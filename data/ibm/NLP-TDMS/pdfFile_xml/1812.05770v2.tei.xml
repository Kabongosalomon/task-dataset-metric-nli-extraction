<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Machine: Rethinking Action Recognition in Trimmed Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
							<email>zhujiagang2015@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS) 3 Horizon Robotics, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
							<email>wei.zou@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Hu</surname></persName>
							<email>huyiming2016@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS) 3 Horizon Robotics, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
							<email>zhuzheng2014@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS) 3 Horizon Robotics, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manyu</forename><surname>Chang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
							<email>huangjunjie2016@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS) 3 Horizon Robotics, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
							<email>dalong.du@horizon.aichangmanyu@stu.xmu.edu.cn</email>
						</author>
						<title level="a" type="main">Action Machine: Rethinking Action Recognition in Trimmed Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing methods in video action recognition mostly do not distinguish human body from the environment and easily overfit the scenes and objects. In this work, we present a conceptually simple, general and high-performance framework for action recognition in trimmed videos, aiming at person-centric modeling. The method, called Action Machine, takes as inputs the videos cropped by person bounding boxes. It extends the Inflated 3D ConvNet (I3D) by adding a branch for human pose estimation and a 2D CNN for pose-based action recognition, being fast to train and test. Action Machine can benefit from the multi-task training of action recognition and pose estimation, the fusion of predictions from RGB images and poses. On NTU RGB-D, Action Machine achieves the state-of-the-art performance with top-1 accuracies of 97.2% and 94.3% on crossview and cross-subject respectively. Action Machine also achieves competitive performance on another three smaller action recognition datasets: Northwestern UCLA Multiview Action3D, MSR Daily Activity3D and UTD-MHAD. Code will be made available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the release of Kinetics-400 <ref type="bibr" target="#b4">[5]</ref> and Kinetics-600 <ref type="bibr" target="#b3">[4]</ref> in the last two years, action recognition in videos has shown similar trend as the object recognition due to the Ima-geNet <ref type="bibr" target="#b10">[11]</ref>. A variety of tasks including trimmed video classification <ref type="bibr" target="#b50">[51]</ref>, temporal action recognition in untrimmed videos <ref type="bibr" target="#b27">[28]</ref>, spatial-temporal action detection <ref type="bibr" target="#b8">[9]</ref>, have been quite popular in recent competitions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>This paper studies action recognition in trimmed videos. To some extent, advances in this field are hampered by the biases in datasets collection, lack of annotations and object recognition in images <ref type="bibr" target="#b10">[11]</ref>. For example, the videos <ref type="bibr">(a)</ref> (b) (c) (d) <ref type="figure">Figure 1</ref>. Visualizing the class-specific activation maps of Inflated 3D ConvNet (I3D) <ref type="bibr" target="#b4">[5]</ref> with the Class Activation Mapping (CAM) <ref type="bibr" target="#b57">[58]</ref>. The video frames of two action classes from Northwestern UCLA Multiview Action3D <ref type="bibr" target="#b47">[48]</ref> are displayed, i.e., drop trash, carry, which are acted by a man and a woman respectively. The results of our person-centric modeling method (subfigure (c) and (d)) are more related to body movements, while the baseline I3D (subfigure (a) and (b)) overfits the trash can.</p><p>in UCF-101 <ref type="bibr" target="#b42">[43]</ref> and HMDB-51 <ref type="bibr" target="#b24">[25]</ref> are rich in scenes and objects, while missing person bounding box annotations. <ref type="bibr" target="#b0">1</ref> Previous methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b4">5]</ref>, which do not directly distinguish human body from videos, tend to predict an action according to the scenes and objects, since convolutional neural networks (CNNs) make it easier to classify the objects and things than human motions. They can be easily distracted by some irrelevant cues of videos when recognizing an action. As shown in <ref type="figure">Fig. 1(b)</ref>, the video frame with ground-truth class carry is predicted as a wrong action drop trash by the baseline Inflated 3D ConvNet (I3D) <ref type="bibr" target="#b4">[5]</ref>.</p><p>Because the model has learned that the trash can and the action drop trash always appear in a video together ( <ref type="figure">Fig. 1(a)</ref>). This motivates us to design a model that can explicitly capture human body movements from videos, simultaneously follows the stream of RGB and CNN-based methods in action recognition. Pose (skeleton) data is lightweight, easy to understand and highly relevant to human action. It can be readily estimated by deep models, due to the re-cent advances of human pose estimation in a single image <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref> and in videos <ref type="bibr" target="#b52">[53]</ref>. The pose estimation methods are usually based on person bounding boxes, which can greatly filter out non-human clutters in RGB images. In view of this, person-centric action recognition has the potential to benefit from the joint training with pose estimation. Thanks to the large-scale annotated datasets <ref type="bibr" target="#b28">[29]</ref> and powerful deep networks <ref type="bibr" target="#b17">[18]</ref>, the poses estimated from images in the wild are more robust than the skeleton captured by depth sensor like Kinect which is limited to indoor pose-based action recognition <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b53">54]</ref>. Thus, action recognition in videos can be naturally formulated as a multi-task learning problem including RGB-based action recognition, pose estimation and pose-based action recognition.</p><p>In this work, a person-centric modeling method for human action recognition is proposed, called Action Machine, which shares similar spirit with Convolutional Pose Machines (CPM) <ref type="bibr" target="#b51">[52]</ref> in sequential fashion model design. The proposed method ( <ref type="figure" target="#fig_4">Fig. 6</ref>) extends the Inflated 3D ConvNet (I3D) <ref type="bibr" target="#b4">[5]</ref> by adding a branch for human pose estimation and a 2D CNN for pose-based action recognition. In details, the video frames are cropped by the bounding boxes of target persons and are taken as the inputs of I3D. For frame-wise pose estimation, a 2D deconvolution head is added to the last convolutional layer of I3D, in parallel with the existing head for RGB-based action recognition. After the pose estimation task, a 2D CNN is applied to the pose sequences for pose-based action recognition. At test time, the predictions of two classification heads are fused by summation. Some class-specific activation maps of Action Machine are shown in <ref type="figure">Fig. 1</ref>(c) and (d), indicating only the regions that really correspond to the action are activated. The main contributions of this work are summarized as follows:</p><p>1. We present a conceptually simple and general framework for action recognition in trimmed videos, called Action Machine, aiming at person-centric modeling.</p><p>2. The proposed techniques of explicitly modeling human body movements including person cropping, multi-task training of action recognition and pose estimation, the fusion of predictions from RGB images and poses can help to improve the model performance.</p><p>3. We showcase the generality of our framework via extensive experiments on four human action datasets. Action Machine achieves the state-of-the-art performance on NTU RGB-D <ref type="bibr" target="#b37">[38]</ref>, Northwestern UCLA Multiview Action3D <ref type="bibr" target="#b47">[48]</ref>. It also achieves competitive performance on MSR Daily Activity3D <ref type="bibr" target="#b46">[47]</ref> and UTD-MHAD <ref type="bibr" target="#b5">[6]</ref>. Action Machine is a high-performance framework while being fast to train and test.</p><p>In the remainder of this paper, related works are given in Section 2. Section 3 describes our proposed approach. In Section 4, our method is evaluated on the datasets. Finally, discussions and conclusions are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>2.1. Deep learning for action recognition RGB-based methods. Two-stream ConvNet <ref type="bibr" target="#b41">[42]</ref> employs RGB images and optical flow stacks as the inputs of two networks and fuses their predictions by late fusion. Temporal Segment Network (TSN) <ref type="bibr" target="#b48">[49]</ref> improves the performance of two-stream ConvNet by sparsely sampling video frames and learning video-level predictions. Deep networks with Temporal Pyramid Pooling (DTPP) <ref type="bibr" target="#b58">[59]</ref> samples enough frames from videos and learns video-level representation end-to-end. Using one network, C3D <ref type="bibr" target="#b44">[45]</ref> learns spatial-temporal patterns from video clips by 3D convolutions. In 2017, DeepMind released a large-scale video action datasets Kinetics <ref type="bibr" target="#b4">[5]</ref> and proposed Inflated 3D Con-vNet (I3D). Non-local Net <ref type="bibr" target="#b50">[51]</ref> equips I3D with attention mechanism, extracting long-range interactions in spatialtemporal domain. The above models take as inputs the random spatial crops of video frames during training and can easily overfit the scenes and objects in videos because of failing to focus on human body explicitly. Different from them, we use the detected bounding boxes to crop the target persons from videos as the inputs of model, eliminating the effect of background context.</p><p>Pose (Skeleton)-based methods. Compared with RGB images, skeleton data has the merits of being lightweight and free from scene cues. Previous studies on pose-based action recognition can be categorized into RNN-based <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b60">61]</ref>, CNN-based <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b56">57]</ref> and GCN-based(Graph Convolution Network) <ref type="bibr" target="#b53">[54]</ref> methods. RNN-based <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b60">61]</ref> methods treat the skeleton data as vectors and capture the sequence information of skeleton. CNN-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref> represent a skeleton sequence as a pseudo-image and recognize the underlying action in the same way as image classification. GCN-based methods <ref type="bibr" target="#b53">[54]</ref> capture joint interactions on the skeleton graphs, explicitly considering the adjacent relationship among joints in a non-Euclidean space. In this work, we follow the CNN-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b56">57]</ref> and use the 2D CNN for the pose-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Human pose estimation</head><p>Human pose estimation can fall into top-down methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17]</ref> in which a pose estimator is applied to the output of a person detector, and bottom-up methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b51">52]</ref>, in which keypoint proposals are grouped together into person instances. In this work, we adopt a topdown method and resort to a off-the-shelf detector <ref type="bibr" target="#b9">[10]</ref> for bounding boxes. For pose estimation, a 2D deconvolution  <ref type="figure">Figure 2</ref>. Action Machine. It consists of the following steps: First, the videos after person cropping are used as the inputs of I3D for RGBbased action recognition. Then a 2D deconvolution head is added to the last convolutional layer of I3D for frame-wise pose estimation. Third, the estimated pose sequences are fed into a 2D CNN for pose-based action recognition. The proposed method is trained in a multi-task manner. Finally, the predictions of two heads for action recognition are fused by summation at test time.</p><p>head is added to the last convolutional layer of I3D. This is inspired from Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>, which extends Faster R-CNN <ref type="bibr" target="#b36">[37]</ref> to support keypoint estimation. Action Machine does not involve detection task during training and the person cropping operations are imposed on the images instead of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-task learning for action recognition</head><p>Chained multi-stream network <ref type="bibr" target="#b62">[63]</ref> unifies three sources: RGB images, optical flow and body part mask for action recognition and detection. It introduces a Markov chain model to fuse these cues successively. In <ref type="bibr" target="#b33">[34]</ref>, Soft-argmax is extended to regress 2D and 3D pose directly, leading to the end-to-end training of pose estimation and action recognition. Different from the above two works, Action Machine is based on I3D, which has less parameters than C3D <ref type="bibr" target="#b44">[45]</ref> because of 2D+1D convolution <ref type="bibr" target="#b50">[51]</ref>. It is also easy to train because of transferring pre-trained weights from 2D CNN and does not need the costly optical flow maps compared to two-stream ConvNet <ref type="bibr" target="#b41">[42]</ref>. The pose estimation method we use is detection-based, detecting keypoint by regressing heatmap and can get more accurate pose than the regression-based pose estimation in <ref type="bibr" target="#b33">[34]</ref>. Meanwhile, the pose estimation head can benefit from the temporal context of the I3D output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Action Machine</head><p>As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, the pipeline of Action Machine consists of the following steps: First, the videos after person cropping are taken as the inputs of I3D for RGB-based action recognition. Then a 2D deconvolution head is added to the last convolutional layer of I3D for frame-wise pose estimation. The heatmaps produced by the pose estimation head are converted into joint coordinates by an argmax operation. Third, the transformed joint coordinates with 2D shape are taken as inputs by a 2D CNN for pose-based action recognition. The proposed method is trained in a multitask manner. Finally, two sources of predictions, i.e., RGB images and poses, are fused by summation at test time.</p><p>Network input. All the video frames are fed to a published detector, i.e., Deformable CNN <ref type="bibr" target="#b9">[10]</ref> for person bounding boxes and the category confidence threshold is set to 0.99 to avoid most false positive detections. The minimal bounding box enclosing all the detected boxes in a video is used for person cropping. In this way, the problem of detection-missing in videos is mostly solved. And cropping video by a shared box for all frames can align the feature on the temporal dimension.</p><p>Backbone. We use the I3D with ResNet-50 <ref type="bibr" target="#b17">[18]</ref> backbone shown in <ref type="table" target="#tab_10">Table 10</ref> for feature extraction. In order to estimate the pose of each frame, we remove the temporal max pooling after the first stage of I3D. The output feature map of the backbone has a size of 2048×8×7×7, used both by RGB-based action recognition and pose estimation.</p><p>RGB-based action recognition. As shown in <ref type="figure" target="#fig_5">Fig. 7</ref>, global average pooling is performed after the last convolutional layer of I3D to get a 2048-d feature P rgb .</p><p>Consider a dataset of N videos with n categories</p><formula xml:id="formula_0">{(X i , y i )} N i=1 , where y i ∈ {1, .</formula><p>. . , n} is the label. Formally, the prediction can be obtained directly</p><formula xml:id="formula_1">Y rgb = ϕ(W c P rgb + b c ),<label>(1)</label></formula><p>where ϕ is the softmax operation, Y rgb ∈ R n . W c and b c are the parameters of the fully connected layer. In the training stage, combining with cross-entropy loss, the final loss function is</p><formula xml:id="formula_2">L r = − N i=1 log(Y rgb (y i )),<label>(2)</label></formula><p>where Y rgb (y i ) is the value of the y i -th dimension of Y rgb . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB-based action recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose estimation</head><p>Pose-based action recognition <ref type="figure">Figure 3</ref>. We extend I3D by adding a branch for human pose estimation and a 2D CNN for pose-based action recognition. Numbers denote spatial resolution and channels. Arrows denote either conv, deconv, or fc layers as can be inferred from context (conv preserves spatial dimension while deconv increases it). The output conv of heatmap and offsetmap is 1×1, deconvs are 4×4 with stride 2. 'res5' denotes the fifth stage of I3D with ResNet-50. '8×' denotes the shared operations of 2D pose estimation on the temporal dimension.  <ref type="table">Table 1</ref>. Our used ResNet-50 I3D model for video. The dimensions of 3D output maps and filter kernels are in T×H×W (2D kernels in H×W), with the number of channels following. The input size is 8×224×224. Residual blocks are shown in brackets.</p><p>Pose estimation. Given the output features of I3D, the pose estimation is performed on each temporal dimension. Inspired from Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>, a 2D deconvolution head is added to the last convolutional layer of I3D, as shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. By default, two deconvolutional layers with batch normalization <ref type="bibr" target="#b19">[20]</ref> and ReLU activation <ref type="bibr" target="#b23">[24]</ref> are used. Each layer has 256 filters with 4×4 kernel and the stride is 2. Following <ref type="bibr" target="#b35">[36]</ref>, a 1×1 convolutional layer is added at last to generate predicted heatmaps for all K keypoints (one channel per keypoint) and offsets (two channels per keypoint for the x and y-directions) for a total of 3K output channels, where K = 17 is the number of keypoints.</p><p>Given the image crop, let f k (x i ) = 1 if the k-th keypoint is located at position x i and 0 otherwise. Here k ∈ 1, ..., K indexes the keypoint type and i ∈ 1, ..., Q indexes the pixel locations on the image crop grid. For each position x i and each keypoint k, we compute the probability h k (x i ) = 1 if ||x i − l k || ≤ M , which means the point x i is within a disk of radius M from the location l k of the k-th keypoint. K such heatmaps are trained by solving a binary classification problem for each position and keypoint independently. For each position x i and each keypoint k, we also predict the 2D offset vector F k (x i ) = l k − x i from the pixel to the corresponding keypoint. K such vector fields are trained by solving a 2D regression problem for each position and keypoint independently.</p><p>The output of the heatmap branch yields the heatmap probabilities h k (x i ) for each position x i and each keypoint k. The training target for the heatmap branch h k (x i ) is a map of zeros and ones, with h k (x i ) = 1 if ||x i − l k || ≤ M and 0 otherwise. The corresponding loss function L h (θ) is the sum of smooth L 1 loss for each position and keypoint independently</p><formula xml:id="formula_3">L h (θ) = 1 K K k=1 i R(h k (x i ), h k (x i )),<label>(3)</label></formula><p>where R is the smooth L 1 loss. For training the offset regression branch, the differences between the predicted and ground truth offsets are penalized by smooth L 1 loss. The offset loss is only computed for positions x i within a disk of radius M from each keypoint.</p><formula xml:id="formula_4">L o (θ) = 1 K K k=1 i:||l k −xi||≤M R(F k (x i ), (l k − x i )),<label>(4)</label></formula><p>The final loss function for pose estimation has the form</p><formula xml:id="formula_5">L p = λ h L h (θ) + λ o L o (θ),<label>(5)</label></formula><p>where λ h = 0.5 and λ o = 0.5 are two scalar factors to balance the loss function. At test time, for the k-th keypoint, the argmax operation is performed on the k-th heatmap to yield the coarse location</p><formula xml:id="formula_6">x k = arg max xi (h k (x i ), i ∈ 1, ..., Q).<label>(6)</label></formula><p>The accurate coordinate of the k-th keypoint is obtained by adding the corresponding offset F k (x k ) to x k .</p><p>Pose-based action recognition. The coordinates of 2D pose can be transformed into a tensor of a size 2×T ×K, where T denotes the number of input frames. An extra confidence channel is added for each predicted joints, which is obtained by max pooling over the heatmap and passed to the ReLU activation. Then the tensor is fed into the ResNet-18 <ref type="bibr" target="#b17">[18]</ref> for pose-based action recognition, as shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. Due to the low spatial dimension of the input pose sequences, in the used ResNet-18, all the pooling operations are removed and all the stride 2 operations in the convolutional layers are replaced with 1. Global average pooling is performed after the last convolutional layer of ResNet-18 to get a 512-d feature. The prediction of pose stream Y paction is optimized with cross-entropy loss</p><formula xml:id="formula_7">L paction = − N i=1 log(Y paction (y i )).<label>(7)</label></formula><p>Multi-task training. Action Machine has three tasks: RGB-based action recognition, pose estimation and posebased action recognition. They are jointly optimized by the following loss function:</p><formula xml:id="formula_8">L = λ 1 L r + λ 2 L p + λ 3 L paction ,<label>(8)</label></formula><p>where λ 1 , λ 2 and λ 3 are the loss weights of RGB-based action recognition, pose estimation and pose-based action recognition respectively. <ref type="bibr" target="#b1">2</ref> They are all set to 1.0 by default. Fusion of RGB and pose-based action recognition. In order to combine the strengths of predictions from RGB images and poses, the predicted probabilities of two heads are fused by summation at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The proposed method has been evaluated on five datasets: on COCO <ref type="bibr" target="#b28">[29]</ref> for pose estimation, and on four trimmed video action datasets: NTU RGB-D <ref type="bibr" target="#b37">[38]</ref>, Northwestern-UCLA Multiview Action 3D <ref type="bibr" target="#b47">[48]</ref>, MSR 2 Note that the gradients of pose-based action recognition don't backpropagate into the pose estimation head because of the argmax operation in equation 6. We do not use the Soft-argmax in <ref type="bibr" target="#b33">[34]</ref> on heatmap for endto-end training because we find the keypoints quality of this approach on COCO <ref type="bibr" target="#b28">[29]</ref> is lower than ours.</p><p>Daily Activity3D <ref type="bibr" target="#b46">[47]</ref> and UTD-MHAD <ref type="bibr" target="#b5">[6]</ref> for action recognition. <ref type="bibr" target="#b2">3</ref> COCO <ref type="bibr" target="#b28">[29]</ref>. The COCO train, validation, and test sets contain more than 200k images and 250k person instances annotated with keypoints. 150k instances of them are publicly available for training and validation. Our models are trained on COCO train2017 dataset (includes 57K images and 150K person instances) and tested on the val2017 set.</p><p>NTU RGB-D <ref type="bibr" target="#b37">[38]</ref>. This dataset is acquired with a Kinect v2 sensor. It contains more than 56K videos and 4 million frames with 60 different activities including individual activities, interactions between two people, and healthrelated events. The actions are performed by 40 subjects and recorded from 80 viewpoints. We follow the crosssubject and cross-view protocol from <ref type="bibr" target="#b37">[38]</ref>.</p><p>Northwestern-UCLA Multiview Action 3D (N-UCLA) <ref type="bibr" target="#b47">[48]</ref>. This dataset contains 1494 sequences, covering 10 action categories, such as drop trash or sit down. Each sequence is captured simultaneously by 3 Kinect v1 cameras. Each action is performed one to six times by ten different subjects. We follow the cross-view protocol defined by <ref type="bibr" target="#b47">[48]</ref>. It has three cross-view combinations: xview1, xview2 and xview3. The combination xview1 means that the samples from view 2 and 3 are for training, and the samples from view 1 are for testing.</p><p>MSR Daily Activity3D (MSR) <ref type="bibr" target="#b46">[47]</ref>. This dataset contains 320 videos shot with a Kinect v1 sensor. 16 daily activities are performed twice by 10 subjects from a single viewpoint. Following <ref type="bibr" target="#b46">[47]</ref>, we use the videos from subject 1, 3, 5, 7 and 9 for training, and the remaining ones for testing.</p><p>UTD-MHAD <ref type="bibr" target="#b5">[6]</ref>. This dataset is collected using a Microsoft Kinect sensor and a wearable inertial sensor in an indoor environment. It contains 27 actions performed by 8 subjects and has 861 sequences. Cross-subject protocol <ref type="bibr" target="#b5">[6]</ref> is used for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental settings for pose estimation</head><p>Training. The ground truth human box is made to a fixed aspect ratio (height : width = 4 : 3) by extending the box in height or width. It is then cropped from the image and resized to a fixed resolution. The default resolution is 384×288. Data augmentation includes scale(±30%), rotation(±30 degrees) and flip. Our models are pre-trained on ImageNet <ref type="bibr" target="#b10">[11]</ref>. ResNet-50 is used by default. We train our models on a 4-GPU machine and each GPU has 2 images in a mini-batch (so in total with a mini-batch size of 8 <ref type="bibr" target="#b2">3</ref> Because the videos in these datasets usually have one person (except the 11 classes of NTU RGB-D where the videos have two persons), which are appropriate for validating the effect of person-centric modeling only by simple human detection. Datasets like UCF-101 <ref type="bibr" target="#b42">[43]</ref> and HMDB-51 <ref type="bibr" target="#b24">[25]</ref> have some group activities (sports) with more than one person and a variety of scenes. It may need tracking and person re-identification for our use and additional annotation cost, which is beyond the scope of this paper. images). We train our models for 22 epochs in total, starting with a learning rate of 0.01 and reducing it by a factor of 10 according to a schedule of <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>. SGD is used, with a momentum of 0.9 and a weight decay of 0.0001. The L1 norm of all gradients is clipped by 2 independently. MXNet <ref type="bibr" target="#b6">[7]</ref> is used for implementation.</p><p>Testing. The detected person bounding boxes on COCO val2017 are used. Following the common practice in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b52">53]</ref>, the joint location is predicted on the averaged heatmaps of the original images and their horizontal flips. Following <ref type="bibr" target="#b0">[1]</ref>, we use the mean average precision (AP) over 10 OKS (object keypoint similarity) thresholds for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental settings for action recognition</head><p>Preprocessing. As shown in <ref type="table" target="#tab_2">Table 12</ref>, for all datasets, we resize their videos to make them smaller and keep their aspect ratios. The sampling stride is selected according to the frame rate of videos and model performance.</p><p>Training. Our models are pre-trained on ImageNet <ref type="bibr" target="#b10">[11]</ref>. Then the pre-trained weights are inflated from 2D to 3D, as shown in <ref type="table" target="#tab_10">Table 10</ref>. For small datasets, including N-UCLA <ref type="bibr" target="#b47">[48]</ref>, MSR <ref type="bibr" target="#b46">[47]</ref> and UTD-MHAD <ref type="bibr" target="#b5">[6]</ref>, we also try pre-training our models on NTU RGB-D <ref type="bibr" target="#b37">[38]</ref>. The models are fine-tuned using 8-frame clips with sampling stride shown in <ref type="table" target="#tab_2">Table 12</ref>. The start frame is randomly sampled during training. Data augmentation includes the bounding box center, width, height jittering and random mirror. Then the bounding box is made to a fixed aspect ratio (height : width = 1 : 1 4 ) by extending the box in height or width. It is then cropped from the image and resized to a fixed resolution. The default resolution is 224×224. We train our models on a 4-GPU machine and each GPU has 4 clips (32 images) in a mini-batch (in total with a mini-batch size of 16 clips). We train our models for 85 epochs in total, starting with a learning rate of 0.01 and reducing it by a factor of 10 according to a schedule of <ref type="bibr" target="#b41">[42,</ref><ref type="bibr">68]</ref>. SGD is used, with a momentum of 0.9 and a weight decay of 0.0001. The L1 norm of all gradients is clipped by 2 independently. Dropout with ratio of 0.5 is used before the fully connected layer of RGBbased and pose-based action recognition. The pose annotations of video frames are obtained by using the detection boxes and models trained on COCO <ref type="bibr" target="#b28">[29]</ref>. MXNet <ref type="bibr" target="#b6">[7]</ref> is used for implementation.</p><p>Testing. Following <ref type="bibr" target="#b50">[51]</ref>, the fully convolutional inference is performed spatially on videos, including three crops, i.e., the up left, center, down right of bounding box center. 10 clips are evenly sampled from a full-length video and the softmax scores are computed on them individually. The final prediction is the averaged softmax scores of all clips. <ref type="bibr" target="#b3">4</ref> We do not keep the same aspect ratio as 2D pose estimation in COCO, because we use a shared box for all frames in a video and a moving person in a video is likely to cover a range different from a still person in a single image.   We report the top-1 accuracy using the model of the last epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Pose estimation on COCO</head><p>As shown in <ref type="table" target="#tab_3">Table 3</ref>, our method is compared with stateof-the-art methods: Hourglass <ref type="bibr" target="#b34">[35]</ref>, CPN <ref type="bibr" target="#b7">[8]</ref> and Simple Baseline <ref type="bibr" target="#b52">[53]</ref> on COCO val2017. Our method achieves competitive performance with the above methods. <ref type="figure">Fig. 4</ref> shows some results of our pose estimation method on COCO val2017 dataset. The proposed method equipped with I3D is used for the next action recognition experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Action recognition</head><p>In this section, Action Machine is compared with other approaches on four human action datasets. Results are shown in <ref type="table" target="#tab_3">Table 13</ref>, 5, 6, 7, where denotes that the corresponding modality is used as the input of model in testing. Note that Action Machine does not take as input human poses in testing because it has learned to estimate poses from RGB images after training.</p><p>Performance on NTU RGB-D. In <ref type="table" target="#tab_3">Table 13</ref>, our model is compared with pose-based methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b40">41]</ref> and RGB-based methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">32]</ref>. Action Machine with single modality (RGB) as input at test time achieves the state-of-the-art performance. Specifically, Action Machine outperforms PoseMap <ref type="bibr" target="#b31">[32]</ref>    is conceptually simple and easy to implement. Performance on N-UCLA. In <ref type="table" target="#tab_6">Table 5</ref>, our model is compared with pose-based methods: <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b25">26]</ref> and RGB-based methods <ref type="bibr" target="#b1">[2]</ref>. Action Machine with single modality (RGB) as input outperforms previous state-of-theart approaches. Without using LSTM and extra handcrafted rules as Glimpse Clouds <ref type="bibr" target="#b1">[2]</ref>, Action Machine has a accuracy gain of 4.7 points in average top-1 accuracy on cross-view.</p><p>Performance on MSR. In <ref type="table">Table 6</ref>, our model is compared with pose-based methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b43">44]</ref>, depthbased methods <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>. Without using depth modality, Action Machine achieves competitive performance compared to DSSCA-SSLM <ref type="bibr" target="#b39">[40]</ref>, which is based on handcrafted feature including RGB and depth. However, on the cross-subject of NTU RGB-D <ref type="table" target="#tab_3">(Table 13)</ref>, a larger dataset than MSR, DSSCA-SSLM <ref type="bibr" target="#b39">[40]</ref> is lower than ours for 19.4 points, showing the robustness of our method against the amount of data.</p><p>Performance on UTD-MHAD. In <ref type="table">Table 7</ref>, our model is compared with <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>. Without using the 3D pose extracted by depth sensor as these methods, Action Machine with RGB modality as input achieves competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation study</head><p>Ablation studies are performed on NTU RGB-D and N-UCLA to verify the effectiveness of our techniques for person-centric modeling in Action Machine. There are four  <ref type="table">Table 6</ref>. Performance on MSR Daily Activity3D, accuracy(%).</p><p>Pose RGB xsub JTM <ref type="bibr" target="#b49">[50]</ref> -85.8 Optical Spectra <ref type="bibr" target="#b18">[19]</ref> -86.9 JDM <ref type="bibr" target="#b26">[27]</ref> -88.1 PoseMap <ref type="bibr" target="#b31">[32]</ref> 94.5 Action Machine (Ours) -92.5 <ref type="table">Table 7</ref>. Performance on UTD-MHAD, accuracy(%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>basic configurations, as illustrated below:</head><p>RGBAction random crop. The baseline I3D model takes as inputs the random crops of videos and performs action recognition using RGB feature.</p><p>RGBAction person crop. The I3D model takes as inputs the videos after person cropping and performs action recognition using RGB feature.</p><p>KPS RGBAction. The I3D model takes as inputs the videos after person cropping, performs action recognition using RGB feature, and adds a head for pose estimation.</p><p>KPS PoseAction RGBAction. The I3D model takes as inputs the videos after person cropping, adds a head for pose estimation, and performs action recognition using RGB and pose feature. The model trained from KPS RGBAction is used as the pre-trained model. We fix it and only train the ResNet-18 or ResNet-50 for pose-based action recognition. We report the results of pose-based action recognition and the sum fusion of predictions from RGB images and poses.</p><p>As shown in <ref type="table">Table 8</ref>, on the cross-subject of NTU RGB-D, person cropping can improve the model accuracy by 0.9 points over random crop. Our full model outperforms the baseline RGBAction random crop by 2 points. Due to the high accuracy of baseline, the improvement on the crossview is not obvious. Similar gain potential can also be observed on the small subsets of NTU RGB-D (xview-s, xsubs), which are originally used for the fast training and testing in our implementation.</p><p>As shown in <ref type="table">Table 9</ref>, on N-UCLA, Action Machine outperforms the baseline by a large margin. Specifically, RG-BAction person crop with the person cropping technique can improve the accuracy by 1.6 and 4.3 points on xview1 and xview3 over the baseline RGBAction random crop respectively. Person cropping does not bring accuracy gain on xview2, because the test crops of front view images <ref type="figure" target="#fig_6">(Fig. 8</ref>,  <ref type="figure">Figure 5</ref>. Visualizing the class-specific activation maps of our model with the Class Activation Mapping (CAM) <ref type="bibr" target="#b57">[58]</ref>. Activation maps of video snippets of three action classes, i.e., sit down, stand up, carry are shown from top to the bottom. It is clear that the multi-task training of RGB-based action recognition and pose estimation can make the model focus on the spatial-temporal regions related to the action class.  <ref type="table">Table 8</ref>. Ablation studies on NTU RGB-D, accuracy(%). In the rows which have slash /, the number on the left of slash is the accuracy of pose-based action recognition, the right is the accuracy of fusion of RGB and pose results. xview-s and xsub-s denote the small subsets of NTU RGB-D cross-view and cross-subject respectively.</p><p>the first and second row) on this dataset are close to that cropped by person boxes. Jointly training pose estimation and RGB-based action recognition, i.e., KPS RGBAction, can improve about 3 to 7 points. Overall, using ResNet-18, our final model exceeds the baseline by 7.2 points. By using a stronger backbone, i.e., ResNet-50 for pose-based action recognition and NTU RGB-D pre-training, the accuracies of our models, either solely by poses or the fusion of RGB images and poses, are further improved.</p><p>To better understand how our approach learns discriminative feature for action recognition, the class-specific activation maps of our models are visualized with the Class Activation Mapping (CAM) <ref type="bibr" target="#b57">[58]</ref> approach in <ref type="figure" target="#fig_6">Fig. 8</ref>. The videos are sampled from N-UCLA, including three classes (sit down, stand up, carry). These maps show that, jointly training RGB-based action recognition with pose estimation can make the model focus on the motions of human body. For example, KPS RGBAction ( <ref type="figure" target="#fig_6">Fig. 8(b)</ref>) pays more attention on the standing and sitting process, while RGBAction person crop ( <ref type="figure" target="#fig_6">Fig. 8(a)</ref>) seems to focus on the object (chair). Particularly, in the carry example, KPS RGBAction is significantly activated only by the hand (center of the body). Nevertheless, RGBAction person crop is activated by the trash can, leading to a wrong prediction (drop trash).  <ref type="table">Table 9</ref>. Ablation studies on N-UCLA, accuracy(%). In the rows which have slash /, the number on the left of slash is the accuracy of pose-based action recognition, the right is the accuracy of fusion of RGB and pose results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Timing</head><p>Inference: We train a ResNet-50-I3D model that shares features between RGB-based action recognition and pose estimation with two deconvolutional layers. And it is followed by a ResNet-50 for pose-based action recognition. This model runs at ∼55ms per clip (8 frames) on an Nvidia TitanX GPU. As the dimension of pose sequences is small, substituting ResNet-50 with ResNet-18 for pose-based action recognition don't cause much difference: it finally takes ∼50ms. I3D takes ∼30ms. Action Machine is fast to test and adds only a small overhead to I3D.</p><p>Training: Action Machine is also fast to train. Training with ResNet-50-I3D on the cross-view of NTU RGB-D takes 32 hours (0.66s per 16 clips (128 frames) mini-batch) in our synchronized 4-GPU implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions and Conclusions</head><p>We propose a person-centric modeling method: Action Machine, for human action recognition in trimmed videos. It has three complementary tasks: RGB-based action recognition, pose estimation and pose-based action recognition. By using person bounding boxes and human poses, Action Machine achieves competitive perfor-mance compared with other approaches on four video action datasets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b5">6]</ref>. However, in our implementation, it is hard to discard non-human clutters strictly (e.g., the trash can in <ref type="figure" target="#fig_6">Fig. 8</ref>) because of the bounding box quality and other postprocessing steps. Besides, in our multi-task training, the ground-truth pose annotations are estimated by the model trained on COCO <ref type="bibr" target="#b28">[29]</ref> and may not be abundant enough for the training of pose estimation task due to the paucity of videos. The joint training of pose estimation on COCO and action recognition on videos may relieve the problem, as we can exploit the data richness of COCO.</p><p>In Section 4.6, for ablation studies of different configurations of our models, we use the small subsets of NTU RGB-D (xview-s, xsub-s), which are designed by us for the fast training and testing.</p><p>For xview-s, the sample videos of the original cross-view split with subject ID larger than 5 are discarded. For this evaluation, the training and testing sets have 3, 839 and 1, 917 samples (about 1/10th of the full cross-view split), respectively.</p><p>For xsub-s, based on the original cross-subject split, we pick all the samples of camera 1 and discard samples of cameras 2 and 3. The sample videos with subject ID larger than 10 are discarded. For this evaluation, the training and testing sets have 4, 317 and 1, 439 samples (about 1/10th of the full cross-subject split), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Cross-dataset recognition task</head><p>In order to show the advantage of person-centric modeling over baseline, we further test our trained models on another different datasets. Specifically, we train our models on NTU RGB-D cross-subject and test them on the test sets of   <ref type="table">Table 11</ref>. Shared category mapping between MSR Daily and NTU RGB-D.</p><p>UTD-MHAD NTU RGB-D sit to stand standing up (from sitting position) stand to sit sitting down <ref type="table" target="#tab_2">Table 12</ref>. Shared category mapping between UTD-MHAD and NTU RGB-D.</p><p>N-UCLA, MSR Daily and UTD-MHAD respectively. The shared category mappings between the smaller dataset and NTU RGB-D are shown in <ref type="table" target="#tab_10">Table 10</ref>, 11, 12 and the test videos are limited to have these ground-truth classes. Because of the different sources of videos, the scene contexts and objects in training dataset are largely different from the testing dataset. A model without capturing human body motion will behave worse than the model which learns to focus on. Results are shown in <ref type="table" target="#tab_3">Table 13</ref>. It is clearly observed that our proposed person-centric modeling techniques including: person cropping, multi-task training of action recognition and pose estimation, the fusion of predictions from RGB images and poses can help to improve the performance of baseline model RGBAction random crop on different datasets. Existing methods based on RGB images easily overfit the scenes and objects of specific datasets without focusing on human body movements, though they may have high performance. In contrast, Action Machine is more generalizable and extendable.  <ref type="table" target="#tab_3">Table 13</ref>. Cross-dataset testing on N-UCLA (xview3), MSR Daily and UTD-MHAD, accuracy(%). The models on the first column are trained on NTU RGB-D cross-subject. In the rows which have slash /, the number on the left of slash is the accuracy of posebased action recognition, the right is the accuracy of fusion of RGB and pose results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Pose estimation results on action recognition datasets</head><p>We visualize the video frames with detected boxes and estimated poses on action recognition datasets in <ref type="figure" target="#fig_4">Fig. 6, 7</ref> <ref type="bibr">, 8, 9.</ref> We use the testing model KPS RGBAction, which performs RGB-based action recognition and pose estimation simultaneously. In general, the estimated poses are accurate.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Activation maps of RGBAction person crop (b) Activation maps of KPS RGBAction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Visualizing the video frames of the test set of NTU RGB-D cross-view. The video frames with detected boxes (yellow) and estimated poses of eight action classes, i.e., wear a shoe, cheer up, pointing to something with finger, falling, writing, put the palms together, reading, brushing teeth, tear up paper are shown from top row to the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Visualizing the video frames of the test set of N-UCLA xview3. The video frames with detected boxes (yellow) and estimated poses of four action classes, i.e., sit down, throw, donning, pick up with two hands are shown from top row to the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Visualizing the video frames of the test set of MSR Daily. The video frames with detected boxes (yellow) and estimated poses of four action classes, i.e., sit down, call cellphone, play guitar, walk are shown from top row to the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Visualizing the video frames of the test set of UTD-MHAD. The video frames with detected boxes (yellow) and estimated poses of four action classes, i.e., two hand push, front boxing, cross arms in the chest, forward lunge (left foot forward) are shown from top row to the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Video preprocessing and configurations.</figDesc><table><row><cell cols="3">Figure 4. Results of our pose estimation method on COCO [29].</cell></row><row><cell>Method</cell><cell cols="2">Backbone Input Size AP</cell></row><row><cell>8-stage Hourglass [35]</cell><cell>-</cell><cell>256×192 66.9</cell></row><row><cell>CPN [8]</cell><cell cols="2">ResNet-50 384×288 70.6</cell></row><row><cell cols="3">Simple Baseline [53] ResNet-50 384×288 72.2</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-50 384×288 72.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Comparison with Hourglass [35], CPN [8] and Simple Baseline [53] on COCO val2017 dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Performance on NTU RGB-D, accuracy(%).</figDesc><table><row><cell></cell><cell cols="5">Pose RGB xview1 xview2 xview3 Avg</cell></row><row><cell>Lie Group [46]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.2</cell></row><row><cell>H-RNN [13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>78.5</cell></row><row><cell>Enhanced viz. [31]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.1</cell></row><row><cell>Ensemble TS-LSTM [26]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.2</cell></row><row><cell>Glimpse Clouds [2]</cell><cell>-</cell><cell>83.4</cell><cell>89.5</cell><cell cols="2">90.1 87.6</cell></row><row><cell>Action Machine (Ours)</cell><cell>-</cell><cell>88.3</cell><cell>92.2</cell><cell cols="2">96.5 92.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>Performance on N-UCLA, accuracy(%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Shared category mapping between N-UCLA and NTU RGB-D.</figDesc><table><row><cell>MSR Daily</cell><cell>NTU RGB-D</cell></row><row><cell>drink</cell><cell>drink water</cell></row><row><cell>eat</cell><cell>eat meal/snack</cell></row><row><cell>read book</cell><cell>reading</cell></row><row><cell cols="2">call cellphone make a phone call/answer phone</cell></row><row><cell>write on a paper</cell><cell>writing</cell></row><row><cell>cheer up</cell><cell>cheer up</cell></row><row><cell>stand up</cell><cell>standing up (from sitting position)</cell></row><row><cell>sit down</cell><cell>sitting down</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Except their subsets, UCF-24<ref type="bibr" target="#b42">[43]</ref> and JHMDB-21<ref type="bibr" target="#b20">[21]</ref>, which are for spatial-temporal action detection.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coco: Coco Leaderboard</surname></persName>
		</author>
		<ptr target="http://cocodataset.org.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Glimpse clouds: Human activity recognition from unstructured feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A short note about kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1512.01274</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatiotemporally localized atomic visual actions</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>J. M. Chunhui Gu, Chen Sun</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eweiwi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The activitynet large-scale activity recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Dao</surname></persName>
		</author>
		<idno>abs/1808.03766</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<idno>abs/1710.08011</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Skeleton optical spectra-based action recognition using convolutional neural networks. TCSVT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>NIPS. 2012. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hmdb51: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing in Science and Engineering</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint distance maps based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Group sparsity and geometry constrained dictionary learning for action recognition from depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>NIPS. 2015. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multimodal multipart learning for action recognition in depth videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep multimodal feature analysis for action recognition in rgb+d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. 2014. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Moving poselets: A discriminative and interpretable skeletal motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Action recognition based on joint trajectory maps with convolutional neural networks. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<idno>abs/1804.07453</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">End-to-end video-level representation learning for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Two stream gated fusion convnets for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fusing multiple features for depth-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
