<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial Networks for Classification of Noisy Handwritten Bangla Characters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Louisiana State University</orgName>
								<address>
									<postCode>70803</postCode>
									<settlement>Baton Rouge</settlement>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Collier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Louisiana State University</orgName>
								<address>
									<postCode>70803</postCode>
									<settlement>Baton Rouge</settlement>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supratik</forename><surname>Mukhopadhyay</surname></persName>
							<email>supratik@csc.lsu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Louisiana State University</orgName>
								<address>
									<postCode>70803</postCode>
									<settlement>Baton Rouge</settlement>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial Networks for Classification of Noisy Handwritten Bangla Characters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Progressively Training · General Adversarial Networks · Classification · Noisy Characters · Handwritten Bangla</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the sparsity of features, noise has proven to be a great inhibitor in the classification of handwritten characters. To combat this, most techniques perform denoising of the data before classification. In this paper, we consolidate the approach by training an all-in-one model that is able to classify even noisy characters. For classification, we progressively train a classifier generative adversarial network on the characters from low to high resolution. We show that by learning the features at each resolution independently a trained model is able to accurately classify characters even in the presence of noise. We experimentally demonstrate the effectiveness of our approach by classifying noisy versions of MNIST [13], handwritten Bangla Numeral, and Basic Character datasets [5], <ref type="bibr" target="#b5">[6]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Early work in neural networks focused on classification of handwritten characters <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Since then, there has been a lot of research on character recognition. While in many cases, text processing deals directly with the character strings themselves, there are a growing number of use cases for recognizing characters and text in physical real world prints and documents. This includes processing receipts and bank statements, transcribing books and medical prescriptions, or translating text. Aside from the large amounts of computer generated text, there are vast quantities of scanned handwritten text that can be processed. Such text is generally collected as images, which invariably introduces some noise (e.g., damaged documents, noise added due to camera motion, etc.). While computer generated text classification might be more stable to noise, recognition of handwritten text breaks down with the introduction of noise.</p><p>In this work, we build on recent work in adversarial training <ref type="bibr" target="#b13">[14]</ref> to improve on the state-of-the-art in representing sparse features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>. We define sparse representations as noisy, generally compact, representations of signals <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b6">[7]</ref>. This is the case for many real world images which contain various sources of noise that can distort their true representation. Such noise can easily reduce the quality of classifications and challenge the power of classifiers <ref type="bibr" target="#b21">[22]</ref>  <ref type="bibr" target="#b18">[19]</ref>. Most algorithms include denoising step for the images before classification <ref type="bibr" target="#b12">[13]</ref>, while our approach can directly classify without denoising step due to progressively learn features at increasing resolutions to accurately classify the noisy digits/characters. <ref type="figure" target="#fig_0">Fig.1</ref> shows the architecture of our approach. We utilized the progressive technique which is a newly proposed method <ref type="bibr" target="#b13">[14]</ref>, for training Auxiliary Classifier Generative Adversarial Networks (ACGAN) that has been attractive due to its ability to improve and stabilize the network. It facilitates networks to learn features in a generic to specific manner as the input progresses down the model <ref type="bibr" target="#b22">[23]</ref>. Low resolution features are more resistant to noise due to their generic nature. By individually learning representations at each resolution, our method is able to leverage the noise-resistant generic features to make more accurate and better predictions for noisy handwritten characters to achieve state-of-theart performance.</p><p>In general, our framework uses a generative adversarial network (GAN) <ref type="bibr" target="#b8">[9]</ref> as a basic component for its noise-resilient ability and the discriminative power of its discriminator for classification. GANs contain two competing networks, a generator and a discriminator <ref type="bibr" target="#b8">[9]</ref>, playing a minmax game. The discriminator <ref type="bibr" target="#b8">[9]</ref> tries to discern real samples from fake ones generated by the generator in an attempt to fool the discriminator. Because of this behavior, as one network tries to minimize its own loss, this in turn maximizes the other network's loss. Generators from GANs have been shown to generate outputs that are almost indiscernible from real samples, while the disrcriminator trained by a GAN has more discriminative power with respect to classification being exposed to both real and fake data, where much of the fake data contains some noise.</p><p>We used the discriminator in the Classifier GAN for our Classification Network for the noisy handwritten characters, but to make it more robust to noise and resolution we novelly adopted the innovative GAN training technique, Progressive growing <ref type="bibr" target="#b13">[14]</ref>, to our Classifier GAN. In progressive growing each layer of the GAN is trained individually on increasing resolutions. This allows each feature to specialize and simplifies the problem at the individual layers.</p><p>To the best of our knowledge, this is the first work that designs a Progressive trained Classifier GAN (PCGAN) for retaining the noise-resistant discriminator in a classifier GAN for robust classification in noisy settings. This paper makes the following contributions.</p><p>-It presents a novel robust noise-resilient classification framework using progressively trained classifier general adversarial networks. -The proposed classification framework can directly classify raw noisy data without any preprocessing steps that include complex techniques such as denoising or reconstruction. -It experimentally demonstrates the effectiveness of the framework on the Noisy Bangla Numeral, the Noisy Bangla Characters, and the Noisy MNIST benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Handwritten character based datasets have become benchmarks in computer vision research. Handwritten characters contain sparse representations, or features, while also containing significant amounts of noise <ref type="bibr" target="#b2">[3]</ref>. Early work on classification of handwritten characters focused on dimensionality reduction and denoising. This includes the use of quadtrees <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18]</ref> and intermediate layers of Convolutional Neural Networks for representations and Deep Belief Networks (DBN) for denoising <ref type="bibr" target="#b2">[3]</ref>.</p><p>Researchers have tried a variety of methods to solve the noisy character classification problem. For example, multi-stage approaches have used chain code histogram features to discriminate classes in <ref type="bibr" target="#b4">[5]</ref>. Similar to this work, increasing resolutions are used to assist in classification. Other multistage approaches have used modified quadratic discriminant function (MQDF) and gradients from neural networks to classify characters from many classes <ref type="bibr" target="#b5">[6]</ref>.</p><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref> have been widely used in image processing, and are increasingly being used in character and text classification <ref type="bibr" target="#b12">[13]</ref>. The performance of CNNs can be greatly affected by blurry images, where noise increases the separation between the output and the ground truth in feature space <ref type="bibr" target="#b20">[21]</ref>. While Euclidean distance has shown to be a decent method for measuring the closeness between two images in feature space, it is difficult to measure the sharpness and quality of images. Through adversarial training, Generative adversarial networks (GANs) can both imagine structure where there is none, to produce sharp images, and discern real from fake <ref type="bibr" target="#b8">[9]</ref>. Progressively growing GANs are an improvement on the GAN architecture that can produce sharp realistic images <ref type="bibr" target="#b13">[14]</ref>. This innovation is important for GANs operate on high resolution input. When handling high resolution data it can be too easy for the GAN to discriminate between the fake, generally low resolution imagery, and the real, which are high resolution. Progressively grown GANs learn the resolutions at each layer in isolation by training each layer almost independently, before adding the next layer and training again. This process utilizes transfer learning and the generic to specific learning behaviour exhibited in neural networks <ref type="bibr" target="#b22">[23]</ref>. Auxiliary Classifier GANs (ACGAN) <ref type="bibr" target="#b19">[20]</ref> has introduced class labels in GANs by adding an auxiliary classifier which leveraged model in its prior, their research focused on image synthesis tasks and has shown better performance. The focus of our paper is classification of noisy handwritten (Bangla) characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we present an overview of our proposed method. We separate our methodology into three subsections; Generative Adversarial Networks, Progressively Trained Classifier GAN, and Classification Network. In the Generative Adversarial Network section, we outline the formulation of our GAN objective function. We describe the progressive growing structure of the GAN in Section 3.2, along with a general training algorithm. Details about the Classification Network are provided in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative Adversarial Network</head><p>A Generative Adversarial Network (GAN) <ref type="bibr" target="#b8">[9]</ref> comprises of two networks: a generator and a discriminator. The generator G takes as input a random noise vector z and outputs a fake image G(z). It learns a mapping function, z → y, between the latent space z and the feature space defined by the task y. The discriminator D takes as input x either a real image or a fake image produced by G to calculate class probabilities. It attempts to learn a mapping between feature space y and a discriminatory space q, y → q. The goal is to map all true y to the positive class while all fake y f to the negative class.</p><p>To learn these mappings the generator G plays with the discriminator D a two-player min/max game, min G max D L(G, D), with a loss L(G, D). The objective function <ref type="bibr" target="#b8">[9]</ref> is defined as follows.</p><formula xml:id="formula_0">min G max D L(G, D) =E y [logD(y)] + E x,z [log(1 − D(x|G(x|z)))]<label>(1)</label></formula><p>where the discriminator D will try to maximize the log-likelihood which is the first term, while the generator G try to minimize the second term. The classifier GAN <ref type="bibr" target="#b19">[20]</ref> takes a class label l and a random noise vector z to generate fake images X f through its generator G, denoted as G(l, z). The real images X r are the training images in the noisy dataset under consideration. The objective function of the classification GAN comprises of two components each involving two log-likelihood L 1 and L 2 . The log-likelihood L 1 involves the conditional probability distribution P (guess | x) where the input x can be a fake image X f (generated by the generator) or a real image X r (a training image from the noisy dataset under consideration) and guess can take two values fake or real . Formally <ref type="bibr" target="#b19">[20]</ref>,</p><formula xml:id="formula_1">L 1 = E[log P (real | X r )] + E[log P (fake | X f )].</formula><p>(</p><p>The log-likelihood L 2 involves the conditional probability distribution P (l | x) where l is the class label of x and x is a real image X r or a fake image X f . Formally,</p><formula xml:id="formula_3">L 2 = E[log P (l | X r )] + E[log P (l | X f )].<label>(3)</label></formula><p>The generator G will try to maximize L 2 − L 1 , but the discriminator D will try to maximize L 1 + L 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Progressively Trained Classifier GAN</head><p>Progressive growing is a recent development from <ref type="bibr" target="#b13">[14]</ref> that uses transfer learning to improve the quality of the learned models. Training is performed individually for the layers of the generator G and the discriminator D. New mirroring layers are added to G and D before a new training iteration is run. This increases the spatial resolution of the output image for each layer progressively added. Layers in G and D become more specialized to spatial resolution, resulting in them learning finer features. In addition to making the layers more specialized, progressively growing also simplifies the problem at each layer. This creates a more stable generative model with finer outputs.</p><p>Deep neural networks learn features in a low resolution to high resolution manner, or generic to specific. Progressive growing takes advantage of this behavior by transferring the weights learned from all previous training iterations to identical layers for the next step. Each training iteration then only contains one untrained layer, the newest layer, allowing for features at each resolution to be learned independently and in relative isolation from the other layers. The discriminator grows in parallel with the generator with each layer learning to discriminate specific resolutions.</p><p>Most progressive GANs are designed with generative tasks in mind, putting a focus on the generator. In our work, however, we use the well trained discriminator that is the end result of progressive training. Each layer of the discriminator specializes at specific resolutions allowing it to learn more fine-grain features. Additionally, the discriminator has seen a wide range of samples produced by the generator as it learns; from noisy to sharp. We call this discriminator "well trained" because it is more robust to noise and sparse features, characteristics inherent in handwritten characters.</p><p>We show the general training algorithm for our proposed framework for progressively training the classifier GAN in Algorithm 1 for a given number of progressive stages. As seen in the Algorithm, we first initialize the progressive stages (indicated as modules in <ref type="figure" target="#fig_0">Fig.1</ref>), for learning essential features at different resolutions. The input resolution for the discriminator increases from 7 × 7 for the first module to 28 × 28 for the third module as seen in <ref type="figure" target="#fig_0">Fig.1</ref>. After initializing a module, our algorithm begins training the GAN using the normal training procedure. Since our GAN is used for classification purposes, unlike other variants of GANs used to synthesize high quality images, which mainly focus on the generator, we focus on the discriminator and aim to improve its classification ability. Similar to ACGAN <ref type="bibr" target="#b19">[20]</ref>, we add an auxiliary classifier to the discriminator to compute class labels, apart from using a binary classifier for discerning if an image is fake or real. The latter classifier corresponds to the loss L discern as shown in Algorithm 1; this loss is used to enhance the robustness of the discriminator by learning better representations of variations within a class <ref type="bibr" target="#b10">[11]</ref>. After a module is trained through epochs, the weights in its generator and discriminator are transferred to the next module (as seen in <ref type="figure" target="#fig_0">Fig.1</ref>, the weights from Module i is transferred to Module i+1 , with i ≥ 1, with the number of layers increasing as we progress from the ith module to the i + 1th module for augmenting resolution). This method of progressive training continues until training for the last module has converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classification Network</head><p>After being progressively trained, the discriminator of the classifier GAN has learned the input space in such a way that the lower layers specialize on low resolutions while the higher layers specialize on high resolutions. Progressive training results in a stabilized discriminator that has learned the essential features from noisy data at multiple resolutions, resulting in better classification performance. As we can see in <ref type="figure" target="#fig_0">Fig.1</ref>, as we go from the ith module to the i + 1th module, the number of layers increases. The input resolution for discriminator increases from 7 × 7 in the first module to 28 × 28 in the third module. The weights of trained discriminator in Module 3 are transferred to the classification network, which is a convolutional neural network as described below. To further improve Bz ← Sample a batch of n random vectors 7 B l ← Sample a batch of n random labels <ref type="bibr" target="#b7">8</ref> Update the parameters in the discriminator regarding gradients,</p><formula xml:id="formula_4">∇ θ d 1 2n</formula><p>r∈Br ,z∈G(Bz ,B l ) L discern (r, z)</p><formula xml:id="formula_5">+ 1 n r∈Br L class (r) 9</formula><p>Bz ← Sample a batch of 2n random vectors 10 B l ← Sample a batch of 2n random labels <ref type="bibr" target="#b10">11</ref> Set discriminator trainable to false, update the parameters in the generator regarding gradients,</p><formula xml:id="formula_6">∇ θg 1 2n z,z ∈G(Bz ,B l ) L discern (z, z ) + 1 2n z,z ∈G(Bz ,B l ) L class (z, z ) 12 end 13 end 14</formula><p>Transfer weights to Next(module) 15 end the performance of the classification network, we finetuned the softmax layer. The details of the classification network are shown in <ref type="figure" target="#fig_0">Fig.1</ref>. It includes three convolutional layers, each with LeakyReLU activation function. Two dropout layers and a batch normalization layer are added to the classification network as showed in <ref type="figure" target="#fig_0">Fig.1</ref>. Finally, a softmax layer added as the last layer for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>The experiments mainly focus on Indian handwritten digits and characters datasets <ref type="bibr" target="#b12">[13]</ref>, namely, Noisy Bangla Numeral and Noisy Bangla Characters, are publicly available datasets we downloaded from online 1 , it provided a training dataset and a test dataset. For a comparative study, we also conducted experiments on a noisy version of a commonly used handwritten digits dataset, Noisy MNIST Dataset <ref type="bibr" target="#b12">[13]</ref>. The original non-noisy versions of the Bangla Numeral and Character datasets are from <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. We consider three different versions of each dataset, the first with Added White Gaussian Noise (AWGN), the second with Reduced Contrast with white Gaussian noise (Contrast), and the third with Motion Blurred noise (Motion). Sample data from each of the three different versions of the noisy Bangla character dataset are shown in <ref type="figure" target="#fig_1">Fig.2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Noisy Bangla Numeral has three different versions, each with different type of noise added, AWGN, Contrast, and Motion. For each version, there are 10 classes of Bangla Numerals with a total of 23330 black and white images with image size 32 × 32.</p><p>Noisy Bangla Characters contains 76000 black and white images for 50 classes of Bangla Characters with image size 32 × 32, in each version. There are three different versions one for each type of added noise as stated above.</p><p>Noisy MNIST Dataset is the same as the original MNIST dataset except for added noise. Again, there are three different versions, one for each of the three types of noise considered. Each version contains 10 classes with a total of 70000 black and white images with image size 28 × 28.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>We used an auxiliary classifier GAN (ACGAN) <ref type="bibr" target="#b19">[20]</ref> as the classifier GAN in our framework. We considered 28 × 28 (the resolution of the noisy MNIST dataset) as the input resolution for our framework. The images in the noisy Bangla dataset were resized to 28 × 28 before being input to our framework.</p><p>The architecture of generators is the reverse of that of the discriminators (see <ref type="figure" target="#fig_0">Fig.1</ref>). Specifically, for the generators, we used a dense layer to transform an input (latent z) to a format that corresponds to the input of the convolutional transpose layers for generating multiresolution images. For Module 1 , after the first convolutional transpose layer, we used a batch normalization layer and another convolutional transpose layer to transform the feature maps to an output image with resolution 7 × 7 using a filter with kernel size 1 × 1. A similar configuration is used generating output images with resolutions 14 × 14 and 28 × 28 respectively for Module 2 and Module 3 during the progressive training procedure. For the discriminator, during progressive training, we started from the input resolution of 7 × 7 (Module 1 ). The real images (i.e., the training images from the noisy dataset under consideration) are downscaled to 7 × 7. The downscaled real images are combined with fake images produced by the generator and are fed to the discriminator for feature extraction and classification. The discriminator not only computes the class label but also discerns fake images from real ones. Similar downscaling and combination operations are performed before feeding inputs to the discriminators for the second and the third modules.</p><p>During the progressive training, the GAN learns, in addition to other features, low resolution features that are more tolerant to noise, being generic in nature. While the GAN learns low resolution as well as noisy features, it never learns to denoise an image. At no point, does our framework produce a denoised image or learns the representation of a denoised image. Instead, low-resolution features that are not disrupted by noise are learned though progressive training. By individually learning representations at each resolution, our method is able to leverage the noise-resistant generic low resolution features to provide better classification performance even for noisy character data. This produces a robust discriminator that can classify noisy handwritten characters. Thus, in our framework, there is no preprocessing step that denoises the input explicitly or implicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>We have evaluated our framework on Noisy Bangla Numeral, Noisy Bangla Characters, and Noisy MNIST datasets with respect to classification accuracy as shown in Tables 1, 2, and 3, respectively. <ref type="figure">Fig.3</ref> shows how the accuracy varies with the number of training epochs for Noisy Bangla Numeral ( <ref type="figure">Fig.3(a)</ref>), Noisy Bangla Characters ( <ref type="figure">Fig.3(b)</ref>, and Noisy MNIST ( <ref type="figure">Fig.3(c)</ref>) datasets.</p><p>For each version (corresponding to each of the three types of added noise) of each dataset, we used the provided training and testing dataset splits <ref type="bibr" target="#b12">[13]</ref>. As we can see in <ref type="table">Table 1</ref>, for the Noisy Bangla Numeral dataset, our approach achieved the best performance (in terms of accuracy): 96.68% on AWGN noise, 98.18% on Motion noise, and 94.60% on Contrast noise, which surpassed the second best ones by 1.22%, 1.13%, 1.75%, respectively. In <ref type="figure">Fig.3(a)</ref>, one can see that the classification accuracy of our framework remains almost the same on AWGN noise with increasing number of epochs. For the same dataset, but with added Motion noise, the classification accuracy initially increases before stabilizing. In case of added Contrast noise, for the same dataset, the classification accuracy remains relatively unstable with increasing number of epochs.</p><p>In <ref type="table">Table 2</ref>, for Noisy Bangla Characters, our framework obtained better performance than the state-of-the-art in the case of added AWGN noise (79.85%) and added Motion noise (89.54%) surpassing the second best by 3.11% and 5.95%, respectively. In the case of added Contrast noise, our framework achieved an accuracy of 68.41%, which is (-1.25%) slightly less than the state-of-the-art 69.66%.</p><p>The performances of all methods on the Noisy Bangla Characters are much worse than those on the Noisy Bangla Numeral and the Noisy MNIST Datasets. Noisy Bangla Characters are relatively harder to classify than the other two datasets as this dataset has 50 classes versus 10 classes for each of the other two datasets. Additionally, among the three types of added noises, all methods have relatively poor performances on Contrast noise compared to their performances on AWGN noise and Motion noise. In <ref type="figure">Fig.3(b)</ref>, we can see that the classification accuracy of our framework remains relatively stable with increasing number of epochs on all the three types of added noise. We also conducted experiments on the Noisy MNIST Dataset. The classification accuracies are shown in <ref type="table">Table 3</ref>. In <ref type="table">Table 3</ref>, it can be seen that the accuracy obtained by our framework exceeds the state-of-the-art by 0.81% in the case of added AWGN noise, by 0.62% in the case of added Motion noise, and by 2.21% in the case of added Contrast noise, yielding best classification accuracies of 98.43%, 99.20%, and 97.25%, respectively. In <ref type="figure">Fig.3(c)</ref> for the Noisy MNIST Dataset, we can see the classification accuracies of our framework already surpassed the state-of-the-art after 5 epochs for all the three types of added noise.</p><p>To understand the statistical significance of the performance improvements obtained by our framework over <ref type="bibr" target="#b12">[13]</ref>, we used McNemars test (since our framework and <ref type="bibr" target="#b12">[13]</ref> had same test datasets). Following are the results of the McNemars tests.</p><p>For noisy Bangla numeral with AWGN noise added: χ 2 = 47.02, df = 1, p &lt; 7.025e − 12; with reduced contrast and white Gaussian noise: χ 2 = 68.014, df = 1, p &lt; 2.2e − 16; here df represents degrees of freedom. For noisy Bangla characters with added AWGN: χ 2 = 398, df = 1, p &lt; 2.2e − 16.</p><p>For noisy MNIST with added AWGN: χ 2 = 79.012, df = 1, p &lt; 2.2e − 16; with reduced contrast and white Gaussian noise: χ 2 = 219, df = 1, p &lt; 2.2e−16.</p><p>Based on the results of the McNemar tests, the improvements obtained over <ref type="bibr" target="#b12">[13]</ref>, even in the case of AWGN and contrast variations are statistically significant.</p><p>The discriminator in our framework is trained to learn representations progressively from lower resolution to higher. Each layer of the discriminator specializes at specific resolutions allowing it to learn more fine-grain features. Lower resolution features are more resistant to noise due to their generic nature. Since the discriminator in our framework has been trained with a combination of fake images produced by the generator and real images belonging to the noisy dataset under consideration, it learned better representations of the variability within the classes. This explains the robustness of our framework to noise and sparse features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented a novel robust noise-resilient classification framework for noisy handwritten (Bangla) characters using progressively trained classification general adversarial networks. The proposed classification framework can directly classify raw noisy data without any preprocessing. We experimentally demonstrated the effectiveness of the framework on the Noisy Bangla Numeral, the Noisy Bangla Basic Characters, and the Noisy MNIST benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>arXiv:1908.08987v1 [cs.CV] 11 Aug 2019 Overview of our proposed Progressively Trained Classifier Generative Adversarial Networks (PCGAN-CHAR) architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Sample data for different types of added noise for Noisy Bangla Characters. Three types of noisy data, Added White Gaussian Noise, Reduced Contrast with white gaussian noise, Motion Blurred noise, shown above from top to bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 : 2 Initialize(module) 3 for 4 for number of batches do 5 Br←</head><label>12345</label><figDesc>General Training Algorithm 1 for number of modules do epoch=1,2,...,K do Sample a batch of n real images with labels 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .Table 3 .</head><label>123</label><figDesc>Comparison of classification accuracy (%) on three types of Noisy Bangla Numeral Comparison of classification accuracy (%) on three types of Noisy Bangla characters Comparison of classification accuracy (%) on three types of Noisy MNIST dataset</figDesc><table><row><cell>Methods</cell><cell cols="2">AWGN Motion Contrast</cell></row><row><cell>Basu et al. [3]</cell><cell>91.34 92.66</cell><cell>87.31</cell></row><row><cell>Dropconnect [13]</cell><cell>91.18 97.05</cell><cell>85.79</cell></row><row><cell cols="2">Karki et al. (w/o Saliency) [13] 95.08 94.88</cell><cell>92.60</cell></row><row><cell>Karki et al. (Saliency) [13]</cell><cell>95.46 95.04</cell><cell>92.85</cell></row><row><cell>PCGAN-CHAR (Ours)</cell><cell cols="2">96.68 98.18 94.60</cell></row><row><cell>Methods</cell><cell cols="2">AWGN Motion Contrast</cell></row><row><cell>Basu et al. [3]</cell><cell>57.31 58.80</cell><cell>46.63</cell></row><row><cell>Dropconnect [13]</cell><cell>61.14 83.59</cell><cell>48.07</cell></row><row><cell cols="2">Karki et al. (w/o Saliency) [13] 70.64 74.36</cell><cell>58.89</cell></row><row><cell>Karki et al. (Saliency) [13]</cell><cell cols="2">76.74 77.22 69.66</cell></row><row><cell>PCGAN-CHAR (Ours)</cell><cell cols="2">79.85 89.54 68.41</cell></row><row><cell>Methods</cell><cell cols="2">AWGN Motion Contrast</cell></row><row><cell>Basu et al. [3]</cell><cell>90.07 97.40</cell><cell>92.16</cell></row><row><cell>Dropconnect [13]</cell><cell>96.02 98.58</cell><cell>93.24</cell></row><row><cell>Karki et al. (Saliency) [13]</cell><cell>97.62 97.20</cell><cell>95.04</cell></row><row><cell>PCGAN-CHAR (Ours)</cell><cell cols="2">98.43 99.20 97.25</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://en.wikipedia.org/wiki/List of datasets for machinelearning research#Handwriting and character recognition</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Decomposing a window into maximal quadtree blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Aref</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="425" to="439" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepsat: a learning framework for satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Nemani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems<address><addrLine>Bellevue, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning sparse feature representations using probabilistic quadtrees and deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gayaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nemani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="855" to="867" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A theoretical analysis of deep neural networks for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gayaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks, IJCNN 2016</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="992" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Handwritten numeral databases of indian scripts and multistage recognition of mixed numerals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="444" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Offline recognition of handwritten bangla characters: an efficient two-stage approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Parui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="445" to="458" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse feature learning for deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cactusnets: Layer applicability as a metric for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-08" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sparse representation for signal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aviyente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS. vol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="609" to="616" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pixellevel reconstruction and classification for noisy handwritten bangla characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICFHR-2018.2018.00095</idno>
		<ptr target="https://doi.org/10.1109/ICFHR-2018" />
	</analytic>
	<monogr>
		<title level="m">16th International Conference on Frontiers in Handwriting Recognition, ICFHR 2018</title>
		<meeting><address><addrLine>Niagara Falls, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">95</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno>abs/1710.10196</idno>
		<ptr target="http://arxiv.org/abs/1710.10196" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quad tree structures for image compression applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Markas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="707" to="721" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Arxiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? In: Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
