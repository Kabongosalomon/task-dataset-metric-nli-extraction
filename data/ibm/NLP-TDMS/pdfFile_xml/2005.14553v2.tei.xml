<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Map-Guided Curriculum Domain Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20201">DECEMBER 2020 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
						</author>
						<title level="a" type="main">Map-Guided Curriculum Domain Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<date type="published" when="20201">DECEMBER 2020 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Domain adaptation</term>
					<term>semantic segmentation</term>
					<term>nighttime</term>
					<term>evaluation</term>
					<term>curriculum learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of semantic nighttime image segmentation and improve the state-of-the-art, by adapting daytime models to nighttime without using nighttime annotations. Moreover, we design a new evaluation framework to address the substantial uncertainty of semantics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmentation models from day to night through progressively darker times of day, exploiting cross-time-of-day correspondences between daytime images from a reference map and dark images to guide the label inference in the dark domains; 2) a novel uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, including image regions beyond human recognition capability in the evaluation in a principled fashion; 3) the Dark Zurich dataset, comprising 2416 unlabeled nighttime and 2920 unlabeled twilight images with correspondences to their daytime counterparts plus a set of 201 nighttime images with fine pixel-level annotations created with our protocol, which serves as a first benchmark for our novel evaluation. Experiments show that our map-guided curriculum adaptation significantly outperforms state-of-the-art methods on nighttime sets both for standard metrics and our uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can improve results on data with ambiguous content such as our benchmark and profit safety-oriented applications involving invalid inputs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE state of the art in semantic segmentation is rapidly improving in recent years. Despite the advance, most methods are designed to operate at daytime, under favorable illumination conditions. However, many outdoor applications require robust vision systems that perform well at all times of day, under challenging lighting conditions, and in bad weather <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Currently, the popular approach to solving perceptual tasks such as semantic segmentation is to train deep neural networks <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> using large-scale human annotations <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. This supervised scheme has achieved great success for daytime images, but it scales badly to adverse conditions like nighttime. The adversity of nighttime poses further challenges for perceptual tasks compared to daytime. The extracted features become corrupted due to visual hazards <ref type="bibr" target="#b8">[9]</ref> such as underexposure, noise, and motion blur. In this work, we focus on semantic segmentation at nighttime, both at the method level and the evaluation level.</p><p>At the method level, this work adapts semantic segmentation models from daytime to nighttime, without annotations in the latter domain. To this aim, we propose a new method called Map-Guided Curriculum Domain Adaptation (MGCDA). The underpinnings of MGCDA are threefold: continuity of time, prior knowledge of place, and power of data. Time: environmental illumination changes continuously from daytime to nighttime. This enables adding intermediate domains between the two to smoothly transfer semantic knowledge. This idea is found to be effective in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>; we extend it by adding two more modules. Place: images taken over different time but with the same 6D camera pose share a large portion of content. The shared content can be used to guide the knowledge transfer process from a favorable condition (daytime) to an adverse condition (nighttime). We formalize this observation and propose a method for largescale applications. The method stores the daytime images and the distilled semantic knowledge into a digital map and enhances the semantic nighttime image segmentation by this geo-referenced map in an adaptive fusion framework. This supplement is especially important for nighttime perception as observing partial information and uncertain data is a frequent situation at nighttime. Data: MGCDA takes advantage of the powerful image translation techniques to stylize real annotated daytime datasets to darker target domains in order to perform standard supervised learning.</p><p>At the evaluation level, this work proposes an uncertaintyaware annotation and evaluation framework for semantic segmentation. The degradation of regions of nighttime images affected by visual hazards is often so intense that they are rendered indiscernible, i.e. determining their semantic content is impossible even for humans. We term such regions as invalid for the task of semantic segmentation. A robust model should predict with high uncertainty on invalid regions while still being confident on valid (discernible) regions, and a sound evaluation framework should reward such behavior. The above requirement is particularly significant for safety-oriented applications such as autonomous cars, since having the vision system declare a prediction as invalid can help the downstream driving system avoid the fatal consequences of this prediction being false, e.g. when a pedestrian is missed.</p><p>To this end, we design a generic uncertainty-aware annotation and evaluation framework for semantic segmentation in adverse arXiv:2005.14553v2 [cs.CV] 7 Jan 2021 conditions which explicitly distinguishes invalid from valid regions of input images, and apply it to nighttime. On the annotation side, our novel protocol leverages privileged information in the form of daytime counterparts of the annotated nighttime scenes, which reveal a large portion of the content of invalid regions. This allows to reliably label invalid regions and to indeed include invalid regions in the evaluation, contrary to existing semantic segmentation benchmarks <ref type="bibr" target="#b6">[7]</ref> which completely exclude them from evaluation. Moreover, apart from the standard class-level semantic annotation, each image is annotated with a mask which designates its invalid regions. On the evaluation side, we allow the invalid label in predictions and adopt from <ref type="bibr" target="#b11">[12]</ref> the principle that for invalid pixels with legitimate semantic labels, both these labels and the invalid label are considered correct predictions. However, this principle does not cover the case of valid regions. We address this by introducing the concept of false invalid predictions. This enables calculation of uncertainty-aware intersection-over-union (UIoU), a joint performance metric for valid and invalid regions which generalizes standard IoU, reducing to the latter when no invalid prediction exists. UIoU rewards predictions which exhibit confidence that is consistent to human annotators, i.e. which have higher confidence on valid regions than invalid ones, meeting the aforementioned requirement.</p><p>Finally, we present Dark Zurich, a dataset of 8779 real images which contains corresponding images of the same driving scenes at daytime, twilight and nighttime. We use this dataset to feed real data to MGCDA and to create a benchmark with 201 nighttime images for our uncertainty-aware evaluation. Our dataset is publicly available <ref type="bibr" target="#b0">1</ref> and is used for hosting a CVPR 2020 challenge on nighttime segmentation 2 . Our code is publicly available <ref type="bibr" target="#b2">3</ref> .</p><p>An earlier version of this work has appeared in the International Conference on Computer Vision <ref type="bibr" target="#b12">[13]</ref>. Compared to the conference version, this paper makes the following additional contributions:</p><p>1) An improved version of our domain adaptation method which involves a geometry-aware formulation for refining semantic predictions via cross-time-of-day correspondences and leads to improved performance over the conference version. 2) An extension of the annotated nighttime part of our Dark Zurich dataset with 50 additional images, leading to a total of 201 annotated nighttime images. 3) Substantially more extensive experiments, including i.a. detailed comparisons with more recent state-of-the-art domain adaptation methods for semantic segmentation, evaluation on additional nighttime sets, thorough ablation studies for the components of our method, and application of our approach at test time. 4) Other enhanced parts, including related work and dataset statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Vision at Nighttime. Nighttime has attracted a lot of attention in the literature due to its ubiquitous nature. Several works pertain to human detection at nighttime, using FIR cameras <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, visible light cameras <ref type="bibr" target="#b15">[16]</ref>, or a combination of both <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. In driving scenarios, a few methods have been proposed to detect cars <ref type="bibr" target="#b18">[19]</ref> and vehicles' rear lights <ref type="bibr" target="#b19">[20]</ref>. Contrary to these domainspecific methods, previous work also includes both methods designed for robustness to illumination changes, by employing domain-invariant representations <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> or fusing information from complementary modalities and spectra <ref type="bibr" target="#b22">[23]</ref>, and datasets with adverse illumination <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. A recent work <ref type="bibr" target="#b10">[11]</ref> on semantic nighttime segmentation shows that images captured at twilight are helpful for supervision transfer from daytime to nighttime. Our work is partially inspired by <ref type="bibr" target="#b10">[11]</ref> and extends it by proposing a map-guided curriculum adaptation framework which learns jointly from stylized images and unlabeled real images of increasing darkness and exploits the prior knowledge from a map.</p><p>There is a rich literature on low-light image enhancement <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, which is also relevant to our work. However, its focus is on the low-level goal of visual quality improvement rather than the high-level goal of accurate semantic scene understanding.</p><p>Domain Adaptation. Performance of semantic segmentation on daytime scenes has increased rapidly in recent years. As a consequence, attention is now turning to adaptation to adverse conditions <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. A case in point are recent efforts to adapt clear-weather models to fog <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, by using both labeled synthetic images and unlabeled real images of increasing fog density. This work instead focuses on the nighttime domain, which poses very different and-as we would claim-greater challenges than the foggy domain (e.g. artificial light sources casting very different illumination patterns at night). A major class of adaptation approaches, including <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, involves adversarial confusion or feature alignment between domains. The general concept of curriculum learning has been successfully applied to domain adaptation by ordering tasks <ref type="bibr" target="#b50">[51]</ref>, target-domain pixels <ref type="bibr" target="#b51">[52]</ref>, or domains <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Our method belongs to the last group. Cross-domain correspondences as guidance have only been used very recently in <ref type="bibr" target="#b53">[54]</ref>, which requires pixel-level matches to be given, while we require more generic image-level correspondences.</p><p>Semantic Segmentation Evaluation. Semantic segmentation evaluation is commonly performed with the IoU metric <ref type="bibr" target="#b5">[6]</ref>. Cityscapes <ref type="bibr" target="#b6">[7]</ref> introduced an instance-level IoU (iIoU) to remove the large-instance bias, as well as mean average precision for the task of instance segmentation. The two tasks have recently been unified into panoptic segmentation <ref type="bibr" target="#b54">[55]</ref>, with a respective panoptic quality metric. The most closely related work to ours in this regard is WildDash <ref type="bibr" target="#b11">[12]</ref>, which uses standard IoU together with a finegrained evaluation to measure the impact of visual hazards on performance. In contrast, we introduce UIoU, a new semantic segmentation metric that handles images with regions of uncertain semantic content and is suited for adverse conditions. Our uncertainty-aware evaluation is complementary to uncertaintyaware methods such as <ref type="bibr" target="#b55">[56]</ref> and <ref type="bibr" target="#b56">[57]</ref> that explicitly incorporate uncertainty in their model formulation and aims to promote the development of such methods, as UIoU rewards models that accurately capture heteroscedastic aleatoric uncertainty <ref type="bibr" target="#b55">[56]</ref> in the input images through the different treatment of invalid and valid regions.</p><p>Map-Guided Vision Applications. One of the major application domains of maps is robot localization, which is a large research field on its own and has a rich literature <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Maps have also been enriched to be leveraged for other vision tasks beyond localization such as road surface detection <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, navigation <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, object detection <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, tracking <ref type="bibr" target="#b65">[66]</ref> and forecasting <ref type="bibr" target="#b66">[67]</ref>. This work uses a new form of map-based prior knowledge, i.e. daytime images and their distilled semantics, to supplement the task of semantic image segmentation. This supplement is especially important when the online segmentation system operates in challenging weather or lighting conditions, e.g. at nighttime. Our learning method uses geo-referenced maps as an additional source of information in an adaptive fusion scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Daytime model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guided segmentation refinement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MAP-GUIDED CURRICULUM DOMAIN ADAPTA-TION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>MGCDA involves a source domain S, an ultimate target domain T , and an intermediate target domainṪ . In this work, S is daytime, T is nighttime, andṪ is twilight time with an intermediate level of darkness between S and T . These domains are formally defined according to the solar elevation angle <ref type="bibr" target="#b10">[11]</ref>, which can be derived from geographical location and time of capture. MGCDA adapts semantic segmentation models through this sequence of domains (S,Ṫ , T ), which is sorted in ascending order with respect to level of darkness. The approach proceeds progressively and adapts the model from one domain in the sequence to the next. The knowledge is transferred through the domain sequence via this gradual adaptation process. The transfer is performed using two coupled branches: 1) learning from labeled synthetic stylized images and 2) learning from real data without annotations, to jointly leverage the assets of both. Stylized images inherit the human annotations of their original counterparts but contain unrealistic artifacts, whereas real images have less reliable pseudolabels but are characterized by artifact-free textures. An overview of MGCDA is presented in <ref type="figure" target="#fig_5">Fig. 1</ref>. Let us use z ∈ {1, 2, 3} as the index in (S,Ṫ , T ). Once the model for the current domain z is trained, its knowledge can be distilled on unlabeled real data from z, and then used, along with a new version of synthetic data from the next domain z + 1 to adapt the current model to z + 1.</p><p>Before diving into the details, we first define all datasets used. The inputs for MGCDA consist of: 1) a labeled daytime set with M real images D 1</p><formula xml:id="formula_0">lr = {(I 1 m , Y 1 m )} M m=1 , e.g. Cityscapes [7], where Y 1 m (i, j) ∈ C = {1, .</formula><p>.., C} is the ground-truth label of pixel (i, j) of I 1 m ; 2) an unlabeled daytime set of N 1 images D 1 ur = {I 1 n } N1 n=1 ; 3) an unlabeled twilight set of N 2 images D 2 ur = {I 2 n } N2 n=1 ; and 4) an unlabeled nighttime set of N 3 images D 3 ur = {I 3 n } N3 n=1 . In order to perform knowledge transfer with annotated data, D 1 lr is rendered in the style of D 2 ur and D 3 ur . We use CycleGAN <ref type="bibr" target="#b67">[68]</ref> to perform this style transfer, leading to two more sets:</p><formula xml:id="formula_1">D 2 ls = {(Ī 2 m , Y 1 m )} M m=1 and D 3 ls = {(Ī 3 m , Y 1 m )} M m=1 , whereĪ 2</formula><p>m andĪ 3 m are the stylized twilight and nighttime version of I 1 m respectively, and labels are copied. For z = 1, the semantic segmentation model φ 1 is trained directly on D 1 lr . In order to perform knowledge transfer with unlabeled data, pseudo-labels for all three unlabeled real datasets need to be generated. The pseudo-labels for D 1 ur are generated using the model φ 1 viâ Y 1 n = φ 1 (I 1 n ). For z &gt; 1, training φ z and generatingŶ z m is performed progressively as MGCDA proceeds, as is detailed in Sec. 3.1.1. All six datasets are summarized in <ref type="table" target="#tab_1">Table 1</ref>. In <ref type="figure" target="#fig_0">Fig. 2</ref>, we show visual examples from the six training sets. Cityscapes <ref type="bibr" target="#b6">[7]</ref> is used to instantiate the labeled sets, while our Dark Zurich dataset, which we detail in Sec. 5, is used to instantiate the unlabeled sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Map-Guided Curriculum Domain Adaptation</head><p>Since the method proceeds in an iterative manner, we present the algorithmic details only for a single adaptation step from z − 1 to z. The presented algorithm is straightforward to generalize to multiple intermediate target domains. In order to adapt the semantic segmentation model φ z−1 from the previous domain  z − 1 to the current domain z, we generate synthetic stylized data in domain z: D z ls . For real unlabeled images, since no human annotations are available, we rely on a strategy of self-learning or curriculum learning. Our motivating assumption is that objects are generally easier to recognize in lighter conditions, so the tasks are solved in ascending order with respect to the level of darkness and the easier, solved tasks are used to re-train the model to solve the harder tasks. This is in line with the concept of curriculum learning <ref type="bibr" target="#b68">[69]</ref>. In particular, the model φ z−1 for domain z − 1 can be applied to the unlabeled real images of domain z − 1 to generate supervisory labels for training φ z . Specifically, the dataset of real images with pseudo-labels for adaptation to domain</p><formula xml:id="formula_2">{(I 1 m , Y 1 m )} M m=1 {(I 1 n ,Ŷ 1 n )} N 1 n=1 2. Twilight time {(Ī 2 m , Y 1 m )} M m=1 {(I 2 n ,Ŷ 2 n )} N 2 n=1 3. Nighttime {(Ī 3 m , Y 1 m )} M m=1 {(I 3 n ,Ŷ 3 n )} N 3 n=1 (a) D 1 lr : Cityscapes (b) D 1 ur : Dark Zurich-day (c) D 2 ls : Cityscapes-twilight style (d) D 2 ur : Dark Zurich-twilight (e) D 3 ls : Cityscapes-nighttime style (f) D 3 ur : Dark Zurich-night</formula><formula xml:id="formula_3">z is D z−1 ur = {(I z−1 n ,Ŷ z−1 n )} Nz−1 n=1 , whereŶ z−1 n</formula><p>denotes the predicted labels of image I z−1 n . A simple way to get these labels is by directly feeding I z−1 n to φ z−1 , similar to the approach of <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b34">[35]</ref> for the case of fog. This choice, however, suffers from accumulation of substantial errors in the prediction of φ z−1 into the subsequent training step if domain z − 1 is not the daytime domain. We instead propose a method to refine these errors by using weak supervision from the semantics of a daytime image I 1 n that corresponds to I z−1 n , i.e. depicts roughly the same scene as I z−1 n (the difference in the camera pose is small):</p><formula xml:id="formula_4">Y z−1 n = G φ z−1 (I z−1 n ), I z−1 n , φ 1 (I 1 Az−1→1(n) ), I 1 Az−1→1(n) ,<label>(1)</label></formula><p>where G is a guidance function which will be defined in Sec. 3.2 and z − 1 &gt; 1. A z−1→1 (n) is the correspondence function giving the index of the daytime image that corresponds to I z−1 n . Once we have the two training sets D z−1 ur (with labels inferred through (1)) and D z ls , learning φ z is performed by optimizing a loss function that involves both datasets:</p><formula xml:id="formula_5">min φ z (I,Y ) ∈D z ls L(φ z (I), Y ) + µ (I,Ŷ ) ∈D z−1 ur L(φ z (I),Ŷ ) ,<label>(2)</label></formula><p>where L(., .) is the cross entropy loss and µ is a hyper-parameter balancing the contribution of the two datasets. In order to leverage the map prior at large scale to improve predictions through the guided label refinement defined in (1), specific aligned training datasets need to be compiled. With this aim, we collected the Dark Zurich dataset by driving several laps in disjoint areas of Zurich; each lap was driven multiple times during the same day, starting from daytime through twilight to nighttime. The recording overhead is moderate compared to the unsupervised setting where only a single nighttime recording would be performed. The recordings include GPS readings and are split into three sets: daytime, twilight and nighttime (cf. Sec. 5). Since different drives of the same lap correspond to the same route, the camera orientation at a certain point of the lap is similar across all drives. We implement the correspondence function A z→1 that assigns to each image in domain z its daytime counterpart using a GPS-based nearest neighbor assignment, as shown in <ref type="figure" target="#fig_1">Fig. 3(a)</ref>. Thus, the requirements for the training dataset compared to the unsupervised adaptation case do not significantly restrict the applicability of MGCDA. The method presented in Sec. 3.2 carefully handles the effects of misalignment and dynamic objects in paired images.</p><p>The geo-referenced daytime images, along with their semantic pseudo-labels, are used as a new form of map knowledge. We acknowledge that our method uses a very simple map-matching method that may not be sufficient for other tasks. However, we show that our learning method is able to benefit from the corresponding map data already. Developing and using more sophisticated map-matching algorithms is orthogonal to our learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Geometrically Guided Segmentation Refinement</head><p>In the following presentation of our guided segmentation refinement for dark images using corresponding daytime images, we drop for brevity the subscript which was used to indicate this correspondence. In the conference version of this paper, the specific formulation of the guidance function G for our refinement approach which was introduced in a general form in (1) was</p><formula xml:id="formula_6">G φ z (I z ), I z , φ 1 (I 1 ) = R φ z (I z ), B(φ 1 (I 1 ), I z ) ,<label>(3)</label></formula><p>i.e. the composition of a cross bilateral filter B on the daytime predictions, which aligns them to the dark image, with a fusion function R, which adaptively combines the aligned daytime predictions with the initial dark image predictions to refine the latter. In this extended version, we propose an improved, geometryaware formulation for the alignment of the daytime predictions to the dark image, which explicitly incorporates the respective two-view geometry and performs the alignment by warping the daytime predictions to the viewpoint of the dark image. The specification of the guidance function G in the newly proposed, geometrically guided refinement is</p><formula xml:id="formula_7">G φ z (I z ), I z , φ 1 (I 1 ), I 1 = R φ z (I z ), Q(φ 1 (I 1 ), I 1 , I z ) ,<label>(4)</label></formula><p>where the fusion function R is the same as in <ref type="formula" target="#formula_6">(3)</ref> and the cross bilateral filter B has been replaced by a warping function Q which maps the daytime predictions to the dark view I z . This warping function can be further analyzed as</p><formula xml:id="formula_8">Q(φ 1 (I 1 ), I 1 , I z ) = W (φ 1 (I 1 ), d(I 1 ), δT (I 1 , I z )), (5)</formula><p>where d(I 1 ) = d 1 is the estimated depth map for the daytime image and δT (I 1 , I z ) is the estimated camera motion between the daytime view and the dark view. In the remainder of this section, we present the details of the individual modules of our guided segmentation refinement corresponding to functions B, Q and R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Cross Bilateral Filter for Prediction Alignment</head><p>The correspondences between real images that are used in MGCDA are not perfect, in the sense that they are not aligned at a pixel-accurate level. Therefore, to leverage the prediction for the daytime image I 1 as guidance for refining the respective prediction for the dark image I z , it is necessary to first align the former prediction to I z . To this end, we operate on soft predictions and define a cross bilateral filter on the initial soft prediction map S 1 = φ 1 (I 1 ) which uses the color of the dark image I z as reference:</p><formula xml:id="formula_9">S 1 (p) = q∈N (p) G σs ( q − p )G σr ( I z (q) − I z (p) )S 1 (q) q∈N (p) G σs ( q − p )G σr ( I z (q) − I z (p) )</formula><p>. <ref type="formula">(6)</ref> In <ref type="bibr" target="#b5">(6)</ref>, p and q denote pixel positions, N (p) is the neighborhood of p, G σs is the spatial-domain Gaussian kernel and G σr is the color-domain kernel. The definition of the filter implies that only pixels q with similar color to the examined pixel p in the dark image I z contribute to the outputS 1 (p), which shifts salient edges in the initial daytime prediction to their correct position in the dark image. For the color-domain kernel, we use the CIELAB version of I z , as it is more appropriate for measuring color similarity <ref type="bibr" target="#b69">[70]</ref>. We set the spatial parameter σ s to 80 to account for large misalignment, and σ r to 10 following <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b69">[70]</ref>. In different settings from ours, σ s needs to be scaled proportionally to the image dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Depth-Based Warping for Prediction Alignment</head><p>An important drawback of the above prediction alignment approach with a cross bilateral filter is its uniform operation on all image regions, despite the fact that the magnitude of misalignment between corresponding points in the two views varies across the image, depending on the depth of the examined points as well as the particular camera motion between the two views. Furthermore, in case the magnitude of misalignment is larger than the diameter of the respective ground-truth semantic segment, there is no common support between the regions this segment occupies in S 1 and I z , inevitably leading to erroneous outputs of the filter on small objects.</p><p>In order to address this issue, we explicitly model the twoview geometry which pertains to the daytime image and the dark image at hand, and use the estimated camera motion together with the depth map for the daytime view to apply a dense pixellevel warping of the daytime predictions to the target viewpoint that corresponds to the dark image. In this way, we are able to capture the diverse magnitudes and directions of pixel flow within the image, aligning the daytime predictions accurately in a dense pixel-level fashion. We illustrate this process in <ref type="figure" target="#fig_1">Fig. 3</ref></p><formula xml:id="formula_10">(b).</formula><p>More formally, we first establish dense correspondences from the pixel grid of the daytime image to that of the dark image. Consider a pixel p = (x p , y p ) T in the daytime image. We denote the depth value at this pixel by d 1 (p). Moreover, we denote the calibration matrices for the two views by K 1 and K z , and the transformation from the coordinate system of the daytime view to that of the dark view, which models camera motion δT (I 1 , I z ), by (R|t). The point p in the dark image that corresponds to p is identified by first back-projecting p into 3D space and then reprojecting this 3D point to the dark view, which can be expressed as</p><formula xml:id="formula_11">p 1 ∼ K z (R|t)   d 1 (p) K 1 −1 p 1 1   , (7)</formula><p>where ∼ denotes equality up to scale. These dense 2D-2D correspondences enable us to warp the soft prediction map S 1 for the daytime image I 1 to the viewpoint of the dark image I z . Note that although inverse warping, which would use the depth map of the target dark view, is known to perform better in the literature <ref type="bibr" target="#b70">[71]</ref> and to avoid discretization artifacts, we choose instead to apply forward warping, which uses the depth map of the source daytime view, as shown in <ref type="formula">(7)</ref>. The reasoning behind this choice is that a ground-truth depth map is not available for either of the views and  the consequent monocular depth estimation outputs much more reliable results on the easier, daytime domain of the source view I 1 than on the adverse, dark domain of I z .</p><formula xml:id="formula_12">(a) Dark image I z (b) Initial prediction S z for I z (c) Daytime image I 1 (d) Initial prediction S 1 for I 1 (e) Aligned</formula><p>In particular, we apply this forward warping by defining a quadrilateral mesh on the pixel grid of I 1 using 4-connectivity to form the quads. This mesh is deformed through <ref type="bibr" target="#b6">(7)</ref>, which results in fractional coordinates for the quad vertices in general. For each pixel p in the dark view, we assign it the quad q that contains it and calculate the warped soft predictionS 1 (p) by performing bilinear interpolation of the soft predictions on the four original vertices of q in the daytime view. The bilinear weights are defined by the position of p inside the deformed q.</p><p>One of the challenges that occur in the aforementioned setting are potential fold-backs of the mesh, which correspond to occlusions of visible parts of I 1 in I z . In this case, for an affected pixel there are more than one candidate quads that contain it, corresponding to the occluder and one or more occludees. We solve this ambiguity by assigning to the pixel the quad with the smallest depth value d 1 (computed as the average over all quad vertices), which corresponds to the occluder that is actually visible in the target dark view. Another challenge is related to disocclusion, in which case the assigned quads of disoccluded pixels are elongated across the direction perpendicular to the associated depth edge. We handle this case as follows: we first detect such irregular quads in the deformed mesh by calculating the ratio of lengths of the second longest side of each quad to its third longest side; if this ratio is larger than 5, the quad is deemed as irregular. These quads accept a clear binary cut which corresponds to removing their two longest sides and reflects the depth discontinuity within them. Since the content of disoccluded pixels is generally similar to the region in the source view which is on the distant side of the associated depth edge, we modify the interpolation weights so that only those quad vertices which are on the side of the aforementioned cut with the larger values for d 1 are considered for the interpolation. Finally, for pixels in I z that are not assigned any quad, which implies that they are invisible in I 1 , the soft prediction is directly copied from the pixel with the same position in S 1 , based on the prior that the mean of the distribution of camera motion between the two views is zero.</p><p>We now elaborate further on the implementation details for computing the dense correspondences between I 1 and I z established via <ref type="bibr" target="#b6">(7)</ref>. As far as the depth map d 1 is concerned, we use a pre-trained Monodepth2 <ref type="bibr" target="#b71">[72]</ref> model (trained with stereo supervision at 1024 × 320 resolution) to obtain an absolute estimate for d 1 . We process this estimate further by setting all pixels for which φ 1 predicts sky to the maximum possible value for Monodepth2, which is d max = 540 m. In order to compute the camera motion between the two views, we first apply SURF <ref type="bibr" target="#b72">[73]</ref> to extract keypoints and respective descriptors in both images. The descriptors for I z are matched with their nearest neighbors in I 1 , using three criteria for rejecting matches. In particular, <ref type="bibr" target="#b0">(1)</ref> we only accept mutual nearest neighbors, (2) we apply a threshold θ sec = 0.7 to the ratio of squared Euclidean distances of each nearest neighbor to the respective second nearest neighbor, and <ref type="formula" target="#formula_6">(3)</ref> we apply a threshold θ rel = 20 to the ratio of squared Euclidean distances of the current match to the match with globally minimum distance. After identifying the putative matches, we run the 7-point RANSAC algorithm to compute the final inlier set of matches and obtain the fundamental matrix F, using 1000 iterations and an inlier threshold of t = 2 pixels for RANSAC. We then compute the essential matrix E = (K z ) T FK 1 ; this step assumes that the camera is calibrated for both images. E is decomposed into the rotational component R and the translational component t of the camera motion we are after, only that t is determined at this point only up to scale. We recover the scale of t by triangulating the matched points and estimating the median scaling factor that needs to be applied to their Z-coordinates so that these match the respective values of the depth map d 1 .</p><p>We compare the two approaches for prediction alignment on a pair of a twilight image and a corresponding daytime image from Dark Zurich in <ref type="figure" target="#fig_2">Fig. 4</ref>, where we depict hard predictions for easier visualization. As can be seen from <ref type="figure" target="#fig_2">Fig. 4</ref>(e) and 4(f), the depthbased warping preserves small-scale objects such as traffic signs and poles well in the aligned predictionS 1 , in contrast to the cross bilateral filter which completely extinguishes such objects due to its inability to handle misalignments that are very large relative to the object's scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Confidence-Adaptive Prediction Fusion</head><p>The final step in our refinement approach, which is applied after either of the preceding prediction alignment approaches, is to fuse the aligned predictionS 1 for I 1 with the initial prediction S z = φ z (I z ) for I z in order to obtain the refined prediction S z , the hard version of which is subsequently used in training. We propose an adaptive fusion scheme, which uses the confidence associated with the two predictions at each pixel to weigh their contribution in the output and addresses disagreements due to dynamic content by properly adjusting the fusion weights. Let us denote the confidence of the aligned predictionS 1 for I 1 at pixel p by F 1 (p) = max c∈CS 1 c (p) and respectively the confidence of the initial prediction S z for I z by F z (p). Our confidence-adaptive fusion is then defined aŝ</p><formula xml:id="formula_13">S z = F z F z + αF 1 S z + αF 1 F z + αF 1S 1 ,<label>(8)</label></formula><p>where 0 &lt; α = α(p) ≤ 1 may vary and we have completely dropped the pixel argument p for brevity. In this way, we allow the daytime image prediction to have a greater effect on the output at regions of the dark image which were not easy for model φ z to classify, while preserving the initial prediction S z at lighter regions of the dark image where S z is more reliable. Our fusion distinguishes between dynamic and static scene content by regulating α. In particular, α downweightsS 1 to induce a preference towards S z when both predictions have high confidence. However, apart from imperfect alignment, the two scenes also differ due to dynamic content. Intuitively, the prediction of a dynamic object in the daytime image should be assigned an even lower weight in case the corresponding prediction in the dark image does not agree, since this object might only be present in the former scene. More formally, we denote the subset of C that includes dynamic classes by C d and define</p><formula xml:id="formula_14">α(p) =        α l , if c 1 = arg max c∈CS 1 c (p) ∈ C d and S z c1 (p) ≤ η or c 2 = arg max c∈C S z c (p) ∈ C d andS 1 c2 (p) ≤ η, α h otherwise.<label>(9)</label></formula><p>The majority of pixels falls in the second case of (9) and the choice of α h is important, as it involves a trade-off between exploiting the daytime prediction and avoiding overly relying on it. In our experiments, we manually tune α l = 0.3, α h = 0.6 and η = 0.2 on a couple of training images (no grid search), which implies that our method is not sensitive to the exact value of these parameters. An illustration of the effect of α h on the prediction fusion results is included in Appendix C. Comparative results of our complete guided segmentation refinement for the two prediction alignment approaches are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>(g) and 4(h). Note the improved correction of the sky region on the top right part of the image as well as the better preservation of fine objects such as distant traffic signs achieved with depth-based warping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UNCERTAINTY-AWARE EVALUATION</head><p>Images taken under adverse conditions such as nighttime contain invalid regions, i.e. regions with indiscernible semantic content. Invalid regions are closely related to the concept of negative test cases which was considered in <ref type="bibr" target="#b11">[12]</ref>. However, invalid regions constitute intra-image entities and can co-exist with valid regions in the same image, whereas a negative test case refers to an entire image that should be treated as invalid. We build upon the evaluation of <ref type="bibr" target="#b11">[12]</ref> for negative test cases and generalize it to be applied uniformly to all images in the evaluation set, whether they contain invalid regions or not. Our annotation and evaluation framework includes invalid regions in the set of evaluated pixels, but treats them differently from valid regions to account for the high uncertainty of their content. In the following, we elaborate on the generation of ground-truth annotations using privileged information through the day-night correspondences of our dataset and present our UIoU metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Annotation with Privileged Information</head><p>For each image I, the annotation process involves two steps: 1) creation of the ground-truth invalid mask J, and 2) creation of the ground-truth semantic labeling H.</p><p>For the semantic labels, we consider a predefined set C of C classes, which is equal to the set of Cityscapes <ref type="bibr" target="#b6">[7]</ref> evaluation classes (C = 19). The annotator is first presented only with I and is asked to mark the valid regions in it as the regions which she can unquestionably assign to one of the C classes or declare as not belonging to any of them. The result of this step is the invalid mask J, which is set to 0 at valid pixels and 1 at invalid pixels.</p><p>Secondly, the annotator is asked to mark the semantic labels of I, only that this time she also has access to an auxiliary image I . This latter image has been captured with roughly the same 6D camera pose as I but under more favorable conditions. In our dataset, I is captured at daytime whereas I is captured at nighttime. The large overlap of static scene content between the two images allows the annotator to label certain regions in H with a legitimate semantic label from C, even though the same regions have been annotated as invalid (and are kept as such) in J. This allows joint evaluation on valid and invalid regions, as it creates regions which can accept both the invalid label and the ground-truth label from C as correct predictions. Due to the imperfect match of the camera poses for I and I , the labeling of invalid regions in H is done conservatively, marking a coarse boundary which may leave unlabeled zones around the true semantic boundaries in I, so that no pixel is assigned a wrong label. The parts of I which remain indiscernible even after inspection of I are left unlabeled in H. These parts as well as instances of classes outside C are not considered during evaluation. We illustrate a visual example of our annotation inputs and outputs in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Uncertainty-Aware Predictions</head><p>The semantic segmentation prediction that is fed to our evaluation is expected to include pixels labeled as invalid. Instead of defining a separate, explicit invalid class, which would potentially require the creation of new training data to incorporate this class, we allow a more flexible approach for soft predictions with the original set of semantic classes by using a confidence threshold, which affords an evaluation curve for our UIoU metric by varying this threshold.</p><p>In particular, we assume that the evaluated method outputs an intermediate soft prediction S(p) at each pixel p as a probability distribution among the C classes, which is subsequently converted to a hard assignment by outputting the classH(p) = arg max c∈C {S c (p)} with the highest probability. In this case, SH (p) (p) ∈ [1/C, 1] is the effective confidence associated with the prediction. This assumption is not very restrictive, as most recent semantic segmentation methods are based on CNNs with a softmax layer that outputs such soft predictions. The final evaluated outputĤ is computed based on a free parameter θ ∈ [1/C, 1] which acts as a confidence threshold by invalidating those pixels where the confidence of the prediction is lower than θ, i.e.Ĥ(p) =H(p) if SH (p) (p) ≥ θ and invalid otherwise. Increasing θ results in more pixels being predicted as invalid. This approach is motivated by the fact that ground-truth invalid regions are identified during annotation by the uncertainty of their semantic content, which implies that a model should ideally place lower confidence (equivalently higher uncertainty) in predictions on invalid regions than on valid ones, so that the former get invalidated for lower values of θ than the latter. The formulation of our UIoU metric rewards this behavior as we shall see next. Note that our evaluation does not strictly require soft predictions, as UIoU can be normally computed for fixed, hard predictionsĤ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">UIoU</head><p>We propose UIoU as a generalization of the standard IoU metric for evaluation of semantic segmentation predictions which may contain pixels labeled as invalid. UIoU reduces to standard IoU if no pixel is predicted to be invalid, e.g. when θ = 1/C.</p><p>The calculation of UIoU for class c involves five sets of pixels, which are listed along with their symbols: true positives (TP), false positives (FP), false negatives (FN), true invalids (TI), and false invalids (FI). Based on the ground-truth invalid masks J, the ground-truth semantic labelings H and the predicted labelsĤ for the set of evaluation images, these five sets are defined as follows: </p><formula xml:id="formula_15">TP = {p : H(p) =Ĥ(p) = c},<label>(10)</label></formula><formula xml:id="formula_16">FP = {p : H(p) = c andĤ(p) = c},<label>(11)</label></formula><formula xml:id="formula_17">FN = {p : H(p) = c andĤ(p) / ∈ {c, invalid}},<label>(12)</label></formula><p>UIoU for class c is then defined as</p><formula xml:id="formula_19">UIoU = |TP| + |TI| |TP| + |TI| + |FP| + |FN| + |FI| .<label>(15)</label></formula><p>Note that a true invalid prediction results in equal reward to predicting the correct semantic label of the pixel. Moreover, an invalid prediction does not come at no cost: it incurs the same penalty on valid pixels as predicting an incorrect label. When dealing with multiple classes, we modify our notation to UIoU (c) (similarly for the five sets of pixels related to class c), which we avoided in the previous definitions to reduce clutter. The overall semantic segmentation performance on the evaluation set is reported as the mean UIoU over all C classes. By varying the confidence threshold θ and using the respective output, we obtain a parametric expression UIoU(θ). When θ = 1/C, no pixel is predicted as invalid and thus UIoU(1/C) = IoU. We motivate the usage of UIoU instead of standard IoU in case the test set includes ground-truth invalid masks by showing in Th. 1 that UIoU is guaranteed to be larger than IoU for some θ &gt; 1/C under the assumption that predictions on invalid regions are associated with lower confidence than those on valid regions, which lies in the heart of our evaluation framework. The proof is in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE DARK ZURICH DATASET</head><p>Dark Zurich was recorded in Zurich using a 1080p GoPro Hero 5 camera, mounted on top of the front windshield of a car. The collection protocol with multiple drives of several laps to establish correspondences is detailed in Sec. 3.</p><p>We split Dark Zurich and reserve one lap for validation and another lap for testing. The rest of the laps remain unlabeled and are used for training. They comprise 3041 daytime, 2920 twilight and 2416 nighttime images extracted at 1 fps, which are named Dark Zurich-{day, twilight, night} respectively and correspond to the three sets in the rightmost column of <ref type="table" target="#tab_1">Table 1</ref>. From the validation and testing night laps, we extract one image every 50m or 20s, whichever comes first, and assign to it the corresponding daytime image to serve as the auxiliary image I in our annotation (cf. Sec. 4.1). We annotate 201 nighttime images (151 from the testing lap and 50 from the validation lap) with fine pixel-level Cityscapes labels and invalid masks following our protocol and name these sets Dark Zurich-test and Dark Zurichval respectively. In total, 366.8M pixels have been annotated with semantic labels and 90.2M of these pixels are marked as invalid. Detailed annotation statistics are provided in <ref type="figure" target="#fig_6">Fig. 6</ref>   the quality of our annotations by having 20 images annotated twice by different subjects and measuring consistency. 93.5% of the labeled pixels are consistent in the semantic annotations and respectively 95% in the invalid masks. We compare to existing annotated nighttime sets in <ref type="table" target="#tab_3">Table 2</ref>, noting that most large-scale sets for road scene parsing, such as Cityscapes <ref type="bibr" target="#b6">[7]</ref> and Mapillary Vistas <ref type="bibr" target="#b7">[8]</ref>, contain few or no nighttime scenes. Nighttime Driving <ref type="bibr" target="#b10">[11]</ref> and Raincouver <ref type="bibr" target="#b73">[74]</ref> only include coarse annotations. Dark Zurich contains fifteen times more annotated nighttime images than WildDash <ref type="bibr" target="#b11">[12]</ref>-the only other dataset with fine and reliable nighttime annotations. Detailed inspection showed that ∼70% of the 345 densely annotated nighttime images of BDD100K <ref type="bibr" target="#b74">[75]</ref> contain severe labeling errors which render them unsuitable for evaluation, especially in dark regions we treat as invalid (e.g. sky is often mislabeled as building). Our annotation protocol helps avoid such errors by properly defining invalid regions and using daytime images to aid annotation, and the labeled part of Dark Zurich is an initial high-quality benchmark to promote our uncertainty-aware evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>Our architecture of choice for implementing MGCDA is Re-fineNet <ref type="bibr" target="#b3">[4]</ref>. We use the publicly available RefineNet-res101-Cityscapes model, trained on Cityscapes, as the baseline model to be adapted to nighttime. Throughout our experiments, we train this model with a constant learning rate of 5 × 10 −5 on minibatches of size 1. To obtain the synthetic labeled datasets for MGCDA, we stylize Cityscapes to twilight using a CycleGAN model that is trained to translate Cityscapes to Dark Zurichtwilight (respectively to nighttime with Dark Zurich-night). The real training datasets for MGCDA are Dark Zurich-day, instantiating D 1 ur , and Dark Zurich-twilight, instantiating D 2 ur . Each adaptation step comprises 30k SGD iterations and uses µ = 1. For the second step, we apply our guided refinement to the labels of Dark Zurich-twilight that are predicted by model φ 2 fine-tuned in the first step, using the correspondences of Dark Zurich-twilight to Dark Zurich-day. In particular, we experiment with both variants of our guided refinement, i.e., the original variant which was presented in our conference paper <ref type="bibr" target="#b12">[13]</ref> and uses cross bilateral filtering (Sec. 3.2.1), and the new upgraded variant which uses depth-based warping (Sec. 3.2.2). We refer to the original variant of our complete pipeline as GCMA and to the upgraded variant as MGCDA. For MGCDA, we note that for the corresponding image pairs of Dark Zurich-twilight and Dark Zurich-day on which the RANSAC step of the depth-based warping variant for guided refinement detects less than 14 inliers, we fall back to cross bilateral filtering. This pertains to 1852/2920 pairs. Moreover, MGCDA uses an improved configuration for CycleGAN-based stylization compared to GCMA, which is detailed in Sec. 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison to Other Adaptation Methods</head><p>Our first experiment compares MGCDA and GCMA to state-ofthe-art approaches for adaptation of semantic segmentation models to nighttime. We evaluate MGCDA and GCMA on Dark Zurichtest against the state-of-the-art adaptation approaches AdaptSeg-Net <ref type="bibr" target="#b39">[40]</ref>, BDL <ref type="bibr" target="#b45">[46]</ref>, ADVENT <ref type="bibr" target="#b46">[47]</ref> and DMAda <ref type="bibr" target="#b10">[11]</ref> and report  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4</head><p>Performance comparison of our method with state-of-the-art approaches and daytime-trained baselines on Nighttime Driving <ref type="bibr" target="#b10">[11]</ref>.</p><p>Read as <ref type="table" target="#tab_5">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mIoU (%)</head><p>RefineNet <ref type="bibr" target="#b3">[4]</ref> 31.5 DeepLab-v2 <ref type="bibr" target="#b75">[76]</ref> 32.6</p><p>AdaptSegNet-Cityscapes→DZ-night <ref type="bibr" target="#b39">[40]</ref> 34.5 ADVENT-Cityscapes→DZ-night <ref type="bibr" target="#b46">[47]</ref> 34.7 BDL-Cityscapes→DZ-night <ref type="bibr" target="#b45">[46]</ref> 34.7 DMAda <ref type="bibr" target="#b10">[11]</ref> 36.1 Ours: GCMA <ref type="bibr" target="#b12">[13]</ref> 45.6 Ours: MGCDA 49. <ref type="table">4   TABLE 5</ref> Performance comparison of our method with state-of-the-art approaches and daytime-trained baselines on BDD100K-night <ref type="bibr" target="#b74">[75]</ref>.</p><p>Read as <ref type="table" target="#tab_5">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mIoU (%)</head><p>RefineNet <ref type="bibr" target="#b3">[4]</ref> 26.6 DeepLab-v2 <ref type="bibr" target="#b75">[76]</ref> 22.9</p><p>AdaptSegNet-Cityscapes→DZ-night <ref type="bibr" target="#b39">[40]</ref> 22.0 ADVENT-Cityscapes→DZ-night <ref type="bibr" target="#b46">[47]</ref> 22.6 BDL-Cityscapes→DZ-night <ref type="bibr" target="#b45">[46]</ref> 22.8 DMAda <ref type="bibr" target="#b10">[11]</ref> 28.3 Ours: GCMA <ref type="bibr" target="#b12">[13]</ref> 33.2 Ours: MGCDA 34.9</p><p>standard IoU performance in <ref type="table" target="#tab_5">Table 3</ref>, including invalid pixels which are assigned a legitimate semantic label in the evaluation. We have trained AdaptSegNet, BDL and ADVENT to adapt from Cityscapes to Dark Zurich-night. For fair comparison, we also report the performance of the respective baseline Cityscapes models for each method. RefineNet is the common baseline of MGCDA, GCMA and DMAda, while DeepLab-v2 <ref type="bibr" target="#b75">[76]</ref> is the common baseline of AdaptSegNet, BDL and ADVENT. The fact that both baseline models feature a ResNet-101 backbone <ref type="bibr" target="#b76">[77]</ref> allows a direct comparison. Both MGCDA and GCMA significantly outperform the other methods for most classes and achieve a substantial 10% improvement in the overall mIoU score against the next best method. The improvement with MGCDA and GCMA is pronounced for classes which usually appear dark at nighttime, such as sky, vegetation, building and person, indicating that our method successfully handles large domain shifts from its source daytime domain. These findings are supported by visually assessing the predictions of the compared methods, as in the examples of <ref type="figure" target="#fig_7">Fig. 7</ref>.</p><p>In order to reinforce these conclusions and show that our method generalizes very well to different datasets, we repeat the above comparison on two additional sets. More specifically, we evaluate the various approaches on Nighttime Driving <ref type="bibr" target="#b10">[11]</ref> and report the results in <ref type="table">Table 4</ref>. In addition, we consider BDD100K <ref type="bibr" target="#b74">[75]</ref> as a candidate benchmark, even though it presents the difficulty of unreliable ground-truth annotations, as mentioned in Sec. 5. We overcome this issue by manually identifying a list of 87 images (out of a total of 345) whose annotations are free from obvious errors. We name this subset BDD100K-night and restrict ourselves to it for evaluation. The associated results are reported in <ref type="table">Table 5</ref>. Indeed, MGCDA and GCMA are by far the best-performing adaptation methods on Nighttime Driving and BDD100K-night. The rest of the methods generally deliver only slight improvements compared to their respective daytime baselines. MGCDA improves upon RefineNet by very large margins: 17.9% on Nighttime Driving and 8.3% on BDD100K-night. This large improvement on BDD100K-night is achieved even though MGCDA has not been presented with any image from the particular domain of BDD100K during training. Equally importantly, MGCDA brings a significant benefit of 3.8% on Nighttime Driving and 1.7% on BDD100K-night compared to GCMA, which supports the utility of the novel geometrically guided refinement via depthbased warping for adaptation thanks to more accurate resulting pseudo-labels. The superiority of MGCDA is demonstrated in the qualitative results of <ref type="figure" target="#fig_9">Fig. 8</ref> on BDD100K-night.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Image Translation with CycleGAN</head><p>Compared to GCMA, in MGCDA we have implemented a different configuration for training and testing CycleGAN to stylize images as twilight or nighttime for generating our synthetic training sets. More specifically, the default CycleGAN configuration, which we used in GCMA, involves training the entire architecture on small 256×256 crops of the input images, while at test time the full images are passed to the generator. We have observed that this discrepancy between the field of view of the CycleGAN model at training versus test time leads to a smaller degree of translation of the overall image appearance than desired, as shown in <ref type="figure" target="#fig_10">Fig. 9</ref>. To resolve this issue in the synthetic data stream of our pipeline, we downsize input images from both domains to 360×720 resolution,   so that the entire architecture fits into GPU memory, and train CycleGAN on the full downsized images. At test time, the same downsized images as in training are input to the generator, and the stylized 360×720 outputs are upsampled to the original resolution using joint bilateral upsampling <ref type="bibr" target="#b77">[78]</ref>. In this way, the generator is presented with the entire pattern of appearance changes across different image regions and it is able to learn better the global shift in illumination between the two domains, as can be seen in <ref type="figure" target="#fig_10">Fig. 9</ref>. Apart from this visual comparison, we also demonstrate in <ref type="table" target="#tab_6">Table 6</ref> the induced improvement in image translation from full-image training in the target context of semantic segmentation adaptation, using Cityscapes images stylized as nighttime with the two examined CycleGAN variants to adapt the baseline RefineNet model to nighttime in a single step. We therefore use full-image CycleGAN training for generating the synthetic images in our upgraded MGCDA pipeline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Study for MGCDA</head><p>We measure the individual effect of the main components of MGCDA in <ref type="table" target="#tab_7">Table 7</ref> by evaluating its ablated versions on Dark Zurich-test. The naive baseline adapting directly to nighttime delivers only a slight benefit compared to the daytime-trained model. By comparison, adaptation to nighttime with our twostage curriculum training involving twilight data delivers a much larger benefit of 9.7%, thanks to the fact that pseudo-labels of real images are always inferred by models already adapted to the respective domain. On the contrary, the corresponding adaptation variant that is trained without a curriculum, i.e. in one stage with 60k SGD iterations (equal to the total iterations for the two stages of MGCDA) and data of varying darkness presented in mixed order, cannot profit from data of intermediate darkness. Applying our guided segmentation refinement in its original, cross bilateral filtering variant that we have used in GCMA significantly improves upon the basic curriculum approach. Finally, the upgraded, depth-based warping variant of our guided refinement that we use in MGCDA brings an additional 2.2% benefit over the original variant, as it corrects even more errors in the pseudo-labels of the real images, which helps compute more reliable gradients from the corrected loss during the subsequent training. Note that only with the fully fledged MGCDA with our novel geometrically guided refinement are we able to adequately leverage real data and significantly improve upon the powerful synthetic-only adaptation baseline with CycleGAN from <ref type="table" target="#tab_6">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Map Guidance at Test Time</head><p>In the exposition of our MGCDA method as well as in the preceding experiments, we have considered map guidance for segmentation refinement only in the training stage. However, guidance from maps is fully relevant at test time too, when e.g. the semantic segmentation model is deployed on an autonomous vehicle. To investigate this scenario, we consider two models, corresponding to MGCDA and DMAda <ref type="bibr" target="#b10">[11]</ref>, and compare in <ref type="table">Table 8</ref> the performance on Dark Zurich-test using 1) the original predictions of the models, and 2) the refined predictions that are obtained after guided refinement using the predictions of RefineNet <ref type="bibr" target="#b3">[4]</ref> on the corresponding daytime images. The performance of both models is boosted significantly with the use of map guidance for refining the initial predictions, showing that our proposed geometrically guided segmentation refinement is applicable to and beneficial for more general semantic segmentation settings beyond our MGCDA framework. A visual comparison for map guidance at test time with MGCDA is included in <ref type="figure" target="#fig_5">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparison with Preprocessing Baselines</head><p>For the sake of completeness, we consider the straightforward alternative to our approach of applying a preprocessing step to the images at test time and then using a pre-trained daytime segmentation model on the processed images to get the predictions. Such preprocessing can be accomplished via different approaches. We select the following representative methods for our experiment: ZeroDCE <ref type="bibr" target="#b29">[30]</ref> and MBLLEN <ref type="bibr" target="#b26">[27]</ref> for low-light image enhancement, CLAHE [79] for histogram equalization, and CycleGAN <ref type="bibr" target="#b67">[68]</ref> for image translation from nighttime to daytime.</p><p>In <ref type="table" target="#tab_8">Table 9</ref>, we compare the performance of the daytime Re-fineNet model on Dark Zurich-test without preprocessing versus preprocessing with each of the aforementioned methods. A visual comparison is provided in <ref type="figure" target="#fig_5">Fig. 11</ref>. Although preprocessing can generally enhance the contrast and visibility in nighttime images, it does not help improve the performance of the daytime model. The reason is the domain mismatch between the preprocessed nighttime images at test time and the daytime images presented to the segmentation model at training time. The finding that contrary to expectation, mere test-time preprocessing does not benefit segmentation at nighttime is in congruence with the results of <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, where dehazing preprocessing is also found  <ref type="table" target="#tab_6">Table 6</ref> adapted on synthetic nighttime images without enhancement, as additional artifacts are introduced to the images, which makes the image domain harder. This comparison reveals the limitations of preprocessing in serving adaptation of segmentation to nighttime and stresses the need for a more sophisticated learning scheme such as MGCDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Comparisons with UIoU</head><p>In <ref type="figure" target="#fig_0">Fig. 12</ref>  <ref type="figure" target="#fig_0">Fig. 12</ref>. Uncertainty-aware evaluation of RefineNet <ref type="bibr" target="#b3">[4]</ref>, DMAda <ref type="bibr" target="#b10">[11]</ref>, GCMA and MGCDA on Dark Zurich-test. We evaluate mean UIoU across the entire range [1/C, 1] of confidence threshold θ. For each method, the point at which mean UIoU is maximized is marked black and labeled with this maximum mean UIoU value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we have introduced MGCDA, a method to gradually adapt semantic segmentation models from daytime to nighttime with stylized data and unlabeled real data of increasing darkness, as well as UIoU, a novel evaluation metric for semantic segmentation designed for images with indiscernible content. We have also presented Dark Zurich, a large-scale dataset of real scenes captured at multiple times of day with cross-time-of-day correspondences, and annotated 201 nighttime scenes of it with a new protocol which enables our evaluation. Detailed evaluation with standard IoU on real nighttime sets demonstrates the merit of MGCDA, which substantially improves upon competing stateof-the-art methods. Finally, evaluation on our benchmark with UIoU shows that invalidating predictions is useful when the input includes ambiguous content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOF OF THEOREM 1</head><p>Proof. For brevity in the proof, we drop the class superscript (c) which is used in the statement of the theorem.</p><p>Firstly, we draw an association between pixel sets related to the standard IoU = UIoU(1/C) and their counterparts for UIoU defined in (10)- <ref type="bibr" target="#b13">(14)</ref>. In particular, the following holds true:</p><formula xml:id="formula_20">|TP(1/C)| + |FN(1/C)| = |TP(θ)| + |FN(θ)| + |TI(θ)| + |FI(θ)|, ∀θ ∈ [1/C, 1].<label>(16)</label></formula><p>The first assumption of Th. 1 implies that FI(θ 1 ) = ∅, because ∀θ &lt; θ 2 (including θ 1 ) there exists no false invalid pixel for the examined class. Thus, applying (16) for θ = θ 1 leads to</p><formula xml:id="formula_21">|TP(1/C)| = |TP(θ 1 )|+|TI(θ 1 )|+|FN(θ 1 )|−|FN(1/C)|. (17)</formula><p>Secondly, we plug the proposition of the first assumption of the theorem into the proposition of the second assumption to obtain</p><formula xml:id="formula_22">(FN(1/C) ∪ FP(1/C)) \ (FN(θ 1 ) ∪ FP(θ 1 )) = ∅.<label>(18)</label></formula><p>We further elaborate on (18) by observing that FN(1/C) ∩ FP(1/C) = ∅, FN(θ 1 ) ⊆ FN(1/C) and FP(θ 1 ) ⊆ FP(1/C) to arrive at</p><formula xml:id="formula_23">(|FN(1/C)| − |FN(θ 1 )|) + (|FP(1/C)| − |FP(θ 1 )|) &gt; 0. (19)</formula><p>Both terms on the left-hand side of <ref type="bibr" target="#b18">(19)</ref> are nonnegative based on our previous observations, while at the same time <ref type="bibr" target="#b18">(19)</ref> implies that at least one of the two is strictly positive. To complete the proof, we distinguish between the two corresponding cases.</p><p>In the first case, the first term in <ref type="bibr" target="#b18">(19)</ref> is strictly positive, so (17) implies</p><formula xml:id="formula_24">|TP(1/C)| &lt; |TP(θ 1 )| + |TI(θ 1 )|.<label>(20)</label></formula><p>We establish the inequality we are after by writing</p><formula xml:id="formula_25">IoU = = |TP(1/C)| |TP(1/C)| + |FN(1/C)| + |FP(1/C)| = |TP(1/C)| |TP(θ 1 )| + |FN(θ 1 )| + |TI(θ 1 )| + |FI(θ 1 )| + |FP(1/C)| ≤ |TP(1/C)| |TP(θ 1 )| + |TI(θ 1 )| + |FP(θ 1 )| + |FN(θ 1 )| + |FI(θ 1 )| &lt; |TP(θ 1 )| + |TI(θ 1 )| |TP(θ 1 )| + |TI(θ 1 )| + |FP(θ 1 )| + |FN(θ 1 )| + |FI(θ 1 )| = UIoU(θ 1 ),<label>(21)</label></formula><p>where we have used the definition of IoU in the second line, <ref type="bibr" target="#b15">(16)</ref> in the third line, FP(θ 1 ) ⊆ FP(1/C) in the fourth line, <ref type="bibr" target="#b19">(20)</ref> in the fifth line, and the definition of UIoU that has been introduced in (15) in the last line.</p><p>In the second case, the second term in <ref type="bibr" target="#b18">(19)</ref> is strictly positive, which implies that</p><formula xml:id="formula_26">|FP(1/C)| &gt; |FP(θ 1 )|.<label>(22)</label></formula><p>Besides, applying the nonnegativity of the first term in <ref type="bibr" target="#b18">(19)</ref> to <ref type="formula" target="#formula_4">(17)</ref> leads to |TP(1/C)| ≤ |TP(θ 1 )| + |TI(θ 1 )|.</p><p>Similarly to the first case, we establish the inequality we are after by writing IoU = = |TP(1/C)| |TP(θ 1 )| + |TI(θ 1 )| + |FP(1/C)| + |FN(θ 1 )| + |FI(θ 1 )| &lt; |TP(1/C)| |TP(θ 1 )| + |TI(θ 1 )| + |FP(θ 1 )| + |FN(θ 1 )| + |FI(θ 1 )| ≤ |TP(θ 1 )| + |TI(θ 1 )| |TP(θ 1 )| + |TI(θ 1 )| + |FP(θ 1 )| + |FN(θ 1 )| + |FI(θ 1 )| = UIoU(θ 1 ),</p><p>where we have used the definition of IoU as well as <ref type="bibr" target="#b15">(16)</ref> in the second line, <ref type="bibr" target="#b21">(22)</ref> in the third line, <ref type="bibr" target="#b22">(23)</ref> in the fourth line, and the definition of UIoU in the last line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B ADDITIONAL QUALITATIVE RESULTS</head><p>In <ref type="figure" target="#fig_1">Fig. 13</ref>, we compare our MGCDA approach against our original GCMA approach, AdaptSegNet <ref type="bibr" target="#b39">[40]</ref> and DMAda <ref type="bibr" target="#b10">[11]</ref> on additional images from Dark Zurich-test, further demonstrating the superiority of MGCDA. For these images, we also present our annotations for invalid masks and semantic labels, which show that a significant portion of ground-truth invalid regions is indeed assigned a reliable semantic label through our annotation protocol and can thus be included in the evaluation. <ref type="figure" target="#fig_1">Fig. 13</ref>. Examples of our annotations and qualitative semantic segmentation results on Dark Zurich-test. From top to bottom row: nighttime image, invalid mask annotation overlaid on the image (valid pixels are colored green), semantic annotation, AdaptSegNet <ref type="bibr" target="#b39">[40]</ref>, DMAda <ref type="bibr" target="#b10">[11]</ref>, GCMA (ours), and MGCDA (ours).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C PARAMETER SELECTION FOR PREDICTION FUSION</head><p>In <ref type="figure" target="#fig_2">Fig. 14,</ref> we illustrate the effect of parameter α h of our confidence-adaptive prediction fusion presented in Sec. 3.2.3. <ref type="figure" target="#fig_2">Fig. 14(f)-(h)</ref> show the refined predictions after fusion for increasing values of α h . Setting α h to a low value in <ref type="figure" target="#fig_2">Fig. 14(f)</ref> prevents the refinement of some erroneous parts of the initial prediction for the dark image, such as the region in the central part of the image which is incorrectly labeled as vegetation. Increasing α h to 0.6 in <ref type="figure" target="#fig_2">Fig. 14(g)</ref> corrects a large part of this region to sky, as the aligned daytime prediction ( <ref type="figure" target="#fig_2">Fig. 14(e)</ref>), which contains the correct label sky in this region, is assigned a higher weight in the prediction fusion. However, when α h is further increased to 1 in <ref type="figure" target="#fig_2">Fig. 14(h)</ref>, the initially correctly segmented traffic sign and pole in the right part of the image have their labels aggravated due to the overly high weight assigned to the daytime prediction, which is imperfectly aligned to the dark image for these particular objects (cf. <ref type="figure" target="#fig_2">Fig. 14(e)</ref>). This demonstrates the trade-off in the selection of α h , which requires striking a balance between exploiting the daytime prediction, as it is performed in an easier domain, and not relying on it too much, due to its imperfect alignment to the dark image. We expect that improving the prediction alignment, primarily via increasing the accuracy of the depth map for the daytime view, will enable a better exploitation of the daytime prediction in our geometrically guided segmentation refinement by placing a higher weight on it in the prediction fusion step. [79] S. M. Pizer, E. P. Amburn, J. D. Austin, R. Cromartie, A. Geselowitz, T. Greer, B. ter Haar Romeny, J. B. Zimmerman, and K. Zuiderveld, "Adaptive histogram equalization and its variations," Luc Van Gool is a full professor for Computer Vision at ETH Zurich and the KU Leuven. He leads research and teaches at both places. He has authored over 300 papers. He has been a program committee member of several, major computer vision conferences (e.g. Program Chair ICCV'05, Beijing, General Chair of ICCV'11, Barcelona, and of ECCV'14, Zurich). His main interests include 3D reconstruction and modeling, object recognition, and autonomous driving. He received several Best Paper awards (e.g. David Marr Prize '98, Best Paper CVPR'07). He received the Koenderink Award in 2016 and the 'Distinguished Researcher' nomination by the IEEE Computer Society in 2017. In 2015 he also received the 5-yearly Excellence Prize by the Flemish Fund for Scientific Research. He was the holder of an ERC Advanced Grant (VarCity). Currently, he leads computer vision research for autonomous driving in the context of the Toyota TRACE labs in Leuven and at ETH, and has an extensive collaboration with Huawei on the topic of image and video enhancement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Sample images from the training sets used in MGCDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Establishment of dense pixel-level correspondences Illustration of two components of MGCDA involving the identification of cross-time-of-day correspondences at image level and pixel level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of guided segmentation refinement using prediction alignment with cross bilateral filter versus depth-based warping on an example pair of corresponding images from Dark Zurich. Best viewed on a screen with zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Input image I (b) Auxiliary image I (c) GT invalid mask J (d) GT semantic labeling H Example input images from Dark Zurich-test and output annotations with our protocol. Valid pixels in J are marked green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>TI</head><label></label><figDesc>= {p : H(p) = c andĤ(p) = invalid and J(p) = 1}, (13) FI = {p : H(p) = c andĤ(p) = invalid and J(p) = 0}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 1 .</head><label>1</label><figDesc>Assume that there exist θ 1 , θ 2 such that θ 1 &lt; θ 2 , ∀p :J(p) = 1 ⇒ SH (p) (p) ≤ θ 1 and J(p) = 0 ⇒ SH (p) (p) ≥ θ 2 .If we additionally assume that ∃p ∈ FN (c) (1/C) ∪ FP (c) (1/C) : J(p) = 1, then IoU (c) &lt; UIoU (c) (θ 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Number of annotated pixels per class in Dark Zurich.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative semantic segmentation results on Dark Zurich-test. "AdaptSegNet" adapts from Cityscapes to Dark Zurich-night.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>8 22.0 20.0 41.2 40.5 41.6 64.8 31.0 32.1 53.5 47.5 75.5 39.2 0.0 49.6 30.7 21.0 42.0 Ours: MGCDA 80.3 49.3 66.2 7.8 11.0 41.4 38.9 39.0 64.1 18.0 55.8 52.1 53.5 74.7 66.0 0.0 37.5 29.1 22.7 42.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative semantic segmentation results on BDD100K-night. "BDL" adapts from Cityscapes to Dark Zurich-night.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Comparison of CycleGAN configurations for generation of synthetic stylized data from Cityscapes and Dark Zurich. From left to right: input Cityscapes image, stylized nighttime image with training on 256 × 256 crops, stylized nighttime image with training on full 360 × 720 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Qualitative comparison on Dark Zurich-test of MGCDA without and with map guidance at test time. TG stands for test-time guidance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>( a )= 1 Fig. 14 .</head><label>a114</label><figDesc>Dark image I z (b) Initial prediction S z for I z (c) Daytime image I 1 (d) Initial prediction S 1 for I 1 (e) Aligned predictionS 1 for I 1 with depth-based warping (f) Refined predictionŜ z for I z with α h = 0.3 (g) Refined predictionŜ z for I z with α h = 0.6(h) Refined predictionŜ z for I z with α h Comparison of confidence-adaptive prediction fusion results for different values of α h on an example image pair from Dark Zurich. The other two parameters of prediction fusion are fixed to their default values α l = 0.3 and η = 0.2 across all compared cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>The training sets used in MGCDA. I indicates an image and Y its label map;Ī is a synthetic image andŶ a pseudo-label map. See the text for details.</figDesc><table><row><cell>Labeled</cell><cell></cell><cell>Unlabeled</cell></row><row><cell>Real</cell><cell>Synthetic</cell><cell>Real</cell></row><row><cell>1. Daytime</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 Comparison</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">of Dark Zurich against related datasets with nighttime</cell></row><row><cell cols="5">semantic annotations. "Night annot.": annotated nighttime images,</cell></row><row><cell cols="4">"Invalid": can invalid regions get legitimate labels?</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">Night annot. Classes Reliable GT Fine GT Invalid</cell></row><row><cell>WildDash [12]</cell><cell>13</cell><cell>19</cell><cell></cell><cell>×</cell></row><row><cell>Raincouver [74]</cell><cell>95</cell><cell>3</cell><cell>×</cell><cell>×</cell></row><row><cell>BDD100K [75]</cell><cell>345</cell><cell>19</cell><cell>×</cell><cell>×</cell></row><row><cell>Nighttime Driving [11]</cell><cell>50</cell><cell>19</cell><cell>×</cell><cell>×</cell></row><row><cell>Dark Zurich</cell><cell>201</cell><cell>19</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Performance comparison of our method with state-of-the-art approaches and daytime-trained baselines on our Dark Zurich-test dataset. Cityscapes→DZ-night denotes adaptation from Cityscapes to Dark Zurich-night. 23.2 46.8 20.8 12.6 29.8 30.4 26.9 43.1 14.3 0.3 36.9 49.7 63.6 6.8 0.2 24.0 33.6 9.3 28.5 DeepLab-v2 [76] 79.0 21.8 53.0 13.3 11.2 22.5 20.2 22.1 43.5 10.4 18.0 37.4 33.8 64.1 6.4 0.0 52.3 30.4 7.4 28.8</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sidew.</cell><cell>build.</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veget.</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorc.</cell><cell>bicycle</cell><cell>mIoU</cell></row><row><cell cols="21">RefineNet [4] 68.8 AdaptSegNet-Cityscapes→DZ-night [40] 86.1 44.2 55.1 22.2 4.8 21.1 5.6 16.7 37.2 8.4 1.2 35.9 26.7 68.2 45.1 0.0 50.1 33.9 15.6 30.4</cell></row><row><cell>ADVENT-Cityscapes→DZ-night [47]</cell><cell cols="20">85.8 37.9 55.5 27.7 14.5 23.1 14.0 21.1 32.1 8.7 2.0 39.9 16.6 64.0 13.8 0.0 58.8 28.5 20.7 29.7</cell></row><row><cell>BDL-Cityscapes→DZ-night [46]</cell><cell cols="20">85.3 41.1 61.9 32.7 17.4 20.6 11.4 21.3 29.4 8.9 1.1 37.4 22.1 63.2 28.2 0.0 47.7 39.4 15.7 30.8</cell></row><row><cell>DMAda [11]</cell><cell cols="20">75.5 29.1 48.6 21.3 14.3 34.3 36.8 29.9 49.4 13.8 0.4 43.3 50.2 69.4 18.4 0.0 27.6 34.9 11.9 32.1</cell></row><row><cell>Ours: GCMA [13]</cell><cell cols="3">81.7 46.9 58.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>Comparison on Dark Zurich-test of CycleGAN configurations for generation of synthetic stylized data to adapt to nighttime. CycleGAN-crops stands for the default training of CycleGAN with 256 × 256 crops, whereas CycleGAN-full stands for training of CycleGAN with full 360 × 720 images.</figDesc><table><row><cell>Method</cell><cell>mIoU (%)</cell></row><row><cell>Daytime baseline: RefineNet [4]</cell><cell>28.5</cell></row><row><cell>Previous state of the art: DMAda [11]</cell><cell>32.1</cell></row><row><cell>CycleGAN-crops adaptation</cell><cell>37.1</cell></row><row><cell>CycleGAN-full adaptation</cell><cell>40.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>Ablation study of the components of MGCDA on Dark Zurich-test, reporting mIoU (%). CBF stands for cross bilateral filtering and DBW for depth-based warping. W/o twilight means not using data from the intermediate domain. W/o curriculum means single-stage adaptation using all available data. The third and fourth rows are with reference to the respective previous row. The fifth and sixth rows are both with reference to the fourth row.</figDesc><table><row><cell cols="2">Daytime-trained baseline: RefineNet [4]</cell><cell>28.5</cell></row><row><cell cols="3">Adaptation to nighttime (w/o twilight/curriculum/guided refinement) 33.6</cell></row><row><cell cols="2">+twilight (w/o curriculum/guided refinement)</cell><cell>34.0</cell></row><row><cell cols="2">+curriculum (MGCDA w/o guided refinement)</cell><cell>38.2</cell></row><row><cell>+guided refinement-CBF</cell><cell></cell><cell>40.3</cell></row><row><cell cols="2">+guided refinement-DBW (MGCDA)</cell><cell>42.5</cell></row><row><cell></cell><cell>TABLE 8</cell><cell></cell></row><row><cell cols="3">Comparison on Dark Zurich-test focusing on the usage of map</cell></row><row><cell cols="3">guidance at test time and reporting mIoU (%).</cell></row><row><cell>Method</cell><cell cols="2">w/o test guidance w/ test guidance</cell></row><row><cell>DMAda [11]</cell><cell>32.1</cell><cell>34.8</cell></row><row><cell>Ours: MGCDA</cell><cell>42.5</cell><cell>44.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9</head><label>9</label><figDesc>Comparison on Dark Zurich-test of different preprocessing baselines, using the daytime RefineNet [4] model for predictions and reporting mIoU (%). No PP stands for no preprocessing. to help segmentation in fog. To close the aforementioned domain gap, we additionally experiment with enhancement both at training and test time with the state-of-the-art ZeroDCE, adapting the daytime model on ZeroDCE-enhanced versions of synthetic nighttime Cityscapes images generated with CycleGAN. The adapted model achieves 38.4% mIoU, significantly outperforming the daytime model but falling short of the model from</figDesc><table><row><cell cols="5">No PP CycleGAN [68] CLAHE [79] MBLLEN [27] ZeroDCE [30]</cell></row><row><cell>28.5</cell><cell>7.2</cell><cell>23.0</cell><cell>22.4</cell><cell>17.9</cell></row><row><cell cols="5">Fig. 11. Qualitative example from Dark Zurich-test of effect of prepro-</cell></row><row><cell cols="5">cessing on semantic predictions. From first to fifth column, left to right:</cell></row><row><cell cols="5">image (top row) and prediction of daytime baseline (bottom row) for</cell></row><row><cell cols="5">no preprocessing, CycleGAN, CLAHE, MBLLEN, and ZeroDCE. Sixth</cell></row><row><cell cols="5">column: semantic GT (top row) and prediction of MGCDA (bottom row).</cell></row></table><note>not</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>, we use our novel UIoU metric to evaluate MGCDA and GCMA against DMAda and our baseline RefineNet model on Dark Zurich-test for varying confidence threshold θ and plot the resulting mean UIoU(θ) curves. Note that standard mean IoU can be read out from the leftmost point of each curve. First, our expectation based on Th. 1 is confirmed for all methods, i.e. maximum UIoU values over the range of θ are larger than IoU by ca. 2-3%. This implies that on Dark Zurich-test, these models generally have lower confidence on invalid regions than valid ones, although they do not explicitly model uncertainty, as there is no distinction between valid and invalid regions during training. Modeling confidence explicitly along the lines of<ref type="bibr" target="#b55">[56]</ref>,<ref type="bibr" target="#b56">[57]</ref> could further increase UIoU and is an interesting direction for future work. Second, the comparative performance of the methods is the same across all values of θ, except the pair of MGCDA and GCMA. MGCDA slightly outperforms GCMA for low values of θ, but for high values GCMA achieves significantly higher UIoU, which implies that MGCDA places relatively low confidence on valid regions, which prevents its UIoU from increasing further as θ increases. In any case, MGCDA and GCMA substantially outperform the other two methods. Overall, UIoU is generally consistent with standard IoU and is a suitable substitute of the latter in adverse settings where selective invalidation of the predictions is relevant.</figDesc><table><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mean UIoU 0.441</cell><cell></cell><cell>0.458</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.355</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.318</cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell cols="2">RefineNet DMAda</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>GCMA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MGCDA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Confidence threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Computer Vision, Graphics, and Image Processing, vol. 39, no. 3, pp. 355-368, 1987. 12 Christos Sakaridis is a PhD candidate at Computer Vision Lab, ETH Zurich, since 2016. His broad research fields are Computer Vision and Machine Learning, while his current focus is on visual recognition in adverse conditions, domain adaptation, and segmentation. Prior to joining Computer Vision Lab, he received the MSc degree in Computer Science from ETH Zurich in 2016 and the Diploma degree in Electrical and Computer Engineering from National Technical University of Athens in 2014. Dai is a senior scientist working with the Computer Vision Lab at ETH Zurich. In 2016, he obtained his PhD in Computer Vision at ETH Zurich. Since then he is the Team Leader of the TRACE-Zurich team, working on Autonomous Driving in collaboration with Toyota. His research interests lie in autonomous driving, robust perception in adverse weather and lighting conditions, automotive sensors, and learning under limited supervision. He has organized a CVPR Workshop series ('19, '20) on Vision for All Seasons: Bad Weather and Nighttime, and has organized an ICCV'19 workshop on Autonomous Driving. He is the lead guest editor for the IJCV special issue Vision for All Seasons, and an area chair for WACV'20 and CVPR'21.</figDesc><table><row><cell>Dengxin</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">. https://www.trace.ethz.ch/publications/2019/GCMA UIoU 2. https://competitions.codalab.org/competitions/23553 3. https://github.com/sakaridis/MGCDA</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is funded by Toyota Motor Europe via the research project TRACE-Zürich. We thank Simon Hecker for his advice on decoding GoPro GPS data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision and the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LAP-Net: Level-aware progressive network for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Mapillary Vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How good is my test data? Introducing safety analysis for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Humenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Herzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="109" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">WildDash -creating hazard-aware benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Fernandez</forename><surname>Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guided curriculum model adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pedestrian detection and tracking with night vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fujimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="71" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time pedestrian detection and tracking at nighttime for driver-assistance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networkbased human detection in nighttime images using visible light camera sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Night-time pedestrian detection by visual-infrared video fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Congress on Intelligent Control and Automation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaist multi-spectral day/night data set for autonomous and assisted driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="934" to="948" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayes saliency-based object proposal generator for nighttime traffic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="814" to="825" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Looking at vehicles in the night: Detection and dynamics of rear lights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Road detection based on illuminant invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="193" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised image transformation for outdoor semantic labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">AdapNet: Adaptive semantic segmentation in adverse environmental conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">1 year, 1000 km: The Oxford RobotCar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Benchmarking 6DOF outdoor visual localization in changing conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A*3D dataset: Towards autonomous driving in challenging environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sevestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Pahwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference in Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MBLLEN: Low-light image/video enhancement using CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Underexposed photo enhancement using deep illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Zeroreference deep curve estimation for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Benchmarking image sensors under adverse weather conditions for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bijelic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Addressing appearance change in outdoor robotics with adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Continuous manifold based adaptation for evolving visual domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Curriculum model adaptation with synthetic and real data for semantic foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic understanding of foggy scenes with purely synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Zaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd IEEE International Conference on Intelligent Transportation Systems</title>
		<meeting>the 22nd IEEE International Conference on Intelligent Transportation Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ROAD: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Incremental adversarial domain adaptation for continually changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Penalizing top performers: Conservative loss for semantic segmentation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DCAN: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ADVENT: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Content-consistent matching for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pixellevel cycle association: A new perspective for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno>abs/2011.00147, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Vijaya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Open compound domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A cross-season correspondence dataset for robust semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in Bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Simultaneous semantic segmentation and outlier detection in presence of domain shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krešo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oršić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Šegvić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Robust vehicle localization in urban environments using probabilistic maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Video based localization for bertha</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lategahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Knöppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haueis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Drivable space characterization using automotive lidar and georeferenced map information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S A</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Drevelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dherbomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cherfaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bonnifait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Road recognition from a single image using prior information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Autonomous robot navigation based on openstreetmap geodata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hentschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">End-to-end learning of driving models with surround-view cameras and route planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Map-aided evidential grids for driving scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kurdej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cherfaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bonnifait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Transportation Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="41" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hdnet: Exploiting hd maps for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Action sequence predictions of vehicles in urban environments using map and social context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Zaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liniger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14251</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A fast approximation of the bilateral filter using a signal processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="52" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Monocular neural image based rendering with continuous view control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">SURF: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The Raincouver scene parsing benchmark for self-driving in adverse weather and at night</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">BDD100K: a diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1805.04687</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Joint bilateral upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2007 Papers</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
