<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Group-Free 3D Object Detection via Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
							<email>liuze@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Yue Cao</surname></persName>
							<email>yuecao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
							<email>xtong@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Group-Free 3D Object Detection via Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/zeliu98/ Group-Free-3D</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, directly detecting 3D objects from 3D point clouds has received increasing attention. To extract object representation from an irregular point cloud, existing methods usually take a point grouping step to assign the points to an object candidate so that a PointNet-like network could be used to derive object features from the grouped points. However, the inaccurate point assignments caused by the hand-crafted grouping scheme decrease the performance of 3D object detection.</p><p>In this paper, we present a simple yet effective method for directly detecting 3D objects from the 3D point cloud. Instead of grouping local points to each object candidate, our method computes the feature of an object from all the points in the point cloud with the help of an attention mechanism in the Transformers <ref type="bibr" target="#b42">[42]</ref>, where the contribution of each point is automatically learned in the network training. With an improved attention stacking scheme, our method fuses object features in different stages and generates more accurate object detection results. With few bells and whistles, the proposed method achieves state-of-the-art 3D object detection performance on two widely used benchmarks, Scan-Net V2 and SUN RGB-D. The code and models are publicly available at https://github.com] have been proposed for various point cloud based learning tasks.</p><p>[13] provides a good taxonomy and review of all these architectures, and discussing all of them is beyond the scope of this paper. Our method can take any point cloud architecture as the backbone network for computing the point features. We adopt PointNet++ [30]  used in previous methods <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b51">51]</ref> in our implementation for a fair comparison.</p><p>Attention Mechanism/Transformer in NLP and 2D Image Recognition The attention-based Transformer is the dominant network architecture for the learning tasks in the field of NLP <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b22">21]</ref>. They have been also applied in the field of 2D image recognition <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b46">46]</ref> as a strong competitor to the dominant grid/dense modeling tools such as ConvNets and RoI-Align. The most related works in 2D</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection on point cloud simultaneously localizes and recognizes 3D objects from a 3D point set. As a fundamental technique for 3D scene understanding, it plays an important role in many applications such as autonomous driving, robotics manipulation, and augmented reality.</p><p>Different from 2D object detection that works on 2D regular images, 3D object detection takes irregular and sparse * This work is done when Ze Liu is an intern at MSRA. † Contact person</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoI-Pooling</head><p>Voting</p><p>Group-Free Scene <ref type="figure">Figure 1</ref>. With the heuristic point grouping step, all points in blue box of RoI-Pooling or blue ball of Voting are assigned and aggregated to derive the object features, resulting in wrong assignments. Our group-free based approach automatically learn the contribution of all points to each object, which has ability to alleviate the drawbacks of the hand-crafted grouping. point cloud as input, which makes it difficult to directly apply techniques used for 2D object detection techniques. Recent studies <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b51">51]</ref> infer the object location and extract object features directly from the irregular input point cloud for object detection. In these methods, a point grouping step is required to assign a group of points to each object candidate, and then computes object features from assigned groups of points. For this purpose, different grouping strategies have been investigated. Frustum-PointNet <ref type="bibr" target="#b27">[27]</ref> applies the Frustum envelop of a 2D proposal box for point grouping. Point R-CNN <ref type="bibr" target="#b35">[35]</ref> groups points within the 3D box proposals to objects. VoteNet <ref type="bibr" target="#b26">[26]</ref> determines the group as the points which vote to the same (or spatially-close) center point. Although these hand-crafted grouping schemes facilitate 3D object detection, the complexity and diversity of objects in real scene may lead to wrong point assignments (shown in <ref type="figure">Figure.</ref> 1) and degrade the 3D object detection performance.</p><p>In this paper, we propose a simple yet effective technique for detecting 3D objects from point clouds without the handcrafted grouping step. The key idea of our approach is to take all points in the point cloud for computing features for each object candidate, in which the contribution of each point is determined by an automatically learned attention module. Based on this idea, we adapt the Transformer to fit for 3D object detection, which could simultaneously model the object-object and object-pixel relationships, and extract the object features without handcrafted grouping.</p><p>To further release the power of the transformer architecture, we improve it in two aspects. First, we propose to iteratively refine the prediction of objects by updating the spatial encoding of objects in different stages, while the original application of Transformers adopt the fixed spatial encoding. Second, we use the ensemble of detection results predicted at all stages during inference, instead of only using the results in the last stage as the final results. These two modifications efficiently improve the performance of 3D object detection with few computational overheads.</p><p>We validate our method with both ScanNet V2 <ref type="bibr" target="#b6">[6]</ref> and SUN RGB-D <ref type="bibr" target="#b52">[52]</ref> benchmarks. Results show that our method is effective and robust to the quality of initial object candidates, where even a simple farthest point sampling approach has been able to produce strong results on ScanNet V2 and SUN RGB-D benchmarks. For the SUN RGB-D dataset, our method with the ensemble scheme results in significant performance improvement (+3.8 mAP@0.25). With few bells and whistles, the proposed approach achieved state-of-the-art performance on both benchmarks.</p><p>We believe that our method also advocates a strong potential by using the attention mechanism or Transformers for point cloud modeling, as it naturally addresses the intrinsic irregular and sparse distribution problems encountered by 3D point clouds. This is contrary to 2D image modeling, where such modeling tools mainly act as a challenger or a complementary component to the mature grid modeling tools such as ConvNets variants <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b46">46]</ref> and RoI Align <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Grid Projection/Voxelization based Detection Early 3D object detection approaches project point cloud to 2D grids or 3D voxels so that the advanced convolutional networks can be directly applied. A set of methods <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b50">50]</ref> project point cloud to the bird's view and then employ 2D ConvNets for learning features and generate 3D boxes. These methods are mainly applied for the outdoor scenes in autonomous driving where objects are distributed on a horizontal plane so that their projections on the bird-view are occlusion-free. Note these approaches also need to address the irregular and sparse distribution issues of the 2D point projections, usually by pixelization. Other methods <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b48">48]</ref> project point clouds into frontal views and then apply 2D ConvNets for object detection. Voxel-based methods <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b53">53]</ref> convert points into voxels and employ 3D Con-vNets to generate features for 3D box generation. All these projection/voxelization based methods suffer from quantization errors. The voxel-based methods also suffer from the large memory and computational cost of 3D convolutions.</p><p>Point based Detection Recent methods directly process point clouds for 3D object detection. A core task of these methods is to compute object features from the irregularly and sparsely distributed points. All existing methods first assign a group of points to each object candidate and then compute object features from each point group. Frustum-PointNet <ref type="bibr" target="#b27">[27]</ref> groups points by the 3D Frustum envelope of a 2D box detected using an RGB object detector, and applies a PointNet on the grouped points to extract object features for 3D box prediction. Point R-CNN <ref type="bibr" target="#b35">[35]</ref> directly computes 3D box proposals, where the points within this 3D box are used for object feature extraction. PV-RCNN <ref type="bibr" target="#b34">[34]</ref> leverages the voxel representation to complement the pointbased representation in Point R-CNN <ref type="bibr" target="#b35">[35]</ref> for 3D object detection and achieves better performance.</p><p>VoteNet <ref type="bibr" target="#b26">[26]</ref> groups points according to their voted centers and extract object features from grouped points by the PointNet. Some follow-up works further improve the point group generation procedure <ref type="bibr" target="#b51">[51]</ref> or the object box localization and recognition procedure <ref type="bibr" target="#b3">[3]</ref>.</p><p>Our method is also a point-based detection approach. Unlike existing point-based approaches, our method involves all the points for computing the features of each object candidate by an attention module. We also stack the attention modules to iteratively refine the detection results while maintaining the simplicity of our method.  <ref type="figure">Figure 2</ref>. This figure illustrates the simple architecture of our approach, including three major components: a backbone network to extract feature representations for each point in the point cloud, a sampling method to generate initial object candidates, and stacked attention modules to extract and refine object representations from all points. image recognition to this paper are those who apply the attention mechanism or Transformer architectures into 2D object detection <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b2">2]</ref>.</p><p>Among these approaches, our method is most similar to <ref type="bibr" target="#b2">[2]</ref>, which also applies a Transformer architecture for 2D object detection. However, we found that directly applying this method to point clouds leads to significantly lower performance than our approach in 3D object detection task. On the one hand, this is caused by the new technologies we proposed, and on the other hand, it probably because our method better integrated the advantage of traditional 3D detection framework. We discussed these factors in Sec. 4.6.</p><p>Our approach improves the Transformer models to better adapt the 3D object detection task, including the update of object query locations in the multi-stage iterative box prediction, and an ensemble of detection results of stages. Although the attention mechanisms still have a certain performance gap compared to the dominant convolution-based methods in other tasks, we found that this architecture may well address the point grouping issue for object detection on point clouds. As a result, we advocate a strong potential of this architecture for modeling irregular 3D point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In 3D object detection on point clouds, we are given a set of N points S ∈ R N ×3 and the goal is to produce a set of 3D (oriented) bounding boxes with categorization scores O S to cover all ground-truth objects. Our overall architecture is illustrated in <ref type="figure">Figure 2</ref>, involving three major components: a backbone network to extract feature representations for each point in point clouds, a sampling method to generate initial object candidates, and stacked attention modules to extract and refine object representations from all points.</p><p>Backbone Architecture While our framework can leverage any point cloud network to extract point features, we adopt PointNet++ <ref type="bibr" target="#b30">[30]</ref> as the backbone network for a fair comparison with the recent methods <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b51">51]</ref>.</p><p>The backbone network receives a point cloud of N points (i.e. 2048) as input. We follow the encoder-decoder architecture in <ref type="bibr" target="#b30">[30]</ref> to first down-sample the point cloud input into 8× resolution (i.e. 256 points) through four stages of set abstraction layers, and then up-sample it to the resolution of 2× (i.e. 1024 points) by feature propagation layers. The network will produce a C-channel vector representation for each point on the 2× resolution, denoted as</p><formula xml:id="formula_0">{z i } M i=1</formula><p>, which are then used in the initial object candidates sampling module and the stacked attention modules. In the following parts, we will first describe these two modules in detail, and then present the loss function and head design for this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Initial Object Candidate Sampling</head><p>While object detection on 2D images usually adopts data-independent anchor boxes as initial object candidates, it is generally intractable or impractical for 3D object detection to apply this simple top-down strategy, as the number of anchor boxes in 3D search space is too huge to handle. Instead, we follow recent practice <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b26">26]</ref> to sample initial object candidates directly from the points on a point cloud, by a bottom-up way.</p><p>We consider three simple strategies to sample initial object candidates from a point cloud:</p><p>• Farthest Point Sampling (FPS). The FPS approach has been widely adopted to generate a point cloud from a 3D shape or to down-sample the point clouds to a lower resolution. This method can be also employed to sample initial candidates from a point cloud. Firstly, a point is randomly sampled from the point cloud. Then the farthest point to the already-chosen point set is iteratively selected until the number of chosen points meets the candidate budget. Though it is simple, we show in experiments that this sampling approach along with our framework has been able to be comparable to the previous state-of-the-art 3D object detectors.</p><p>• k-Closest Points Sampling (KPS). In this approach, we classify each point on a point cloud to be a real object candidate or not. The label assignment in training follows this rule: a point is assigned positive if it is inside a ground-truth object box and it is one of the k-closest points to the object center. In inference, the initial candidates are selected according to the classification score of the point.</p><p>• KPS with non-maximal suppression (KPS-NMS). Built on the above KPS method, we introduce an additional non-maximal suppression (NMS) step, which iteratively removes spatially close object candidates, to improve the recall of sampled object candidates given a fixed number of objects, following the common practice in 2D object detection. In addition to the objectness scores, we predict also the object center that each point belongs to, where the NMS is conducted accordingly. Specifically, the candidates locating within a radius of the selected object center will be suppressed. The radius is set to 0.05 in our experiments.</p><p>In experiments, we will demonstrate that our framework has strong compatibility with the choice of these sampling approaches, mainly ascribed to the robust object feature extraction approach described in the next subsection (see <ref type="table">Table 3</ref>). We use the KPS approach by default, due to its better performance than the FPS approach, and the same effectiveness as the more complex KPS-NMS approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Iterative Object Feature Extraction and Box Prediction by Transformer Decoder</head><p>With the initial object candidates generated by a sampling approach, we adopt the Transformer as the decoder to leverage all points on a point cloud to compute the object feature of each candidate. The multi-head attention network is the foundation of Transformer, it has three input sets: query set, key set and value set. Usually, the key set and value set are different projections of the same set of elements. Given a query set {q i } and a common element set {p k } of key set and value set, the output feature of the multi-head attention of each query element is the aggregation of the values that weighted by the attention weights, formulated as:</p><formula xml:id="formula_1">Att(q i , {p k }) = H h=1 W h ( K k=1 A h i,k · V h p k ),<label>(1)</label></formula><formula xml:id="formula_2">A h i,k = exp[(Q h q i ) T (U h p k )] K k=1 exp[(Q h q i ) T (U h p k )]<label>(2)</label></formula><p>where h indexes over attention heads, A h is the attention weight, Q h , V h , U h , W h indicate the query projection weight, value projection weight, key projection weight, and output projection weight, respectively. While the standard Transformer predicts the sentence of a target language sequentially in an auto-regressive way, our Transformer computes object features and predicts 3D object boxes in parallel. The Transformer consists of several stacked multi-head self-attention and multi-head crossattention modules, as illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>.</p><p>Denote the input point features at stage l as {z</p><formula xml:id="formula_3">(l) i } M i=1</formula><p>and the object features at the same stage as {o attention module models interaction between object features, formulated as:</p><formula xml:id="formula_4">(l) i } K i=1 . A self- multi-head self-attention Q K &amp; V add &amp; norm multi-head</formula><formula xml:id="formula_5">Self-Att(o (l) i , {o (l) j }) = Att(o (l) i , {o (l) j }),<label>(3)</label></formula><p>A cross-attention module leverages point features to compute object features, formulated as:</p><formula xml:id="formula_6">Cross-Att(o (l) i , {z (l) j }) = Att(o (l) i , {z (l) j }),<label>(4)</label></formula><p>where the notations are similar to those in Eq. (3). After the object feature are updated through the self-attention module and cross attention module, a feed-forward network (FFN) is then applied to further transformed feature of each object.</p><p>There are a few differences compared to the original Transformer decoders, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Object Box Prediction and Spatial Encoding</head><p>The original Transformer adopts a fixed spatial encoding for all of the stacked attention modules, indicating the indices of each word. The application of Transformers to 2D object detection <ref type="bibr" target="#b2">[2]</ref> instantiate the spatial encoding (object prior) as a learnable weight. During inference, the spatial encoding is fixed and same for any images.</p><p>In this work, we propose to refine the spatial encodings of an object candidate stage by stage. Specifically, we predict the 3D box locations and categories at each decoder stage, and the predicted location of a box in one stage will be used to produce the refined spatial encoding of the same object, the refined spatial encoding vector is then added to the output feature of this decoder stage and fed into the next stage. The spatial encodings of an object and a point are computed by applying independent linear layers on the parameterization vector of a 3D box (x, y, z, l, h, w) and a point (x, y, z), respectively. In the experiments, we will show this approach can improve the mAP@0.25 and mAP@0.5 by 1.6 and 5.0 on the ScanNet V2 benchmark, compared to the approach without iterative refinement.</p><p>Ensemble from Multi-Stage Predictions Another difference is that we ensemble the predictions of different stages to produce final detection results, while previous methods usually adopt the output of the last stage as the final results. Concretely, the detection results of different stages are combined and they together go through an NMS (IoU threshold of 0.25) procedure to generate the final object detection results. We find this approach can significantly improve the performance of some benchmarks, e.g. +3.8 mAP@0.25 on the SUN RGB-D dataset. Also note the overhead of this ensembling approach is marginal, mainly ascribed to the multi-stage nature of the Transformer decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Heads and Loss Functions</head><p>Decoder Head We apply head networks on all decoder stages, with each mostly following the setting in <ref type="bibr" target="#b26">[26]</ref>. There are 5 prediction tasks: objectness prediction with a binary focal loss <ref type="bibr" target="#b21">[20]</ref> L obj , box classification with a cross entropy loss L cls , center offset prediction with a smooth-L1 loss L center off , size classification with a cross entropy loss L sz cls , and size offset prediction with a smooth-L1 loss L sz off . Also, all 5 prediction tasks are obtained by a shared 2-layer MLP and an independent linear layer.</p><p>The loss of l-th decoder stage is the combination of these 5 loss terms by weighted summation:</p><formula xml:id="formula_7">L (l) decoder = β 1 L (l) obj +β 2 L (l) cls +β 3 L (l) center off +β 4 L (l) sz cls +β 5 L (l) sz off ,<label>(5)</label></formula><p>where the balancing factors are set default as β 1 = 0.5, β 2 = 0.1, β 3 = 1.0, β 4 = 0.1 and β 5 = 0.1. The losses on all decoder stages are averaged to form the final loss:</p><formula xml:id="formula_8">L decoder = 1 L L l=1 L (l) decoder .<label>(6)</label></formula><p>Sampling Head The head designs and the loss functions of the sampling module are similar to those of the decoders. There are two differences: firstly, the box classification task is not involved; secondly, the objectness task follows the label assignment as described in Sec. 3.1. Our final loss is the sum of decoder and sampling heads:</p><formula xml:id="formula_9">L = L decoder + L sampler<label>(7)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Protocol</head><p>We validate our approach on two widely-used 3D object detection datasets: ScanNet V2 <ref type="bibr" target="#b6">[6]</ref> and SUN RGB-D <ref type="bibr" target="#b36">[36]</ref>, and we follow the standard data splits <ref type="bibr" target="#b26">[26]</ref> for them both. ScanNet V2 <ref type="bibr" target="#b6">[6]</ref> is constructed from an 3D reconstruction dataset of indoor scenes by enriched annotations. It consists of 1513 indoor scenes and 18 object categories. The annotations of per-point instance, semantic labels, and 3D bounding boxes are provided. We follow a standard evaluation protocol <ref type="bibr" target="#b26">[26]</ref> by using mean Average Precision(mAP) under different IoU thresholds, without considering the orientation of bounding boxes. SUN RGB-D <ref type="bibr" target="#b36">[36]</ref> is a single-view RGB-D dataset for 3D scene understanding, consisting of ∼5K indoor RGB and depth images. The annotation consists of per-point semantic labels and oriented bounding object bounding boxes of 37 object categories. The standard mean Average Precision is used as evaluation metrics and the evaluation is reported on the 10 most common categories, following <ref type="bibr" target="#b26">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>ScanNet V2 We follow recent practice <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b31">31]</ref> to use PointNet++ as default backbone network for a fair comparison. The backbone has 4 set abstraction layers and 2 feature propagation layers. For each set abstraction layer, the input point cloud is sub-sampled to 2048, 1024, 512, and 256 points with the increasing receptive radius of 0.2, 0.4, 0.8, and 1.2, respectively. Then, two feature propagation layers successively up-sample the points to 512 and 1024. More training details are given in Appendix. SUN RGB-D The implementation mostly follow <ref type="bibr" target="#b26">[26]</ref>. We use 20k points as input for each point cloud. The network architecture and the data augmentation are the same as that for ScanNet V2. As the orientation of the 3D box is required in evaluation, we include an additional orientation prediction branch for all decoder stages. More training details are given in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">System-level Comparison</head><p>In this section, we compare with previous state-of-thearts on ScanNet V2 and SUN RGB-D. Since previous works <ref type="bibr" target="#b26">[26,</ref><ref type="bibr">24]</ref> usually report the best results of multiple times on training and testing in the system-level comparison, we report both best results and average results 1 ScanNet V2 The results are shown in <ref type="table">Table 1</ref>. With the same backbone network of a standard PointNet++, the proposed approach achieves 67.3 mAP@0. <ref type="bibr" target="#b25">25</ref>  mAP@0.5 using 6 decoder stages and 256 object candidates, which is 2.8 and 5.5 better than previous best results using the same backbones. By more decoder stages as 12, the gap increases to 6.3 on mAP@0.5.</p><p>With stronger backbones and more sampled object candidates, i.e. 2× more channels and 512 candidates, the performance of the proposed approach is improved to 69.1 mAP@0.25 and 52.8 mAP@0.5, outperforming previous best method by a large margin.</p><p>SUN RGB-D We also compare the proposed approach with previous state-of-the-arts on the SUN RGB-D dataset, which is another widely used 3D object detection benchmark. In this dataset, the ensemble approach over multiple stages is used by default during inference. The results are shown in <ref type="table">Table.</ref> 2. Our base model achieves 63.0 on mAP@0.25 and 45.2 on mAP@0.5, which outperforms all previous state-of-the-arts that only use the point cloud. In particular, it outperforms the H3DNet on mAP@0.5 by 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we validate our key designs on ScanNet V2. If not specified, all models have 6 attention modules, <ref type="bibr" target="#b2">2</ref> We report the results of MMDetection3D(https://github.com/openmmlab/mmdetection3d) instead of the official paper, which reported 46.8 mAP@0. <ref type="bibr" target="#b25">25</ref>  256 sampled candidates, and are equipped with the proposed iterative object prediction approach. In evaluation, we report the average performance of 25 trials by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling Strategy</head><p>We first ablate the effects of different sampling strategies in <ref type="table">Table.</ref> 3. It shows that our approach performs well by using different sampling strategies. It also works well in a wide range of hyper-parameters, such as k in the KPS sampling approach (see <ref type="table">Table.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4).</head><p>These results indicate the robustness of our framework for choosing different sampling approaches. method where no spatial encoding is involved in the decoder stages, the approach shows reasonably good performance of 64.7 mAP@0.25 and 43.4 mAP@0. <ref type="bibr" target="#b25">25</ref>, likely because the location information may have been implicitly included in the input object features. Actually, an additional fixed position encoding does not improve detection performance (64.6 mAP@0.25 and 43.5 mAP@0.5). By refining the encodings of the box location stage by stage, the localization ability of the approach is significantly improved of the 4.1 points gains on the mAP@0.5 metric over the naive implementation <ref type="bibr">(47.5 vs. 43.4)</ref>. Also, more detailed spatial encoding by both box center and size is beneficial, compared to that only encodes box centers (66.3 vs. 65.2 on mAP@0.25 and 48.5 vs. 47.5 on mAP@0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Box Prediction</head><p>Table. 6 shows the performance of iterative box prediction with different decoder stages. More stages can bring significant performance improvement, especially in the mAP@0.5. Compared with not applying any attention modules, our 6-stage model performs better on mAP@0.25 and mAP@0.5 by 3.0 and 7.8, respectively.</p><p>Ensemble Multi-stage Predictions Each decoder stage of our approach will predict a set of 3D boxes. It is natural to ensemble these results of different decoder stages in expecting better final detection results. <ref type="table">Table 7</ref> shows the results, where significantly performance improvements are observed on SUN RGB-D (+3.8 mAP@0.25 and +1.9 mAP@0.5) and maintained performance on ScanNet V2. We hypothesize that it is because the point clouds of SUN RGB-D have lower quality than those of ScanNet V2: SUN RGB-D adopts real RGB-D signals to generate point clouds that many objects have missing parts due to occlusion, while the ScanNet V2 generate point clouds from 3D shape meshes which are more complete. The ensemble method can boost the performance more on real 3D scenes.</p><p>Comparison with Group-based Approaches Aggregating point features through RoI-Pooing, or according to the voted centers are two typical handcrafted grouping strategies <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b26">26]</ref> in 3D object detection. We refer these two grouping strategies as baselines and compare with them. For a fair comparison, we only switch the feature aggregation mechanism while all other settings (e.g. the 6-stage decoder) remain unchanged. More details are in Appendix. <ref type="table" target="#tab_4">Table 8</ref> show the results. Although RoI-Pooling outperforms than the voting approach, it is still worse than our group-free approach by 1.2 points on mAP@0.25 and 4.1 points on mAP@0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Inference Speed</head><p>The computational complexity of the attention model is determined by the number of points in a point cloud and the number of sampled object candidates. In our approach, only a small number of object candidates are sampled, which makes the cost of the attention model insignificant. With our default setting (256 object candidates, 1024 output points), stacking one attention model brings 0.95 GFLOPs, which is quite light compared to the backbone.</p><p>In addition, the realistic inference speed of our method is also very competitive, compared to other state-of-the-art methods. For a fair comparison, all experiments are run on the same workstation (single Titan-XP GPU, 256G RAM, and Xeon E5-2650 v3) and environment (Ubuntu-16.04, Python 3.6, Cuda-10.1, and PyTorch-1.3.1). The official code of other methods is used for evaluation. The batch size of all experiments is set to 1 (i.e. single image). The results are shown in <ref type="table">Table.</ref> 9. Our method achieves better performance and also higher inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposal Layer6 Layer3</head><p>ScanNet V2 SUN  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB-D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Comparison with DETR</head><p>DETR <ref type="bibr" target="#b2">[2]</ref> is a pioneer work that applies the Transformer to 2D object detection. Compared with DETR, our method involves more domain knowledge, such as the datadependent initial object candidate generation, where DETR uses a data-independent object prior to representing each object candidate and is automatically learned without explicit supervision. Moreover, there is no iterative refinement on spatial encodings in DETR as in our approach. We evaluate these differences in 3D object detection. For a fair comparison, the backbone and decoder heads used in DETR are the same as in ours. We carefully tune the hyper-parameters for DETR and chose the best setting in comparison.</p><p>The results are shown in <ref type="table" target="#tab_5">Table 10</ref>. With the same training length of 400 epochs, DETR achieves 39.6 mAP@0.25 and 21.4 mAP@0.5, significantly worse than our method. We guess it is mainly because of optimization difficulty by the data-independent object representation. The fixed spatial encoding also may contribute to inferior performance. In fact, the performance can be improved significantly by bridging these differences, reaching 59.9 mAP@0.25 and 42.9 mAP@0.5 using the same training epochs, and 61.8 mAP@0.25 and 45.2 mAP@0.5 by longer training.</p><p>The remaining performance gap is due to the difference in ground-truth assignments, where DETR adopts a set loss to automatically determine the assignments by detection losses and our approach manually assigns object candidates to ground-truths. This assignment may also be difficult for a network to learn. <ref type="figure" target="#fig_1">Fig. 4</ref> illustrates the qualitative results on both ScanNet V2 and SUN RGB-D. As the decoder networks go deeper, the more accurate detection results are observed. <ref type="figure" target="#fig_2">Fig. 5</ref> visualizes the learned cross-attention weights of different decoder stages. We could observe that the model of the lower stage always focuses on the surrounding points without considering the geometry. With the refinement, the model of the higher stage could focus more on the geometry and extract more high-quality object features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a simple yet effective 3D object detector based on the attention mechanism in Transformers. Unlike previous methods that require a grouping step for object feature computation, this detector is group-free which computes object features from all points in a point cloud, with the contribution of each point automatically determined by the attention modules. The proposed method achieves state-of-the-art performance on ScanNet V2 and SUN RGB-D benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Training Details</head><p>A1.1. Our Approach ScanNet V2 We follow recent practice <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b31">31]</ref> to use the PointNet++ as our default backbone network for a fair comparison. The backbone network has four set abstraction layers and two feature propagation layers. For each set abstraction layer, the input point cloud is sub-sampled to 2048, 1024, 512, and 256 points with the increasing receptive radius of 0.2, 0.4, 0.8, and 1.2, respectively. Then, two feature propagation layers successively up-sample the points to 512 and 1024, respectively.</p><p>In the training phase, we use 50k 1 points as input and adopt the same data augmentation as in <ref type="bibr" target="#b26">[26]</ref>, including a random flip, a random rotation between [−5 • , 5 • ], and a random scaling of the point cloud by [0.9, 1.1]. The network is trained from scratch by the AdamW optimizer (β 1 =0.9, β 2 =0.999) with 400 epochs. The weight decay is set to 5e-4. The initial learning rate is 0.006 and decayed by 10× at the 280-th epoch and the 340-th epoch. The learning rate of the attention modules is set as 1/10 of that in the backbone network. The gradnorm clip is applied to stabilize the training dynamics. Following <ref type="bibr" target="#b26">[26]</ref> we use classaware head for box size prediction.</p><p>SUN RGB-D The implementation settings mostly follow <ref type="bibr" target="#b26">[26]</ref>. We use 20k points as input for each point cloud. The network architecture and the data augmentation are the same as that for ScanNet V2. As the orientation of the 3D box is required in evaluation, we include an additional orientation prediction branch for all decoder layers. The orientation branch contains a classification task and an offset regression task with loss weights of 0.1 and 0.04, respectively.</p><p>In training, the network is trained from scratch by the AdamW optimizer (β 1 =0.9, β 2 =0.999) with 600 epochs if not specified. The initial learning rate is 0.004 and decayed by 10× at the 420-th epoch, the 480-th epoch, and the 540th epoch. The learning rate of attention modules is set as 1/20 of the backbone network. The weight decay is set to 1e-7, and the gradnorm clip is used. We use class-agnostic head for size prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.2. Other Pooling Mechanisms</head><p>For a fair comparison, we only switch the feature aggregation mechanism while all other settings remain unchanged. In the following, we will introduce the implementation details of RoI-Pooling and Voting aggregation mechanism.</p><p>RoI-Pooling For a given object candidate, the points within the predicted box of the object candidate are aggregated together, and the refined box is predicted from the aggregated features. The same as our group-free approach, the multi-stage refinement is also adopted. Thus the aggregated points and features will be updated and refined in multiple stages. Also, we tried two different strategies for feature aggregation: average-pooling and max-pooling. The results are shown in <ref type="table">Table.</ref> 11. We could find that the approach with max-pooling performs better, so we use it for comparison by default.</p><p>Voting The voting mechanism is first introduced by VoteNet [26] and we implement it in our framework. Specifically, each point predicts the center of its corresponding object, and if the distance between the predicted center of points and the center of an object candidate is less than a threshold (set to 0.3 meters), then these points and the candidate are grouped. Further, a two-layer MLP with maxpooling is used to form the aggregation feature of the object candidate, and the refined boxes are predicted from the aggregated features in the multi-stage refinement process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. More Results</head><p>We show per-category results on ScanNet V2 and SUN RGB-D under different IoU thresholds. <ref type="table" target="#tab_1">Table 12</ref> and <ref type="table" target="#tab_8">Table 13</ref> show the results of mAP@0.25 and mAP@0.5 on ScanNet V2, respectively. <ref type="table" target="#tab_2">Table 14</ref> and <ref type="table">Table 15</ref> show the results of mAP@0.25 and mAP@0.5 on SUN RGB-D, respectively.</p><p>We also show more qualitative results of our method on ScanNet V2 and SUN RGB-D. The results are shown in <ref type="figure">Figure 6</ref> (ScanNet V2) and <ref type="figure">Figure 7</ref> (SUN RGB-D).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of the attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results of different decoder stages. The first row is the results on SUN RGB-D, and the second row is the results on ScanNet V2. The color of bounding boxes represents their category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Visualizations on cross-attention weight in different decoder stages. The green point represents the reference object candidates. The redder color represent higher attention weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2</head><label>12</label><figDesc>System level comparison on ScanNet V2 with state-of-the-arts. The main comparison is based on the best results of multiple experiments between different methods, and the number within the bracket is the average result.</figDesc><table><row><cell>and 48.9</cell></row></table><note>. System level comparison on SUN RGB-D with state-of-the-arts. The main comparison is based on the best results of multiple experiments between different methods, and the number within the bracket is the average result.* imVoteNet use RGB images as addition inputs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>and 24.7 mAP@0.5 on ScanNet V2, and 57.7 mAP@0.25 and 32.0 mAP@0.5 on SUN RGB-D. Ablation study on different values of k in KPS sampling strategy.</figDesc><table><row><cell cols="3">sampling method mAP@0.25 mAP@0.5</cell></row><row><cell>FPS</cell><cell>64.5</cell><cell>46.2</cell></row><row><cell>KPS-NMS</cell><cell>65.8</cell><cell>48.7</cell></row><row><cell>KPS</cell><cell>66.3</cell><cell>48.5</cell></row><row><cell cols="3">Table 3. Ablation study on applying different sampling strategies.</cell></row><row><cell cols="3">k mAP@0.25 mAP@0.5</cell></row><row><cell>1</cell><cell>65.7</cell><cell>48.7</cell></row><row><cell>2</cell><cell>65.8</cell><cell>48.3</cell></row><row><cell>4</cell><cell>66.3</cell><cell>48.5</cell></row><row><cell>6</cell><cell>66.1</cell><cell>48.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 Table 7</head><label>57</label><figDesc></figDesc><table><row><cell>ablates several design</cell></row></table><note>. Ablation study on the effectiveness of multi-stage ensem- ble.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 .</head><label>8</label><figDesc>Comparison with grouping-based approaches.</figDesc><table><row><cell>method</cell><cell></cell><cell cols="2">mAP@0.25 mAP@0.5</cell></row><row><cell cols="2">RoI-Pooing</cell><cell>65.1</cell><cell>44.4</cell></row><row><cell>Voting</cell><cell></cell><cell>64.2</cell><cell>44.1</cell></row><row><cell>Ours</cell><cell></cell><cell>66.3</cell><cell>48.5</cell></row><row><cell>method</cell><cell></cell><cell>backbone</cell><cell>mAP 0.25 0.5</cell><cell>frames/s</cell></row><row><cell>MLCVNet [31]</cell><cell></cell><cell>PointNet++</cell><cell>64.5 41.4</cell><cell>5.44</cell></row><row><cell>H3DNet [51]</cell><cell cols="3">4×PointNet++ 67.2 48.1</cell><cell>3.76</cell></row><row><cell>Ours (L6, O256)</cell><cell></cell><cell>PointNet++</cell><cell>67.3 48.9</cell><cell>6.71</cell></row><row><cell>Ours (L12, O256)</cell><cell></cell><cell>PointNet++</cell><cell>67.2 49.7</cell><cell>5.70</cell></row><row><cell cols="4">Ours (L12, O256) PointNet++w2× 68.8 52.1</cell><cell>5.23</cell></row><row><cell cols="4">Ours (L12, O512) PointNet++w2× 69.1 52.8</cell><cell>5.17</cell></row><row><cell cols="5">Table 9. Comparison on realistic inference speed on ScanNet V2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 10 .</head><label>10</label><figDesc>The comparison between DETR and our method on ScanNet V2. KPS represent k-Closest Points Sampling, iter pred represents iterative prediction.</figDesc><table><row><cell>method</cell><cell cols="3">epoch mAP@0.25 mAP@0.5</cell></row><row><cell>DETR</cell><cell>400</cell><cell>39.6</cell><cell>21.4</cell></row><row><cell>DETR+KPS</cell><cell>400</cell><cell>59.6</cell><cell>41.0</cell></row><row><cell>DETR+KPS+iter pred</cell><cell>400</cell><cell>59.9</cell><cell>42.9</cell></row><row><cell>DETR+KPS+iter pred</cell><cell>1200</cell><cell>61.8</cell><cell>45.2</cell></row><row><cell>Ours</cell><cell>400</cell><cell>66.3</cell><cell>48.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 11 .</head><label>11</label><figDesc>Comparison between average-pooling and max-pooling on ScanNet V2.</figDesc><table><row><cell cols="3">method mAP@0.25 mAP@0.5</cell></row><row><cell>average</cell><cell>64.2</cell><cell>44.2</cell></row><row><cell>max</cell><cell>65.1</cell><cell>44.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 12 .</head><label>12</label><figDesc>methods backbone cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP VoteNet [26] PointNet++ 47.7 88.7 89.5 89.3 62.1 54.1 40.8 54.3 12.0 63.9 69.4 52.0 52.5 73.3 95.9 52.0 92.5 42.4 62.9 MLCVNet [31] PointNet++ 42.5 88.5 90.0 87.4 63.5 56.9 47.0 56.9 11.9 63.9 76.1 56.7 60.9 65.9 98.3 59.2 87.2 47.9 64.5 H3DNet [51] 4×PointNet++ 49.4 88.6 91.8 90.2 64.9 61.0 51.9 54.9 18.6 62.0 75.9 57.3 57.2 75.3 97.9 67.4 92.5 53.6 67.2 Ours (L6, O256) PointNet++ 54.1 86.2 92.0 84.8 67.8 55.8 46.9 48.5 15.0 59.4 80.4 64.2 57.2 76.3 97.6 76.8 92.5 55.0 67.3 Ours (L12, O256) PointNet++ 55.4 86.6 91.8 86.6 73.0 54.5 49.4 47.7 13.1 63.3 82.4 63.3 53.2 74.0 99.2 67.7 91.7 55.8 67.2 Ours (L12, O256) PointNet++w2× 56.5 88.2 92.5 88.2 71.6 57.5 48.3 53.7 17.5 71.0 79.5 63.4 58.1 71.7 99.4 71.1 93.0 57.8 68.8 Ours (L12, O512) PointNet++w2× 52.1 91.9 93.6 88.0 70.7 60.7 53.7 62.4 16.1 58.5 80.9 67.9 47.0 76.3 99.6 72.0 95.3 56.4 69.1 Performance of mAP@0.25 for each category on the ScanNet V2 dataset. methods backbone cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP VoteNet [26] PointNet++ 14.6 77.8 73.1 80.5 46.5 25.1 16.0 41.8 2.5 22.3 33.3 25.0 31.0 17.6 87.8 23.0 81.6 18.7 39.9 H3DNet [51] 4×PointNet++ 20.5 79.7 80.1 79.6 56.2 29.0 21.3 45.5 4.2 33.5 50.6 37.3 41.4 37.0 89.1 35.1 90.2 35.4 48.1 Ours (L6, O256) PointNet++ 23.0 78.4 78.9 68.7 55.1 35.3 23.6 39.4 7.5 27.2 66.4 43.3 43.0 41.2 89.7 38.0 83.4 37.3 48.9 Ours (L12, O256) PointNet++ 23.8 77.2 81.6 65.1 62.8 35.0 21.3 39.4 7.0 33.1 66.3 39.3 43.9 47.0 91.2 38.5 85.2 37.4 49.7 Ours (L12, O256) PointNet++w2× 26.2 80.7 83.5 70.7 57.0 37.4 21.2 47.7 8.8 45.3 60.7 42.2 43.5 42.7 95.5 42.3 89.7 43.4 52.1 Ours (L12, O512) PointNet++w2× 26.0 81.3 82.9 70.7 62.2 41.7 26.5 55.8 7.8 34.7 67.2 43.9 44.3 44.1 92.8 37.4 89.7 40.6 52.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 13 .</head><label>13</label><figDesc>Performance of mAP@0.5 for each category on the ScanNet V2 dataset. 85.8 31.9 75.8 26.5 31.3 61.5 66.3 50.4 89.1 59.8 HGNet [3] PointNet++ w/ FPN 78.0 84.5 35.7 75.2 34.3 37.6 61.7 65.7 51.6 91.1 61.6 H3DNet [51] 4×PointNet++ 73.8 85.6 31.0 76.7 29.6 33.4 65.5 66.5 50.8 88.2 60.1 Ours (L6, O256) PointNet++ 80.0 87.8 32.5 79.4 32.6 36.0 66.7 70.0 53.8 91.1 63.0 Table 14. Performance of mAP@0.25 for each category on the SUN RGB-D validation set. methods backbone bathtub bed bkshf chair desk drser nigtstd sofa table toilet mAP</figDesc><table><row><cell>methods</cell><cell>backbone</cell><cell>bathtub bed bkshf chair desk drser nigtstd sofa table toilet mAP</cell></row><row><cell>VoteNet [26]</cell><cell>PointNet++</cell><cell>75.5 85.6 31.9 77.4 24.8 27.9 58.6 67.4 51.1 90.5 59.1</cell></row><row><cell>MLCVNet [31]</cell><cell>PointNet++</cell><cell>79.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We train each setting 5 times and test each training trial 5 times. The average performance of these 25 trials is reported to account for algorithm randomness.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We evaluate our model on 40k points on ScanNet V2 according to previous works and the performance is similar: 66.3(40k) vs. 66.2(50k) on mAP@0.25, and 48.5(40k) vs. 48.6(50k) on mAP@0.5.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Performance of mAP@0.5 for each category on the SUN RGB-D validation set</title>
		<imprint/>
	</monogr>
	<note>Table 15</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A hierarchical graph network for 3d object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biwen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15831</idno>
		<title level="m">Relationnet++: Bridging visual representations for object detection via transformer decoder</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bastian Leibe, and Matthias Nießner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9031" to="9040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gvcnn</surname></persName>
		</author>
		<title level="m">Group-view convolutional neural networks GT Ours Image Figure 7. Qualitative results on SUN RGB-D</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">for 3d shape recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flex-convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><forename type="middle">Pa</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning region features for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-view 3d object retrieval with deep embedding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5526" to="5537" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generative sparse detection networks for 3d single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12356</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4452" to="4461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01294</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imvotenet: Boosting 3d object detection in point clouds with image votes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mlcvnet: Multi-level context votenet for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename><surname>Yu-Kun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhoutao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Yiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Standalone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2088" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rgcnn: Regularized graph cnn for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gusi</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="746" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive o-cnn: A patch-based deep representation of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pixor: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">H3dnet: 3d object detection using hybrid geometric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05682</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
