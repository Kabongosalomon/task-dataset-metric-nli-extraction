<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Trajectory Dependencies for Human Motion Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Australia Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
							<email>mathieu.salzmann@epfl.ch</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<email>hongdong.li@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Australia Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Trajectory Dependencies for Human Motion Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human motion prediction, i.e., forecasting future body poses given observed pose sequence, has typically been tackled with recurrent neural networks (RNNs). However, as evidenced by prior work, the resulted RNN models suffer from prediction errors accumulation, leading to undesired discontinuities in motion prediction. In this paper, we propose a simple feed-forward deep network for motion prediction, which takes into account both temporal smoothness and spatial dependencies among human body joints. In this context, we then propose to encode temporal information by working in trajectory space, instead of the traditionallyused pose space. This alleviates us from manually defining the range of temporal dependencies (or temporal convolutional filter size, as done in previous work). Moreover, spatial dependency of human pose is encoded by treating a human pose as a generic graph (rather than a human skeletal kinematic tree) formed by links between every pair of body joints. Instead of using a pre-defined graph structure, we design a new graph convolutional network to learn graph connectivity automatically. This allows the network to capture long range dependencies beyond that of human kinematic tree. We evaluate our approach on several standard benchmark datasets for motion prediction, including Human3.6M, the CMU motion capture dataset and 3DPW. Our experiments clearly demonstrate that the proposed approach achieves state of the art performance, and is applicable to both angle-based and position-based pose representations. The code is available at https: //github.com/wei-mao-2019/LearnTrajDep arXiv:1908.05436v3 [cs.CV] 7 Jul 2020</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human motion prediction is key to the success of applications where one needs to forecast the future, such as human robot interaction <ref type="bibr" target="#b14">[15]</ref>, autonomous driving <ref type="bibr" target="#b17">[18]</ref> and human tracking <ref type="bibr" target="#b7">[8]</ref>. While traditional data-driven approaches, such as Hidden Markov Model <ref type="bibr" target="#b2">[3]</ref> and Gaussian Process latent variable models <ref type="bibr" target="#b23">[24]</ref>, have proved effective for simple periodic motions and acyclic motions, such as <ref type="bibr">Figure 1</ref>. Human motion prediction. The left frames correspond to the observations. From top to bottom, we show the ground truth, and predictions obtained by the methods of <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b15">[16]</ref>, and by our approach on joint angles and 3d coordinates. Our predictions better match the ground truth.</p><p>walking and golf swing, more complicated ones are typically tackled using deep networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Because of the temporal nature of the signal of interest, the most common trend consists of using Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9]</ref>. However, as argued in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> , besides their well-known training difficulty <ref type="bibr" target="#b18">[19]</ref>, RNNs for motion prediction suffer from several drawbacks: First, existing works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref> that use the estimation at the current RNN step as input to the next prediction tend to accumulate errors throughout the generated sequence, leading to unrealistic predictions at inference time. Second, as observed in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, earlier RNN-based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> often produce strong discontinuities between the last observed frame and the first predicted one. These discontinuities are partially due to the frame-by-frame regression procedure that does not encourage global smoothness of the sequence <ref type="bibr" target="#b8">[9]</ref>. As a consequence, several works have proposed to rely on feed-forward networks for motion prediction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>. In this paper, we introduce a new feed-forward approach to motion prediction, leading to more accurate predictions than RNN ones, as illustrated in <ref type="figure" target="#fig_5">Fig. 1</ref>.</p><p>When using feed-forward networks for a time-related problem such as motion prediction, the question of how to encode the temporal information naturally arises. In <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>, this was achieved by using convolutions across time on the observed poses. The temporal dependencies that such an approach can encode, however, strongly depend on the size of the convolutional filters.</p><p>To remove such a dependency, here, we introduce a drastically different approach to modeling temporal information for motion prediction. Inspired by ideas from the nonrigid structure-from-motion literature <ref type="bibr" target="#b0">[1]</ref>, we propose to represent human motion in trajectory space instead of pose space, and thus adopt the Discrete Cosine Transform (DCT) to encode temporal information. Specifically, we represent the temporal variation of each human joint as a linear combination of DCT bases, and, given the DCT coefficients of the observed poses, learn to predict those of the future ones. This strategy applies to both angle-based pose representations and 3D joint positions. As discussed in our experiments, the latter has the advantage of not suffering from ambiguities, in contrast to angle-based ones, where two different sets of angles can represent the exact same pose. As a consequence, reasoning in terms of 3D joint positions allows one not to penalize configurations that differ from ground truth while depicting equivalent poses.</p><p>The other question that arises when working with human pose is how to encode the spatial dependencies among the joints. In <ref type="bibr" target="#b4">[5]</ref>, this was achieved by exploiting the human skeleton, and in <ref type="bibr" target="#b15">[16]</ref> by defining a relatively large spatial filter size. While the former does not allow one to model dependencies across different limbs, such as left-right symmetries, the latter again depends on the size of the filters.</p><p>In this paper, we propose to overcome these two issues by exploiting graph convolutions <ref type="bibr" target="#b12">[13]</ref>. However, instead of using a pre-defined, sparse graph as in <ref type="bibr" target="#b12">[13]</ref>, we introduce an approach to learning the graph connectivity. This strategy allows the network to capture joint dependencies that are neither restricted to the kinematic tree, nor arbitrarily defined by a convolutional kernel size.</p><p>In summary, our contributions are (i) a natural way to encode temporal information in feed-forward networks for motion prediction via the DCT; (ii) learnable graph convolutional networks to capture the spatial structure of the motion data. Our experiments on standard human motion prediction benchmarks evidence the benefits of our approach; our model yields state-of-the-art results in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>RNN-based human motion prediction. Because of their success at sequence-to-sequence prediction <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14]</ref>, RNNs have become the de facto model for human motion prediction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref>. This trend was initiated by Fragkiadaki et al. <ref type="bibr" target="#b6">[7]</ref>, who proposed an Encoder-Recurrent-Decoder (ERD) model that incorporates a nonlinear encoder and decoder before and after recurrent layers. Error accumulation was already observed in this work, and a curriculum learning strategy was adopted during training to prevent it. In <ref type="bibr" target="#b10">[11]</ref>, Jain et al. proposed to further encode the spatial and temporal structure of the pose pre-diction problem via a Structural-RNN model relying on high-level spatio-temporal graphs. These graphs, however, were manually designed, which limits the flexibility of the framework, not letting it discover long-range interactions between different limbs. While the two previous methods directly estimated absolute human poses, Martinez et al. <ref type="bibr" target="#b16">[17]</ref> introduced a residual architecture to predict velocities. Interestingly, it was shown in this work that a simple zero-velocity baseline, i.e., constantly predicting the last observed pose, led to better performance than <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>. While <ref type="bibr" target="#b16">[17]</ref> outperformed this baseline, the predictions produced by the RNN still suffer from discontinuities between the observed poses and the predicted future ones. To overcome this, Gui et al. proposed to rely on adversarial training, so as to generate smooth sequences that are indistinguishable from real ones <ref type="bibr" target="#b8">[9]</ref>. While this approach constitutes the state of the art, its use of an adversarial classifier, which notoriously complicates training <ref type="bibr" target="#b1">[2]</ref>, makes it difficult to deploy on new datasets. Feed-forward approaches to human motion prediction. Feed-forward networks, such as fully-connected and convolutional ones, were studied as an alternative solution to avoiding the discontinuities produced by RNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>. In particular, in <ref type="bibr" target="#b4">[5]</ref>, Butepage et al. proposed to treat a recent pose history as input to a fully-connected network, and introduced different strategies to encode additional temporal information via convolutions and spatial structure by exploiting the kinematic tree. The use of a kinematic tree, however, does not reflect the fact that, as discussed in <ref type="bibr" target="#b15">[16]</ref>, stable motion requires synchronizing different body parts, even distant ones not directly connected by the kinematic tree. To capture such dependencies, Li et al. <ref type="bibr" target="#b15">[16]</ref> built a convolutional sequence-to-sequence model processing a 2 dimensional matrix whose columns represent the pose at every time step. The range of the spatial and temporal dependencies captured by this model is then determined by the size of the convolutional filters. In this paper, as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>, we also rely on a feed-forward network for motion prediction. However, we introduce a drastically different way to modeling temporal information, which, in contrast to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>, does not require manually defining convolutional kernel sizes. Specifically, we propose to perform motion prediction in trajectory space instead of pose space. Furthermore, to model the spatial dependencies between the joints, we propose to exploit graph convolutional networks. Graph Convolutional Networks (GCNs). GCNs generalize the convolution operation to data whose structure is defined by a graph, such as user data from social networks, data defined on 3D meshes and gene data on biological regulatory networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>. The main advances in this context can be categorized as spectral <ref type="bibr" target="#b12">[13]</ref> and non-spectral <ref type="bibr" target="#b21">[22]</ref> methods. In particular, Kipf and Welling <ref type="bibr" target="#b12">[13]</ref> use filters that depend on the graph structure, which limits the gener- <ref type="figure" target="#fig_6">Figure 2</ref>. Network architecture. We first apply the DCT to encode temporal pose information in trajectory space. The DCT coefficients are treated as features input to graph convolutional layers. We use 12 blocks of graph convolutional layers with residual connections and two additional graph convolutional layers, one at the beginning and one at the end, to encode the temporal information and decode the features to the residual DCT coefficients, respectively. In each block, we depict how our framework aggregates information from multiple nodes via learned adjacency matrices. ality of their approach. By contrast, Velicković et al. <ref type="bibr" target="#b21">[22]</ref> rely on self-attention to determine the neighborhood structure to be considered, thus providing more flexibility to the network. A straightforward approach to exploiting graph convolutions for motion prediction would consist of relying on the kinematic tree to define the graph. This strategy has been employed for action recognition <ref type="bibr" target="#b24">[25]</ref>, by using a GCN to capture the temporal and spatial dependencies of human joints via a graph defined on temporally connected kinematic trees. For motion prediction, however, this would suffer from the same limitations as the strategy of <ref type="bibr" target="#b4">[5]</ref> discussed above. Therefore, here, inspired by <ref type="bibr" target="#b21">[22]</ref>, we design a GCN able to adaptively learn the necessary connectivity for the motion prediction task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>Let us now introduce our approach to human motion prediction. As existing methods, we assume to be given a history motion sequence X 1:N = [x 1 , x 2 , x 3 , · · · , x N ] consisting of N consecutive human poses, where x i ∈ R K , with K the number of parameters describing each pose. Our goal then is to predict the poses X N +1:N +T for the future T time steps. To this end, we propose to make use of a feedforward deep network that models the temporal and spatial structure of the data. Below, we introduce our approach to encoding these two types of information and then provide the details of our network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DCT-based Temporal Encoding</head><p>In the motion prediction literature, the two standard ways to represent human pose are joint angles and 3D joint coordinates. These two representations, however, are purely static. Here, instead, we propose to directly encode the temporal nature of human motion in our representation and work in trajectory space. Note that, ultimately, we nonetheless need to produce human poses in a standard representation, and, as evidenced by our experiments, our formalism applies to both of the above-mentioned ones.</p><p>Our temporal encoding aims to capture the motion pattern of each joint. Recall that each column of X 1:N represents the human pose at a specific time step. Conversely, each row of X 1:N describes the motion of each joint (angle or coordinate). Let us denote byx k = (x k,1 , x k,2 , x k,3 , · · · , x k,N ) the trajectory for the k th joint across N frames. While one could directly use such trajectories as input and output for motion prediction, inspired by ideas from the nonrigid-structure-from-motion literature <ref type="bibr" target="#b0">[1]</ref>, we propose to adopt a trajectory representation based on the Discrete Cosine Transform (DCT). The main motivation behind this is that, by discarding the high frequencies, the DCT can provide a more compact representation, which nicely captures the smoothness of human motion, particularly in terms of 3D coordinates. Detailed analysis about the number of DCT coefficients used is in the supplementary material.</p><p>Specifically, given a trajectoryx k , the corresponding l th DCT coefficient can be computed as <ref type="bibr" target="#b0">(1)</ref> where δ ij denotes the Kronecker delta function with</p><formula xml:id="formula_0">C k,l = 2 N N n=1 x k,n 1 √ 1+δl1 cos π 2N (2n − 1)(l − 1) ,</formula><formula xml:id="formula_1">δ ij = 1 if i = j 0 if i = j.<label>(2)</label></formula><p>In practice, l ∈ {1, 2, · · · , N }, but one can often ignore the higher values, which, in our context, translates to removing the high motion frequencies. In short, Eq. 1 allows us to model the temporal information of each joint using DCT coefficients. Given such coefficients, the original pose representation (angles or coordinates) can be obtained via the Inverse Discrete Cosine Transform (IDCT) as</p><formula xml:id="formula_2">x k,n = 2 N N l=1 C k,l 1 √ 1+δl1 cos π 2N (2n − 1)(l − 1) , (3)</formula><p>where n ∈ {1, 2, · · · , N }. Note that, if all DCT coefficients are used, the resulting representation is lossless. However, as mentioned before, truncating some of the high frequencies can prevent generating jittery motion.</p><p>To make use of the DCT representation, instead of treating motion prediction as the problem of learning a mapping from X 1:N to X N +1:N +T , we reformulate it as one of learning a mapping between observed and future DCT coefficients. Specifically, given a temporal sequence X 1:N , we first replicate the last pose, x N , T times to generate a temporal sequence of length N + T . We then compute the DCT coefficients of this sequence, and aim to predict those of the true future sequence X 1:N +T . This naturally translates to estimating a residual vector in frequency space and was motivated by the zero-velocity baseline in <ref type="bibr" target="#b16">[17]</ref>. As will be shown in our experiments, this residual approach, with padding by replicating the last pose, has proven much more effective than other strategies.</p><p>Our DCT representations could be directly employed in a standard fully-connected network, either by stacking the DCT representations of all joints in a single vector, which would yield to a network with many parameters, or by treating the different DCT coefficients as different channels, thus using a K × L matrix as input to the network, with L the number of retained DCT coefficients. While this latter strategy results in a more compact network, it does not model the spatial dependencies between the joints. In the next section, we introduce an approach to doing so using GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Graph Convolutional Layer</head><p>To encode the spatial structure of human pose, we make use of GCNs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>. Here, instead of relying on a predefined, sparse graph, as in <ref type="bibr" target="#b12">[13]</ref>, we propose to learn the graph connectivity during training, thus essentially learning the dependencies between the different joint trajectories.</p><p>To this end, let us assume that the human body is modeled as a fully-connected graph with K nodes. The strength of the edges in this graph can then be represented by a weighted adjacency matrix A ∈ R K×K . A graph convolutional layer p then takes as input a matrix H (p) ∈ R K×F , with F the number of features output by the previous layer. For example, for the first layer, the network takes as input the K × L matrix of DCT coefficients. Given this information and a set of trainable weights W (p) ∈ R F ×F , a graph convolutional layer outputs a matrix of the form</p><formula xml:id="formula_3">H (p+1) = σ(A (p) H (p) W (p) ) ,<label>(4)</label></formula><p>where A (p) is the trainable weighted adjacency matrix for layer p and σ(·) is an activation function, such as tanh(·).</p><p>Following the standard deep learning formalism, multiple such layers can be stacked to form a GCN. Since all operations are differentiable, w.r.t. both A (p) and W (p) , the resulting network can be trained using standard backpropagation. In the next section, we provide additional detail about the network structure used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Structure</head><p>As discussed in Section 3.1, we aim to learn the residuals between the input and output DCT representations. More precisely, we learn the residuals between the DCT coefficients obtained from the input sequence with replicated last pose, and that of the sequence X 1:N +T . We therefore design a residual graph convolutional network. The network structure is shown in <ref type="figure" target="#fig_6">Fig. 2</ref>. It consists of 12 residual blocks, each of which comprises 2 graph convolutional layers and two additional graph convolutional layers, one at the beginning and one at the end, to encode the temporal information and decode the features to the residual DCT coefficients, respectively. Each layer p relies on a learnable weight matrix W (p) of size 256 × 256 and a learnable weighted adjacency matrix A (p) . Using a different learnable A for every graph convolutional layer allows the network to adapt the connectivity for different operations. This gives our framework a greater capacity than a GCN with a fixed adjacency matrix. Nevertheless, because, in each layer p, the weight matrix W (p) is shared by the different joints to further extract motion patterns from feature matrix, the overall network remains compact; the size of the models used in our experiments is around 2.6M for both angle and 3D representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>As mentioned before, joint angles and 3D coordinates are the two standard representations for human pose, and we will evaluate our approach on both. Below, we discuss the loss function we use to train our network in each case. For joint angles, following the literature, we use an exponential map representation. Given the training angles, we apply the DCT to obtain the corresponding coefficients, train our model and employ the IDCT to the predicted DCT coefficients so as to retrieve the corresponding angles X 1:N +T . To train our network, we use the average 1 distance between the ground-truth joint angles and the predicted ones. Formally, for one training sample, this gives the loss</p><formula xml:id="formula_4">a = 1 (N +T )K N +T n=1 K k=1 |x k,n − x k,n | ,<label>(5)</label></formula><p>wherex k,n is the predicted k th angle in frame n and x k,n the corresponding ground-truth one. Note that we sum 1 errors over both the future and observed time steps. This provides us with additional signal to learn to predict the DCT coefficients, which represent the entire sequence.</p><p>For the coordinate-based representation, we adopt the standard body model of <ref type="bibr" target="#b9">[10]</ref> to convert the joint angles to 3D coordinates. The 3D joint positions are then preprocessed so as to be centred at the origin, and the global rotations are removed. Going from 3D coordinates to DCT coefficients and back follows exactly the same procedure as in the angle case. To train our model, we then make use of the Mean Per Joint Position Error (MPJPE) proposed in <ref type="bibr" target="#b9">[10]</ref>, which, for one training sample, translates to the loss</p><formula xml:id="formula_5">m = 1 J(N +T ) N +T n=1 J j=1 p j,n − p j,n 2 ,<label>(6)</label></formula><p>wherep j,n ∈ R 3 denotes the predicted jth joint position in frame n, p j,n the corresponding ground-truth one, and J the number of joints in the human skeleton.  <ref type="table">Table 2</ref>. Short-term prediction of 3D joint positions on H3.6M. A 3D in the method's name indicates that it was directly trained on 3D joint positions. Otherwise, the results were obtained by converting the angle predictions to 3D positions. Note that we outperform the baselines by a large margin, particularly when training directly on 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our model on several benchmark motion capture (mocap) datasets, including Human3.6M (H3.6M) <ref type="bibr" target="#b9">[10]</ref>, the CMU mocap dataset 1 , and the 3DPW dataset <ref type="bibr" target="#b22">[23]</ref>. Below, we first introduce these datasets, the evaluation metrics we use and the baselines we compare our method with. We then present our results using both joint angles and 3D coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Human3.6M. To the best of our knowledge, Human3.6M (H3.6M) <ref type="bibr" target="#b9">[10]</ref> is the largest dataset for human motion analysis. It depicts seven actors performing 15 actions, such as walking, eating, discussion, sitting, and phoning. The actors are represented by a skeleton of 32 joints. Follow-1 Available at http://mocap.cs.cmu.edu/ ing the data processing of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>, we remove the global rotations and translations as well as constant angles. The sequences are down-sampled to 25 frames per second and we test on the same sequences of subject 5 (S5) as previous work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. CMU-Mocap. Following <ref type="bibr" target="#b15">[16]</ref>, we also report results on the CMU mocap dataset (CMU-Mocap). For a fair comparison, we adopt the same data representation and training/test splits as in <ref type="bibr" target="#b15">[16]</ref>, provided in their released code and data. Based on <ref type="bibr" target="#b15">[16]</ref>, eight actions are selected for evaluation after pre-processing the entire dataset by removing sequences depicting multiple people, sequences with less training data and actions with repetitions. We apply the same pre-processing as on H3.6M. 3DPW. The 3D Pose in the Wild dataset (3DPW) <ref type="bibr" target="#b22">[23]</ref> is a recently published dataset which has more than 51k frames with 3D annotations for challenging indoor and outdoor ac-   <ref type="bibr" target="#b16">[17]</ref>, convSeq2Seq <ref type="bibr" target="#b15">[16]</ref>, our approach based on angles, and our approach based on 3D positions. The results evidence that our approach generates high-quality predictions in both cases.  tivities. We use the official training, test and validation sets. The frame rate of the 3D annotation is 30Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics and Baselines</head><p>Metrics. We follow the standard evaluation protocol used in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9]</ref>, and report the Euclidean distance between the predicted and ground-truth joint angles in Euler angle representation. We further report results in terms of 3D error. To this end, we make use of the Mean Per Joint Position Error (MPJPE) <ref type="bibr" target="#b9">[10]</ref> in millimeter, commonly used for image-based 3D human pose estimation. As will be shown later, 3D errors can be measured either by directly train a model on the 3D coordinates (via the DCT in our case), or by converting the predicted angles to 3D. Baselines. We compare our approach with two recent RNN-based methods, namely, Residual sup. <ref type="bibr" target="#b16">[17]</ref> and AGED (w or w/o adv) <ref type="bibr" target="#b8">[9]</ref>, and with one feedforward model, convSeq2Seq <ref type="bibr" target="#b15">[16]</ref>. When reporting angular errors, we di-rectly make use of the results provided in the respective papers of these baselines. Because these works do not report 3D error, in this case, we rely on the code provided by the authors of <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>, which we adapted so as to take 3D coordinates as input and output. Note that the code of <ref type="bibr" target="#b8">[9]</ref> is not available, and we were unable to reproduce their method so as to obtain reliable results with their adversarial training strategy 2 . Therefore, we only report the results of this method in angle space. Implementation details. We implemented our network using Pytorch <ref type="bibr" target="#b19">[20]</ref>, and we used ADAM <ref type="bibr" target="#b11">[12]</ref> to train our model. The learning rate was set to 0.0005 with a 0.96 decay every two epochs. The batch size was set to 16 and the gradients were clipped to a maximum 2-norm of 1. It takes 30ms for one forward pass and back-propagation on an NVIDIA Titan V GPU. Our models are trained for 50 epochs. More details about the experiments are included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>To be consistent with the literature, we report our results for short-term (&lt; 500ms) and long-term (&gt; 500ms) predictions. For all datasets, we are given 10 frames (400 milliseconds) to predict the future 10 frames (400 milliseconds) for short-term prediction and to predict the future 25 frames (1 second) for long-term prediction. Human 3.6M. In <ref type="table">Table 1</ref>  of the baselines for short-term prediction in angle space on H3.6M. <ref type="table">Table 1</ref> reports the errors for the activities "Walking", "Eating", "Smoking" and "Discussion", which have been the focus of the comparisons in the literature. It also provides the results for the other 11 activities and the average over the 15 activities. Note that we outperform all the baselines on average. We provide qualitative comparisons in <ref type="figure" target="#fig_1">Fig. 3</ref>. They further evidence that our predictions are closer to the ground truth than that of the baselines for all 3 actions. More visualizations are included in the supplementary material.</p><p>To analyze the failure cases of our approach, such as for "Phoning", we converted the predicted angles to 3D coordinates so as to visualize the poses. We were then surprised to realize that a high error in angle space did not necessarily translate to a high error in 3D space. This is due to the fact that the angle representation is ambiguous, and thus two very different sets of angles can yield the same pose. To evidence this, in <ref type="figure" target="#fig_2">Fig. 4</ref>, we plot the angle error for three methods, including ours, on the same sequence, as well as the corresponding 3D errors obtained by simply converting the angles to 3D coordinates. Note that, while all three methods have comparable errors in angle space, two of them, including ours, have a much lower error than the third one in 3D space. This makes us argue that angles are not a good representation to evaluate motion prediction.</p><p>Motivated by this observation, in <ref type="table">Table 2</ref>, we report the 3D errors for short-term prediction on H3.6M. As mentioned before, there are two ways to achieve this: Converting the predicted angles to 3D or directly training the models on 3D coordinates. We report the results of both strategies. Note that, having access to neither the code nor  <ref type="bibr" target="#b16">[17]</ref> yields a much higher one in 3D. This is also reflected by the qualitative comparison in (b). In the predictions of <ref type="bibr" target="#b16">[17]</ref> (2nd row), the 3D location of the right hand and left leg are too high and far away from the ground truth, leading to unrealistic poses. By contrast, the predictions of <ref type="bibr" target="#b15">[16]</ref> (3rd row) and our method (last row) are closer to the ground truth.</p><p>the angle predictions of <ref type="bibr" target="#b8">[9]</ref>, we are unable to provide the 3D results for this method. When considering the remaining baselines, our approach consistently outperforms them, yielding the best results when directly using the 3D information (via the DCT) during training.</p><p>In <ref type="table" target="#tab_2">Table 3</ref>, we report the long-term prediction errors on H3.6M in angle space and 3D space. In angle space, our approach yields the best results for 500ms, but a higher error than that of <ref type="bibr" target="#b8">[9]</ref> for 1000ms. Note that, based on our previous analysis, it <ref type="table" target="#tab_2">Walking   Eating  Smoking  Discussion  Average  dct padding resi  80  160 320 400  80  160 320 400  80  160 320 400  80  160 320 400  80  160 320</ref>   <ref type="table">Table 6</ref>. Influence of the DCT representation, the padding strategy, and the residual connections on 4 actions of H3.6M. Top: angle error; Bottom: 3D error (Models are trained on 3D). Note that, on average, all components of our model contribute to its accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Walking</head><p>Eating <ref type="table" target="#tab_2">Smoking  Discussion  Average  80  160 320 400  80  160 320 400  80  160 320 400  80  160 320 400  80  160 320</ref>    <ref type="table">Table 7</ref>. Influence of GCNs and of learning the graph connectivity. Top: angle error; Bottom: 3D error. Note that GCNs with a pre-defined connectivity yield much higher errors than learning this connectivity as we do.</p><p>is unclear if this is due to actual worse predictions or to the ambiguities of the angle representation. In terms of 3D errors, as shown in <ref type="table" target="#tab_2">Table 3</ref>, our approach yields the best results by a large margin, particularly when trained using 3D coordinates. CMU-Mocap &amp; 3DPW. We report the results on the CMU dataset in terms of angle errors and 3D errors in <ref type="table">Table 4</ref>, and those on the 3DPW in <ref type="table" target="#tab_4">Table 5</ref>. In essence, the conclusions remain unchanged: Our method consistently outperforms the baselines for both short-term and long-term prediction, with the best results obtained when working directly with the 3D representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To provide a deeper understanding of our approach, we now evaluate the influence of its several components. In particular, we investigate the importance of relying on the DCT to represent the temporal information. To this end, we compare our approach with a graph convolutional network trained using the joint angles or 3D coordinates directly as input. Furthermore, we study the influence of padding the input sequence with replicates of the last observed time step, instead of simply taking a shorter sequence as input, and the impact of using residual connections in our network.</p><p>The results of these different experiments are provided in <ref type="table">Table 6</ref>. These results show that using our padding strategy provides a significant boost in accuracy, and so do the residual connections. In angle space, the influence of the DCT representation is sometimes small, but it remains important for some activities, such as "Discussion". By contrast, in 3D space, using the DCT representation yields significantly better results in all cases.</p><p>Finally, we evaluate the importance of using GCNs vs fully-connected networks and of learning the connectivity in the GCN instead of using a pre-defined adjacency matrix based on the kinematic tree. The results of these experiments, provided in <ref type="table">Table 7</ref>, demonstrate the benefits of both using GCNs and learning the corresponding graph structure. Altogether, this ablation study evidences the importance of both aspects of our contribution: Using the DCT to model temporal information and learning the connectivity in GCNs to model spatial structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have introduced an approach to human motion prediction that jointly encodes temporal information, via the use of the DCT, and spatial structure, via GCNs with learnable connectivity. This leads to a compact, feed-forward network with proven highly effectiveness for the prediction task. Our approach achieves state-of-the-art results on standard human motion prediction benchmarks. Experiments have also revealed an interesting phenomenon: evaluating motion prediction in angle space is unreliable, as the angle representation has ambiguities such that two very different sets of angles can share the same 3D pose. We thus argue that, in contrast to the main trend in the literature, motion prediction should be performed in 3D space. This was confirmed by our experiments, in which the models trained on 3D coordinates consistently outperform those trained on angles. Our future work will focus on a systematic analysis of this phenomenon. 1 Australian National University, 2 CVLab, EPFL, 3 Australia Centre for Robotic Vision {wei.mao, miaomiao.liu, hongdong.li}@anu.edu.au, mathieu.salzmann@epfl.ch</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Datasets</head><p>Below, we provide more detail on the datasets used in our experiments. Human3.6M. In H3.6M, each pose has 32 joints. Removing the global rotation, translation and constant angles, leaves us with a 48 dimensional vector for each human motion, denoting the exponential map representation of the joint angles. Furthermore, a 3D human pose can also be represented by a 66 dimensional vector of 3D coordinates after removing the global rotation, translation and stationary joints across time. We use the same training and test split as previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9]</ref>. That is, we test our model on the same image sequence of subject 5 as previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9]</ref>. For training, we keep subject 11 as validation set to choose the best model (the one that achieves the least average error across all future frames) and use the remaining 5 subjects as training set. CMU-Mocap. In this dataset, we use a 64 dimensional vector to represent every human pose by removing the global rotation, translation and joint angles with constant values. Each component of the vector denotes the exponential map representation of the joint angle. We further use 75 dimensional vectors for the 3D joint coordinate representation. We do not use a validation set due to limited training data. 3DPW. The human skeleton in this dataset uses 24 joints, yielding a 72 dimensional vector for the angle representation. For the 3D joint coordinate one, we obtain a 69 dimensional vector after removing the global translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Visualizing the Results on H3.6M in Video</head><p>We provide more visualization of the results on H3.6M in a video (See the supplementary video). In particular, the video compares our approach with the state of the art on periodic actions, such as walking, and aperiodic ones, such as eating and direction. Our approach shows better performance than the state-of-the-art ones.</p><p>Furthermore, in the video, we provide additional (quantitative and qualitative) visualization of cases where large er-  <ref type="bibr" target="#b16">[17]</ref>, results of <ref type="bibr" target="#b15">[16]</ref> and our results. The highlighted results in the box show that we can make better predictions on the legs and arms of the subject.</p><p>rors are observed according to the angle representation but small errors in 3D space. This confirms that ambiguities exist in angle space for human motion prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Visualizing the Results on CMU-Mocap</head><p>We provide a qualitative visualisation of the 3D human pose prediction on the "basketball", "basketball signal" and "direction traffic" actions of the CMU-Mocap dataset in <ref type="figure" target="#fig_1">Fig. 1, Fig. 2 and Fig. 3</ref>, respectively. Again, our approach outperforms the state-of-the-art ones (see highlighted poses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Number of DCT Coefficients</head><p>In this section, we first present the intuition behind using fewer DCT coefficients to represent the whole sequence. We then compare the performance of using different number of DCT coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Using Fewer Coefficients</head><p>Given a smooth trajectory, it is possible to discard some high frequency DCT coefficients without losing prediction   accuracy. To evidence this, in <ref type="figure" target="#fig_2">Fig. 4</ref>, we show the effect of the number of DCT components in reconstructing a sequence of 35 frames for the one human joint predicted using our approach. Note that, since we use 35 frames, 35 DCT coefficients yield a lossless reconstruction. Nevertheless, even 10 DCT coefficients are enough to reconstruct the trajectory with a very low error. This is due to the smoothness of the joint trajectory in 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on H3.6M</head><p>Experiment setup. Based on the previous discussion, we perform more experiments to evaluate the influence of the number of input DCT coefficients on human motion prediction. In the following experiments, we assume that we observe 10 frames to predict the future 25 frames. Following the same formulation as in our submission, the observed sequence is padded with the last observed frame replicated 25 times and then transformed to DCT coefficients. The target is the DCT coefficients of the whole sequence (35 frames). We perform several experiments by preserving different number of DCT coefficients. For instance, 'dct n=5' means that we only use the first 5 DCT coefficients for temporal reconstruction. The experiments are performed on both 3D and angle representation. <ref type="figure">Fig. 6</ref> shows the error for short-term prediction at 160ms and long-term prediction at 560ms in angle representation as a function of the number of DCT coefficients. In general, the angle error decreases with the increase of number of DCT coefficients. Similarly, in <ref type="figure">Fig. 7</ref>, we plot the motion prediction error in 3D coordinates at 160ms and 560ms as a function of the number of DCT coefficients. Here, 10 DCT coefficients already give a very small prediction error. Interestingly, when we use more DCT coefficients, the average error sometimes increases (see the plot for prediction at 560ms). This pattern confirms our argument in the submission that the use of truncated DCT coefficients can prevent a model from generating jittery motion, because the 3D coordinate representation of human motion trajectory is smooth.</p><p>To analyse the different patterns of the prediction error w.r.t. the number of DCT coefficients shown in angle representation ( <ref type="figure">Fig. 6</ref>) and 3D representation ( <ref type="figure">Fig. 7)</ref>, we looked into the dataset and found that there are large discontinuities in the trajectories of angles. As shown in <ref type="figure">Fig. 8</ref>, these large jumps make the reconstruction of trajectories with fewer DCT coefficients lossy.</p><p>In summary, we can discard some of the high frequency coefficients to achieve better performance in 3D space. In our experiments, we use the first 15 DCT coefficients as input to our network for short-term prediction and 30 coefficients for long-term prediction in 3D space. As the joint trajectory in angle representation is not smooth and has large discontinuities, we therefore take the full frequency as input to our network for motion prediction in angle representation. In our experiments, we therefore use 20 DCT coefficients as input to our network for short-term prediction and 35 for long-term prediction in angle representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study Details</head><p>Fully-connected Network. In our ablation study, we also compare the motion prediction using a graph network  <ref type="figure">Figure 6</ref>. Average angle prediction error over 4 actions ("walking","eating","smoking","discussion") using different number of DCT coefficients at 160ms (blue) and 560ms (red). <ref type="figure">Figure 7</ref>. Average 3D prediction error over 4 actions ("walking","eating","smoking","discussion") using different number of DCT coefficients at 160ms (blue) and 560ms (red).</p><p>with that of a fully-connected network structure. We apply the same process of encoding temporal information via the DCT. Before being fed to the network, the DCT coefficients of the past sequence padded with last frame are flattened to a vector and the network learns the residual between the past temporal encoding and the future one. To this end, we adopt the network structure shown in <ref type="figure" target="#fig_9">Fig. 5</ref>. Instead of using graph convolutional layers, we rely on 2 fully connected layers with residual connections. We additionally use two fully connected layers at the start of the network for encod- <ref type="figure">Figure 8</ref>. The temporal trajectory of one joint angle reconstructed using different number of DCT coefficients. Note that the trajectory is not smooth and has large jumps. The full frequency (35 DCT coefficients) leads to lossless temporal reconstruction of the trajectory.</p><p>ing the DCT coefficients and at the end for decoding the feature to the residual of the DCT coefficient.</p><p>The implementation details for this network are the same as our Graph Convolutional Network. We implemented this network using Pytorch <ref type="bibr" target="#b19">[20]</ref>, and we used ADAM <ref type="bibr" target="#b11">[12]</ref> to train this model. The learning rate was set to 0.0005 with a 0.96 decay every two epochs. The batch size was set to 16 and the gradients were clipped to a maximum 2-norm of 1. The model was trained for 50 epochs. As reported in the submission, the fully-connected network structure cannot learn a better representation than the Graph Convolutional Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Mean Pose Problem</head><p>As explained in <ref type="bibr" target="#b15">[16]</ref>, the mean pose problem typically occurs when using recurrent neural networks (RNNs) to encode temporal dynamics, where the past information may vanish during long propagation paths. By not relying on RNNs, but directly encoding the trajectory of the whole sequence, our method inherently prevents us from losing the past information. This is evidenced by <ref type="figure">Fig. 9</ref>, where our method yields poses significantly further from the mean pose than the RNN-based method <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr">Figure 9</ref>. Prediction up to 4 seconds for the Phoning action of Hu-man3.6m. From top to bottom, we show the ground truth, the poses predicted by <ref type="bibr" target="#b16">[17]</ref> , and by our method. Note that, after the highlighted frame, the poses predicted by the RNN of <ref type="bibr" target="#b16">[17]</ref> have indeed converged to the mean pose (shown in the last column), whereas in our predictions the legs continue to move.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative comparison of short-term ("Smoking" and "Walking") and long-term ("Walking Dog") predictions on H3.6M. From top to bottom, we show the ground truth, and the results of Residual sup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Drawbacks of the angle-based representation. (a) Joint angle error (top) and 3D position error (bottom) for each predicted frame on the Phoning H3.6M action. While all methods have a similar error in angle space, Residual sup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>400 0.20 0.33 0.52 0.59 0.17 0.30 0.50 0.62 0.22 0.41 0.83 0.78 0.24 0.60 0.91 0.97 0.21 0.41 0.69 0.74 0.34 0.46 0.65 0.71 0.33 0.44 0.63 0.76 0.47 0.60 0.94 0.95 0.40 0.70 0.95 1.00 0.39 0.55 0.79 0.86 0.25 0.41 0.62 0.69 0.26 0.39 0.60 0.73 0.31 0.49 0.89 0.89 0.34 0.72 0.97 1.02 0.29 0.50 0.77 0.83 0.18 0.31 0.49 0.56 0.16 0.29 0.50 0.62 0.22 0.41 0.86 0.80 0.20 0.51 0.77 0.85 0.19 0.38 0.66 0.71 11.4 19.5 32.9 38.3 10.6 21.4 41.1 48.0 9.4 16.7 27.2 32.2 14.1 29.6 49.9 54.1 11.4 21.8 37.8 43.1 19.1 24.7 37.3 41.5 24.7 30.4 48.6 55.8 40.5 41.0 48.9 53.0 22.6 29.9 46.7 51.3 26.7 31.5 45.4 50.4 18.3 25.9 39.7 43.7 20.1 29.4 48.8 56.7 29.0 34.2 43.8 49.3 23.3 31.2 46.8 51.0 22.7 30.2 44.8 50.2 8.9 15.7 29.2 33.4 8.8 18.9 39.4 47.2 7.8 14.9 25.3 28.7 9.8 22.1 39.6 44.1 8.8 17.9 33.4 38.4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>400 Fully-connected network 0.20 0.34 0.54 0.61 0.18 0.31 0.53 0.66 0.22 0.43 0.85 0.83 0.28 0.64 0.87 0.93 0.22 0.43 0.70 0.76 with pre-defined connectivity 0.25 0.46 0.70 0.8 0.23 0.41 0.68 0.83 0.24 0.46 0.93 0.91 0.27 0.62 0.89 0.97 0.25 0.49 0.80 0.88 with learnable connectivity 0.18 0.31 0.49 0.56 0.16 0.29 0.50 0.62 0.22 0.41 0.86 0.80 0.20 0.51 0.77 0.85 0.19 0.38 0.66 0.71 Fully-connected network 11.2 18.6 33.5 38.8 9.0 18.8 39.0 48.0 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 .</head><label>1</label><figDesc>Motion prediction in 3D space on the "basketball action of CMU-Mocap. From top to bottom: Ground truth, results of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .</head><label>2</label><figDesc>Motion prediction in 3D space on the "basketball signal" action of CMU-Mocap. From top to bottom: Ground truth, results of<ref type="bibr" target="#b16">[17]</ref>, results of<ref type="bibr" target="#b15">[16]</ref> and our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Motion prediction in 3D space on the "directing traffic" action of CMU-Mocap. From top to bottom: Ground truth, results of<ref type="bibr" target="#b16">[17]</ref>, results of<ref type="bibr" target="#b15">[16]</ref> and our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Temporal trajectory of the x coordinate of one joint reconstructed using different number of DCT coefficients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Fully-connected Network Structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b16">[17]</ref> 0.39 0.68 0.99 1.15 0.27 0.48 0.73 0.86 0.26 0.48 0.97 0.95 0.31 0.67 0.94 1.04 Residual sup.<ref type="bibr" target="#b16">[17]</ref> 0.28 0.49 0.72 0.81 0.23 0.39 0.62 0.76 0.33 0.61 1.05 1.15 0.31 0.68 1.01 1.09 convSeq2Seq<ref type="bibr" target="#b15">[16]</ref> 0.33 0.54 0.68 0.73 0.22 0.36 0.58 0.71 0.26 0.49 0.96 0.92 0.32 0.67 0.94 1.01 AGED w/o adv [9] 0.28 0.42 0.66 0.73 0.22 0.35 0.61 0.74 0.3 0.55 0.98 0.99 0.30 0.63 0.97 1.06 AGED w/adv [9] 0.22 0.36 0.55 0.67 0.17 0.28 0.51 0.64 0.27 0.43 0.82 0.84 0.27 0.56 0.76 0.83 ours 0.18 0.31 0.49 0.56 0.16 0.29 0.50 0.62 0.22 0.41 0.86 0.80 0.20 0.51 0.77 0.85 .39 0.59 0.79 0.89 0.54 0.89 1.30 1.49 0.64 1.21 1.65 1.83 0.28 0.57 1.13 1.37 0.62 0.88 1.19 1.27 0.40 1.63 1.02 1.18 Residual sup. [17] 0.26 0.47 0.72 0.84 0.75 1.17 1.74 1.83 0.23 0.43 0.69 0.82 0.36 0.71 1.22 1.48 0.51 0.97 1.07 1.16 0.41 1.05 1.49 1.63 convSeq2Seq [16] 0.39 0.60 0.80 0.91 0.51 0.82 1.21 1.38 0.59 1.13 1.51 1.65 0.29 0.60 1.12 1.37 0.63 0.91 1.19 1.29 0.39 0.61 1.02 1.18 AGED w/o adv [9] 0.26 0.46 0.71 0.81 0.61 0.95 1.44 1.61 0.23 0.42 0.61 0.79 0.34 0.70 1.19 1.40 0.46 0.89 1.06 1.11 0.46 0.87 1.23 1.51 AGED w/adv [9] 0.23 0.39 0.63 0.69 0.56 0.81 1.30 1.46 0.19 0.34 0.50 0.68 0.31 0.58 1.12 1.34 0.46 0.78 1.01 1.07 0.41 0.76 1.05 1.19 Ours 0.26 0.45 0.71 0.79 0.36 0.60 0.95 1.13 0.53 1.02 1.35 1.48 0.19 0.44 1.01 1.24 0.43 0.65 1.05 1.13 0.29 0.45 0.80 0.97 .39 0.74 1.07 1.19 0.25 0.51 0.79 0.92 0.34 0.67 1.22 1.47 0.60 0.98 1.36 1.50 0.33 0.66 0.94 0.99 0.40 0.78 1.07 1.21 Residual sup. [17] 0.39 0.81 1.40 1.62 0.24 0.51 0.90 1.05 0.28 0.53 1.02 1.14 0.56 0.91 1.26 1.40 0.31 0.58 0.87 0.91 0.36 0.67 1.02 1.15 convSeq2Seq [16] 0.41 0.78 1.16 1.31 0.23 0.49 0.88 1.06 0.30 0.62 1.09 1.30 0.59 1.00 1.32 1.44 0.27 0.52 0.71 0.74 0.38 0.68 1.01 1.13 AGED w/o adv [9] 0.38 0.77 1.18 1.41 0.24 0.52 0.92 1.01 0.31 0.64 1.08 1.12 0.51 0.87 1.21 1.33 0.29 0.51 0.72 0.75 0.32 0.62 0.96 1.Residual sup. 3D[17] 23.8 40.4 62.9 70.9 17.6 34.7 71.9 87.7 19.7 36.6 61.8 73.9 31.7 61.3 96.0 103.5 convSeq2Seq [16] 21.8 37.5 55.9 63.0 13.3 24.5 48.6 60.0 15.4 25.5 39.3 44.5 23.6 43.6 68.4 74.9 convSeq2Seq 3D [16] 17.1 31.2 53.8 61.5 13.7 25.9 52.5 63.3 11.1 21.0 33.4 38.3 18.9 39.3 67.7 75.Residual sup. 3D [17] 28.6 55.3 101.6 118.9 23.6 47.4 94.0 112.7 29.5 60.5 119.9 140.6 60.5 101.9 160.8 188.3 23.5 45.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Walking</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Eating</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Smoking</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Discussion</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">milliseconds</cell><cell>80</cell><cell cols="4">160 320 400</cell><cell>80</cell><cell></cell><cell cols="3">160 320 400</cell><cell></cell><cell>80</cell><cell cols="3">160 320 400</cell><cell></cell><cell>80</cell><cell cols="4">160 320 400</cell><cell></cell></row><row><cell cols="4">zero-velocity Directions</cell><cell></cell><cell></cell><cell cols="2">Greeting</cell><cell></cell><cell></cell><cell cols="2">Phoning</cell><cell></cell><cell></cell><cell cols="2">Posing</cell><cell></cell><cell></cell><cell cols="2">Purchases</cell><cell></cell><cell></cell><cell cols="2">Sitting</cell><cell></cell></row><row><cell>milliseconds</cell><cell>80</cell><cell cols="3">160 320 400</cell><cell>80</cell><cell cols="3">160 320 400</cell><cell>80</cell><cell cols="3">160 320 400</cell><cell>80</cell><cell cols="3">160 320 400</cell><cell>80</cell><cell cols="3">160 320 400</cell><cell>80</cell><cell cols="3">160 320 400</cell></row><row><cell cols="4">zero-velocity [17] 0Sitting Down</cell><cell></cell><cell></cell><cell cols="2">Taking Photo</cell><cell></cell><cell></cell><cell cols="2">Waiting</cell><cell></cell><cell></cell><cell cols="2">Walking Dog</cell><cell></cell><cell cols="4">Walking Together</cell><cell></cell><cell cols="2">Average</cell><cell></cell></row><row><cell>milliseconds</cell><cell>80</cell><cell cols="3">160 320 400</cell><cell>80</cell><cell cols="3">160 320 400</cell><cell>80</cell><cell cols="3">160 320 400</cell><cell>80</cell><cell cols="3">160 320 400</cell><cell>80</cell><cell cols="3">160 320 400</cell><cell>80</cell><cell cols="3">160 320 400</cell></row><row><cell cols="25">zero-velocity [17] 007</cell></row><row><cell>AGED w/adv [9]</cell><cell cols="24">0.33 0.62 0.98 1.1 0.23 0.48 0.81 0.95 0.24 0.50 1.02 1.13 0.50 0.81 1.15 1.27 0.23 0.41 0.56 0.62 0.31 0.54 0.85 0.97</cell></row><row><cell>Ours</cell><cell cols="24">0.30 0.61 0.90 1.00 0.14 0.34 0.58 0.70 0.23 0.50 0.91 1.14 0.46 0.79 1.12 1.29 0.15 0.34 0.52 0.57 0.27 0.51 0.83 0.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Walking</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Eating</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Smoking</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Discussion</cell><cell></cell><cell></cell></row><row><cell cols="2">milliseconds</cell><cell></cell><cell>80</cell><cell cols="4">160 320 400</cell><cell>80</cell><cell></cell><cell cols="4">160 320 400</cell><cell>80</cell><cell cols="4">160 320 400</cell><cell>80</cell><cell cols="3">160 320</cell><cell>400</cell><cell></cell></row><row><cell cols="3">Residual sup. [17]</cell><cell cols="21">21.7 38.1 58.9 68.8 15.1 28.6 54.8 67.4 20.8 39.0 66.1 76.1 26.2 51.2 85.8 94.6</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Directions</cell><cell></cell><cell></cell><cell cols="2">Greeting</cell><cell></cell><cell></cell><cell cols="2">Phoning</cell><cell></cell><cell></cell><cell cols="2">Posing</cell><cell></cell><cell></cell><cell cols="2">Purchases</cell><cell></cell><cell></cell><cell cols="2">Sitting</cell><cell></cell></row><row><cell>milliseconds</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell></row><row><cell>Residual sup.[17]</cell><cell cols="3">27.9 44.8 63.5</cell><cell cols="21">78.6</cell></row><row><cell>convSeq2Seq[16]</cell><cell cols="3">26.7 43.3 59.0</cell><cell cols="8">72.4 30.4 58.6 110.0 122.8 22.4 38.4 65.0</cell><cell cols="3">75.4 22.4 42.1</cell><cell cols="5">87.3 106.1 28.4 53.8 82.1</cell><cell cols="5">93.1 24.7 50.0 88.6 100.4</cell></row><row><cell cols="4">convSeq2Seq 3D [16] 22.0 37.2 59.6</cell><cell cols="8">73.4 24.5 46.2 90.0 103.1 17.2 29.7 53.4</cell><cell cols="3">61.3 16.1 35.6</cell><cell cols="5">86.2 105.6 29.4 54.9 82.2</cell><cell cols="4">93.0 19.8 42.4 77.0</cell><cell>88.4</cell></row><row><cell>Ours</cell><cell cols="3">11.2 23.2 52.7</cell><cell cols="4">64.1 14.2 27.7 67.1</cell><cell cols="4">82.9 13.5 22.5 45.2</cell><cell cols="3">52.4 11.1 27.1</cell><cell>69.4</cell><cell cols="4">86.2 20.4 42.8 69.1</cell><cell cols="4">78.3 11.7 27.0 55.9</cell><cell>66.9</cell></row><row><cell>Ours 3D</cell><cell cols="3">12.6 24.4 48.2</cell><cell cols="4">58.4 14.5 30.5 74.2</cell><cell cols="4">89.0 11.5 20.2 37.9</cell><cell>43.2</cell><cell>9.4</cell><cell>23.9</cell><cell>66.2</cell><cell cols="4">82.9 19.6 38.5 64.4</cell><cell cols="4">72.2 10.7 24.6 50.6</cell><cell>62.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Sitting Down</cell><cell></cell><cell></cell><cell cols="2">Taking Photo</cell><cell></cell><cell></cell><cell cols="2">Waiting</cell><cell></cell><cell></cell><cell cols="2">Walking Dog</cell><cell></cell><cell cols="4">Walking Together</cell><cell></cell><cell cols="2">Average</cell><cell></cell></row><row><cell>milliseconds</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell></row><row><cell>Residual sup. [17]</cell><cell cols="3">33.0 64.1 121.7</cell><cell>146</cell><cell cols="3">21.2 40.3 72.2</cell><cell cols="12">86.2 24.9 50.0 96.5 114.0 53.8 90.9 134.6 156.9 19.7 38.2 62.9</cell><cell cols="5">72.3 27.9 51.6 88.9 103.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0 71.3</cell><cell cols="5">82.8 30.8 57.0 99.8 115.5</cell></row><row><cell>convSeq2Seq [16]</cell><cell cols="3">23.9 39.9 74.6</cell><cell cols="4">89.8 18.4 32.1 60.3</cell><cell cols="12">72.5 24.9 50.2 101.6 120.0 56.4 94.9 136.1 156.3 21.1 38.5 61.0</cell><cell cols="4">70.4 24.9 44.9 75.9</cell><cell>88.1</cell></row><row><cell cols="4">convSeq2Seq 3D [16] 17.1 34.9 66.3</cell><cell cols="4">77.7 14.0 27.2 53.8</cell><cell cols="4">66.2 17.9 36.5 74.9</cell><cell cols="8">90.7 40.6 74.7 116.6 138.7 15.0 29.9 54.3</cell><cell cols="4">65.8 19.6 37.8 68.1</cell><cell>80.2</cell></row><row><cell>Ours</cell><cell cols="3">11.5 25.4 53.9</cell><cell>65.6</cell><cell cols="3">8.3 15.8 38.5</cell><cell cols="4">49.1 12.1 27.5 67.3</cell><cell cols="8">85.6 35.8 63.6 106.7 126.8 11.7 23.5 46.0</cell><cell cols="4">53.5 13.5 27.0 54.2</cell><cell>65.0</cell></row><row><cell>Ours 3D</cell><cell cols="3">11.4 27.6 56.4</cell><cell>67.6</cell><cell cols="3">6.8 15.2 38.2</cell><cell>49.6</cell><cell cols="3">9.5 22.0 57.5</cell><cell cols="8">73.9 32.2 58.0 102.2 122.7 8.9 18.4 35.3</cell><cell cols="4">44.3 12.1 25.0 51.0</cell><cell>61.3</cell></row></table><note>Table 1. Short-term prediction of joint angles on H3.6M for all actions. Our method outperforms the state of the art for most time horizons.7 Ours 11.1 19.0 32.0 39.1 9.2 19.5 40.3 48.9 9.2 16.6 26.1 29.0 11.3 23.7 41.9 46.6 Ours 3D 8.9 15.7 29.2 33.4 8.8 18.9 39.4 47.2 7.8 14.9 25.3 28.7 9.8 22.1 39.6 44.12 29.3 56.0 110.2 125.6 28.7 50.9 88.0 99.7 30.5 59.4 118.7 144.7 33.3 58.2 85.4 93.7 32.6 65.2 113.7 126.2 Residual sup. 3D [17] 36.5 56.4 81.5 97.3 37.9 74.1 139.0 158.8 25.6 44.4 74.0 84.2 27.9 54.7 131.3 160.8 40.8 71.8 104.2 109.8 34.5 69.9 126.3 141.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Long-term prediction of joint angles (top) and 3D joint positions (bottom) on H3.6M.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, we compare our results to those .80  1.27 1.45 1.78 0.41 0.76 1.32 1.54 2.15 0.33 0.59 0.93 1.10 2.05 0.56 0.88 1.77 2.02 2.4 0.33 0.50 0.66 0.75 1.00 convSeq2Seq [16] 0.37 0.62 1.07 1.18 1.95 0.32 0.59 1.04 1.24 1.96 0.25 0.56 0.89 1.00 2.04 0.39 0.6 1.36 1.56 2.01 0.28 0.41 0.52 0.57 0.67 Ours 0.33 0.52 0.89 1.06 1.71 0.11 0.20 0.41 0.53 1.00 0.15 0.32 0.52 0.60 2.00 0.31 0.49 1.23 1.39 1.80 0.33 0.55 0.73 0.74 0.95 .51 0.88 0.99 1.72 0.35 0.47 0.60 0.65 0.88 0.30 0.46 0.72 0.91 1.36 0.38 0.62 1.02 1.18 1.67 convSeq2Seq [16] 0.26 0.44 0.75 0.87 1.56 0.35 0.44 0.45 0.50 0.78 0.30 0.47 0.80 1.01 1.39 0.32 0.52 0.86 3D[17] 18.4 33.8 59.5 70.5 106.7 12.7 23.8 40.3 46.7 77.5 15.2 29.6 55.1 66.1 127.1 36.0 68.7 125.0 145.5 195.5 15.6 19.4 31.2 36.2 43.3 convSeq2Seq 3D[16] 16.7 30.5 53.8 64.3 91.5 8.4 16.2 30.8 37.8 76.5 10.6 20.3 38.7 48.4 115.5 22.4 44.0 87.5 106.3 162.6 14.3 16.3 18.0 20.2 27.5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Basketball</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Basketball Signal</cell><cell></cell><cell>Directing Traffic</cell><cell></cell><cell></cell><cell>Jumping</cell><cell>Running</cell></row><row><cell>milliseconds</cell><cell>80</cell><cell cols="4">160 320 400 1000</cell><cell>80</cell><cell cols="2">160 320 400 1000</cell><cell>80</cell><cell>160 320 400 1000</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>1000</cell><cell>80</cell><cell>160 320 400 1000</cell></row><row><cell>Residual sup. [17]</cell><cell cols="3">0.50 0Soccer</cell><cell></cell><cell></cell><cell></cell><cell>Walking</cell><cell></cell><cell></cell><cell>Washwindow</cell><cell></cell><cell></cell><cell>Average</cell></row><row><cell>milliseconds</cell><cell>80</cell><cell cols="4">160 320 400 1000</cell><cell>80</cell><cell cols="2">160 320 400 1000</cell><cell>80</cell><cell>160 320 400 1000</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>1000</cell></row><row><cell>Residual sup. [17]</cell><cell cols="14">0.29 00.99</cell><cell>1.55</cell></row><row><cell>Ours</cell><cell cols="13">0.18 0.29 0.61 0.71 1.40 0.33 0.45 0.49 0.53 0.61 0.22 0.33 0.57 0.75 1.20 0.25 0.39 0.68</cell><cell>0.79</cell><cell>1.33</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Basketball</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Basketball Signal</cell><cell></cell><cell>Directing Traffic</cell><cell></cell><cell></cell><cell>Jumping</cell><cell>Running</cell></row><row><cell>milliseconds</cell><cell>80</cell><cell cols="4">160 320 400 1000</cell><cell>80</cell><cell cols="2">160 320 400 1000</cell><cell>80</cell><cell>160 320 400 1000</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>1000</cell><cell>80</cell><cell>160 320 400 1000</cell></row><row><cell>Residual sup. Ours 3D</cell><cell cols="6">14.0 25.4 49.6 61.4 106.1 3.5</cell><cell cols="2">6.1 11.7 15.2 53.9</cell><cell cols="5">7.4 15.1 31.7 42.2 152.4 16.9 34.4 76.3</cell><cell>96.8 164.6 25.5 36.7 39.3 39.9 58.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Soccer</cell><cell></cell><cell></cell><cell></cell><cell>Walking</cell><cell></cell><cell></cell><cell>Washwindow</cell><cell></cell><cell></cell><cell>Average</cell></row><row><cell>milliseconds</cell><cell>80</cell><cell cols="4">160 320 400 1000</cell><cell>80</cell><cell cols="2">160 320 400 1000</cell><cell>80</cell><cell>160 320 400 1000</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>1000</cell></row><row><cell cols="4">Residual sup. 3D[17] 20.3 39.5 71.3</cell><cell>84</cell><cell cols="4">129.6 8.2 13.7 21.9 24.5 32.2</cell><cell cols="5">8.4 15.8 29.3 35.4 61.1 16.8 30.5 54.2</cell><cell>63.6</cell><cell>96.6</cell></row><row><cell cols="6">convSeq2Seq 3D[16] 12.1 21.8 41.9 52.9 94.6</cell><cell cols="3">7.6 12.5 23.0 27.5 49.8</cell><cell cols="5">8.2 15.9 32.1 39.9 58.9 12.5 22.2 40.7</cell><cell>49.7</cell><cell>84.6</cell></row><row><cell>Ours 3D</cell><cell cols="8">11.3 21.5 44.2 55.8 117.5 7.7 11.8 19.4 23.1 40.2</cell><cell cols="5">5.9 11.9 30.3 40.0 79.3 11.5 20.4 37.8</cell><cell>46.8</cell><cell>96.5</cell></row><row><cell></cell><cell cols="14">Table 4. Short and long-term prediction of joint angles (top) and 3D joint positions (bottom) on CMU-Mocap.</cell></row><row><cell cols="2">milliseconds</cell><cell></cell><cell>200</cell><cell></cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Residual sup. [17]</cell><cell>1.85</cell><cell></cell><cell>2.37</cell><cell>2.46</cell><cell>2.51</cell><cell>2.53</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">convSeq2Seq [16]</cell><cell>1.24</cell><cell></cell><cell>1.85</cell><cell>2.13</cell><cell>2.23</cell><cell>2.26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ours</cell><cell></cell><cell>0.64</cell><cell></cell><cell>0.95</cell><cell>1.12</cell><cell>1.22</cell><cell>1.27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Residual sup. 3D [17] 113.9 173.1 191.9 201.1 210.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">convSeq2Seq 3D [16] 71.6 124.9 155.4 174.7 187.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ours 3D</cell><cell></cell><cell>35.6</cell><cell></cell><cell>67.8</cell><cell cols="3">90.6 106.9 117.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>Short-term and long-term prediction of joint angle (top) and 3D joint positions (bottom) on 3DPW.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the geodesic loss of<ref type="bibr" target="#b8">[9]</ref> does not apply to 3D space.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported in part by the Australia Centre for Robotic Vision (CE140100016), the Australia Research Council DECRA Fellowship (DE180100628), ARC Discovery Grant (DP190102261) and LIEF(LE190100080). The authors would like to thank NVIDIA for the donated GPU (Titan V) and the GPU cluster in NCI Australia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonrigid structure from motion in trajectory space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohaib</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Style machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Butepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-hypothesis motion planning for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Likhachev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="619" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial geometry-aware human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Liang-Yan Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Anticipating human activities for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Hema Swetha Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<meeting><address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">2071</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wee Sun Lee, and Gim Hee Lee. Convolutional sequence to sequence model for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of motion planning and control techniques for self-driving urban vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Paden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sze Zheng</forename><surname>Michalčáp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Yershov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frazzoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on intelligent vehicles</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="55" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
