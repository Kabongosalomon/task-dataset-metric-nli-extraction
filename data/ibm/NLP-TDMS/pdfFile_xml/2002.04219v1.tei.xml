<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Thermal to Visible Face Recognition Using Deep Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alperen</forename><surname>Kantarcı</surname></persName>
							<email>kantarcia@itu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Istanbul Technical University Istanbul</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazım</forename><surname>Kemal Ekenel</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Istanbul Technical University Istanbul</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Thermal to Visible Face Recognition Using Deep Autoencoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional neural networks</term>
					<term>autoencoders</term>
					<term>heterogeneous face recognition</term>
					<term>thermal to visible matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visible face recognition systems achieve nearly perfect recognition accuracies using deep learning. However, in lack of light, these systems perform poorly. A way to deal with this problem is thermal to visible cross-domain face matching. This is a desired technology because of its usefulness in night time surveillance. Nevertheless, due to differences between two domains, it is a very challenging face recognition problem. In this paper, we present a deep autoencoder based system to learn the mapping between visible and thermal face images. Also, we assess the impact of alignment in thermal to visible face recognition. For this purpose, we manually annotate the facial landmarks on the Carl and EURECOM datasets. The proposed approach is extensively tested on three publicly available datasets: Carl, UND-X1, and EURECOM. Experimental results show that the proposed approach improves the state-of-the-art significantly. We observe that alignment increases the performance by around 2%. Annotated facial landmark positions in this study can be downloaded from the following link: github.com/Alpkant/Thermal-to-Visible-Face-Recognition-Using-Deep-Autoencoders .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Face recognition has been a popular research area for both military and commercial purposes. Most of the face recognition technologies use visible spectrum images. In the past few years, many studies presented very high recognition accuracies. However, in the night time, thermal images are better options because thermal cameras capture heat waves of the objects without any illumination source. Night time surveillance is very crucial for the police forces and military operations in order to recognize the suspects. Most of the visible face recognition methods give poor performance under the illumination changes and low lighting. Therefore, face images that are taken at the night time lead to poor face recognition accuracies. Cross-domain face recognition is used between thermal and visible images to overcome this limitation. In thermal to visible face recognition, the task is to match the thermal probe image of a person with the visible image of the same person within the visible gallery. It is a challenging problem to match faces between thermal and visible domains due to the difference in appearance characteristics of these domains. Moreover, most thermal cameras capture low resolution images, whereas visible images are high resolution, and this increases the domain gap.</p><p>In this paper, we aim at learning a mapping between thermal and visible domains using a deep autoencoder model. We use three thermal-visible datasets to evaluate the performance of the proposed system. In addition, we manually annotated six facial landmarks on the Carl and EURECOM datasets in order to align the faces and assess alignment's impact on the performance. Our experiments show around 14% and 3.5% absolute increase in rank-1 accuracies on Carl <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and UND-X1 <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> datasets, respectively. We also present thermal to visible face recognition results on the recently collected EURECOM dataset <ref type="bibr" target="#b4">[5]</ref>.</p><p>The contributions of this paper can be summarized as follows: First, we have shown that deep convolutional autoencoders can learn non-linear mapping between thermal and visible images for cross domain face recognition task. Second, we analyze the impact of alignment for cross domain face recognition. Moreover, these produced annotations can also be used to develop facial landmark detection methods for thermal face images. Finally, we have improved the stateof-the-art face recognition accuracies on publicly available thermal-visible face datasets.</p><p>The remainder of this paper is organized as follows. In Section 2, we give a brief overview of the related work. Then, in Section 3, we explain the utilized model and the approach. In Section 4, details of the datasets, benchmarks, and experimental results are presented and discussed. The last section concludes the paper and gives directions for the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>One of the initial successful works before deep learning approaches for thermal to visible matching is presented in <ref type="bibr" target="#b5">[6]</ref>. In this study, the authors utilized partial least squares method. They investigated the effects of preprocessing and domain gap very profoundly. Mean filtering, geometric normalization, and Difference of Gaussians (DoG) filtering methods were employed and compared. We benefited from the proposed preprocessing methods in this paper.</p><p>One of the first and most successful approaches to the thermal to visible face recognition using neural networks is Deep Perceptual Mapping (DPM) <ref type="bibr" target="#b6">[7]</ref>. In the proposed system, the authors aim at learning a mapping between the thermal and Deep Perceptual Mapping network has been trained with the overlapping patch crops of the faces, which correspond to the same area of the face both in thermal and visible domain. Different from this study, in this paper, we use whole face images instead of small patches of the face, the feature vectors are generated using convolutional neural networks (CNN), and the mapping is learned using a deep convolutional autoencoder.</p><p>After the success of generative adversial networks (GANs) in image synthesis, researchers employed GANs to the thermal to visible face recognition task. TV-GAN <ref type="bibr" target="#b7">[8]</ref> is one of the most successful GAN based approaches to the problem. Aim of the TV-GAN is to transform thermal faces to visible light faces while preserving the identity information. Authors trained the discriminator to perform both binary classification of real or fake and perform closed-set face recognition task. SG-GAN <ref type="bibr" target="#b8">[9]</ref> synthesizes visible images from thermal images by regularizing the GAN training with semantic priors which are extracted by a face parsing network. Main novelty of this work is semantic loss function employed to regularize the training process to reduce the per-pixel loss value calculated between the synthesized visible image and thermal image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>To learn the mapping between the visible and thermal face images, we utilized a deep autoencoder architecture. Specifically, we benefited from the U-Net architecure <ref type="bibr" target="#b9">[10]</ref> and modified it for the purposes of this study.</p><p>Autoencoders are special types of neural networks that learn data embeddings in an unsupervised manner. Autoencoders are useful to encode the data to the latent code and retrieve the original data using this latent code, since their inputs are the same with their outputs. They have been used for different purposes, such as denoising, dimensionality reduction, and image reconstruction. One of the most well-known autoencoder architectures is the U-Net <ref type="bibr" target="#b9">[10]</ref>, which is proposed for the medical segmetation task.</p><p>Our network is based on the U-Net architecture <ref type="bibr" target="#b9">[10]</ref>. U-Net uses 1024 channels and 30 × 30 bottleneck layer. Since the amount of training samples is limited in thermal to visible face recognition, in order to decrease the number of trainable parameters, we use less channels. Our model has 512 channels and 14 × 14 pixels in the bottleneck layer. In the proposed approach, input images are visible face images and outputs are the face images from the LWIR band to learn a mapping between visible face images and thermal face images. The network takes 224 × 224 pixel resolution color images and the output thermal image has the same size as the input. Convolution filter size is 3 × 3 and window size of the max pooling is two, which is the same size as in U-Net <ref type="bibr" target="#b9">[10]</ref>. Activation function after each batch normalization layer is Rectified Linear Unit (ReLU). For the decoder, we use two different upsampling methods. First one is bilinear upsampling, which is a standard interpolation technique. As the number of trainable parameters decrease by using the bilinear upsampling, required GPU memory reduces and training time decreases. However, information for the decoding part is lost and it degrades the performance. Therefore, besides bilinear upsampling, we also employ up convolution with 2 × 2 filter size as proposed in the U-Net <ref type="bibr" target="#b9">[10]</ref>. The number of total trainable parameters of the up convolution version is 14,789,059. The loss function for this network is mean square loss, since the network tries to make its output as similar as possible to the ground truth thermal image. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASETS AND EXPERIMENTS</head><p>This section presents the datasets that have been used to assess the performance of the proposed approach, the experiments carried out, and the obtained results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Three thermal-visible datasets have been used in this paper. All of them are paired LWIR-visible face datasets. For the Carl and EURECOM datasets, we manually annotated six facial landmark points: two corners for each eye and mouth corners.</p><p>Carl Dataset: Carl dataset <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> contains 41 different subjects with NIR, LWIR, and visible paired images. We only use LWIR and visible images. In total, there are 4920 images, half of them are visible images and the other half are LWIR thermal images. We use the same benchmark settings proposed  <ref type="bibr" target="#b6">[7]</ref> in terms of train and test subject ratio. That is, we used images of 20 randomly chosen subjects as training data and the remaining 21 subjects as testing data. Training and testing subjects are completely different. All available thermal images of 21 subjects have been used as thermal probes which sums to 1260. There are three different visible gallery settings that have been used: one image per subject, two images per subject, and all available images of the subjects. So visible gallery contains 21, 42, and 1260 images, respectively.</p><p>UND-X1 Collection: University of Notre Dame X1 Collection dataset <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> contains 82 different subjects with LWIR and visible paired images. There are different expressions and images collected in different sessions. We follow the same benchmarking protocol presented in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b6">[7]</ref>. Training set contains 41 subjects and testing set contains remaining 41 subjects. However, for each subject there are different number of images in this dataset, unlike the other datasets. Visible gallery settings are the same with the ones that we use on the Carl dataset. For each setting the number of total visible images are 41, 82, and 1146.</p><p>EURECOM Dataset: EURECOM dataset has been recently presented in <ref type="bibr" target="#b4">[5]</ref>. It contains seven different facial variations, five different illumination conditions, five different types of occlusions, and four different head poses that belong to 50 subjects. In total, there are 2100 images in the dataset. We manually marked six facial landmarks also on this dataset. However, it is impossible to annotate some facial landmarks in different variations, therefore we excluded some of them. More specifically, we excluded the following variations: all lights off, eyeglasses, sunglasses, mouth occluded by hand, eye occluded by hand, and poses with 30 • left and right. In the end, for each subject, we have twelve images from each domain that sums up to 1200 images. Since we excluded some images and number of available images is relatively smaller than the other datasets, we chose 30 subjects to the training set and used remaining 20 subjects for testing. For testing, all available thermal images of the subjects are used as thermal probes, that sums up to 240 thermal images. For visible gallery image per subject settings are the same with the Carl and UND-X1 datasets. For the three settings 20, 40, and 240 images are used in the gallery, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preprocessing</head><p>Preprocessing of the images is an important task in thermal to visible face recognition. Due to the significant domain gap between the thermal and visible images, preprocessing methods would help to close the gap between these two domains and make it easier to learn the mapping between them. All the images pass through the same preprocessing steps shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. First 3 × 3 mean filter is applied in order to remove the dead pixels in the thermal images, then DoG filter is applied to enhance the edges and then both visible and thermal images are aligned according to the manually annotated facial landmarks. Because of the resolution difference between thermal images and visible images, both thermal and visible images are downsampled to 112 × 112 pixels which is closer to the resolution of the thermal image. Downsampling the high resolution image to lower resolution to match with the thermal image resolution helps to decrease the domain gap between two images, since resolution difference decreases between two images. Then both images are upsampled to 224 × 224, which is input and output resolution of the CNN. We have also tested the network without applying DoG filter and the results are presented in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments</head><p>As a first step, we perform preprocessing on the images. Then for each dataset, these processed thermal and visible pairs are given as input and output to the deep autoencoder architecture, respectively. Initial learning rate is 0.01 with Adam optimizer. Learning rate is halved when validation error does not change multiple epochs. Batch size is 32 for all the experiments. Training time changes according to the dataset size. Average training time takes five hours with Tesla K80 GPU.</p><p>To perform face recognition, each gallery image has been given to the network as input and the network generated corresponding thermal image. Then each thermal probe image has been compared with these thermal images generated by the network. We conduct experiments with three different setups. In the first two experiments we investigate the effect of the upsampling method for the decoder. For these experiments, DoG filter were not applied to images as preprocessing step. For the third experiment, we test the effect of the DoG filter on the upconvolutional autoencoder model. In order to make a fair comparison with the <ref type="bibr" target="#b6">[7]</ref>, we also use face images that are not aligned on the Carl dataset. Results are presented as averaged rank-1 recognition accuracies over ten independent experiments. That is, for each dataset, each model is trained and tested ten different times and the obtained results are averaged. <ref type="table">Table 1</ref>, 2, and 3 present the averaged rank-1 recognition scores on the Carl, UND-X1, and EURECOM datasets, respectively. In the tables, we also include the accuracies of the stateof-the-art methods for comparison purposes. For the Carl and UND-X1 datasets, state-of-the-art models are Deep Perceptual Mapping <ref type="bibr" target="#b6">[7]</ref> and Hu et al. <ref type="bibr" target="#b5">[6]</ref>. On EURECOM dataset, only <ref type="bibr" target="#b10">[11]</ref> presented a study for the thermal to visible recognition. The authors propose an image synthesis method by using cascaded refinement network. However, their benchmark setup is different than ours and earlier thermal to visible face recognition studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head><p>The highest accuracies are reached when preprocessing, up convolution, and alignment are used. As can be seen in <ref type="table" target="#tab_0">Table  I</ref>, the proposed method is superior to the DPM, when two visible images per subject and all available visible images per subject are used in the gallery. <ref type="table" target="#tab_0">Table II</ref> shows that our proposed method outperforms DPM on the UND-X1 dataset in all the gallery settings. We improved the state-of-the-art by an absolute increase of 14% in accuracy on the Carl dataset and by 3.5% on the UND-X1 dataset when all the samples of the subjects are used in the gallery. From <ref type="table" target="#tab_0">Table I</ref>, it can also be observed that alignment improves the performance by around 2%. Up convolution is found to be more useful compared to upsampling leading to increased accuracies on all there datasets. Similarly, applying preprocessing has enhanced the performance further on all three datasets, which indicates its importance. Please note that the results presented in <ref type="table" target="#tab_0">Table III</ref> are not directly comparable with the ones presented in <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b10">[11]</ref> authors use 45 subjects for training and 5 subjects for testing, whereas we use 30 subjects for training and 20 subjects for testing. Considering that we use less amount of training samples and have more classes to discriminate, our setup poses a more challenging problem. <ref type="figure" target="#fig_2">Fig. 3</ref> shows correctly matched and mismatched samples. Top row contains the outputs of the autoencoder and bottom row contains the ground truth thermal images of the subjects. As can be seen, for the correct matches, the autoencoder provides visually satisfying results. However, as in the case of mismatches, if the autoencoder fails to generate a good mapping, then consequently the classification system also fails. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we show that mapping between thermal and visible images can be learned successfully by using a deep convolutional autoencoder. In addition, we show that applying preprocessing and alignment improves the performance further. We improve the state-of-the-art face recognition accuracies on two different and widely used datasets and also present results on a recently collected dataset. As a future work, we will explore different deep convolutional autoencoder architectures. We also plan to address several appearance variations, such as occlusion and head pose, in thermal face images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the network architecture visible spectrum face images. Their simple feed forward neural network uses densely computed hand-crafted feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Preprocessing steps in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>(a) Correctly matched subjects (b) Mismatched subjects Top row shows the outputs of the model and bottom row shows the ground truth thermal images of the subjects. Correctly predicted subject (a) shows visually satisfying outputs and (b) shows the mismatched subject images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="3">RANK-1 RECOGNITION ACCURACIES ON THE CARL DATASET</cell><cell></cell></row><row><cell>Models</cell><cell cols="3">#Visible Image Gallery/Subject</cell></row><row><cell></cell><cell cols="3">1/subject 2/subject all/subject</cell></row><row><cell>Deep Perceptual Mapping [7]</cell><cell>56.33%</cell><cell>60.08%</cell><cell>71%</cell></row><row><cell>Partial Least Squares [6]</cell><cell>31.75%</cell><cell>34.66%</cell><cell>51.58%</cell></row><row><cell>Model with Bilinear Upsample</cell><cell>40.2%</cell><cell>45.8%</cell><cell>75.5%</cell></row><row><cell>Model with Bilinear Upsample (Aligned)</cell><cell>42%</cell><cell>48.75%</cell><cell>77.25%</cell></row><row><cell>Model with Up Convolution</cell><cell>41%</cell><cell>49.75%</cell><cell>80.2%</cell></row><row><cell>Model with Up Convolution (Aligned)</cell><cell>43.75%</cell><cell>52.5%</cell><cell>82.5%</cell></row><row><cell>Model with Up Convolution + DoG Filter</cell><cell>46.8%</cell><cell>58.5%</cell><cell>82.5%</cell></row><row><cell>Model with Up Convolution + DoG Filter (Aligned)</cell><cell>48%</cell><cell>60.25%</cell><cell>85%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RANK</head><label>II</label><figDesc>-1 RECOGNITION ACCURACIES ON THE UND-X1 DATASET</figDesc><table><row><cell>Models</cell><cell cols="3">#Visible Image Gallery/Subject</cell></row><row><cell></cell><cell cols="2">1/subject 2/subject</cell><cell>all/subject</cell></row><row><cell>Deep Perceptual Mapping [7]</cell><cell>55.36%</cell><cell>60.83%</cell><cell>83.73%</cell></row><row><cell>Partial Least Squares [6]</cell><cell>44.75%</cell><cell>50.89%</cell><cell>69.86%</cell></row><row><cell>Model with Bilinear Upsample</cell><cell>42%</cell><cell>50.25%</cell><cell>75.4%</cell></row><row><cell>Model with Up Convolution</cell><cell>49.25%</cell><cell>57.5%</cell><cell>82%</cell></row><row><cell>Model with Up Convolution + DoG Filter</cell><cell>58.75%</cell><cell>65.25%</cell><cell>87.2%</cell></row><row><cell>TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">RANK-1 RECOGNITION ACCURACIES ON THE EURECOM DATASET</cell></row><row><cell>Models</cell><cell cols="3">#Visible Image Gallery/Subject</cell></row><row><cell></cell><cell cols="2">1/subject 2/subject</cell><cell>all/subject</cell></row><row><cell>Model with Bilinear Upsample</cell><cell>50.41%</cell><cell>58.5%</cell><cell>80%</cell></row><row><cell>Model with Up Convolution</cell><cell>53.75%</cell><cell>61.25%</cell><cell>81.25%</cell></row><row><cell>Model with Up Convolution + DoG Filter</cell><cell>57.91%</cell><cell>70%</cell><cell>88.33%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A criterion for analysis of different sensor combinations with an application to face biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Espinosa-Duró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faundez-Zanuy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mekyska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Monte-Moreno</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12559-010-9060-5</idno>
		<ptr target="https://doi.org/10.1007/s12559-010-9060-5" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="135" to="141" />
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new face database simultaneously acquired in visible, near-infrared and thermal spectrums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Espinosa-Duró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faundez-Zanuy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mekyska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="119" to="135" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visible-light and infrared face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multimodal User Authentication</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Assessment of time dependency in face recognition: An initial study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Audio-and Video-Based Biometric Person Authentication</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A benchmark database of visible and thermal paired face images across multiple variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Biometrics Special Interest Group</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Thermal-to-visible face recognition using partial least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="442" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep perceptual mapping for crossmodal face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-016-0933-2</idno>
		<ptr target="https://doi.org/10.1007/s11263-016-0933-2" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="426" to="438" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tv-gan: Generative adversarial network based thermal to visible face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wiliem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2018-02" />
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Matching thermal to visible face images using a semantic-guided generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 14th IEEE International Conference on Automatic Face Gesture Recognition (FG 2019)</title>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Crossspectrum thermal to visible face recognition based on cascaded image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boutros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuijper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
