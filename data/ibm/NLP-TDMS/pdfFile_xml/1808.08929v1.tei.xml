<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A neural attention model for speech command recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Coimbra De Andrade</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Laboratory of Voice, Speech and Singing</orgName>
								<orgName type="institution">Federal University of the State of Rio de Janeiro</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabato</forename><surname>Leo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Adecco Italia S.P.A -GSK Vaccines Srl c CERN</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">Loesener</forename><surname>Da</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silva</forename><surname>Viana</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bernkopf</surname></persName>
						</author>
						<title level="a" type="main">A neural attention model for speech command recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>human voice</term>
					<term>command recognition</term>
					<term>attention mechanism</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a convolutional recurrent network with attention for speech command recognition. Attention models are powerful tools to improve performance on natural language, image captioning and speech tasks. The proposed model establishes a new stateof-the-art accuracy of 94.1% on Google Speech Commands dataset V1 and 94.5% on V2 (for the 20-commands recognition task), while still keeping a small footprint of only 202K trainable parameters. Results are compared with previous convolutional implementations on 5 different tasks (20 commands recognition (V1 and V2), 12 commands recognition (V1), 35 word recognition (V1) and left-right (V1)). We show detailed performance results and demonstrate that the proposed attention mechanism not only improves performance but also allows inspecting what regions of the audio were taken into consideration by the network when outputting a given category.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">et al. (2015)</ref><p>, <ref type="bibr" target="#b5">Chiu et al. (2017)</ref>). However, these systems rely on powerful neural network models that usually run in the cloud due to the computation required to transform speech to text, perform natural language processing of user intent and react appropriately. Simple commands, such as "stop" and "go", that could be processed locally, go through the same processing stages. Therefore, industry applications which do not have the benefit of uninterrupted broadband internet connection cannot incorporate speech recognition in the same manner. The development of lightweight speech command models would enable the development of a multitude of novel engineering applications, such as voice-controlled robots for critical missions and assistive devices that can operate in areas without internet coverage. This aspect is particularly important when designing microcontrollers that support voice-driven commands <ref type="bibr" target="#b24">(Zhang et al. (2017)</ref>).</p><p>This work introduces a novel attention-based recurrent network architecture designed to recognize simple speech commands, while still generating a lightweight model that can be loaded in mobile devices and run locally.</p><p>The main contributions of this work are:</p><p>1. Design of a novel recurrent architecture with attention that achieves state-of-the-art performance in command recognition and language identification from speech and is small enough to be run locally;</p><p>2. Visualization of attention weights and discussion about how attention improves accuracy and makes the speech recognition model explainable;</p><p>3. Source code (to be made available at https://github.com/.... after acceptance -blind review).</p><p>Results are presented using Google Speech Command datasets V1 and V2. For complete details about these datasets, refer to <ref type="bibr" target="#b22">Warden (2018)</ref>. This paper is structured as follows: Section 1.1 discusses previous work on command recognition and attention models. Section 2 presents the proposed neural network architecture. Section 3 shows results obtained on various tasks related to Google Speech Command datasets V1 and V2, as well as attention and confusion matrix plots. Section 4 summarizes 2 this work and presents possible directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Identification of speech commands, also known as keyword spotting (KWS), is important from an engineering perspective for a wide range of applications, from indexing audio databases and indexing keywords <ref type="bibr" target="#b18">(Tabibian et al. (2013)</ref>, <ref type="bibr" target="#b15">Sangeetha and Jothilakshmi (2014)</ref>) to running speech models locally in microcontrollers <ref type="bibr" target="#b24">(Zhang et al. (2017)</ref>).</p><p>The development of neural attention models <ref type="bibr" target="#b3">(Bahdanau et al. (2014)</ref>, <ref type="bibr" target="#b20">Vaswani et al. (2017)</ref>) increased performance on multiple tasks, especially those related to long sequence to sequence models. These models are extremely powerful ways to understand what parts of the input are being used by the neural network to predict outputs, as shown in the case of image captioning <ref type="bibr" target="#b23">(Xu et al. (2015)</ref>). In the case of acoustic models, Connectionist Temporal Classification (CTC) loss shows good performance in English and Mandarin speech to text tasks <ref type="bibr" target="#b1">(Amodei et al. (2015)</ref>). There is also work on sequence discriminative frameworks ). However, to the best of our knowledge, attention for single word recognition has not been investigated.</p><p>Command recognition using deep residual networks has been investigated in <ref type="bibr" target="#b19">Tang and Lin (2017)</ref>, <ref type="bibr" target="#b2">Arik et al. (2017)</ref> and <ref type="bibr" target="#b14">Sainath and Parada (2015)</ref>. In particular, the problem of limited size architectures has been extensively explored in <ref type="bibr" target="#b24">Zhang et al. (2017)</ref>. The best results achieve over 95% accuracy in specific tasks using spectrogram methods. However, most models perform significantly worse than the proposed recurrent neural network (RNN) with attention even with a greater number of parameters. Moreover, the proposed attention mechanism makes results explainable and easy to interpret, which is fundamental for engineering applications and partly solves the problem of "black box in deep learning" <ref type="bibr" target="#b12">(Lei et al. (2018)</ref>). Results using raw waveform without any Fourier analysis have also been investigated <ref type="bibr" target="#b10">(Jansson (2018)</ref>).</p><p>Results are presented on accuracy of left vs right (i.e., identifying only the words "left", "right" or none of those two), 20 commands and 10 non-commands and all 35 words (McMahan and Rao <ref type="formula">(2017)</ref>). Note that while the first version of the dataset only has 30 words in the test set, V2 has all 35 (denoted by "35-word task" in this paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Neural Network Implementation</head><p>The Keras interface <ref type="bibr" target="#b7">(Chollet et al. (2015)</ref>) was used to implement all neural networks on top of Tensorflow backend <ref type="bibr" target="#b0">(Abadi et al. (2015)</ref>). On top of that, Python library kapre <ref type="bibr" target="#b6">(Choi et al. (2017)</ref>) is used to provide Keras layers for mel-scale spectrogram computation. The use of kapre library provides extreme versatility to change spectrogram and mel-frequency parameters without having to preprocess the audio in any way. As a result, the inputs to the models are raw WAV files converted to numpy arrays for efficient load with Keras generators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Proposed Architecture</head><p>Since the audio files contain a single word command that can be anywhere in the 1s length of the WAV file, it is reasonable to assume that any model that is able to classify an audio should also be able to find what is the appropriate region of interest. Thus, the attention mechanism seems appropriate to this particular task.</p><p>The model starts by computing the mel-scale spectrogram of the audio using nontrainable layers implemented in the kapre library. The input to the model is the raw WAV data with original sampling rate of ∼16 kHz. Mel-scale spectrogram is computed using 80-band mel scale, 1024 discrete Fourier transform points and hop size of 128 points (∼8 s). Similar parameters have been successfully used for audio synthesis <ref type="bibr" target="#b21">(Wang et al. (2017)</ref>) and we noticed that they preserve the visual aspect of the voice formants in the spectrogram (which would allow a human specialist to evaluate the sound - <ref type="bibr" target="#b16">Sundberg et al. (2013)</ref>, <ref type="bibr" target="#b17">Sundberg and Thaln (2015)</ref>).</p><p>After mel-scale spectrogram computation, a set of convolutions is applied to the melscale spectrogram (2D output) only in the time dimension to extract local relations in the audio file. A set of two bidirectional long short term memory (LSTM - <ref type="bibr" target="#b9">Hochreiter and Schmidhuber (1997)</ref>) units is used to capture two-way (forward and backward) long term dependencies in the audio file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>At this point, one of the output vectors of the last LSTM layer is extracted, projected using a dense layer and used as query vector to identify what part of the audio is the most relevant. We choose to use the middle vector of the LSTM output since the voice command is expected to be centered in the audio files. This choice is arbitrary and any vector should work since the double stacked LSTM layers should be able to carry enough "memory".</p><p>Finally, the weighted average of the LSTM output is fed into 3 fully connected layers for classification. <ref type="figure" target="#fig_0">Figure 1</ref> summarizes the architecture. For complete details about the implementation, please refer to the repository. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>To facilitate comparison with previous results, 5 different tasks were considered:</p><p>• Recognition of 20 speech commands (+unknown) using Google Speech Dataset V2;</p><p>• Recognition of 20 speech commands (+unknown) using Google Speech Dataset V1;</p><p>• Recognition of 12 speech commands (+unknown) using Google Speech Dataset V1;</p><p>• Recognition of all 35 words (+unknown, only silence samples in this case) using Google Speech Dataset V1;</p><p>• Recognition of left-right words using Google Speech Dataset V1;</p><p>For each task, the proposed Attention RNN model was trained for a maximum of 40 epochs. The model with the best accuracy performance on the validation set was saved and training was stopped if no improvement was made in 10 consecutive epochs. Training was done using the "adam" algorithm <ref type="bibr" target="#b11">(Kingma and Ba (2014)</ref>) with initial learning rate of 0.001 and decay of 0.4 every 10 epochs. The batch size used was 64. Tests in multiple training runs (3 to 5, depending on the task) show that the training procedure is consistent and standard deviation of accuracy results is 0.2% for all models. Each epoch takes approximately 180 s (V2) and 100 s (V1) to run in a Tesla K80 GPU.</p><p>Attention plots and confusion matrices are also presented to allow for better comparison of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Speech Command Recognition Accuracy</head><p>The results obtained with the attention-RNN model are compared with left-right accuracy, 20 command accuracy and 35 word accuracy (McMahan and Rao <ref type="formula">(2017)</ref>), as shown in <ref type="table" target="#tab_0">Table 1</ref>. Attention provides a substantial improvement on these tasks when compared to other models. On the V2 dataset, our results are 94.5% (20-cmd) and 93.9% (35-word)significantly better than the 20-cmd baseline of 88.2% from <ref type="bibr" target="#b22">Warden (2018)</ref>. <ref type="table" target="#tab_1">Table 2</ref> compares results of the attention RNN model when used to recognize only the 12 commands originally proposed in the Kaggle competition <ref type="bibr">(Google Brain (2018)</ref>). Results are also presented for attention RNN trained and tested on V2 dataset.   <ref type="formula">(2018)</ref>) using the held-out data released after the competition ended <ref type="bibr" target="#b22">(Warden (2018)</ref>). Had this been an entry, the leaderboard score would have been 92.8%, which would rank #1 (assuming that the test set released is identical to the one used for scoring).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention Plots</head><p>One of the main advantages of deep learning models with attention is the possibility to explain the results and get intuition about what the model does. Like in the case of images <ref type="bibr" target="#b23">(Xu et al. (2015)</ref>), plotting the attention weights allows visualization of what parts of the audio were most relevant for the classification. Note that the model matches this intuition and attributes the highest weight to transitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Confusion Matrices</head><p>Confusion matrices are presented for the 20-cmd task and the 35-word recognition task on Google Speech Command dataset V1 and V2 <ref type="bibr">(Figures 5,</ref><ref type="bibr">6,</ref><ref type="bibr">7 and 8 respectively)</ref>. It is worth noting that words "three" and "tree" are often confused (which is expected given the similarity of the words), as well as "no" and "down". Proper identification of these words would require contextual information from other words in a sentence. This issue should not be relevant for an engineering application where the designers are allowed to pick possible commands: the choice of "three" and "tree" as possible commands would certainly be a poor choice due to how similar the words are (and also to the fact that non-native speakers sometimes are not even able to pronounce the /θ/ sound in three). The accuracy on those words is the lowest (approximately 90%). Note that in the 35-word task the accuracy on Speech Dataset V2 is 93.9 ± 0.2%, slightly lower than V1 (94.3 ± 0.2%), because V2 has 35 words in the test set (as opposed to 30 in V1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>Speech command recognition is present in a wide range of devices and utilized by many HCI interfaces. In many situations, it is desirable to obtain high accuracy, lightweight models that can run locally. In this work, we introduced a novel attention RNN architecture that achieves state-of-the-art performance on multiple KWS tasks while still keeping a small footprint in terms of trainable parameters. Source code is made available on github (to be posted) to enable further work.</p><p>The proposed architecture uses raw WAV files as inputs, computes mel-scale spectrogram using a non-trainable Keras layer, extracts short and long-term dependencies and uses an attention mechanism to pinpoint which region has the most useful information, that is then fed to a sequence of dense layers.</p><p>The Google Speech Commands datasets V1 and V2 Warden <ref type="formula">(2018)</ref>  In engineering applications, being able to explain what features were used to select a particular category is a desirable element that is not available in previous neural network models. Our results demonstrate that the attention mechanism explains what parts of the audio are important for classification and also matches the intuition that regions of vowel transitions are relevant to recognize words. For completeness, confusion matrices are presented and show that the word pairs tree-three and no-down are difficult to identify without extra context.</p><p>Although data augmentation has been proven to be an important tool to increase model accuracy in visual tasks, the effectiveness of augmenting the audio samples with noise from other datasets was not explored. One possible direction of future work is to investigate the effect of incorporating multiple datasets and using pretrained models. It should also be possible to stack pairs of words for more complex commands and use a sequence-to-sequence model or multiple attention layers. Further investigation will be conducted towards using the proposed attention RNN model for automatic language identification and detection of speech pathologies from audio.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Proposed architecture: recurrent neural network with attention mechanism. Numbers between [brackets] are tensor dimensions. raw len is WAV audio length (16000 in this case). spec len is the sequence length of the generated mel-scale spectrogram. nMel is the number of mel bands. nClasses is the number of desired classes. The activation of the last Dense layer is softmax. The activation of the 64 and 32 dense classification layers is the rectified linear unit (relu).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figures 2 ,</head><label>2</label><figDesc>3 and 4 show attention weights along with waveform and mel-scale spectrogram. For better visualization, attention weights were plotted in a log-scale. Intuitively, one would expect that the network should put more emphasis on vowel transitions, since non-voiced regions (consonants) may be confused with background noise or even not present at all and regions where vowels are sustained do not carry extra information, i.e., a speaker might pronounce the /a/ sound in right for a long time before transitioning to the /i/ sound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :Figure 4 :</head><label>234</label><figDesc>Waveform, mel-frequency spectrogram and attention weights for the word "on" Waveform, mel-frequency spectrogram and attention weights for the word "one" Waveform, mel-frequency spectrogram and attention weights for the word "right"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Confusion matrix for the 20 command task on Google Speech Dataset V1 11 Confusion matrix for the 20 command task on Google Speech Dataset V2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy results on the Google Speech Command Dataset V1. DenseNet-101 results from McMahan and Rao (2017). ConvNet results from Warden (2018). Our attention Model results on the Google Speech Command Dataset V2 are also reported in the last row.</figDesc><table><row><cell>Accuracy (%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy results on 12-commands from Google Speech Command Dataset V1. "res" model results from<ref type="bibr" target="#b22">Warden (2018)</ref>. ConvNet on raw WAV results from<ref type="bibr" target="#b10">Jansson (2018)</ref>. Depthwise Separable Convolutional Neural Network (DS-CNN) from<ref type="bibr" target="#b24">Zhang et al. (2017)</ref>.</figDesc><table><row><cell>Model</cell><cell cols="2">Accuracy (%) Trainable Parameters</cell></row><row><cell>res15</cell><cell>95.8</cell><cell>238K</cell></row><row><cell>res26</cell><cell>95.2</cell><cell>438K</cell></row><row><cell>res8</cell><cell>94.1</cell><cell>110K</cell></row><row><cell>ConvNet on raw WAV</cell><cell>89.4</cell><cell>700K</cell></row><row><cell>DS-CNN</cell><cell>95.4</cell><cell>498K</cell></row><row><cell>Attention RNN (Ours)</cell><cell>95.6</cell><cell>202K</cell></row><row><cell cols="2">Attention RNN (Ours, V2) 96.9</cell><cell>202K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>are used to demonstrate the effectiveness of the attention RNN approach. Attention RNN establishes a new state-of-the-art result on all tasks: 20-cmd, 12-cmd, 35-word and left-right. The accuracies are respectively 94.1%, 95.6%, 93.9% and 99.2% on the V1 dataset and 94.5%, 96.9%, 93.9% and 99.4% on the V2 dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seetapun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="CoRRabs/1512.02595.URLhttp://arxiv.org/abs/1512.02595" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Convolutional recurrent neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Ö</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno>abs/1703.05390</idno>
		<ptr target="http://arxiv.org/abs/1703.05390" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<ptr target="http://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequence discriminative training for deep learning based acoustic keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="100" to="111" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<idno>abs/1712.01769</idno>
		<ptr target="http://arxiv.org/abs/1712.01769" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kapre: On-gpu audio preprocessing layers for a quick implementation of deep neural network models with keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Music Discovery Workshop at 34th International Conference on Machine Learning. ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tensorflow speech recognition challenge</title>
		<ptr target="https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/" />
	</analytic>
	<monogr>
		<title level="j">Google Brain</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Single-word speech recognition with convolutional neural networks on raw waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jansson</surname></persName>
		</author>
		<ptr target="https://www.theseus.fi/bitstream/handle/10024/144982/Jansson_Patrick.pdf?sequence=1" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Opening the black box of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<idno>abs/1805.08355</idno>
		<ptr target="http://arxiv.org/abs/1805.08355" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Listening to the world improves speech command recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<idno>abs/1710.08377</idno>
		<ptr target="http://arxiv.org/abs/1710.08377" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel spoken keyword spotting system using support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sangeetha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jothilakshmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="287" to="293" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Formant tuning strategies in professional male opera singers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Voice</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="278" to="288" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Respiratory and acoustical differences between belt and neutral style of singing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thaln</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Voice</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="418" to="425" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Keyword spotting using an evolutionary-based classifier and discriminative features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tabibian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nasersharif</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0952197613000511" />
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1660" to="1670" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep residual learning for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1710.10361</idno>
		<ptr target="http://arxiv.org/abs/1710.10361" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="CoRRabs/1706.03762.URLhttp://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Tacotron: A fully end-toend text-to-speech synthesis model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<idno>abs/1703.10135</idno>
		<ptr target="http://arxiv.org/abs/1703.10135" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno>abs/1804.03209</idno>
		<ptr target="http://arxiv.org/abs/1804.03209" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1502.03044</idno>
		<ptr target="http://arxiv.org/abs/1502.03044" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hello edge: Keyword spotting on microcontrollers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno>abs/1711.07128</idno>
		<ptr target="http://arxiv.org/abs/1711.07128" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
