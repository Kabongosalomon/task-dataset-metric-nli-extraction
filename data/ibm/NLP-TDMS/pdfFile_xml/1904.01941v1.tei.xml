<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Character Region Awareness for Text Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Baek</surname></persName>
							<email>youngmin.baek@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bado</forename><surname>Lee</surname></persName>
							<email>bado.lee@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
							<email>dongyoon.han@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
							<email>sangdoo.yun@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwalsuk</forename><surname>Lee</surname></persName>
							<email>hwalsuk.lee@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Character Region Awareness for Text Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene text detection methods based on neural networks have emerged recently and have shown promising results. Previous methods trained with rigid word-level bounding boxes exhibit limitations in representing the text region in an arbitrary shape. In this paper, we propose a new scene text detection method to effectively detect text area by exploring each character and affinity between characters. To overcome the lack of individual character level annotations, our proposed framework exploits both the given characterlevel annotations for synthetic images and the estimated character-level ground-truths for real images acquired by the learned interim model. In order to estimate affinity between characters, the network is trained with the newly proposed representation for affinity. Extensive experiments on six benchmarks, including the TotalText and CTW-1500 datasets which contain highly curved texts in natural images, demonstrate that our character-level text detection significantly outperforms the state-of-the-art detectors. According to the results, our proposed method guarantees high flexibility in detecting complicated scene text images, such as arbitrarily-oriented, curved, or deformed texts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene text detection has attracted much attention in the computer vision field because of its numerous applications, such as instant translation, image retrieval, scene parsing, geo-location, and blind-navigation. Recently, scene text detectors based on deep learning have shown promising performance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26]</ref>. These methods mainly train their networks to localize wordlevel bounding boxes. However, they may suffer in difficult cases, such as texts that are curved, deformed, or extremely long, which are hard to detect with a single bounding box. Alternatively, character-level awareness has many advantages when handling challenging texts by linking the successive characters in a bottom-up manner. Unfortunately, most of the existing text datasets do not provide characterlevel annotations, and the work needed to obtain characterlevel ground truths is too costly.</p><p>In this paper, we propose a novel text detector that localizes the individual character regions and links the detected characters to a text instance. Our framework, referred to as CRAFT for Character Region Awareness For Text detection, is designed with a convolutional neural network producing the character region score and affinity score. The region score is used to localize individual characters in the image, and the affinity score is used to group each character into a single instance. To compensate for the lack of character-level annotations, we propose a weaklysupervised learning framework that estimates characterlevel ground truths in existing real word-level datasets. <ref type="figure">Figure.</ref> 1 is a visualization of CRAFT's results on various shaped texts. By exploiting character-level region awareness, texts in various shapes are easily represented. We demonstrate extensive experiments on ICDAR datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref> to validate our method, and the experi-ments show that the proposed method outperforms state-ofthe-art text detectors. Furthermore, experiments on MSRA-TD500, CTW-1500, and TotalText datasets <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3]</ref> show the high flexibility of the proposed method on complicated cases, such as long, curved, and/or arbitrarily shaped texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The major trend in scene text detection before the emergence of deep learning was bottom-up, where handcrafted features were mostly used -such as MSER <ref type="bibr" target="#b26">[27]</ref> or SWT <ref type="bibr" target="#b4">[5]</ref>-as a basic component. Recently, deep learningbased text detectors have been proposed by adopting popular object detection/segmentation methods like SSD <ref type="bibr" target="#b19">[20]</ref>, Faster R-CNN <ref type="bibr" target="#b29">[30]</ref>, and FCN <ref type="bibr" target="#b22">[23]</ref>.</p><p>Regression-based text detectors Various text detectors using box regression adapted from popular object detectors have been proposed. Unlike objects in general, texts are often presented in irregular shapes with various aspect ratios. To handle this problem, TextBoxes <ref type="bibr" target="#b17">[18]</ref> modified convolutional kernels and anchor boxes to effectively capture various text shapes. DMPNet <ref type="bibr" target="#b21">[22]</ref> tried to further reduce the problem by incorporating quadrilateral sliding windows. In recent, Rotation-Sensitive Regression Detector (RSDD) <ref type="bibr" target="#b18">[19]</ref> which makes full use of rotation-invariant features by actively rotating the convolutional filters was proposed. However, there is a structural limitation to capturing all possible shapes that exist in the wild when using this approach.</p><p>Segmentation-based text detectors Another common approach is based on works dealing with segmentation, which aims to seek text regions at the pixel level. These approaches that detect texts by estimating word bounding areas, such as Multi-scale FCN <ref type="bibr" target="#b6">[7]</ref>, Holistic-prediction <ref type="bibr" target="#b36">[37]</ref>, and PixelLink <ref type="bibr" target="#b3">[4]</ref> have also been proposed using segmentation as their basis. SSTD <ref type="bibr" target="#b7">[8]</ref> tried to benefit from both the regression and segmentation approaches by using an attention mechanism to enhance text related area via reducing background interference on the feature level. Recently, TextSnake <ref type="bibr" target="#b23">[24]</ref> was proposed to detect text instances by predicting the text region and the center line together with geometry attributes.</p><p>End-to-end text detectors An end-to-end approach trains the detection and recognition modules simultaneously so as to enhance detection accuracy by leveraging the recognition result. FOTS <ref type="bibr" target="#b20">[21]</ref> and EAA <ref type="bibr" target="#b9">[10]</ref> concatenate popular detection and recognition methods, and train them in an end-to-end manner. Mask TextSpotter <ref type="bibr" target="#b24">[25]</ref> took advantage of their unified model to treat the recognition task as a semantic segmentation problem. It is obvious that training with the recognition module helps the text detector be more robust to text-like background clutters.  <ref type="figure">Figure 2</ref>. Schematic illustration of our network architecture.</p><p>Most methods detect text with words as its unit, but defining the extents to a word for detection is non-trivial since words can be separated by various criteria, such as meaning, spaces or color. In addition, the boundary of the word segmentation cannot be strictly defined, so the word segment itself has no distinct semantic meaning. This ambiguity in the word annotation dilutes the meaning of the ground truth for both regression and segmentation approaches.</p><p>Character-level text detectors Zhang et al. <ref type="bibr" target="#b38">[39]</ref> proposed a character level detector using text block candidates distilled by MSER <ref type="bibr" target="#b26">[27]</ref>. The fact that it uses MSER to identify individual characters limits its detection robustness under certain situations, such as scenes with low contrast, curvature, and light reflection. Yao et al. <ref type="bibr" target="#b36">[37]</ref> used a prediction map of the characters along with a map of text word regions and linking orientations that require character level annotations. Instead of an explicit character level prediction, Seglink <ref type="bibr" target="#b31">[32]</ref> hunts for text grids (partial text segments) and associates these segments with an additional link prediction. Even though Mask TextSpotter <ref type="bibr" target="#b24">[25]</ref> predicts a character-level probability map, it was used for text recognition instead of spotting individual characters. This work is inspired by the idea of WordSup <ref type="bibr" target="#b11">[12]</ref>, which uses a weakly supervised framework to train the characterlevel detector. However, a disadvantage of Wordsup is that the character representation is formed in rectangular anchors, making it vulnerable to perspective deformation of characters induced by varying camera viewpoints. Moreover, it is bound by the performance of the backbone structure (i.e. using SSD and being limited by the number of anchor boxes and their sizes). <ref type="figure">Figure 3</ref>. Illustration of ground truth generation procedure in our framework. We generate ground truth labels from a synthetic image that has character level annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Our main objective is to precisely localize each individual character in natural images. To this end, we train a deep neural network to predict character regions and the affinity between characters. Since there is no public characterlevel dataset available, the model is trained in a weaklysupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>A fully convolutional network architecture based on VGG-16 <ref type="bibr" target="#b33">[34]</ref> with batch normalization is adopted as our backbone. Our model has skip connections in the decoding part, which is similar to U-net <ref type="bibr" target="#b30">[31]</ref> in that it aggregates lowlevel features. The final output has two channels as score maps: the region score and the affinity score. The network architecture is schematically illustrated in <ref type="figure">Fig</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Ground Truth Label Generation</head><p>For each training image, we generate the ground truth label for the region score and the affinity score with characterlevel bounding boxes. The region score represents the probability that the given pixel is the center of the character, and the affinity score represents the center probability of the space between adjacent characters.</p><p>Unlike a binary segmentation map, which labels each pixel discretely, we encode the probability of the character center with a Gaussian heatmap. This heatmap representation has been used in other applications, such as in pose estimation works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref> due to its high flexibility when dealing with ground truth regions that are not rigidly bounded. We use the heatmap representation to learn both the region score and the affinity score. <ref type="figure">Fig. 3</ref> summarizes the label generation pipeline for a synthetic image. Computing the Gaussian distribution value directly for each pixel within the bounding box is very time-consuming. Since character bounding boxes on an image are generally distorted via perspective projections, we use the following steps to approximate and generate the ground truth for both the region score and the affinity score: 1) prepare a 2-dimensional isotropic Gaussian map; 2) compute perspective transform between the Gaussian map region and each character box; 3) warp Gaussian map to the box area.</p><p>For the ground truths of the affinity score, the affinity boxes are defined using adjacent character boxes, as shown in <ref type="figure">Fig. 3</ref>. By drawing diagonal lines to connect opposite corners of each character box, we can generate two triangleswhich we will refer to as the upper and lower character triangles. Then, for each adjacent character box pair, an affinity box is generated by setting the centers of the upper and lower triangles as corners of the box.</p><p>The proposed ground truth definition enables the model to detect large or long-length text instances sufficiently, despite using small receptive fields. On the other hand, previous approaches like box regression require a large receptive field in such cases. Our character-level detection makes it possible for convolutional filters to focus only on intracharacter and inter-character, instead of the entire text instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Weakly-Supervised Learning</head><p>Unlike synthetic datasets, real images in a dataset usually have word-level annotations. Here, we generate character boxes from each word-level annotation in a weaklysupervised manner, as summarized in <ref type="figure" target="#fig_3">Fig. 4</ref>. When a real image with word-level annotations is provided, the learned interim model predicts the character region score of the cropped word images to generate character-level bounding boxes. In order to reflect the reliability of the interim model's prediction, the value of the confidence map over each word box is computed proportional to the number of the detected characters divided by the number of the ground truth characters, which is used for the learning weight dur-   ing training. <ref type="figure">Fig. 6</ref> shows the entire procedure for splitting the characters. First, the word-level images are cropped from the original image. Second, the model trained up to date predicts the region score. Third, the watershed algorithm <ref type="bibr" target="#b34">[35]</ref> is used to split the character regions, which is used to make the character bounding boxes covering regions. Finally, the coordinates of the character boxes are transformed back into the original image coordinates using the inverse transform from the cropping step. The pseudo-ground truths (pseudo-GTs) for the region score and the affinity score can be generated by the steps described in <ref type="figure">Fig. 3</ref> using the obtained quadrilateral character-level bounding boxes.</p><p>When the model is trained using weak-supervision, we are compelled to train with incomplete pseudo-GTs. If the model is trained with inaccurate region scores, the output might be blurred within character regions. To prevent this, we measure the quality of each pseudo-GTs generated by the model. Fortunately, there is a very strong cue in the text annotation, which is the word length. In most datasets, the transcription of words is provided and the length of the words can be used to evaluate the confidence of the pseudo-GTs.</p><p>For a word-level annotated sample w of the training data, let R(w) and l(w) be the bounding box region and the word length of the sample w, respectively. Through the character splitting process, we can obtain the estimated character bounding boxes and their corresponding length of characters l c (w). Then the confidence score s conf (w) for the sample w is computed as,  and the pixel-wise confidence map S c for an image is computed as,</p><formula xml:id="formula_0">s conf (w) = l(w) − min(l(w), |l(w) − l c (w)|) l(w) ,<label>(1)</label></formula><formula xml:id="formula_1">S c (p) = s conf (w) p ∈ R(w), 1 otherwise,<label>(2)</label></formula><p>where p denotes the pixel in the region R(w). The objective L is defined as,</p><formula xml:id="formula_2">L = p S c (p)· ||S r (p) − S * r (p)|| 2 2 + ||S a (p) − S * a (p)|| 2 2 ,<label>(3)</label></formula><p>where S * r (p) and S * a (p) denote the pseudo-ground truth region score and affinity map, respectively, and S r (p) and S a (p) denote the predicted region score and affinity score, respectively. When training with synthetic data, we can obtain the real ground truth, so S c (p) is set to 1.</p><p>As training is performed, the CRAFT model can predict characters more accurately, and the confidence scores s conf (w) are gradually increased as well. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the character region score map during training. At the early stages of training, the region scores are relatively low for unfamiliar text in natural images. The model learns the ap-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Character split</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word-level annotation</head><p>Character-level annotation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word box Region score Watershed labeling Character box</head><p>Cropping Unwarping <ref type="figure">Figure 6</ref>. Character split procedure for achieving character-level annotation from word-level annotation: 1) crop the word-level image; 2) predict the region score; 3) apply the watershed algorithm; 4) get the character bounding boxes; 5) unwarp the character bounding boxes.</p><p>pearances of new texts, such as irregular fonts, and synthesized texts that have a different data distribution against that of the SynthText dataset. If the confidence score s conf (w) is below 0.5, the estimated character bounding boxes should be neglected since they have adverse effects when training the model. In this case, we assume the width of the individual character is constant and compute the character-level predictions by simply dividing the word region R(w) by the number of characters l(w). Then, s conf (w) is set to 0.5 to learn unseen appearances of texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference</head><p>At the inference stage, the final output can be delivered in various shapes, such as word boxes or character boxes, and further polygons. For datasets like ICDAR, the evaluation protocol is word-level intersection-over-union (IoU), so here we describe how to make word-level bounding boxes QuadBox from the predicted S r and S a through a simple yet effective post-processing step.</p><p>The post-processing for finding bounding boxes is summarized as follows. First, the binary map M covering the image is initialized with 0. M (p) is set to 1 if S r (p) &gt; τ r or S a (p) &gt; τ a , where τ r is the region threshold and τ a is the affinity threshold. Second, Connected Component Labeling (CCL) on M is performed. Lastly, QuadBox is obtained by finding a rotated rectangle with the minimum area enclosing the connected components corresponding to each of the labels. The functions like connectedComponents and minAreaRect provided by OpenCV can be applied for this purpose. Note that an advantage of CRAFT is that it does not need any further post-processing methods, like Non-Maximum Suppression (NMS). Since we have image blobs of word regions separated by CCL, the bounding box for a word is simply defined by the single enclosing rectangle. On a different note, our character linking process is conducted at a pixel-level, which differs from other linking-based methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12]</ref> relying on searching relations between text components explicitly.</p><p>Additionally, we can generate a polygon around the entire character region to deal with curved texts effectively. The procedure of polygon generation is illustrated in <ref type="figure" target="#fig_5">Fig. 7</ref>. The first step is to find the local maxima line of character regions along the scanning direction, as shown in the figure with arrows in blue. The lengths of the local maxima lines are equally set as the maximum length among them to prevent the final polygon result from becoming uneven. The line connecting all the center points of the local maxima is called the center line, shown in yellow. Then, the local maxima lines are rotated to be perpendicular to the center line to reflect the tilt angle of characters, as expressed by the red arrows. The endpoints of the local maxima lines are the candidates for the control points of the text polygon. To fully cover the text region, we move the two outer-most tilted local maxima lines outward along the local maxima center line, making the final control points (green dots). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training strategy</head><p>The training procedure includes two steps: we first use the SynthText dataset <ref type="bibr" target="#b5">[6]</ref> to train the network for 50k iter-ations, then each benchmark dataset is adopted to fine-tune the model. Some "DO NOT CARE" text regions in ICDAR 2015 and ICDAR 2017 datasets are ignored in training by setting s conf (w) to 0. We use the ADAM <ref type="bibr" target="#b15">[16]</ref> optimizer in all training processes. For multi-GPU training, the training and supervision GPUs are separated, and pseudo-GTs generated by the supervision GPUs are stored in the memory. During fine-tuning, the SynthText dataset is also used at a rate of 1:5 to make sure that the character regions are surely separated. In order to filter out texture-like texts in natural scenes, On-line Hard Negative Mining <ref type="bibr" target="#b32">[33]</ref> is applied at a ratio of 1:3. Also, basic data augmentation techniques like crops, rotations, and/or color variations are applied.</p><p>Weakly-supervised training requires two types of data; quadrilateral annotations for cropping word images and transcriptions for calculating word length. The datasets meeting these conditions are IC13, IC15, and IC17. Other datasets such as MSRA-TD500, TotalText, and CTW-1500 do not meet the requirements. MSRA-TD500 does not provide transcriptions, while TotalText and CTW-1500 provide polygon annotations only. Therefore, we trained CRAFT only on the ICDAR datasets, and tested on the others without fine-tuning. Two different models are trained with the ICDAR datasets. The first model is trained on IC15 to evaluate IC15 only. The second model is trained on both IC13 and IC17 together, which is used for evaluating the other five datasets. No extra images are used for training. The number of iterations for fine-tuning is set to 25k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Results</head><p>Quadrilateral-type datasets (ICDARs, and MSRA-TD500) All experiments are performed with a single image resolution. The longer side of the images in IC13, IC15, IC17, and MSRA-TD500 are resized to 960, 2240, 2560, and 1600, respectively. <ref type="table">Table 1</ref> lists the h-mean scores of various methods on ICDAR and MSRA-TD500 datasets. To have a fair comparison with end-to-end methods, we include their detection-only results by referring to the original papers. We achieve state-of-the-art performances on all the datasets. In addition, CRAFT runs at 8.6 FPS on IC13 dataset, which is comparatively fast, thanks to the simple yet effective post-processing.</p><p>For MSRA-TD500, annotations are provided at the linelevel, including the spaces between words in the box. Therefore, a post-processing step for combining word boxes is applied. If the right side of one box and the left side of another box are close enough, the two boxes are combined together. Even though fine-tuning is not performed on the TD500 training set, CRAFT outperforms all other methods as shown in <ref type="table">Table 1</ref>. Polygon-type datasets (TotalText, CTW-1500) It is challenging to directly train the model on TotalText and CTW-1500 because their annotations are in polygonal in shape,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>IC13 IC15 IC17</p><p>Mask TextSpotter <ref type="bibr" target="#b24">[25]</ref> 91.7 86.0 -EAA <ref type="bibr" target="#b9">[10]</ref> 90 87 -FOTS <ref type="bibr" target="#b20">[21]</ref> 92.8 89.8 70.8</p><p>CRAFT(ours) 95.2 86.9 73.9 <ref type="table">Table 3</ref>. H-mean comparison with end-to-end methods. Our method is not trained in an end-to-end manner, yet shows comparable results, or even outperforms popular methods.</p><p>which complicates text area cropping for splitting character boxes during weakly-supervised training. Consequently, we only used the training images from IC13 and IC17, and finetuning was not conducted to learn the training images provided by these datasets. At the inference step, we used the polygon generation post-processing from the region score to cope with the provided polygon-type annotations.</p><p>The experiments for these datasets are performed with a single image resolution, too. The longer sides of the images within TotalText and CTW-1500 are resized to 1280 and 1024, respectively. The experimental results for polygontype datasets are shown in <ref type="table" target="#tab_4">Table 2</ref>. The individual-character localization ability of CRAFT enables us to achieve more robust and superior performance in detecting arbitrarily shaped texts compared to other methods. Particularly, the TotalText dataset has a variety of deformations, including curved texts as shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, for which adequate inference by quadrilateral-based text detectors is infeasible. Therefore, a very limited number of methods can be evaluated on those datasets.</p><p>In the CTW-1500 dataset's case, two difficult characteristics coexist, namely annotations that are provided at the line-level and are of arbitrary polygons. To aid CRAFT in such cases, a small link refinement network, which we call the LinkRefiner, is used in conjunction with CRAFT. The input of the LinkRefiner is a concatenation of the region score, the affinity score, and the intermediate feature map of CRAFT, and the output is a refined affinity score adjusted for long texts. To combine characters, the refined affinity score is used instead of the original affinity score, then the polygon generation is performed in the same way as it was performed for TotalText. Only LinkRefiner is trained on the CTW-1500 dataset while freezing CRAFT. The detailed implementation of LinkRefiner is addressed in the supplementary materials. As shown in <ref type="table" target="#tab_4">Table 2</ref>, the proposed method achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussions</head><p>Robustness to Scale Variance We solely performed singlescale experiments on all the datasets, even though the size of texts are highly diverse. This is different from the majority of other methods, which rely on multi-scale tests to handle the scale variance problem. This advantage comes from the property of our method localizing individual characters, not the whole text. The relatively small receptive field is sufficient to cover a single character in a large image, which makes CRAFT robust in detecting scale variant texts. Multi-language issue The IC17 dataset contains Bangla and Arabic characters, which are not included in the synthetic text dataset. Moreover, both languages are difficult to segment into characters individually because every character is written cursively. Therefore, our model could not distinguish Bangla and Arabic characters as well as it does Latin, Korean, Chinese, and Japanese. In East Asian characters' cases, they can be easily separated with a constant width, which helps train the model to high performance via weakly-supervision. Comparison with End-to-end methods Our method is trained with the ground truth boxes only for detection, but it is comparable with other end-to-end methods, as shown in <ref type="table">Table.</ref> 3. From the analysis of failure cases, we expect our model to benefit from the recognition results, especially when the ground truth words are separated by semantics, rather than visual cues. Generalization ability Our method achieved state-of-theart performances on 3 different datasets without additional fine-tuning. This demonstrates that our model is capable of capturing general characteristics of texts, rather than overfitting to a particular dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a novel text detector called CRAFT, which can detect individual characters even when characterlevel annotations are not given. The proposed method provides the character region score and the character affinity score that, together, fully cover various text shapes in a bottom-up manner. Since real datasets provided with character-level annotations are rare, we proposed a weaklysupervised learning method that generates pseudo-ground truthes from an interim model. CRAFT shows state-of-theart performances on most public datasets and demonstrates generalization ability by showing these performances without fine-tuning. As our future work, we hope to train our model with a recognition model in an end-to-end fashion to see whether the performance, robustness, and generalizability of CRAFT translates to a better scene text spotting system that can be applied in more general settings.  <ref type="figure">Figure 9</ref>. Schematic illustration of LinkRefiner architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LinkRefiner for CTW-1500 dataset</head><p>CTW-1500 dataset <ref type="bibr" target="#b37">[38]</ref> provides polygon-only annotations without text transcriptions. Furthermore, annotations of CTW-1500 are provided at the line-level and does not consider spaces as separation cues. This is far from our assumption of affinity, which is that the score for affinity is zero for characters with a space between them.</p><p>To obtain a single-long polygon from the detected characters, we employ a shallow network for link refinement, so called LinkRefiner. The architecture of the LinkRefiner is shown in <ref type="figure">Fig. 9</ref>. The input of the LinkRefiner is a concatenation of the region score, the affinity score, and the intermediate feature map from the network, that is the output of Stage4 of the original CRAFT model. Atrous Spatial Pyramid Pooling (ASPP) in <ref type="bibr" target="#b1">[2]</ref> is adopted to ensure a large receptive field for combining distant characters and words onto the same text line.</p><p>For the ground truth of the LinkRefiner, lines are simply drawn between the centers of the paired control points of the annotated polygons, which is similar to the text line generation used in <ref type="bibr" target="#b8">[9]</ref>. The width of each line is proportional to the distance between paired control points. The ground truth generation for the LinkRefiner is illustrated in <ref type="figure" target="#fig_0">Fig. 10</ref>. The output of the model is called the link score. For training, only the LinkRefiner is trained on the CTW-1500 training dataset, while freezing CRAFT.</p><p>After training, we have the outputs produced by the model, which are the region score, the affinity score, and the link score. Here, the link score is used instead of the original affinity score, and the text polygon is obtained entirely through the same process as done with TotalText. The CRAFT model localizes the individual characters, and the LinkRefiner model combines the characters as well as the words separated by spaces, which are required by the CTW-1500 evaluation.</p><p>The results on the CTW-1500 dataset are shown in : Polygon annotation and its control points : Ground truth for the LinkRefiner <ref type="figure" target="#fig_0">Figure 10</ref>. Ground truth generation for LinkRefiner. <ref type="figure" target="#fig_0">Fig. 11</ref>. Very challenging image samples with long and curved texts are successfully detected by the proposed method. Moreover, with our polygon representation, the curved images can be rectified into straight text images, which are also shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. We believe this ability for rectification can further be of use for recognition tasks. <ref type="figure" target="#fig_0">Figure 11</ref>. Results on CTW-1500 dataset. For each cluster: the input image (top), region score (middle left), link score (middle right), and the resulting rectified polygons for curved texts (bottom, below arrow) are shown. Note that the affinity scores are not rendered and are unused in the CTW-1500 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visualization of character-level detection using CRAFT. (a) Heatmaps predicted by our proposed framework. (b) Detection results for texts of various shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the overall training stream for the proposed method. Training is carried out using both real and synthetic images in a weakly-supervised fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Character region score maps during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>:Figure 7 .</head><label>7</label><figDesc>Local maxima along scanning direction : Center line of local maxima : Line of control points (tilted from local maxima) : Control points of text polygon : Polygon generation for arbitrarily-shaped texts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Results on the TotalText dataset. First row: each column shows the input image (top) with its respective region score map (bottom left) and affinity map (bottom right). Second row: each column only shows the input image (left) and its region score map (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>CRAFT(ours) 79.9 87.6 83.6 81.1 86.0 83.5 Results on polygon-type datasets, such as TotalText and CTW-1500. R, P and H refer to recall, precision and H-mean, respectively. The best score is highlighted in bold.IC15, the text regions in IC17 are also annotated by the 4 vertices of quadrilaterals. MSRA-TD500 (TD500) contains 500 natural images, which are split into 300 training images and 200 testing images, collected both indoors and outdoors using a pocket camera. The images contain English and Chinese scripts. Text regions are annotated by rotated rectangles. TotalText (TotalText), recently presented in ICDAR 2017, contains 1255 training and 300 testing images. It especially provides curved texts, which are annotated by polygons and word-level transcriptions. CTW-1500 (CTW) consists of 1000 training and 500 testing images. Every image has curved text instances, which are annotated by polygons with 14 vertices.</figDesc><table><row><cell>Method</cell><cell>R</cell><cell>TotalText P</cell><cell>H</cell><cell cols="2">CTW-1500 R P H</cell></row><row><cell>CTD+TLOC [38]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">69.8 77.4 73.4</cell></row><row><cell cols="4">MaskSpotter [25] 55.0 69.0 61.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">TextSnake [24] 74.5 82.7 78.4 85.3 67.9 75.6</cell></row><row><cell>4.1. Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICDAR2013 (IC13) was released during the ICDAR 2013</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Robust Reading Competition for focused scene text detec-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tion, consisting of high-resolution images, 229 for training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>and 233 for testing, containing texts in English. The anno-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tations are at word-level using rectangular boxes.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICDAR2015 (IC15) was introduced in the ICDAR 2015</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Robust Reading Competition for incidental scene text de-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tection, consisting of 1000 training images and 500 testing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>images, both with texts in English. The annotations are at</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the word level using quadrilateral boxes.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICDAR2017 (IC17) contains 7,200 training images, 1,800</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>validation images, and 9,000 testing images with texts in 9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>languages for multi-lingual scene text detection. Similar to</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors would like to thank Beomyoung Kim, Daehyun Nam, and Donghyun Kim for helping with extensive experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Ch&amp;apos;ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-scale fcn with cascaded instance aware segmentation for arbitrary oriented word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Accurate text localization in natural image with cascaded convolutional text network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09423</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An end-to-end textspotter with explicit alignment and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5020" to="5029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heras</surname></persName>
		</author>
		<title level="m">Icdar 2013 robust reading competition. In ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rotationsensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fots: Fast oriented text spotting with a unified network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5676" to="5685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multi-oriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3454" to="3461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01544</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02242</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Robust widebaseline stereo from maximally stable extremal regions. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="761" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3482" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Watersheds in digital spaces: an efficient algorithm based on immersion simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Soille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="583" to="598" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09002</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lianwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shuaitao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02170</idno>
		<title level="m">Detecting curve text in the wild: New dataset and new solution</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
