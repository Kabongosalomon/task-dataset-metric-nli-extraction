<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Alleviating Over-segmentation Errors by Detecting Action Boundaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchi</forename><surname>Ishikawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Institute of Advanced Industrial Science and Technology (AIST)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Keio University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seito</forename><surname>Kasai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Institute of Advanced Industrial Science and Technology (AIST)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Keio University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimitsu</forename><surname>Aoki</surname></persName>
							<email>aoki@elec.keio.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Keio University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
							<email>hirokatsu.kataoka@aist.go.jp</email>
							<affiliation key="aff0">
								<orgName type="department">National Institute of Advanced Industrial Science and Technology (AIST)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Alleviating Over-segmentation Errors by Detecting Action Boundaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an effective framework for the temporal action segmentation task, namely an Action Segment Refinement Framework (ASRF). Our model architecture consists of a long-term feature extractor and two branches: the Action Segmentation Branch (ASB) and the Boundary Regression Branch (BRB). The long-term feature extractor provides shared features for the two branches with a wide temporal receptive field. The ASB classifies video frames with action classes, while the BRB regresses the action boundary probabilities. The action boundaries predicted by the BRB refine the output from the ASB, which results in a significant performance improvement. Our contributions are three-fold: (i) We propose a framework for temporal action segmentation, the ASRF, which divides temporal action segmentation into frame-wise action classification and action boundary regression. Our framework refines frame-level hypotheses of action classes using predicted action boundaries. (ii) We propose a loss function for smoothing the transition of action probabilities, and analyze combinations of various loss functions for temporal action segmentation. (iii) Our framework outperforms state-of-the-art methods on three challenging datasets, offering an improvement of up to 13.7% in terms of segmental edit distance and up to 16.1% in terms of segmental F1 score. Our code will be publicly available soon 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kinetics [21, 3] and Sports-1M</head><p>[20] has greatly improved the performance of video classification.</p><p>However, natural videos are untrimmed and may contain multiple action instances. Therefore, analysis of these videos demands the recognition of action sequences. Motivated by this, we address the task of temporal action segmentation, which aims to capture and classify each action segment of an untrimmed video into an action category. Action segmentation has various potential applications in robotics, surveillance and the analysis of human activities.</p><p>Extant research typically approaches this task through two phases: i) extracting spatial or spatiotemporal features with 2D CNNs, two-stream convnets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b12">13]</ref> or 3D CNNs [2] and ii) temporally classifying the extracted features using an RNN [17]  or temporal convolutional net-9876</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, with the exponential increase in the number of videos uploaded on the Internet, more attention is now being paid to video analysis. One of the most active topics in video analysis is video classification, the goal of which is to classify a trimmed video into a single action label <ref type="bibr" target="#b0">[1]</ref>. The rise of sophisticated architectures (e.g. twostream convnets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b12">13]</ref>, 3D CNNs <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11]</ref>, (2+1)D CNN <ref type="bibr" target="#b44">[45]</ref>) and large-scale video datasets such as 1 https://github.com/yiskw713/asrf <ref type="figure">Figure 1</ref>. Overview of proposed framework. The Action Segment Refinement Framework (ASRF) consists of a long-term feature extractor(Section 3.1), an Action Segmentation Branch (ASB; Section 3.2) and a Boundary Regression Branch (BRB; Section 3.3). Video features are input for a long-term feature extractor, and it provides shared features for the ASB and the BRB. Then these two branches output frame-wise action class predictions and boundary probabilities, respectively. The ASRF reassigns frame-level action predictions from the ASB utilizing the action boundaries predicted by the BRB. works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b8">9]</ref>. These models claim better results on datasets with a small number of action classes and video clips than earlier approaches <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19]</ref>. However, it is difficult for these methods to recognize action segments, especially on large datasets with diverse action classes, which results in continuously fluctuating action class predictions, or over-segmentation errors.</p><p>When considering temporal action segmentation, oversegmentation is a critical issue. The prediction in the upper part of <ref type="figure">Figure 1</ref> shows an example prediction with oversegmentation errors. This error is crucial when analyzing untrimmed videos with a sequence of actions. For example, when analyzing the steps that are taken in cooking videos, over-segmentation will result in detecting extra steps. Assuming that actions in videos exist as chunks and do not rapidly change, reducing these errors is essential for improving action segmentation performance.</p><p>To address this problem, we propose a framework for action segmentation that leverages predicted action boundaries for action segmentation, namely Action Segment Refinement Framework (ASRF). The ASRF consists of a longterm feature extractor and two branches, an Action Segmentation Branch (ASB) and a Boundary Regression Branch (BRB) as in <ref type="figure">Figure 1</ref>. The long-term feature extractor expands the temporal receptive field and provides shared features for the following two branches. Then, the ASB broadly predicts frame-wise actions in a video, while the BRB detects action boundaries regardless of action class. Our framework refines the outputs from the ASB using action boundaries predicted by the BRB. The ASB and the BRB are complementary to each other, which enables the reduction of over-segmentation errors. In summary, the main contributions of this work are highlighted as follows:</p><p>1. We propose a simple but effective framework for action segmentation, an ASRF. Our framework decouples frame-wise action classification and action boundary regression, which enables the capturing of reliable segments and correct classification. Having a decoupled architecture enhances single model classification results by up to 10.6% in terms of segmental F1 score and 10.8% in terms of segmental edit distance. 2. We propose a loss function for smoothing the transition of action probabilities and investigate appropriate combinations of loss functions for our framework. Combining this loss function with a class-weighted classification loss function enables a 8.2% improvement of segmental F1 score and a 5.9% improvement of segmental edit distance. 3. Our approach outperforms state-of-the-art methods on three challenging datasets for action segmentation: 50 Salads <ref type="bibr" target="#b40">[41]</ref>, Georgia Tech Egocentric Activities (GTEA) <ref type="bibr" target="#b9">[10]</ref> and the Breakfast dataset <ref type="bibr" target="#b21">[22]</ref>, by up to 16.1% improvement of segmental F1 score, 13.7% improvement of segmental edit distance and 2.6% improvement for frame-wise accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Representation. Action segmentation aims to classify each frame in a video into action classes, assuming that each frame contains a single action. Extant methods are typically divided into two phases: first extracting frame-wise spatio-temporal features by 2DCNNs <ref type="bibr" target="#b29">[30]</ref>, twostream convnets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b12">13]</ref> or 3DCNNs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b10">11]</ref>, and then conducting the frame-wise classification. Some works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35]</ref> focus on extracting effective features for action segmentation. However, our work focuses on the method for classifying action and feature extraction is beyond the scope of this work. Following <ref type="bibr" target="#b8">[9]</ref>, we use I3D <ref type="bibr" target="#b1">[2]</ref> features as input to our framework.</p><p>Action Segmentation. Earlier approaches detect action segments using a sliding window and filter out redundant hypotheses with non-maximum suppression <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19]</ref>. Other approaches model the temporal action sequence with a Markov model <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36]</ref> or an RNN <ref type="bibr" target="#b38">[39]</ref> to classify framewise actions. Lea et al. <ref type="bibr" target="#b25">[26]</ref> propose a spatiotemporal CNN for extracting features, which takes RGB and motion history images as input, and uses a semi-Markovian model for jointly segmenting and classifying actions.</p><p>Following the success in the speech synthesis domain, some studies have adopted a temporal convolutional network from the WaveNet model <ref type="bibr" target="#b45">[46]</ref>. <ref type="bibr">Lea et al. [25]</ref> propose two types of temporal convolutional networks (TCN) for action segmentation. Lei et al. <ref type="bibr" target="#b27">[28]</ref> propose a network with temporal deformable convolutions and a residual stream. These approaches use temporal pooling, which help to capture long-range dependencies between actions. However, pooling operations may lose temporal information that is indispensable for action segmentation.</p><p>To address this problem, Farha et al. <ref type="bibr" target="#b8">[9]</ref> propose a multistage architecture, namely MS-TCN. MS-TCN stacks several TCNs, which have dilated convolutions with residual connections. They also propose a loss function for penalizing over-segmentation errors. This architecture and the loss function enable the refinement of action segmentation results through each stage. Zhang et al. <ref type="bibr" target="#b47">[48]</ref> propose a bilinear pooling module and combines it with MS-TCN. The above two methods are capable of capturing dependencies between actions and reducing over-segmentation errors. Yifei et al. <ref type="bibr" target="#b17">[18]</ref> propose Graph-based Temporal Reasoning Module, which can be added to top of action segmenation models. Combining this module with MS-TCN improves the performance. However, there is still room for improvement, especially on large datasets with diverse action classes such as the Breakfast dataset <ref type="bibr" target="#b21">[22]</ref>. Existing works perform with high frame-wise accuracy, but produce many false positive. which results in over-segmentation error de-spite the multi-stage architecture and the smoothing loss. Our experiments are conducted with comparison to these state-of-the-art methods to demonstrate the efficacy of our proposal. Some works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> apply domain adaptation techniques to action segmentation, but we do not compare these works as their setting is different from our work.</p><p>Action Proposal Generation. Substantial research has been carried out in this domain, some of which is related to our approach. Existing methods for action proposal generation can broadly be divided into two types of approaches: anchor-based approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> and anchor-free approaches <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref>. Anchor-based approaches define multi-scale anchors with even intervals as proposals and generate confidence scores for each proposal. Anchor-free approaches first evaluate actionness or the likelihood of a frame being the start or end of an action, and then generate final predictions by leveraging these cues. Inspired by these anchor-free approaches, we additionally use an action boundary regression network for the action segmentation task. Our action boundary network regresses only boundary probabilities, regardless of it being a start or end of an action. Using predicted action boundaries, our framework refines frame-wise predictions to improve the performance of action segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Proposed Method</head><p>In this section, we introduce our approach for action segmentation, ASRF. Our framework decouples frame-wise action classification and action boundary regression. Our proposed framework consists of a long-term feature extractor and two branches, an Action Segmentation Branch (ASB) and a Boundary Regression Branch (BRB), as in <ref type="figure">Figure 1</ref>. A long-term feature extractor takes video features as input, expands the receptive field and captures long-term dependencies between action segments. Then, both branches take the features as inputs and frame-level action predictions and action boundary probabilities as outputs. Let X = [x 1 , ...x T ] ∈ R T ×D be the input to the ASRF, where T is the number of frames in a video and D is the dimension of the feature. Given X, our goal is to classify frame-level action classes C = [c 1 , ..., c T ]. For each frame, we predict action boundaries B = [b 1 , ..., b T ], and use this to improve the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Long-term Feature Extractor</head><p>Given X, the goal of a long-term feature extractor is to capture long-term dependencies between action segments and extract rich features X ∈ R T ×D , where D is the dimension of the feature. As a long-term feature extractor, we use a temporal convolutional network (TCN) with dilated residual layers proposed in <ref type="bibr" target="#b8">[9]</ref>. This architecture can convolve features with full temporal resolution and a large receptive field. This enables the network to capture long-term dependencies between action segments and extract shared features for the following branches. Our feature extractor consists of 10 dilated residual layers with 64 filters, each of which are followed by a dropout layer with a dropout rate of 0.5. The dilation rate is doubled at every residual convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Action Segmentation Branch</head><p>Given X , the goal of the Action Segmentation Branch (ASB) is to predict frame-wise action classes C. To predict action classes, we simply use a 1D convolutional layer followed by a softmax layer. However this prediction contains some errors such as over-segmentation errors. So we add a multi-stage architecture proposed in <ref type="bibr" target="#b8">[9]</ref> after the output layer. The first layer takes X as input and outputs the initial predictions, and the subsequent stages refine the predictions made from previous stages. This architecture facilitates capturing temporal dependencies and recognizes action segments, preventing over-segmentation errors. Herein, each stage consists of a single temporal convolution with a kernel size of 1 and 64 filters, 10 dilated residual convolutions, and another temporal convolution for reducing the feature dimension to the number of action classes. The parameters of each dilated convolutional layer are the same as in the long-term feature extractor. We set the number of stages to 3 after the initial prediction layer as per <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Boundary Regression Branch</head><p>Although stacking several TCNs improves performance of action segmentation, the predictions still contain oversegmentation errors. To address this, we introduce a Boundary Regression Branch (BRB) in addition to the ASB. Given X , the BRB aims to regress the action boundary probabilities P ∈ [0, 1] T in a video, which are used later for refining the action segmentation results from the ASB (see Section 3.4). Action boundaries are defined as frames when an action starts and ends irrespective of action classes. Unlike the methods which use a Hidden Markov model to determine the most probable sequence of actions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">42]</ref>, the BRB is class-agnostic, which eliminates the need for modeling the probabilities from each class to every other. Rather, the BRB only regresses the likelihood of general action boundaries. Therefore, the BRB requires far less data for training and can improve the robustness in comparison with the class-aware methods. We applies the the same structure as in the ASB to the BRB. The stacked structure also allows the refinement of action boundary predictions within the branch. In Section 4.4.1, we will explore the effect of the number of stages for action boundary regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Refining Action Segmentation Results</head><p>We describe how to refine the action segmentation results C from the ASB using action boundary probabilities P b from the BRB. First, we determine action boundaries B ∈ {0, 1} T from P b . We define B as the frame-level prediction where P b,t scores a local maximum and is over a certain threshold θ p . B is our prediction for action boundaries, so we divide action segments based on these predictions. Assuming that each segment contains a single action, we assign action classes to segments based on the action segmentation predictions from the ASB. We make final predictions by majority voting on the action class within each segment. Note that this refinement process is done only during inference. In the experiments, the efficacy of this refinement strategy will be illuminated (see Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function</head><p>Our framework outputs both frame-wise action predictions and action boundaries. Hence, our loss function is defined as:</p><formula xml:id="formula_0">L = L asb + λL brb<label>(1)</label></formula><p>where L asb and L brb are loss functions for the ASB and the BRB respectively and λ is the weight of the L brb . In our work, we set λ to 0.2 for GTEA and 0.1 for 50 Salads and the Breakfast dataset. In the following sections, we describe loss functions for each branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Loss Function for ASB.</head><p>Existing works usually adopt cross entropy as a classification loss.</p><formula xml:id="formula_1">L ce = 1 T t − log (y t,c )<label>(2)</label></formula><p>where y t,c is the action probability for class c at time t. However, this approach cannot penalize over-segmentation errors because there is no constraint for temporal transition of probabilities. To overcome this, the authors of <ref type="bibr" target="#b8">[9]</ref> additionally use the Truncated Mean Squared Error (TMSE).</p><formula xml:id="formula_2">L T M SE = 1 T N t,c∆ 2 t,c<label>(3)</label></formula><formula xml:id="formula_3">∆ t,c = ∆ t,c : ∆ t,c ≤ τ τ : otherwise (4) ∆ t,c = |log y t,c − log y t−1,c |<label>(5)</label></formula><p>where T is the length of a video, N is the number of classes and τ is a threshold for the transition of probabilities.</p><p>Herein, in addition to these two loss functions, we validate two other loss functions. First, we simply impose a class weight for the cross entropy loss L ce,cw ). The frequency of different action segments differs for each action class, leading to an imbalance during training. For weighting, we use median frequency balancing <ref type="bibr" target="#b7">[8]</ref>, where the weight to each class is calculated by dividing the median of class frequencies by each class frequency. In our experiments, we also compare this weighting method with Focal Loss <ref type="bibr" target="#b33">[34]</ref> (see Section 4.4.3).</p><p>Next, we introduce Gaussian Similarity-weighted TMSE (GS-TMSE) as a loss function, which improves upon TMSE. TMSE penalizes all frames in a video to smooth the transition of action probabilities between frames. However, this results in penalizing the frames where actions actually transition. To address this problem, we apply the Gaussian kernel to TMSE as follows:</p><formula xml:id="formula_4">L GS−T M SE = 1 T N t,c exp − x t − x t−1 2 2σ 2 ∆ 2 t,c<label>(6)</label></formula><p>where x t is an index of similarity for frame t and σ denotes variance. Because of the Gaussian kernel based on the similarity of frames, this function penalizes adjacent frames with large differences with a smaller weight. Here, we use the frame-level input feature for an index of similarity and set σ to 1.0. We also set τ in TMSE and GS-TMSE as 4, following <ref type="bibr" target="#b8">[9]</ref>.</p><p>The loss function for each prediction in the ASB is defined:</p><formula xml:id="formula_5">L as = L ce + L GS−T M SE<label>(7)</label></formula><p>Then we average the losses for each prediciton in the ASB as follows:</p><formula xml:id="formula_6">L asb = 1 N as i L as,i<label>(8)</label></formula><p>where N as is the number of predictions in the ASB (N as = 4 in our framework).</p><p>In the experiments, we compare various loss functions and their combinations for action segmentation as well as Equation <ref type="bibr" target="#b9">10</ref>. When combining them, we simply add each loss function except TMSE. We multiply L T M SE by 0.15 and then add the other loss functions as per <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Loss Function for BRB.</head><p>We use a binary logistic regression loss function for the action boundary regression:</p><formula xml:id="formula_7">L bl = 1 T T t=1 (w p y t · log p t + (1 − y t ) · log (1 − p t )) (9)</formula><p>where y t and p t are the ground truth and the action boundary probability for frame t, respectively. We weight positive samples by w p since the number of frames that are action boundaries is much smaller than that of the others. We calculate the ratio of positive data points over the whole training data and use the reciprocal of this as the weight. As in the ASB, we average the losses for each boundary prediction in the BRB as follow: <ref type="figure">Figure 2</ref>. An example where the prediction refined the by ground truth boundaries is worse than that of the ASB. The color bands differentiate action classes, and the horizontal direction denotes time. Our framework decides action classes of each segment by majority voting. Therefore segments can be reassigned as the wrong class.</p><formula xml:id="formula_8">L brb = 1 N br i L bl,i<label>(10)</label></formula><p>where N br is the number of predictions in the BRB (N br is also 4 in our framework).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. For our evaluation, we use three challenging datasets: 50 Salads <ref type="bibr" target="#b40">[41]</ref>, Georgia Tech Egocentric Activities (GTEA) <ref type="bibr" target="#b9">[10]</ref>, and the Breakfast dataset <ref type="bibr" target="#b21">[22]</ref>. The 50 Salads dataset contains 50 videos in which 25 people in total are preparing two kinds of mixed salads. The videos, each consisting of 9000 to 18000 RGB frames, depth maps and accelerometer data, are annotated with 17 action classes every frame. The GTEA dataset contains 28 videos of 7 types of daily activities, each performed by 4 different subjects. Each video is recorded egocentrically, from a camera mounted on a subject's head. The Breakfast dataset contains over 77 hours of videos, where 10 classes of actions are performed by 52 individuals in 18 different kitchens.</p><p>As per <ref type="bibr" target="#b8">[9]</ref>, for all datasets, we use spatiotemporal features extracted by I3D <ref type="bibr" target="#b1">[2]</ref> as input to both ASB and BRB. Following <ref type="bibr" target="#b8">[9]</ref>, we downsample the videos in the 50 Salads dataset from 30 to 15 fps to ensure consistency between the datasets. For evaluation, we use 5-fold cross-validation on 50 Salads dataset and 4-fold cross-validation on the others as in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Evaluation Metrics for Action Segmentation. The following three metrics are used for action segmentation as in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25]</ref>: frame-wise accuracy (Acc), segmental edit distance (Edit), and segmental F1 score with overlapping threshold k% (F1@k). Although frame-wise accuracy is commonly used as a metric for action segmentation, this measure is not sensitive to over-segmentation errors. Therefore, as well as frame-wise accuracy, we also use segmental edit distance <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26]</ref> and segmental F1 score <ref type="bibr" target="#b24">[25]</ref> because both of the latter penalize over-segmentation errors.</p><p>The segmental edit distance, S edit (G, P ) is a metric for measuring the difference between ground truth segments G = {G 1 , ..., G M } and predicted segments P = {P 1 , ..., P N }. This metric is calculated using the Levenshtein Distance <ref type="bibr" target="#b28">[29]</ref> between hypotheses and ground truths. For the sake of clarity, we report (1 − S edit (G, P )/max(M, N )) × 100 as segmental distance.</p><p>The segmental F1 score is averaged per class, in which a prediction is classified as correct if the temporal Intersection over Union (IoU) is larger than a certain threshold. This metric is invariant with respect to temporal shifts in predictions emanating from the ambiguity of the action boundary or human annotation noise.</p><p>Evaluation Metrics for Boundary Regression. The action boundary F1 score is used as an evaluation metric for the action boundary regression. We define the action boundary F1 score referencing the boundary F1 score used for semantic segmentation <ref type="bibr" target="#b5">[6]</ref>. Let B gt ∈ {0, 1} T denote if each frame is a boundary or not and P b ∈ [0, 1] T denote the predicted boundary probability map. We define B pred ∈ {0, 1} T as the frame-level prediction where P b,t is both over a threshold θ p and is a local maximum. The precision and the recall for action boundaries are defined as: <ref type="bibr" target="#b11">(12)</ref> where I[·] denotes the indicator function, d(·) is the L1 distance for temporal span and θ b is a threshold over timestamp. We set θ b as 5 and θ p as 0.5 in all experiments. Then the boundary F1 metric is defined as BF = 2×P recision×Recall P recision+Recall .</p><formula xml:id="formula_9">P recision = 1 |B pred | x∈B pred I [d (x, B gt ) &lt; θ b ] , (11) Recall = 1 |B gt | x∈Bgt I [d (x, B pred ) &lt; θ b ] ,</formula><p>In addition, assuming that we have an oracle ASB which outputs the ground truth frame-wise labels, we use the BRB and refine the output of this oracle ASB. Using a single BRB, this is the upper bound of the achievable score. Therefore, when comparing multiple BRBs, this upper bound can be used to evaluate the efficacy. Note that we do not assume an oracle BRB for refining action segmentation results from the ASB, as upper bounds are not achievable even with an oracle BRB (See <ref type="figure">Figure 2)</ref>.</p><p>Learning Scheme. We train the entire framework using the Adam optimizer with a learning rate of 0.0005 and batch size of 1 as per <ref type="bibr" target="#b8">[9]</ref>. We also find the optimal number of epochs with nested cross-validation. During inference, action segmentation results from the ASB are refined using predicted action boundaries from the BRB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparing ASRF with the state-of-the-art</head><p>We compare our proposed framework with existing methods on three challenging datasets: 50 Salads, Georgia Tech Egocentric Activities (GTEA) and the Breakfast dataset. <ref type="table" target="#tab_0">Table 1</ref> shows the results for the first two datasets.  Therein, our ASRF is superior to the state-of-the-art in terms of segmental edit distance and segmental F1 score with competitive frame-wise accuracy on each dataset, having up to 8.3% improvement for segmental edit distance, and up to 10.6% improvement for the segmental F1 score on 50 Salads and GTEA. As per <ref type="table" target="#tab_1">Table 2</ref>, our framework also outperforms the existing methods by a large margin on the Breakfast dataset with respect to all evaluation metrics.</p><p>We find that our framework offers better results for stricter overlap thresholds when calculating the segmental F1 score. On GTEA, our framework and MS-TCN with bilinear pooling <ref type="bibr" target="#b47">[48]</ref> perform similarly in terms of F1@10. However, our framework outperforms <ref type="bibr" target="#b47">[48]</ref> on F1@50 by 7.1%. This shows that our framework is capable of recognizing action segments which overlap markedly with the ground truth segments.</p><p>In the case of GTEA, the frame-wise accuracy for our framework is inferior to that of <ref type="bibr" target="#b47">[48]</ref>. We hypothesize that our framework predicted action boundaries off by some margin, therefore affecting the frame-wise accuracy. The rise in the segmental metrics support our hypothesis, as these errors are not accounted for here.</p><p>Qualitative results are presented in <ref type="figure" target="#fig_0">Figure 3</ref>. Predictions before refinement have some over-segmentation errors, but our framework can reduce these using action boundaries <ref type="figure" target="#fig_0">(Figure 3 (a)</ref>, (b), (c)). <ref type="figure" target="#fig_0">Figure 3 (d)</ref> shows a failure case of our framework. The BRB has limits in the sense that it cannot reassign completely incorrect segments of inferred action classes by the ASB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effect of our refining paradigm</head><p>In this section, we show the effect of refinement on the action segmentation metrics using action boundaries predicted by the BRB. In addition to the refinement with the BRB, we evaluate three other postprocessing methods: i) Relabeling. relabeling actions of segments shorter than a certain temporal span θ t with the action of the previous segment, ii) Smoothing. smoothing action probabilities using the 1D Gaussian filter with a kernel size K, and iii) Similarity. refinement with predicted action boundaries based on frame-level similarity. We measure the similarity of frames using frame-level features as in Section 3.5.1, and decide action boundary positions based on where the similarity is a local minimum. <ref type="table">Table 3</ref> shows the action segmentation results of the ASB and those after refinement on the 50 Salads dataset. As  observed, our refinement method with action boundaries improves action segmentation results by over 8.8% for every segmental metric with comparable frame-wise accuracy. Especially the stricter we set the IOU threshold for the segmental F1 score, the better the ASRF scores. This shows that the ASRF not only prevents over-segmentation errors but also generates predictions that highly overlap with the ground truth. In addition, our method is superior to other ways of postprocessing in terms of every metric. Smoothing has less impact on the metrics than our ASRF, and Similarity has a negative effect. Although Relabeling is better than the other two methods, this method is highly dependent on the hyperparameter θ t . With a large θ t , the method can reduce over-segmentation errors, but drops small action segments. On the other hand, our framework can detect action boundaries and capture action segments irrespective of temporal length, which results in better performance. <ref type="figure" target="#fig_1">Figure 4</ref> shows our refinement process on the Breakfast dataset. The BRB outputs some false positives, but they are ignored when the ASB captures action segments without over-segmentation errors (the middle of <ref type="figure" target="#fig_1">Figure 4</ref>). This shows that our framework not only refines frame-level action predictions with boundaries, but selects reasonable action boundaries during refinement as well. Therefore, the ASB and the BRB are mutually supportive. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with segment-level classifier</head><p>Our framework classifies actions by the frame level before predicting action boundaries, then reassigning action classes for each predicted action segment. Another variant is to use a TCN to classify each predicted action segment. Therefore, we compare our framework with a combination of a boundary regression model and segment-level classifier. We use a single-stage TCN with 10 dilated convolutions and two convolutions as a segment-level classifier. Note that we add global average pooling before the last convolutional layer of the single-stage TCN to aggregate temporal features. Otherwise, for both the action boundary regression and segment-level classification, we use the same networks as our method. As seen in <ref type="table" target="#tab_2">Table 4</ref>, our framework outperforms this variant by a large margin. This shows the importance of capturing long-range dependencies between action segments in the context of action segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Effect of the number of stages</head><p>We use a multi-stage architecture for BRB as well as ASB, which must be validated. Therefore we train a variety of multi-stage networks and evaluate their performances. Each network stage has 10 dilated convolutions and two convolu- tions like our proposed BRB. As can be observed in <ref type="table">Table 5</ref>, the three-stage architecture after the initial prediction layer outperforms others in terms of action segmentation. This shows that stacking TCNs helps to regress boundaries as well. However, using four stages on the 50 Salads dataset does not improve the predictions, which is thought to be the result of overfitting due to the size of the 50 Salads dataset.</p><p>We found that the precision of the BRB is low, but overdetected boundaries can still help the refinement of frame-wise action predictions. The reason of the low precision is that it is difficult to predict action boundaries precisely because of the ambiguity of human annotations and the action boundary itself. In addition, the θ b for calculating precision is only 5 frames, which is a strict threshold. Also, we highlight that the low precision of the BRB does not have a negative impact. In our framework, the ASB and the BRB are complementary, so even if nonexistent action boundaries are predicted, some of them disappear when combining outputs from the ASB. In addition, segmental edit distance and segmental F1 score with IoU thresholds tolerate the shifts of action starting and ending points. Results show that there is little to no correlation between boundary scores and action segmentation scores. This is because segmental metrics are tolerant to shifts in action boundary predictions. Therefore, oracle experiments are selected for the evaluation of boundary regression in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Impact of θ p for boundary regression</head><p>As described in Section 3.5.2, we set the threshold θ p = 0.5 for deciding action boundaries from outputs predicted by BRB. <ref type="table">Table 6</ref> shows the impact of θ p for boundary regression. Note that we do not use an oracle ASB in this experiment, because an overdetecting BRB would obtain perfect results, therefore setting θ p to a minimal value would be best performing. As in <ref type="table">Table 6</ref>, our framework is not particularly sensitive to θ p . For our framework, θ p = 0.5 showed the best balance. <ref type="table">Table 7</ref> compares the results of each combination of loss functions. Our proposed smoothing loss L GS−T M SE improves the segmental metrics by up to 3.6%. This shows that L GS−T M SE can smooth the transition of action probabilities without penalizing frames where actions actually transition, which results in alleviating over-segmentation errors. In addition, imposing class weight to the cross also improves all the metrics while L f ocal has a negative influence. Though tuning hyperparameters in L f ocal may still help to improve performance, it is costly to do so for every dataset. Our combination of loss functions enable better performance in comparison with loss functions used in existing works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Comparing loss functions for the ASB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed an effective framework for the action segmentation task. In addition to an action segmentation network, we use an action boundary regression network for refining action segmentation results with predicted action boundaries. We also compared and evaluated various loss functions and their combinations for action segmentation. Through experiments, it was confirmed that our framework is capable of recognizing action segments and alleviat-ing over-segmentation errors. Our framework outperforms state-of-the-art methods on the three challenging datasets, especially on the Breakfast dataset by a large margin, which contains a larger number of videos and action classes than the others.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results using (a) 50 Salads, (b) GTEA and (c) Breakfast dataset in comparison with predictions before refinement. (d) Failure example on Breakfast dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Examples of refinement process on the Breakfast dataset. The first row shows predictions from the ASB and the second row shows predicted boundary probabilities. Combining them, the ASRF outputs final predictions (the third row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparing our proposed method with existing methods on 50 Salads and GTEA.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>50 Salads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GTEA</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">F1@{10, 25, 50}</cell><cell>Edit</cell><cell>Acc</cell><cell cols="3">F1@{10, 25, 50}</cell><cell cols="2">Edit Acc</cell></row><row><cell>Bi-LSTM [39]</cell><cell>62.6 58.3</cell><cell>47.0</cell><cell cols="5">55.6 55.7 66.5 59.0 43.6</cell><cell>-</cell><cell>55.5</cell></row><row><cell>ED-TCN [25]</cell><cell>68.0 63.9</cell><cell>52.6</cell><cell cols="5">59.8 64.7 72.2 69.3 56.0</cell><cell>-</cell><cell>64.0</cell></row><row><cell>TDRN [28]</cell><cell>72.9 68.5</cell><cell>57.2</cell><cell cols="7">66.0 68.1 79.2 74.4 62.7 74.1 70.1</cell></row><row><cell>MS-TCN [25]</cell><cell>76.3 74.0</cell><cell>64.5</cell><cell cols="7">67.9 80.7 85.8 83.4 69.8 79.0 76.3</cell></row><row><cell cols="2">MS-TCN + BPGaussian [48] 78.4 75.8</cell><cell>66.7</cell><cell cols="7">71.0 80.6 86.7 84.3 72.7 77.2 82.3</cell></row><row><cell>MS-TCN + GTRM [18]</cell><cell>75.4 72.8</cell><cell>63.9</cell><cell cols="2">67.5 82.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ASRF</cell><cell>84.9 83.5</cell><cell>77.3</cell><cell cols="7">79.3 84.5 89.4 87.8 79.8 83.7 77.3</cell></row><row><cell>Improvement</cell><cell cols="9">+6.5 +5.1 +10.6 +8.3 +1.9 +2.7 +3.5 +7.1 +4.7 -5.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparing our proposed method with existing methods on the Breakfast dataset. * reported results by the author of<ref type="bibr" target="#b6">[7]</ref>.</figDesc><table><row><cell>Breakfast</cell><cell cols="3">F1@{10, 25, 50}</cell><cell>Edit</cell><cell>Acc</cell></row><row><cell>ED-TCN [25] *</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.3</cell></row><row><cell>HTK [24]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.7</cell></row><row><cell>TCFPN [7]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.0</cell></row><row><cell>HTK(64) [23]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.3</cell></row><row><cell>GRU [7] *</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.6</cell></row><row><cell>MS-TCN [9]</cell><cell>58.2</cell><cell>52.9</cell><cell>40.8</cell><cell>61.4</cell><cell>65.1</cell></row><row><cell>MS-TCN+GTRM [18]</cell><cell>57.5</cell><cell>54.0</cell><cell>43.3</cell><cell>58.7</cell><cell>65.0</cell></row><row><cell>ASRF</cell><cell>74.3</cell><cell>68.9</cell><cell>56.1</cell><cell>72.4</cell><cell>67.6</cell></row><row><cell>Improvement</cell><cell cols="5">+16.1 +14.9 +12.8 +13.7 +2.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparing our proposed method with the segment-level classification method on the 50 Salads dataset.</figDesc><table><row><cell></cell><cell>F1@{10, 20, 50}</cell><cell>Edit Acc</cell></row><row><cell>w/o post-processing</cell><cell cols="2">76.1 74.5 66.7 68.5 82.6</cell></row><row><cell>Relabeling (θt = 5)</cell><cell cols="2">81.5 79.7 71.6 74.8 82.6</cell></row><row><cell cols="3">Relabeling (θt = 15) 82.4 80.6 72.5 76.2 82.7</cell></row><row><cell>Smoothing (K = 5)</cell><cell cols="2">80.7 78.9 70.8 73.5 82.6</cell></row><row><cell cols="3">Smoothing (K = 15) 80.8 79.0 70.9 73.6 82.6</cell></row><row><cell>Similarity</cell><cell cols="2">39.8 30.9 18.1 32.8 40.0</cell></row><row><cell>ASRF</cell><cell cols="2">84.9 83.5 77.3 79.3 84.5</cell></row><row><cell cols="3">Table 3. Effect of refinement strategy using action boundaries from</cell></row><row><cell cols="2">the BRB on the 50 Salads dataset.</cell></row><row><cell></cell><cell>F1@{10, 20, 50}</cell><cell>Edit Acc</cell></row><row><cell cols="3">Segment classifier 51.8 49.0 40.0 42.8 58.5</cell></row><row><cell>ASRF</cell><cell cols="2">84.9 83.5 77.3 79.3 84.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .Table 7 .</head><label>567</label><figDesc>Effect of the number of stages for the BRB after the initial prediction layer on the 50 Salads dataset. Comparing the effect of θp for action boundary decision Lce,cw + LGS−T M SE 84.9 83.5 77.3 79.3 84.5 Comparing combinations of loss functions on the 50 Salads dataset.</figDesc><table><row><cell></cell><cell cols="3">Boundary Regression</cell><cell></cell><cell>Action Segmentation</cell><cell>Action Segmentation (Oracle)</cell></row><row><cell cols="5">Precision Recall F1 Score</cell><cell>F1@{10, 20, 50}</cell><cell>Edit Acc</cell><cell>F1@{10, 20, 50}</cell><cell>Edit Acc</cell></row><row><cell>No stage</cell><cell>18.8</cell><cell>76.4</cell><cell>29.9</cell><cell></cell><cell>82.0 80.4 74.4 76.7 82.7 86.8 86.2 83.5 82.2 88.5</cell></row><row><cell>1 stage</cell><cell>34.8</cell><cell>69.9</cell><cell>46.4</cell><cell></cell><cell>82.5 81.1 73.1 76.8 80.9 86.0 85.4 81.7 81.4 87.2</cell></row><row><cell>2 stages</cell><cell>38.7</cell><cell>66.0</cell><cell>48.7</cell><cell></cell><cell>83.3 82.3 76.4 78.3 82.7 86.2 85.8 84.1 82.7 88.6</cell></row><row><cell>3 stages</cell><cell>37.3</cell><cell>63.2</cell><cell>46.8</cell><cell></cell><cell>84.9 83.5 77.3 79.3 84.5 86.9 86.8 84.5 83.2 89.3</cell></row><row><cell>4 stages</cell><cell>37.4</cell><cell>63.0</cell><cell>46.9</cell><cell></cell><cell>84.4 82.9 75.7 78.1 82.7 86.5 86.0 83.8 82.5 87.8</cell></row><row><cell></cell><cell></cell><cell cols="4">Boundary Regression</cell><cell>Action Segmentation</cell></row><row><cell></cell><cell>θp</cell><cell cols="4">Precision Recall F1 Score</cell><cell>F1@{10, 20, 50}</cell><cell>Edit Acc</cell></row><row><cell></cell><cell>0.1</cell><cell>36.5</cell><cell></cell><cell>63.7</cell><cell>46.3</cell><cell>84.6 83.3 76.7 79.0 84.4</cell></row><row><cell></cell><cell>0.3</cell><cell>37.0</cell><cell></cell><cell>63.4</cell><cell>46.7</cell><cell>84.7 83.4 76.8 78.9 84.4</cell></row><row><cell></cell><cell>0.5</cell><cell>37.3</cell><cell></cell><cell>63.2</cell><cell>46.8</cell><cell>84.9 83.5 77.3 79.3 84.5</cell></row><row><cell></cell><cell>0.7</cell><cell>37.6</cell><cell></cell><cell>63.1</cell><cell>47.1</cell><cell>84.5 83.2 77.3 78.8 83.9</cell></row><row><cell></cell><cell>0.9</cell><cell>37.6</cell><cell></cell><cell>63.1</cell><cell>47.1</cell><cell>84.5 83.2 77.3 78.8 83.9</cell></row><row><cell>Loss Function</cell><cell cols="3">F1@{10, 20, 50}</cell><cell cols="2">Edit Acc</cell></row><row><cell>Lce + LT M SE</cell><cell cols="5">79.7 77.7 69.1 73.4 79.8</cell></row><row><cell>Lce + LGS−T M SE</cell><cell cols="5">80.9 79.4 72.7 74.6 81.6</cell></row><row><cell>L f ocal + LT M SE</cell><cell cols="5">77.3 75.8 67.1 71.3 78.0</cell></row><row><cell cols="6">L f ocal + LGS−T M SE 78.3 76.0 66.3 71.0 78.0</cell></row><row><cell>Lce,cw + LT M SE</cell><cell cols="5">83.3 81.8 75.1 77.0 81.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Short Note on the Kinetics-700 Human Action Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR:1907.06987</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action segmentation with mixed temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action segmentation with joint self-supervised temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What is a good evaluation measure for semantic segmentation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multistage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3575" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional Two-Stream Network Fusion for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3648" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving action segmentation via graph based temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV), THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Zisserman. The Kinetics Human Action Video Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<idno>CoRR:1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal cnns for finegrained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning convolutional action primitives for fine-grained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1642" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6742" to="6751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir Iosifovich Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Physics-Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollr. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning motion in feature space: Locally-consistent deformable convolution networks for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Khoi-Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Mac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><forename type="middle">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Do</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="6282" to="6291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mykhaylo Andriluka, and Bernt Schiele. A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1961" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">First person action recognition using deep learned descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suriya</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2620" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1250" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Comuter Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA Speech Synthesis Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Low-rank random tensor for bilinear pooling. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
