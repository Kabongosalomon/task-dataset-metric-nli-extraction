<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ben-Ari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rotman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the key factors of enabling machine learning models to comprehend and solve real-world tasks is to leverage multimodal data. Unfortunately, annotation of multimodal data is challenging and expensive. Recently, self-supervised multimodal methods that combine vision and language were proposed to learn multimodal representations without annotation. However, these methods often choose to ignore the presence of high levels of noise and thus yield sub-optimal results. In this work, we show that the problem of noise estimation for multimodal data can be reduced to a multimodal density estimation task. Using multimodal density estimation, we propose a noise estimation building block for multimodal representation learning that is based strictly on the inherent correlation between different modalities. We demonstrate how our noise estimation can be broadly integrated and achieves comparable results to state-of-the-art performance on five different benchmark datasets for two challenging multimodal tasks: Video Question Answering and Text-To-Video Retrieval. Furthermore, we provide a theoretical probabilistic error bound substantiating our empirical results and analyze failure cases. Code: https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal learning is a well established methodology for tackling complex and challenging artificial intelligence tasks such as Visual Question Answering <ref type="bibr" target="#b2">(Antol et al. 2015;</ref><ref type="bibr" target="#b15">Jang et al. 2017;</ref><ref type="bibr" target="#b8">Gao et al. 2018;</ref><ref type="bibr" target="#b66">Xu et al. 2017;</ref><ref type="bibr" target="#b6">Fan et al. 2019)</ref> and Text-to-Video Retrieval <ref type="bibr" target="#b23">(Liu et al. 2019;</ref><ref type="bibr" target="#b31">Mithun et al. 2018;</ref><ref type="bibr" target="#b68">Yu, Kim, and Kim 2018;</ref><ref type="bibr" target="#b27">Miech, Laptev, and Sivic 2018;</ref><ref type="bibr" target="#b44">Song and Soleymani 2019)</ref>. The motivation for gleaning information from multiple correlated data sources comes from how we as humans perceive the world and learn from experience. Using the correlation between speech and vision, a person is able to recognize objects by their names while learning the visual characteristics. Additionally, concepts can be learned separately and a combination can be comprehended automatically, e.g., 'running' and 'beach' vs. 'running on the beach'.</p><p>Manual annotation of large-scale datasets and specifically multimodal datasets is challenging and expensive. This difficulty results in a shortage which limits the progress of Copyright Â© 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. supervised machine learning and has become the key development bottleneck. Recently, to combat costs and effort of annotation, self-supervised machine learning <ref type="bibr" target="#b69">(Zhang, Isola, and Efros 2016;</ref><ref type="bibr" target="#b33">Noroozi and Favaro 2016;</ref><ref type="bibr" target="#b35">Pathak et al. 2016;</ref><ref type="bibr" target="#b30">Misra, Zitnick, and Hebert 2016;</ref><ref type="bibr" target="#b63">Wei et al. 2018;</ref><ref type="bibr" target="#b58">Vondrick, Pirsiavash, and Torralba 2016;</ref><ref type="bibr" target="#b45">Srivastava, Mansimov, and Salakhudinov 2015)</ref> presents new ways to better utilize the abundant unlabeled data on the web. However, most selfsupervised systems aim to learn from a single data modality, which limits their applicability.</p><p>In contrast to the above, <ref type="bibr" target="#b28">(Miech et al. 2019</ref><ref type="bibr" target="#b26">(Miech et al. , 2020</ref><ref type="bibr" target="#b0">Amrani et al. 2019;</ref><ref type="bibr" target="#b32">Moriya et al. 2019;</ref><ref type="bibr">Sun et al. 2019b,a)</ref> recently showed that unlabeled instructional videos could be used as training data for a self-supervised multimodal learning system due to the high correlation between the spoken word and the ongoing visuals. Unfortunately, such systems are forced to deal with high noise levels and thus yield suboptimal results as we show in this paper.</p><p>In this paper, we propose a novel noise robust multimodal representation learning building block for self-supervised learning. We utilize the inherent correlation between different modalities for efficient multimodal learning in the presence of extreme levels of noise. Specifically, we show that noise estimation can be reduced to a density estimation problem. We define a multimodal similarity function and show that based on this function, noise is correlated with sparsity and vice versa.</p><p>Ultimately, we integrate our proposed building block into an embedding model and learn superior joint video-text representations that achieve comparable state-of-the-art performance on five datasets: MSRVTT <ref type="bibr" target="#b67">(Xu et al. 2016)</ref>, LSMDC , MSVD (Chen and Dolan 2011), MSRVTT-QA <ref type="bibr" target="#b66">(Xu et al. 2017</ref>) and MSVD-QA <ref type="bibr" target="#b66">(Xu et al. 2017)</ref>; for two different tasks: Video Question Answering and Text to Video Retrieval. Additionally, we provide a theoretical probabilistic error bound substantiating our empirical results and analyze failure cases.</p><p>Contributions. The key contributions of this paper are four fold: 1. We show that the problem of noise estimation for multimodal data can be efficiently reduced to a multimodal density estimation task. 2. We propose a novel building block for noise-robust multimodal representation learning and demonstrate its integra-tion into the max margin ranking loss function. 3. We demonstrate comparable state-of-the-art performance on five datasets for two different challenging multimodal tasks by utilizing our approach for self-supervised multimodal learning with the HowTo100M dataset <ref type="bibr" target="#b28">(Miech et al. 2019</ref>). 4. We substantiate our empirical results with a theoretical analysis of the proposed method that includes a probabilistic error bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Self-Supervised Learning. Self-supervised learning methods strive to learn informative data representations by defining and solving a pretext task. In these tasks, pseudo labels can be generated automatically and compact data representations must be learned in order to solve these tasks. Many pretext tasks were proposed in recent years: colorizing grayscale images <ref type="bibr" target="#b69">(Zhang, Isola, and Efros 2016)</ref>, image jigsaw puzzle <ref type="bibr" target="#b33">(Noroozi and Favaro 2016)</ref>, image inpainting <ref type="bibr" target="#b35">(Pathak et al. 2016)</ref>, video frame order verification <ref type="bibr" target="#b30">(Misra, Zitnick, and Hebert 2016)</ref>, video future prediction <ref type="bibr" target="#b58">(Vondrick, Pirsiavash, and Torralba 2016;</ref><ref type="bibr" target="#b45">Srivastava, Mansimov, and Salakhudinov 2015)</ref>, audio-visual correspondence <ref type="bibr" target="#b20">(Korbar, Tran, and Torresani 2018;</ref><ref type="bibr" target="#b3">Arandjelovic and Zisserman 2017)</ref>, speechvisual correspondence <ref type="bibr" target="#b28">(Miech et al. 2019</ref><ref type="bibr" target="#b26">(Miech et al. , 2020</ref><ref type="bibr" target="#b0">Amrani et al. 2019;</ref><ref type="bibr" target="#b32">Moriya et al. 2019;</ref><ref type="bibr">Sun et al. 2019b,a)</ref>, etc. In this work, we focus on speech-visual correspondence in unlabeled instructional videos, where speech is converted to text using an automatic speech recognition system. Speech-visual correspondence is considered a difficult pretext task due to extremely noisy pseudo labels, yet it can be a highly advantageous task since it provides semantic information of visual features in the form of natural text. Such valuable information can be utilized to solve many challenging multimodal downstream tasks as we show in Section 6.</p><p>Multimodal Representation Learning. The word modality refers to a particular form of sensory perception, such as the visual and auditory modalities. A machine learning task or dataset is said to be multimodal when it includes a number of modalities. Multimodal representation learning frameworks can be divided into three types: (a) joint representation which aims to learn a shared semantic subspace <ref type="bibr" target="#b40">(Salakhutdinov and Hinton 2009;</ref><ref type="bibr" target="#b46">Srivastava and Salakhutdinov 2012;</ref><ref type="bibr" target="#b2">Antol et al. 2015)</ref>; (b) an encoder-decoder framework which aims to translate from one modality into another and keep their semantics consistent <ref type="bibr" target="#b24">(Mao et al. 2014;</ref><ref type="bibr" target="#b57">Venugopalan et al. 2015;</ref><ref type="bibr" target="#b36">Reed et al. 2016)</ref>; and (c) coordinated representation which aims to learn separated yet coordinated representations for each modality under some constraints <ref type="bibr" target="#b64">(Weston, Bengio, and Usunier 2010;</ref><ref type="bibr" target="#b7">Frome et al. 2013;</ref><ref type="bibr" target="#b43">Socher et al. 2014;</ref><ref type="bibr" target="#b60">Wang, Li, and Lazebnik 2016;</ref><ref type="bibr" target="#b56">Vendrov et al. 2016;</ref><ref type="bibr" target="#b53">Tian, Krishnan, and Isola 2019)</ref>. In this work, we focus on coordinated representations that enforce similarity among them. Our goal is to enforce the multimodal representations of similar 'concepts' to be close to each other. E.g., a video of a man running on the beach should be close in representation to the textual representation of 'a man running on the beach' as opposed to 'a man cooking in the kitchen'. The multimodal representation described above is highly valuable for solving multimodal machine learning tasks. If a machine learning model learns to link between the visuals and text of specific concepts it should be able, for example, to answer natural language questions about visual content, or do cross-modal retrieval more easily (Section 5.2).</p><p>Density Estimation. The aim of density estimation is to estimate the probability density function underlying the data, which is assumed to be i.i.d. Existing density estimation algorithms can be divided into two categories: (a) parametric or semi-parametric approaches such as Gaussian Mixture models <ref type="bibr" target="#b25">(McLachlan and Krishnan 2007;</ref><ref type="bibr" target="#b61">Wang and Wang 2015)</ref> and probabilistic graphical models <ref type="bibr" target="#b19">(Koller and Friedman 2009)</ref>; and (b) non-parametric approaches such as histograms, Splines <ref type="bibr" target="#b47">(Stone 1994)</ref>, neural network-based density estimation <ref type="bibr" target="#b54">(Uria, Murray, and Larochelle 2014;</ref><ref type="bibr" target="#b34">Papamakarios, Pavlakou, and Murray 2017)</ref> and Kernel Density Estimation <ref type="bibr" target="#b42">(Silverman 2018;</ref><ref type="bibr" target="#b52">Terrell and Scott 1992)</ref>. For an extended review on density estimation for high-dimensional data see <ref type="bibr" target="#b62">(Wang and Scott 2019)</ref>. In this work, we utilize multimodal k-Nearest Neighbor density estimation, which is a special case of Kernel Density Estimation. With it, we form a novel noise-robust multimodal representation learning model.</p><p>Learning with Noisy Data. Learning with noisy data can be divided into two approaches: (a) formulating explicit or implicit noise models to characterize the distribution of noisy and true labels using neural networks <ref type="bibr" target="#b10">(Goldberger and Ben-Reuven 2017;</ref><ref type="bibr" target="#b16">Jiang et al. 2018;</ref><ref type="bibr" target="#b48">Sukhbaatar et al. 2015)</ref>, graphical models <ref type="bibr" target="#b65">(Xiao et al. 2015;</ref><ref type="bibr" target="#b22">Li et al. 2017)</ref>, etc. and (b) using correction methods. E.g., relabeling the data during training <ref type="bibr" target="#b37">(Reed et al. 2015)</ref>, jointly optimizing the model's parameters and estimating true labels <ref type="bibr" target="#b51">(Tanaka et al. 2018</ref>), using noise-tolerant loss function <ref type="bibr" target="#b9">(Ghosh, Kumar, and Sastry 2017;</ref><ref type="bibr" target="#b55">Van Rooyen, Menon, and Williamson 2015)</ref> or noise tolerant training algorithms <ref type="bibr" target="#b21">(Li et al. 2019</ref>). However, these methods do not deal with multimodal association label (Definition 1) and often require a small set of data with clean labels to be available. In this work, we propose a true label estimation method for multimodal data that does not require availability of clean labels. We base our estimation on the correlation between modalities alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>In multimodal data, a sample is said to be noisy when two or more modalities do not share the same semantic meaning. For example, a video-text pair that is associated with each other, yet the text is not related to the ongoing visuals. Existing multimodal embedding models are susceptible to such noisy data, i.e., the model is likely to adjust itself to the noise in the data and thus yield sub-optimal results. This scenario is very common in the case of self-supervised multimodal learning and even when learning from unlabeled instructional videos. Although in these instructional videos there is some correlation between caption (speech transcription) and vision, unfortunately often a person is talking about something that is not present visually. For example, in the HowTo100M dataset <ref type="bibr" target="#b28">(Miech et al. 2019)</ref>, the authors manually inspected 400 randomly sampled clip-caption pairs and found that in about half there was not a single object or action mentioned in the caption that was also visually present in the video clip. To deal with noise, we suggest to utilize the inherent correlation between the different modalities that is based on the Definition and Assumption below. See <ref type="figure">Fig. 1</ref> for a visualization and a detailed explanation. Definition 1. A correctly (wrongly) associated pair is a clipcaption pair (v, c) that share (do not share) the same semantic meaning or concept, i.e., the caption c describes (does not describe) the ongoing visuals v. Assumption 1 (Mixture Model). The distributions of the videos and captions can be represented using a general mixture model of T components in the corresponding modality.</p><p>Denoting by a, b â {1, . . . , T }, respectively, the concept to which the video v and the caption c belong, we can write v|a â¼ p v (v|a) and c|b â¼ p c (c|b).</p><p>If Assumption 1 holds, then correctly associated pairs form dense clusters in both modalities that contain pairs that are also associated with each other (see <ref type="figure">Fig. 1a</ref>). Thus, by defining a multimodal similarity function (i.e., a similarity measure between pairs), we can formulate the task of finding correctly associated pairs simply as a multimodal density estimation task. In this formulation, pairs in dense areas will be more likely to be correctly associated, while pairs in sparse areas will be more likely to be wrongly associated (see <ref type="figure">Fig. 1b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Notation and Problem Formulation</head><formula xml:id="formula_0">Let {(v i , c i ) â R dv Ã R dc } M i=1</formula><p>denote the set of clip-caption pairs, where for each i, the video clip v i is associated with the caption sentence c i , and M denotes the size of the dataset. Let p i â {0, 1} denote a binary indicator for whether the pair (v i , c i ) is correctly associated (p i = 1) or wrongly associated (p i = 0). Let f v : R dv â R d and f c : R dc â R d denote the embedding functions of the videos and the captions, respectively, into a common representation space. The task of noise robust multimodal representation learning aims to map all of the data modalities to a single embedding space such that for</p><formula xml:id="formula_1">all v i that is correctly associated with c i , f v (v i ) â f c (c i ) in the sense of some similarity function.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Noise Estimation Using Multimodal Density Estimation</head><p>For the ease of notation, we will denote the pair as z i = (v i , c i ). Let us define a similarity function between pairs,</p><formula xml:id="formula_2">S : R dv+dc Ã R dv+dc â R. S(z i , z j ) min s(v i , v j ) âÎ¼ v Ï v , s(c i , c j ) âÎ¼ c Ï c ,<label>(1)</label></formula><p>where s can be, for example, the cosine similarity function s(x, y) = x y x y ;Î¼ v ,Î¼ c andÏ v ,Ï c are the sample means and standard deviations of each modality, i.e., the similarity values of each modality are normalized before taking the minimum. Using (1), a pair z i is close to z j only if v i is close to v j and c i is close to c j as well.</p><p>We denote byp i the estimated probability of z i being correctly associated, and compute it using its local k-NN density estimation normalized such thatp i â [0, 1]:</p><formula xml:id="formula_3">p i S i â min({S i } M i=1 ) max({S i } M i=1 ) â min({S i } M i=1 ) ,<label>(2)</label></formula><p>where,S</p><formula xml:id="formula_4">i = 1 K K k=1 S(z i , z i k ), i â [M ],<label>(3)</label></formula><p>z i k is the k-th nearest neighbor of z i and S is the similarity function defined in (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Soft Max Margin Ranking Loss</head><p>Integrating our noise estimation component from above into a max margin ranking loss function <ref type="bibr" target="#b41">Schroff, Kalenichenko, and Philbin 2015)</ref> is straightforward. We weight each pair z i with its fixed estimated probabilityp i of being correctly associated. We call it Soft Max Margin Ranking:</p><formula xml:id="formula_5">L sof târank = iâP p i jâNi max{0, s ij â s ii + Î´}+ max{0, s ji â s ii + Î´} , (4)</formula><p>where, P is the set of noisy associated (positive) pairs, N i is the set of negative pairs for clip-caption pair (v i , c i ),p i is defined in (2), s ij is the similarity score between the embedding of the clip-caption pair (f v (v i ), f c (c j )), and Î´ is the margin. The first term in the equation above is for matching a video with a negative caption and the second term is for matching a caption with a negative video. We note that a different integration approach is to discard samples withp i below a certain threshold. However, for real-data experiments we found the performance to be substantially worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>We present a theoretical probabilistic error upper bound of our noise estimation approach. For simplicity we assume the data is distributed under a Gaussian Mixture model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Probabilistic Error Upper Bound</head><p>Theorem 1. Let Z = (X, Y ) â R Ã R be a random pair of scalars satisfying Assumption 1 for a Gaussian mixture of T &gt; 1 equi-probable concepts. Denoting by (A, B) â {1, . . . , T } 2 the concepts to which the pair Z be-</p><formula xml:id="formula_6">longs, X|A = a â¼ N (Âµ a , Ï 2 a ) and Y |B = b â¼ N (Âµ b , Ï 2 b )</formula><p>. We further assume that each component of the mixture is 6Ïseparated, i.e., |Âµ i â Âµ j | &gt; 6Ï max and |Âµ i â Âµ j | &gt; 6Ï max for every i = j, where Ï max max t {Ï t } and Ï max max t {Ï t }.</p><p>(a) Multimodal data visualization. Each initial monomodal embedding space contains somewhat dense clusters of 'concepts', where a 'concept' could be a specific object or action (e.g., <ref type="bibr">'cutting', 'knife', 'check', 'tire', 'oven', etc.)</ref>. It is likely that correctly associated (Definition 1) pairs form dense clusters in both modalities that contain pairs that are also associated with each other and of the same 'concept' (GREEN, z1 -z6, z9 -z11). In contrast, a wrongly associated (Definition 1) pair may still belong to dense clusters in both modalities but those clusters are not likely to contain pairs that are associated with each other (RED, z7 and z8). Best viewed in color.</p><p>(b) Multimodal space defined by (1). Each point above ({zi} 11 i=1 ) represents a single pair from the left sub-figure. The distance between points that is visualized is computed based on (1). Given Assumption 1, in the multimodal space above, correctly associated pairs are correlated with high density and vice-versa. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Noise estimation using multimodal density estimation</head><formula xml:id="formula_7">Let P r(A = a = B = b) = Î· T (T â1) and P r(A = B = a) = 1âÎ· T for every a, b â {1, . . . , T }.</formula><p>The binary indicator P denoting whether the pair (X, Y ) is correctly associated consequently has P r(P = 1) = P r(A = B) = 1âÎ·, where Î· is the noise ratio of the dataset.</p><p>Let</p><formula xml:id="formula_8">{z i = (x i , y i )} M i=1</formula><p>be a finite sample of pairs drawn independently from the described model, and letS i be the average similarity between z i and its K nearest neighbors as defined in <ref type="formula" target="#formula_4">(3)</ref>, with S(z i , z j ) s(xi,xj )+s(yi,yj ) 2</p><p>, and s(x, x ) â|x â x |. Then, the following bounds hold for every Ï and t &gt; 0,</p><formula xml:id="formula_9">P (S i â¥ Ï | p i = 0) â¤ MS i | pi=0 (t) e tÏ ,<label>(5)</label></formula><formula xml:id="formula_10">P (S i â¤ Ï | p i = 1) â¤ MS i | pi=1 (ât) e âtÏ ,<label>(6)</label></formula><p>where,</p><formula xml:id="formula_11">MS i | pi (t) is the moment generating function ofS i |p i defined in Appendix A, Eq. (8).</formula><p>The proof is provided in Appendix A. It is important to remark that for the simplicity of analysis we assumed the pairs to be formed of scalars. While, from the first glance, this assumption might severely limit the possible configurations of the concepts in each of the modalities, in our analysis we made no assumptions whatsoever on the way the concepts are collocated in space, except the 6Ï-separation that can hold in any number of dimensions. The validity of the presented analysis in the multidimensional case is corroborated by the toy dataset example in Section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Numerical Simulations and Analysis</head><p>In this section, we present numerical simulations of the probabilistic error bounds in <ref type="formula" target="#formula_9">(5)</ref> and <ref type="formula" target="#formula_10">(6)</ref>. Our goal is to gain insight into: (a) why the method works; (b) the effect of the design choice (K) and dataset properties (Î·, M, T ) on the performance of the model; and (c) analyze possible failure cases. More specifically, we: (a) set Ï = Ï * such that P (S i â¥ Ï * | p i = 0) = P (S i â¤ Ï * | p i = 1), i.e., Ï * = MS i | pi=0 (t) Â· MS i | pi=1 (ât); (b) sweep a single parameter at a time, while the rest are fixed; and (c) optimize for t over <ref type="bibr">[1,</ref><ref type="bibr">100]</ref>.</p><p>Discussion. In <ref type="figure" target="#fig_0">Fig. 2a</ref> we study the effect of K. As expected, increasing K decreases the error bound initially and from a certain value ( K 0 ), the error bound increases. Not surprisingly, K 0 â M T Â· (1 â Î·), which is the average number of correctly associated pairs per concept. Throughout <ref type="figure" target="#fig_0">Figures  2a, 2b, 2c, 2d</ref> we see the error bound is influenced greatly by this equality, i.e., when K &gt; K 0 , the model performs well and when K â¤ K 0 , it starts to fail. Specifically, in <ref type="figure" target="#fig_0">Fig.  2c</ref> we show that the error bound goes to zero as the size of the dataset (M ) increases (another point of view is that K 0 is increased). For this reason we mark the point where KT = M (1âÎ·) by a red dashed line in <ref type="figure" target="#fig_0">Fig. 2</ref>. It is clear that for real-world data, concepts are usually not equi-probable and thus assigning a global value for K is sub-optimal. However, this finding allows us to better understand such failure cases and thus choose a reasonable K value. An additional instance where the method fails, gives us insight into why usually the method succeeds. The method will fail for a small number of concepts (T ) regardless of K 0 , because for a small number of concepts there is a higher chance that two or more wrongly associated pairs belong to the same pair of concepts in both modalities. Fortunately, in real-world data T is almost always large, and additionally as T increases, this problem is alleviated by a factor of O(T 2 ) (see Appendix A, Eq. <ref type="formula" target="#formula_2">(10)</ref>).</p><p>A simulation of this case is presented in Appendix D.</p><p>5 Experimental Settings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>Model. For a fair comparison to the baseline model HTM <ref type="bibr" target="#b28">(Miech et al. 2019)</ref>, we use the same class of non-linear embedding functions:</p><formula xml:id="formula_12">f (v) = (W v 1 v + b v 1 ) â¢ Ï(W v 2 (W v 1 v + b v 1 ) + b v 2 ), g(c) = (W c 1 c + b c 1 ) â¢ Ï(W c 2 (W c 1 c + b c 1 ) + b c 2 ), where W v 1 â R dÃdv , W c 1 â R dÃdc , W v 2 , W c 2 â R dÃd , b v 1 , b v 2 , b c 1 , b c 2 â R d</formula><p>are the learnable parameters, Ï is an element-wise sigmoid activation and â¢ is the element-wise multiplication. We use d v = 4096, d c = 300, and d = 6144.</p><p>Training dataset. We train our model using the HowTo100M <ref type="bibr" target="#b28">(Miech et al. 2019</ref>) narrated video dataset that consists of more than 1.2M videos accompanied with automatically generated speech transcription. Similarly to <ref type="bibr" target="#b28">(Miech et al. 2019)</ref>, we use the provided transcription to create pairs of clip-caption defined by each caption time stamp, where each video clip shorter than 5 seconds is extended symmetrically in time so that the duration is at least 5 seconds. Note that we only use 1.16M videos since some of the videos are no longer available for download.</p><p>Input caption features. For the word representations, we use the standard GoogleNews pre-trained word2vec embedding model . For the input sentence representations used in Section 3.3 we simply average word representation over each sentence.</p><p>Input visual features. We extract 2D features using Im-ageNet pre-trained Resnet-152 <ref type="bibr" target="#b12">(He et al. 2016</ref>) at a rate of 1 frame per second. We extract 3D features using Kinetics (Carreira and Zisserman 2017) pre-trained ResNeXt-101 16frames <ref type="bibr" target="#b11">(Hara, Kataoka, and Satoh 2018)</ref> at a rate of 24 frames per second. After temporal max pooling we concatenate 2D and 3D features to form a single feature vector per video clip.</p><p>Loss &amp; Optimization. We train our model using the Soft Max Margin loss function described in Section 3.4. We use the ADAM (Kingma and Ba 2015) optimizer with a fixed learning rate of 10 â3 .</p><p>Time complexity. Using FAISS (Johnson, Douze, and JÃ©gou 2019), computation of the Multimodal Density Estimation described in Section 3.3 is done in less than 15 hours over 10 CPUs. Training the model on the large HowTo100M dataset is done on a single V100 GPU and takes less than 24 hours.</p><p>Additional implementation details are included in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Downstream Tasks</head><p>Video Visual Question Answering (VQA). The Video VQA task comprises answering questions about videos presented in natural language <ref type="bibr" target="#b2">(Antol et al. 2015)</ref>. Essentially, an instance of VQA includes an input video and a free-form textual query regarding the content in the video, and an expected textual answer. To accommodate this task we fine-tune our learned multimodal representations and evaluate our model on two datasets: MSRVTT-QA and MSVD-QA <ref type="bibr" target="#b66">(Xu et al. 2017)</ref>. These datasets are based on existing video description datasets. See <ref type="table" target="#tab_6">Table 5a</ref> in Appendix G for detailed statistics of each dataset.</p><p>Most VQA models use a video and question as input, and the answer is presented as the output of an LSTM unit <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber 1997)</ref> or a softmax layer over a set of predetermined answers. However, these types of architectures do not fully utilize the information which exists in coordinated representations, i.e., the representation of the correct answer might likely be closely embedded to the visual representation, given the question. To better utilize our learned multimodal representations specifically for the VQA task, we use a similar architecture to <ref type="bibr" target="#b14">(Hu, Chao, and Sha 2018)</ref>, but for video. We learn two more sets of embeddings on top of the pre-trained embeddings that were learned with the HowTo100M dataset: a question+video embedding, i.e., we embed a concatenation of the question and video to a single feature vector; and an answer embedding. We train the model with a max margin ranking loss function to embed an answer close to its question+video. Inference is performed simply with a nearest neighbor search over the set of predetermined answers in the joint video+question and answer space. This model is very simple compared to most VQA models, yet as we show in <ref type="table" target="#tab_2">Table 2</ref> it is very powerful when built on effective self-supervised pre-trained joint embeddings.</p><p>Text-To-Video Retrieval. Text-To-Video Retrieval includes retrieval of video clips based on textual description <ref type="bibr" target="#b23">(Liu et al. 2019;</ref><ref type="bibr" target="#b31">Mithun et al. 2018;</ref><ref type="bibr" target="#b44">Song and Soleymani 2019)</ref>. With a learned joint representation space, retrieval is performed with a nearest neighbor search over the joint embedding space. To evaluate our model we use three differ-  <ref type="figure">. (a, c)</ref>: T-SNE of input embeddings. As we can see, separating between samples with p i = 1 and samples with p i = 0 is non-trivial. (b): Cumulative Distribution Function ofp i of toy dataset. The green solid line is the CDF ofp i |p i = 1, while the red dashed line is the inverse CDF ofp i |p i = 0. Assuming a binary prediction is made based on a hard threshold, it is possible to extract the precision and recall for each threshold from the figure above. For example, for the threshold 0.48, both precision and recall are 0.9. Best viewed in color. ent datasets: MSRVTT, MSVD and LSMDC <ref type="bibr" target="#b67">(Xu et al. 2016;</ref><ref type="bibr" target="#b5">Chen and Dolan 2011;</ref>. We use the standard evaluation metrics: recall at K (R@K) for K = 1, 5, 10 and median recall (MR). See <ref type="table" target="#tab_6">Table 5b</ref> in Appendix G for detailed statistics of each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Analysis</head><p>A toy dataset illustrative results are presented in Section 6.1. Comparison to ablative baselines and state-of-the-art models is presented in Section 6.2. A design choice analysis is presented in Appendix F. Qualitative examples are presented in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Multidimensional Toy Dataset</head><p>In this section, we demonstrate the effectiveness of our method visually using a toy (synthesized) dataset of mixture of Gaussians of T components in each modality, </p><formula xml:id="formula_13">{N (Âµ c t , Î£ c t )} T t=1 and {N (Âµ v t , Î£ v t )} T</formula><formula xml:id="formula_14">v i â¼ N (Âµ v t , Î£ v t ), c i â¼ N (Âµ c t , Î£ c t ), such that t â [T ], i â [M ]. Wrongly associated pairs are repre- sented by v i â¼ N (Âµ v m , Î£ v m ), c i â¼ N (Âµ c n , Î£ c n ), such that m = n, {m, n} â [T ], i â [M ]</formula><p>. Î· â [0, 1] is the noise ratio, such that a wrongly associated pair is sampled with probability Î·, and a correctly associated pair is sampled with probability 1 â Î·.</p><p>In</p><formula xml:id="formula_15">our experiment, v i â R dv , c i â R dc , d v = 128, d c = 128; Âµ c t â R dc , Âµ v t â R dv , ât â [T ]</formula><p>are sampled from a uniform multivariate distribution U dc (0, 1), U dv (0, 1), respectively for caption and video; Î£ c t â R dcÃdc , Î£ v t â R dvÃdv are diagonal matrices where the diagonals are sampled from a multivariate uniform distribution, U dc (0, 0.3), U dv (0, 0.3), respectively for caption and video; Î· = 0.5, M = 1250, T = 50; k = 4 (k-NN parameter).</p><p>In <ref type="figure" target="#fig_1">Figures 3a and 3c</ref> we visualize T-SNE graphs for caption and video embedding spaces, respectively. In <ref type="figure" target="#fig_1">Fig. 3b</ref> we visualize the empirical cumulative distribution function ofp i (2), the estimated probability of being correctly associated. Additionally, in Appendix B <ref type="figure">Fig. 4</ref> we empirically reproduce the theoretical graphs in <ref type="figure" target="#fig_0">Fig. 2 and Fig. 8</ref> using the multidimensional toy dataset. These results corroborate the validity of the analysis presented in Thm. 1 in the multidimensional case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablative Baselines and SOTA Models</head><p>We compare our proposed model against two ablative baselines and multiple task-specific state-of-the-art (SOTA) models:</p><p>HTM-PT <ref type="bibr" target="#b28">(Miech et al. 2019)</ref>. The model (architecture and loss function) used in <ref type="bibr" target="#b28">(Miech et al. 2019</ref>). This baseline is pre-trained (PT) on the HowTo100M dataset. It is the exact architecture described in Section 5.1 for our model. The only differentiating element is the loss function. <ref type="bibr" target="#b28">(Miech et al. 2019</ref>) use the max margin ranking loss function, while we use our proposed Soft Max Margin ranking loss function. Since the model is also trained identically to our own model it is clear that any gain in performance over this baseline is due to our novel noise estimation based density estimation component.</p><p>HTM-no-PT <ref type="bibr" target="#b28">(Miech et al. 2019</ref>). The same model from above, but without pre-training (no-PT) on the HowTo100M dataset. The (under) performance of this baseline on downstream tasks demonstrates the potential gain of utilizing selfsupervised speech-visual correspondence training.</p><p>Task specific state-of-the-art models. After fine-tuning for downstream tasks we compare our proposed model to state-of-the-art models for each task and each dataset. <ref type="bibr" target="#b8">(Gao et al. 2018;</ref><ref type="bibr" target="#b66">Xu et al. 2017;</ref><ref type="bibr" target="#b6">Fan et al. 2019;</ref><ref type="bibr" target="#b15">Jang et al. 2017)</ref> for VQA, and <ref type="bibr" target="#b23">(Liu et al. 2019;</ref><ref type="bibr" target="#b31">Mithun et al. 2018;</ref><ref type="bibr" target="#b68">Yu, Kim, and Kim 2018;</ref><ref type="bibr" target="#b27">Miech, Laptev, and Sivic 2018;</ref><ref type="bibr" target="#b28">Miech et al. 2019</ref><ref type="bibr" target="#b26">Miech et al. , 2020</ref> for Text-To-Video Retrieval. <ref type="table" target="#tab_1">Tables 1 and 2</ref> show the result for Text-To-Video Retrieval and Video Question Answering, respectively. <ref type="table">Table 3</ref> in Appendix E shows the results for Zero-Shot Text-To-Video Retrieval under 'unfair' settings. We summarize key insights below:    <ref type="bibr" target="#b6">(Fan et al. 2019)</ref> tion Answering and Text-To-Video Retrieval on five different datsets.</p><formula xml:id="formula_16">-</formula><p>-We set a new state-of-the-art performance for two Visual Question Answering datasets: MSRVTT-QA and MSVD-QA.</p><p>-We set a new state-of-the-art performance for Zero-Shot Text-To-Video Retrieval on two datasets: LSMDC and MSVD. For MSRVTT, we hypothesize that MIL-NCE <ref type="bibr" target="#b26">(Miech et al. 2020)</ref> performs better due to two reasons: i) they train end-to-end, unlike our baseline model, which operates on pre-proccessed embeddings; and ii) they utilize additional clip-caption pairs by associating a clip with its adjacent (in time) captions. In fact, when those additional pairs are not used our model outperforms theirs.</p><p>-We set a new state-of-the-art performance for (fine-tuned) Text-To-Video Retrieval on two datasets: MSRVTT and MSVD.</p><p>-We demonstrate that our model outperforms or is at least on par with the performance of HTM-PT <ref type="bibr" target="#b28">(Miech et al. 2019)</ref> even given a setting which is a clear disadvantage such as training it without 3D features (i.e., only 2D). See <ref type="table">Table 3</ref> in Appendix E. This shows:</p><p>(i) The power of our noise estimation method and its potential.</p><p>(ii) Integrating our multimodal density estimation component allows saving time and/or computation power by training and running inference with only 2D features, without (or with minor) performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary</head><p>In this work, we showed that the problem of noise estimation in multimodal data can be effectively reduced to a multimodal density estimation task. Based on this efficient noise estimation we proposed a novel building block for noise robust multimodal representation learning that can be integrated into many multimodal learning models and improve their performance instantly. We demonstrated how to integrate our building block into the max margin ranking loss function (Soft Max Margin) and it can similarly be integrated into various architectures and losses. We trained Soft Max Margin on the self-supervised proxy task of speechvisual correspondence that is known to be highly noisy. We further evaluated Soft Max Margin on two different downstream tasks: Visual Question Answering and Text-to-Video Retrieval; and achieved comparable state-of-the-art performance on five different datasets. For supporting the empirical results and analyzing failure cases, we provided a theoretical probabilistic error bound. These results emphasize the importance of self-supervised multimodal representation learning for advancing the state of the art in challenging multimodal artificial intelligence tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Reproducing Empirically the Theoretical Results</head><p>In <ref type="figure">Fig. 4</ref> we empirically reproduce the theoretical graphs in Section 4 <ref type="figure" target="#fig_0">Fig. 2 and Fig. 8</ref> using the multidimensional toy dataset described in Section 6.1. These results corroborate the validity of the analysis presented in Thm. 1 in the multidimensional case. We note that across all sub-figures in <ref type="figure">Fig. 4</ref>, similar trends are observed in comparison to the theoretical graphs in <ref type="figure" target="#fig_0">Fig. 2 and Fig. 8</ref>. More specifically, across <ref type="figure">Fig. 4a</ref>, 4b, 4c, 4d we observe the influence of K0 (mentioned in Section 4) on the performance of our suggested approach. Additionally, in <ref type="figure">Fig. 4e</ref>, we observe the same 'low T failure case' mentioned in Section 4.  <ref type="figure">Figure 4</ref>: Empirical error of multidimensional toy dataset. In the figures above we empirically reproduce the theoretical graphs in <ref type="figure" target="#fig_0">Fig. 2 and Fig. 8</ref> using the multidimensional toy dataset mentioned above. The results help in validating that the analysis that was done in Thm. 1 for a single dimension, also holds for a multidimensional space. The vertical axis is (1 â precision), where the threshold was chosen such that F1 Score is maximized. In each figure we mark with a red dashed vertical line the point at which the equation K = M T Â· (1 â Î·) holds. We note that across all figures, similar trends are observed in comparison to the theoretical graphs in <ref type="figure" target="#fig_0">Fig. 2 and Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 High Score Examples and Their k Nearest Neighbours</head><p>In this section, we present two examples of clip-caption pairs with highp (see Eq. (2)) and their nearest neighbours in the multimodal space that contributed to their high score. One example includes in its caption the word 'bye' <ref type="figure" target="#fig_4">(Fig. 5</ref>) and the other the word 'mix' <ref type="figure" target="#fig_5">(Fig 6)</ref>. The figures below include only one representative frame from each video clip. Thus, it is worth mentioning that all video clips in <ref type="figure" target="#fig_4">Fig. 5 and 6</ref> in fact contain a 'waiving goodbye' action and a 'mixing' action, respectively. It is important to note that these examples were extracted only based on our noise estimation component and are not based on the learned shared embedding space.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Low T Failure Case Simulation</head><p>One instance where the method fails, gives us insight into why usually the method succeeds. The method will fail for a small number of concepts (T ) and regardless of K0, because for a small number of concepts there is a higher chance that two or more wrongly associated pairs belong to the same pair of concepts in both modalities. Fortunately, in real-world data T is almost always large, and additionally as T increases this problem is alleviated by a factor of O(T 2 ) (see Appendix A, Eq. (10)). A simulation of this case is presented in the figure below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Zero-shot Text-To-Video retrieval in 'unfair' settings</head><p>We demonstrate that our model outperforms or is at least on par with the performance of HTM-PT <ref type="bibr" target="#b28">(Miech et al. 2019</ref>) even given a setting which is a clear disadvantage such as training it without 3D features (i.e., only 2D). See <ref type="table">Table 3</ref> in Appendix E. This shows:</p><p>(i) The power of our noise estimation method and its potential.</p><p>(ii) Integrating our multimodal density estimation component allows saving time and/or computation power by training and running inference with only 2D features, without (or with minor) performance degradation.</p><p>We note that specifically for MSRVTT dataset our 2D-based model actually performs slightly better than our 2D+3D-based model. This result requires further investigation. <ref type="table">Table 3</ref>: Zero-shot Text-To-Video retrieval in 'unfair' settings. For MR the lower the better. We show below that our model outperforms or is at least on par with the performance of HTM-PT <ref type="bibr" target="#b28">(Miech et al. 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Design-Choice Analysis</head><p>In this section, we evaluate the effect of two important design choices on the Zero-Shot Text-To-Video Retrieval task: (a) k-NN parameter; and (b) S(Â·, Â·), the multimodal similarity function. In <ref type="table" target="#tab_4">Table 4a (K analysis)</ref> we see a similar trend as in Section 4, <ref type="figure" target="#fig_0">Fig. 2a</ref>, i.e., increasing K decreases the error initially and from a certain value, the error increases. In <ref type="table" target="#tab_4">Table 4b</ref> (S(Â·, Â·) analysis), it is evident that there is a slight advantage of using the minimum function over the mean function, yet it is not conclusive, i.e., the mean function that is used in Thm. 1 for simplicity of analysis is a decent design choice as well.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional Implementation Details</head><p>Sampling strategy. For a fair comparison to the baseline model HTM <ref type="bibr" target="#b28">(Miech et al. 2019)</ref>, we follow the same sampling strategy. More specifically, half of the negative pairs, (vi, cj) : i = j are sampled such that they belong to the same video clip, while the other half are sampled such that they do not.</p><p>k-NN Computation. To compute k-NN efficiently over the entire dataset we use FAISS <ref type="bibr" target="#b17">(Johnson, Douze, and JÃ©gou 2019)</ref>. Due to the high correlation between video segments of the same video, in practice we extract K nearest neighbors that originate from different videos, where K = 4.   <ref type="bibr" target="#b31">(Mithun et al. 2018;</ref><ref type="bibr" target="#b68">Yu, Kim, and Kim 2018;</ref><ref type="bibr" target="#b27">Miech, Laptev, and Sivic 2018)</ref> for a fair comparison.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Numerical simulations of our probabilistic error upper bound. From left to right: sweep over K, Î·, M and T . We mark with a red dashed vertical line where K = M T Â· (1 â Î·) holds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Toy dataset Visualizations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Our model consistently outperforms the baselines (HTM-PT, HTM-no-PT (Miech et al. 2019)) in both Visual Ques-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>'Bye' Cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>'Mix' Cluster C.2 High and Low Score Examples for the Same QueryIn this section, we present ten examples of high and lowp score clip-caption pairs for the same query. These examples visually illustrate how our noise estimation component is able to distinguish between wrongly associated pairs and correctly associated pairs successfully.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>High and lowp score examples for the same query. Each row contains two video clips (represented by a single frame) that include in their caption the same query (right column). The left column (GREEN) contains clips with highp, while the middle column (RED) contains clips with lowp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Low T failure case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>even given a setting which is a clear disadvantage such as training it without 3D features (no-3D), i.e., only 2D features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Text-To-Video retrieval. Zero-Shot: training was done only with HowTo100M dataset. Fine-Tuned: model was fine-tuned with the relevant benchmark dataset. For MR the lower the better. *: results for MSRVTT and LSMDC are from<ref type="bibr" target="#b28">(Miech et al. 2019)</ref>, while results for MSVD have been reproduced.</figDesc><table><row><cell>: CE</cell></row></table><note>â  : MIL-NCE (Miech et al. 2020) use additional clip-caption pairs.â¡</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Video Question Answering. Results of ST-VQA and Co-Mem taken from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Design-Choice Analysis. Recall@5 results for Zero-Shot Text-To-Video Retrieval.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Statistics of datasets. For retrieval we use the same test set split as defined by</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. {Ïi} T i=1 are sampled uniformly from (0, min i,j |Âµi â Âµj|/6], and {Ï i } T i=1 are sampled uniformly from (0, min i,j |Âµ i â Âµ j |/6], such that the three sigma limit assumption in Theorem 1 is met.3. We setÏ = Ï * such that P (Si â¥ Ï * | pi = 0) = P (Si â¤ Ï * | pi = 1), i.e., Ï * = MS i | p i =0 (t) Â· MS i | p i =1 (ât).4. Optimization for t is done over [1, 2, ..., 100].5. Each experiment is repeated 10 times and the average is presented.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Overview</head><p>In Appendix A we present the proof of theorem 1. In Appendix B we empirically reproduce the theoretical graphs in Section 4 using the multidimensional toy dataset described in Section 6.1 for corroborating the validity of the analysis presented in Thm. 1 in the multidimensional case. In Appendix C we present qualitative examples. In Appendix D we present a visualization of the 'low T failure case' mentioned in Section 4.2. In Appendix E we compare the performance of our model trained without 3D features (i.e., only 2D) to the baseline model that is trained with 2D+3D features. In Appendix F we evaluate the effect of two important design choices. In Appendix G we provide additional implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theorem 1</head><p>Proof. Given a pair zi = (xi, yi) with (ai, bi) = (a, b), we find the moment generating function ofSi. We first start with the moment generating functions of s(x ik , xi) and s(y ik , yi). We note that âs(x ik , xi) and âs(y ik , yi) are both distributed according to the folded normal distribution. For the general case, the moment generating function of â|X|, X â¼ (Âµ, Ï), is</p><p>where Î¦(Â·) is the normal cumulative distribution function. Denote the cardinality of the set of pairs that originate from the components (a, b) of zi asmi, i.e.,mi |{zj : aj = a, bj = b, j = 1, . . . , M }|. Recall the three sigma limit assumption between each pair of components. Thus, by using the law of total expectation, the moment generating function ofSi | pi can be expressed as</p><p>where,</p><p>where all (Î±j, Î²j) = (a, b), and</p><p>We used the fact that the moment generating function of V = c1U1 + Â· Â· Â· + cnUn is given by MV (t) = MU 1 (c1t) Â· Â· Â· MU n (cnt), when the ci's are scalars and the Ui's are independent random variables. Applying the Chernoff bound concludes the proof.</p><p>To make sense of this error bound we performed multiple numerical simulations (See Section 4.2), with the following set up:</p><p>1. {Âµi, Âµ i } T i=1 are sampled uniformly from <ref type="bibr">[0,</ref><ref type="bibr">100]</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to Detect and Retrieve Objects From Unlabeled Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hakim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3713" to="3717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Heterogeneous memory enhanced multimodal attention model for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Motion-appearance co-memory networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6576" to="6585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training deep neuralnetworks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning answer embeddings for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5428" to="5436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mentor-Net: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>StockholmsmÃ¤ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7763" to="7774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1910" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Use What You Have: Video Retrieval Using Representations From Collaborative Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6632</idno>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The EM algorithm and extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">382</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Howto100M: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grounding Object Detections With Transcriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Moriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The longshort story of movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="209" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3202" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Density estimation for statistics and data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Polysemous visual-semantic embedding for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1979" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning representations for multimodal data with deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning workshop</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">79</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The use of polynomial splines and their tensor products in multivariate function estimation. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="118" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Estrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Variable kernel density estimation. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Terrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="1236" to="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Order-Embeddings of Images and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>Bengio, Y.</editor>
		<editor>and LeCun, Y.</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Translating Videos to Natural Language Using Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1494" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning deep structurepreserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Nonparametric multivariate density estimation using mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="349" to="364" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Nonparametric density estimation for high-dimensional data-Algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1461</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8052" to="8060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1645" to="1653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
