<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Neural Networks and Log-linear Models to Improve Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-11-18">18 Nov 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><forename type="middle">Huu</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">New York University New York</orgName>
								<address>
									<postCode>10003</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
							<email>grishman@cs.nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Neural Networks and Log-linear Models to Improve Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-11-18">18 Nov 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The last decade has witnessed the success of the traditional feature-based method on exploiting the discrete structures such as words or lexical patterns to extract relations from text. Recently, convolutional and recurrent neural networks has provided very effective mechanisms to capture the hidden structures within sentences via continuous representations, thereby significantly advancing the performance of relation extraction. The advantage of convolutional neural networks is their capacity to generalize the consecutive kgrams in the sentences while recurrent neural networks are effective to encode long ranges of sentence context. This paper proposes to combine the traditional feature-based method, the convolutional and recurrent neural networks to simultaneously benefit from their advantages. Our systematic evaluation of different network architectures and combination methods demonstrates the effectiveness of this approach and results in the state-of-the-art performance on the ACE 2005 and SemEval dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We studies the relation extraction (RE) problem, one of the important problem of information extraction and natural language processing (NLP). Given two entity mentions in a sentence (relation mentions), we need to identify the semantic relationship (if any) between the two entity mentions. One example is the recognition of the Located relation between "He" and "Texas" in the sentence "He lives in Texas".</p><p>The two methods dominating RE research in the last decade are the feature-based method <ref type="bibr" target="#b2">(Kambhatla, 2004;</ref><ref type="bibr">Boschee et al., 2005;</ref><ref type="bibr">Zhou et al., 2005;</ref><ref type="bibr" target="#b0">Grishman et al., 2005;</ref><ref type="bibr" target="#b1">Jiang and Zhai, 2007;</ref><ref type="bibr">Chan and Roth, 2010;</ref><ref type="bibr">Sun et al., 2011)</ref> and the kernel-based method <ref type="bibr">(Zelenko et al., 2003;</ref><ref type="bibr">Culotta and Sorensen, 2004;</ref><ref type="bibr">Bunescu and Mooney, 2005a;</ref><ref type="bibr">Bunescu and Mooney, 2005b;</ref><ref type="bibr">Zhang et al., 2006;</ref><ref type="bibr">Zhou et al., 2007;</ref><ref type="bibr">Qian et al., 2008;</ref><ref type="bibr">Nguyen et al., 2009;</ref><ref type="bibr">Plank and Moschitti, 2013)</ref>. These research extensively studies the leverage of linguistic analysis and knowledge resources to construct the feature representations, involving the combination of discrete properties such as lexicon, syntax, gazetteers. Although these approaches are able to exploit the symbolic (discrete) structures within relation mentions, they also suffer from the difficulty to generalize over the unseen words <ref type="bibr">(Gormley et al., 2015)</ref>, motivating some very recent work on employing the continuous representations of words (word embeddings) to do RE. The most popular method involves neural networks (NNs) that effectively learn hidden structures of relation mentions from such word embeddings, thus achieving the top performance for RE <ref type="bibr">(Zeng et al., 2014;</ref><ref type="bibr">dos Santos et al., 2015;</ref><ref type="bibr">Xu et al., 2015)</ref>.</p><p>The NN research for relation extraction and classification has centered around two main network architectures: convolutional neural networks (CNNs) <ref type="bibr">(dos Santos et al., 2015;</ref><ref type="bibr">Zeng et al., 2015)</ref> and recursive/recurrent neural networks <ref type="bibr">(Socher et al., 2012;</ref><ref type="bibr">Xu et al., 2015)</ref>. The distinction between convolutional neural networks and recurrent neural networks (RNNs) for RE is that the former aim to generalize the local and consecutive context (i.e, the k-grams) of the relation mentions (Nguyen and Grishman, 2015a) while the latter adaptively accumulate the context information in the whole sentence via the memory units, thereby encoding the global and possibly unconsecutive patterns for RE <ref type="bibr">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr">Cho et al., 2014)</ref>. Consequently, the traditional feature-based method (i.e, the log-linear or MaxEnt model with hand-crafted features), the CNNs and the RNNs tend to focus on different angles for RE. Guided from this intuition, in this work, we propose to combine the three models to further improve the performance of RE.</p><p>While the architecture design of CNNs for RE is quite established due to the extensive studies in the last couple of years, the application of RNNs to RE is only very recent and the optimal designs of RNNs for RE are still an ongoing research. In this work, we first perform a systematic exploration of various network architectures to seek the best RNN model for RE. In the next step, we extensively study different methods to assemble the log-linear model, CNNs and RNNs for RE, leading to the combined models that yield the state-of-the-art performance on the ACE 2005 and SemEval dataset. To the best of our knowledge, this is the first work to systematically examine the RNN architectures as well as combine them with CNNs and the traditional feature-based approach for RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>Relation mentions consist of sentences marked with two entity mentions of interest. In this paper, we examine two different representations for the sentences in RE: (i) the standard representation, called SEQ that takes all the words in the sentences into account and (ii) the dependency representation, called DEP that only considers the words along the dependency paths between the two entity mention heads of the sentences. In the following, unless indicated specifically, all the statements about the sentences hold for both representations SEQ and DEP.</p><p>Throughout this paper, for convenience, we assume that the input sentences of the relation mentions have the same fixed length n. This can be achieved by setting n to the length of the longest input sentences and padding the shorter sentences with a special token. Let W = w 1 w 2 . . . w n be the input sentence of some relation mention, where w i is the i-th word in the sentence. Also, let w i 1 and w i 2 be the two heads of the two entity mentions of interest. In order to prepare the relation mention for neural networks, we first transform each word w i into a real-valued vector x i using the concatenation of the following seven vectors, motivated by the previous research on neural networks and feature analysis for RE <ref type="bibr">(Zhou et al., 2005;</ref><ref type="bibr">Sun et al., 2011;</ref><ref type="bibr">Gormley et al., 2015)</ref>.</p><p>-The real-valued word embedding vector e i of w i , obtained by looking up the word embedding table E.</p><p>-The real-valued distance embedding vectors d i 1 , d i 2 to encode the relative distances i − i 1 and i − i 2 of w i to the two entity heads of interest w i 1 and w i 2 :</p><formula xml:id="formula_0">d i 1 = D[i − i 1 ], d i 2 = D[i − i 2 ] where D is the distance embedding table (initialized randomly).</formula><p>The objective is to inform the networks the positions of the two entity mentions for relation prediction.</p><p>-The real-valued embedding vectors for entity types t i and chunks q i to embed the entity type and chunking information for w i . These vectors are generated by looking up the entity type and chunk embedding tables (also initialized randomly) (i.e, T and Q respectively) for the entity type ent i and chunking label chunk i of w i :</p><formula xml:id="formula_1">t i = T [ent i ], q i = Q[chunk i ].</formula><p>-The binary vector p i with one dimension to indicate whether the word w i is on the dependency path between w i 1 and w i 2 or not.</p><p>-The binary vector g i whose dimensions correspond to the possible relations between words in the dependency trees. The value at a dimension of g i is only set to 1 if there exists one edge of the corresponding relation connected to w i in the dependency tree.</p><p>The transformation from the word w i to the vector x i = [e i , d i 1 , d i 2 , t i , q i , p i , g i ] essentially converts the relation mention with the input sentence W into a real-valued matrix X = [x 1 , x 2 , . . . , x n ], to be used by the neural networks presented below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Separate Models</head><p>We describe two typical NN architectures for RE underlying the combined models in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">The Convolutional Neural Networks</head><p>In CNNs <ref type="bibr">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b3">Kim, 2014)</ref>, given a window size of k, we have a set of c k feature maps (filters). Each feature map f is a weight matrix f = [f 1 , f 2 , . . . , f k ] where f i is a vector to be learnt during training as the model parameters. The core of CNNs is the application of the convolutional operator on the input matrix X and the filter matrix f to produce a score sequence s f = [s f 1 , s f 2 , . . . , s f n−k+1 ], interpreted as a more abstract representation of the input matrix X:</p><formula xml:id="formula_2">s f i = g( k−1 j=0 f j+1 x j+i + b)</formula><p>where b is a bias term and g is the tanh function.</p><p>In the next step, we further abstract the scores in s f by aggregating it via the max function to obtain the max-pooling score s f max . We then repeat this process for all the c k feature maps with different window sizes k to generate a vector of the maxpooling scores. In the final step, we pass this vector into some standard multilayer neural network, followed by a softmax layer to produce the probabilistic distribution p C (y|X) over the possible relation classes y in the prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">The Recurrent Neural Networks</head><p>In RNNs, we consider the input matrix X = [x 1 , x 2 , . . . , x n ] as a sequence of column vectors indexed from 1 to n. At each step i, we compute the hidden vector h i from the current input vector x i and the previous hidden vector h i−1 using the non-linear transformation function Φ:</p><formula xml:id="formula_3">h i = Φ(x i , h i−1 ).</formula><p>This recurrent computation can be done via three different directional mechanisms: (i) the forward mechanism that recurs from 1 to n and generate the forward hidden vector sequence: R(x 1 , x 2 , . . . , x n ) = h 1 , h 2 , . . . , h n , (ii) the backward mechanism that runs RNNs from n to 1 and results in the backward hidden vector sequence R(x n , x n−1 , . . . , x 1 ) = h ′ n , h ′ n−1 , . . . , h ′ 1 1 , and (iii) the bidirectional mechanism that performs RNNs in both directions to produce the forward and backward hidden vector sequences, and then concatenate them at each position to generate the new hidden vector</p><formula xml:id="formula_4">sequence h b 1 , h b 2 , . . . , h b n : h b i = [h i , h ′ i ].</formula><p>1 The initial hidden vectors are set to the zero vector.</p><p>Given the hidden vector sequence h 1 , h 2 , . . . , h n obtained from one of the three mechanisms above, we study two following strategies to generate the representation vector v R for the initial relation mention. Note that this representation vector can be again fed into some standard multilayer neural network with a softmax layer in the end, resulting in the distribution p R (y|X) for the RNN models:</p><p>-The HEAD strategy: In this strategy, v R is the concatenation of the hidden vectors at the positions of the two entity mention heads of interest:</p><formula xml:id="formula_5">v R = [h i 1 , h i 2 ]</formula><p>. This is motivated by the importance of the two mention heads in <ref type="bibr">RE (Sun et al., 2011;</ref><ref type="bibr">Nguyen and Grishman, 2014)</ref>.</p><p>-The MAX strategy: This strategy is similar to our max-pooling mechanism in CNNs. In particular, v R is obtained by taking the maximum along each dimension of the hidden vectors h 1 , h 2 , . . . , h n . The idea is to further abstract the hidden vectors by retaining only the most important feature in each dimension.</p><p>Regarding the non-linear function, the simplest form of Φ in the literature considers it as a one-layer feed-forward neural network, called</p><formula xml:id="formula_6">F F : h i = F F (x i , h i−1 ) = φ(U x i + V h i−1 ) where φ is the sigmoid function.</formula><p>Unfortunately, the application of F F causes the socalled "vanishing/exploding gradient" problems <ref type="bibr">(Bengio et al., 1994)</ref>, making it challenging to train RNNs properly <ref type="bibr">(Pascanu et al., 2012)</ref>. These problems are overcome by the long-short term memory units (LSTM) (Hochreiter and Schmidhuber, 1997; <ref type="bibr" target="#b0">Graves et al., 2009)</ref>. In this work, we apply a variant of the memory units: the Gated Recurrent Units from Cho et al. <ref type="formula">(2014)</ref>, called GRU . GRU is shown to be much simpler than LSTM in terms of computation but still achieves the comparable performance (Cho et al., 2014).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Combined Models</head><p>We first present three different methods to assemble CNNs and RNNs: ensembling, stacking and voting, to be investigated in this work. The combination of the neural networks with the log-linear model would be discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Ensembling</head><p>In this method, we first run some CNN and RNN in Section 2.1 over the input matrix X to gather the corresponding distributions p C (y|X) and p R (y|X). We then combine the CNN and RNN by multiplying their distributions (element-wise):</p><formula xml:id="formula_7">p ensemble (y|X) = 1 Z p C (y|X)p R (y|X) (Z is a normalization constant).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Stacking</head><p>The overall architecture of the stacking method is to use one of the two network architectures (i.e, CNNs and RNNs) to generalize the hidden vectors of the other architecture. The expectation is that we can learn more effective features for RE via such a deeper architecture by alternating between the local and global representations provided by CNNs and RNNs.</p><p>We examine two variants for this method. The first variant, called RNN-CNN, applies the CNN model in Section 2.1.1 on the hidden vector sequence generated by some RNN in Section 2.1.2 to perform RE. The second variant, called CNN-RNN, on the other hand, utilize the CNN model to acquire the hidden vector sequence, that is, in turn, fed as the input into some RNN for RE. For the second variant, as the length of the hidden vector</p><formula xml:id="formula_8">s f = [s f 1 , s f 2 , .</formula><p>. . , s f n−k+1 ] in the CNN model depends on the specified window size k for the feature map f , we need to pad the input matrix X with ⌊ k 2 ⌋ zero column vectors on both sides to ensure the same fixed length n for all the hidden vectors:</p><formula xml:id="formula_9">s f = [s f 1 , s f 2 , . . . , s f n ].</formula><p>Besides, we need to rearrange the scores in the hidden vectors from different feature maps of the CNN so they are grouped according to the positions in the sentence, thus being compatible with the input requirement of RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Voting</head><p>Instead of integrating CNNs and RNNs at the model level as the two previous methods, the voting method makes decision for a relation mention X by voting the individual decisions of the different models. While there are several voting schemes in the literature, for this work, we employ the simplest scheme of majority voting. If there are more than one relation classes receiving the highest number of votes, the relation class returned by a model and having the highest probability would be chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Hybrid Models</head><p>In order to further improve the RE performance of models above, we investigate the integration of these neural network models with the traditional loglinear model that relies on various linguistic features from the past research on <ref type="bibr">RE (Zhou et al., 2005;</ref><ref type="bibr">Sun et al., 2011;</ref><ref type="bibr">Gormley et al., 2015)</ref>. Specifically, in such integration models (called the hybrid models), the relation class distribution is obtained from the element-wise multiplication between the distributions of the neural network models and the log-linear model. Let us take the ensembling model in Section 2.2.1 as an example. The corresponding hybrid model in this case would be:</p><formula xml:id="formula_10">p hybrid (y|X) = 1 Z p C (y|X)p R (y|X)p login (y|X)</formula><p>, assuming p login (y|X) be the distribution of the loglinear model and Z be the normalization constant. The parameters of the log-linear model are learnt jointly with the parameters of the neural networks.</p><p>Hypothesis: Let S be the set of relation mentions correctly predicted by some neural network model in some dataset (the coverage set). The introduction of the log-linear model into this neural network model essentially changes the coverage set of the network, resulting in the new coverage set S ′ that might or might not subsume the original set S. In this work, we hypothesize that although S and S ′ overlap, there are still some relation mentions that only belong to either set. Consequently, we propose to implement a majority voting system (called the hybrid-voting system) on the outputs of the network and its corresponding hybrid model to enhance both models.</p><p>Note that the voting models in Section 2.2.3 involve the voting on two models (i.e, CNN and RNN). In order to integrate the log-linear model into such voting models, we first augment the separate CNN and RNN models with the log-linear model before we perform the voting procedure on the resulting models. Finally, the corresponding hybridvoting systems would involve the voting on four models (CNN, hybrid CNN, RNN and hybrid RNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>We train the models by minimizing the negative log-likelihood function using the stochastic gradient descent algorithm with shuffled mini-batches and the AdaDelta update rule <ref type="bibr">(Zeiler, 2012;</ref><ref type="bibr" target="#b3">Kim, 2014)</ref>.</p><p>The gradients are computed via back-propagation while regularization is executed by a dropout on the hidden vectors before the the multilayer neural networks <ref type="bibr">(Hinton et al., 2012)</ref>. During training, besides the weight matrices, we also optimized the embedding tables E, D, T, Q to achieve the optimal state. Finally, we rescale the weights whose l 2 -norms exceed a hyperparameter <ref type="bibr" target="#b3">(Kim, 2014;</ref><ref type="bibr">Nguyen and Grishman, 2015a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Resources and Parameters</head><p>For all the experiments below, we utilize the pretrained word embeddings word2vec with 300 dimensions from <ref type="bibr" target="#b5">Mikolov et al. (2013)</ref> to initialize the word embedding table E. The parameters for CNNs and traning the networks are inherited from the previous studies, i.e, the window size set for feature maps = {2, 3, 4, 5}, 150 feature maps for each window size, 50 dimensions for all the embedding tables (except the word embedding table E), the dropout rate = 0.5, the mini-batch size = 50, the hyperparameter for the l 2 norms = 3 <ref type="bibr" target="#b3">(Kim, 2014;</ref><ref type="bibr">Nguyen and Grishman, 2015a)</ref>. Regarding RNNs, we employ 300 units in the hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset</head><p>We evaluate our models on two datasets: the ACE 2005 dataset for relation extraction and the SemEval-2010 Task 8 dataset <ref type="bibr">(Hendrickx et al., 2010)</ref> for relation classification.</p><p>The ACE 2005 corpus comes with 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). Following the common practice of domain adaptation research on this dataset <ref type="bibr">(Plank and Moschitti, 2013;</ref><ref type="bibr">Nguyen and Grishman, 2014;</ref><ref type="bibr">Nguyen et al., 2015c;</ref><ref type="bibr">Gormley et al., 2015)</ref>, we use news (the union of bn and nw) as the training data, a half of bc as the development set and the remainder (cts, wl and the other half of bc) as the test data. Note that we are using the data prepared by Gormley et. al <ref type="formula">(2015)</ref>, thus utilizing the same data split on bc as well as the same data processing and NLP toolkits. The to-tal number of relations in the training set is 43,497 2 . We employ the BIO annotation scheme to capture the chunking information for words in the sentences and only mark the entity types of the two entity mention heads (obtained from human annotation) for this dataset.</p><p>The SemEval dataset concerns the relation classification task that aims to determine the relation type (or no relation) between two entities in sentences. In order to make it compatible with the previous research <ref type="bibr">(Socher et al., 2012;</ref><ref type="bibr">Gormley et al., 2015)</ref>, for this dataset, besides the word embeddings and the distance embeddings, we apply the name tagging, part of speech tagging and WordNet features (inherited from <ref type="bibr">Socher et al. (2012)</ref> and encoded by the real-valued vectors for each word). The other settings are also adopted from the past studies <ref type="bibr">(Socher et al., 2012;</ref><ref type="bibr">Xu et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RNN Architectures</head><p>This section evaluates the performance of various RNN architectures for RE on the development set. In particular, we compare different design combinations of the four following factors: (i) sentence representations (i.e, SEQ or DEP), (ii) transformation functions Φ (i.e, FF or GRU), (iii) the strategies to employ the hidden vector sequence for RE (i.e, HEAD or MAX), and (iv) the directions to run RNNs (i.e, forward (→), backward (←) or bidirectional ( ⇀ ↽)). The main conclusions include:</p><p>(i) Assuming the same choices for the other three corresponding factors, GRU is more effective than FF, SEQ is better than DEP most of the time and HEAD outperforms MAX (except the case where SEQ and GRU are applied) for RE with RNNs.</p><p>(ii) Regarding the direction mechanisms, the bidirectional mechanism achieves the best performance for the HEAD strategy while the forward direction is the best mechanism for the MAX strategy. This can be partly explained by the lack of past or future context information in the HEAD strategy when we follow the backward or forward direction respectively.</p><p>The best performance corresponds to the application of the SEQ representation, the GRU function and the MAX strategy that would be used in all the RNN models below. We call such RNN models with the forward, backward and bidirectional mechanism FORWARD, BACKWARD and BIDIRECT respectively. We also apply the SEQ representation for the CNN model (called CNN) in the following experiments for consistency. We evaluate the combination methods for CNNs and RNNs presented in Section 2.2. In particular, for each method, we examine three models that are combined from one of the three RNN models FOR-WARD, BACKWARD, BIDIRECT and the CNN model. For instance, in the stacking method, the three combined models corresponding to the RNN-CNN variant are FORWARD-CNN, BACKWARD-CNN, BIDIRECT-CNN while the three combined models corresponding to the CNN-RNN variant are CNN-FORWARD, CNN-BACKWARD, CNN-BIDIRECT. The notations for the other methods are self-explained. The model performance on the development set is given in <ref type="table" target="#tab_3">Table 3</ref>.4 that also includes the performance of the separate models (i.e, CNN, FORWARD, BACKWARD, BIDIRECT) for convenient comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluating the Combined Models</head><p>The first observation is that the ensembling method is not an effective way to combine CNNs and RNNs as its performance is worse than the separate models. Second, regarding the stacking method, the best way to combine CNNs and RNNs in this framework is to assemble the CNN model and the FORWARD model. In fact, the combination of the CNN and FORWARD models helps to improve the performance of the separate models in both variants of this method (referring to the models CNN-FORWARD and FORWARD-CNN). Finally, the voting method is also helpful as it outperforms the separate models with the CNN-BIDIRECT and CNN-BACKWARD combinations.</p><p>For the following experiments, we would only focus on the three best combined models in this section, i.e, the CNN-FORWARD model in the stacking method (called STACK-FORWARD) and the CNN-BIDIRECT, CNN-BACKWARD models in the voting methods (called VOTE-BIDIRECT and VOTE-BACKWARD respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Evaluating the Hybrid Models</head><p>This section investigates the hybrid and hybridvoting models (Section 2.3) to see if they can further improve the performance of the neural network models. In particular, we evaluate the separate models: CNN, BIDIRECT, FORWARD, BACKWARD, and the combined models: STACK-FORWARD, VOTE-BIDIRECT and VOTE-BACKWARD when they are augmented with the traditional log-linear model (the hybrid models). Besides, in order to verify the hypothesis in Section 2.3, we also test the corresponding hybrid-voting models. The experimental results are shown in <ref type="table" target="#tab_3">Table 3</ref>. There are three main conclusions:</p><p>(i) For all the models in columns "Neural Networks", "Hybrid Models" and "Hybrid-Voting Mod-   els", we see that the combined models outperform their corresponding separate models (only except the hybrid model of VOTE-BACKWARD), thereby further confirming the benefits of the combined models.</p><p>(ii) Comparing columns "Neural Networks" and "Hybrid Models", we find that the traditional loglinear model significantly helps the CNN model. The effects on the other models are not clear.</p><p>(iii) More interestingly, for all the neural networks being examined (either separate or combined), the corresponding hybrid-voting systems substantially improve both the neural network models as well as the corresponding hybrid models, testifying to the hypothesis about the hybrid-voting approach in Section 2.3. Note that the simpler voting systems on three models: the log-linear model, the CNN model and some RNN model (i.e, either BIDIRECT, FOR-WARD or BACKWARD) produce the worse performance than the hybrid-voting methods (the respective performance is 66.13%, 65.27%, and 65.96%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Comparing to the State-of-the-art</head><p>The state-of-the-art system on the ACE 2005 for the unseen domains has been the feature-rich composi-tional embedding model (FCM) and the hybrid FCM model from <ref type="bibr">Gormley et al. (2015)</ref>. In this section, we compare the proposed hybrid-voting systems with these state-of-the-art systems on the test domains bc, cts, wl. <ref type="table" target="#tab_4">Table 4</ref> reports the results. For completeness, we also include the performance of the log-linear model and the separate models CNN, BIDIRECT, FORWARD, BACKWARD, serving as the other baselines for this work.</p><p>From the table, we see that although the separate neural networks outperform the FCM model across domains, they are still worse than the hybrid FCM model due to the introduction of the log-linear model into FCM. However, when the networks are combined and integrated with the log-linear model, they (the hybrid-voting systems) become significantly better than the FCM models across all domains (up to 2% improvement on the average absolute F score), yielding the state-of-the-art performance for the unseen domains in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Relation Classification Experiments</head><p>We further evaluate the proposed systems for the relation classification task on the SemEval dataset. Ta-ble 5 presents the performance of the seprate models, the proposed systems as well as the other representative systems on this task. The most important observation is that the hybrid-voting systems VOTE-BIDIRECT and VOTE-BACKWARD achieve the state-of-the-art performance for this dataset, further highlighting their benefit for relation classification. The hybrid-voting STACK-FORWARD system performs less effectively in this case, possibly due to the small size of the SemEval dataset that is not sufficient to training such a deep model.   One of the main insights is although CNN and BIDIRECT have the comparable overall perfor-mance, their recalls on individual relations are very diverged. In particular, the BIDIRECT has much better recall for the PHYS relation while the recalls of CNN are significantly better for the ART, ORG-AFF and GEN-AFF relations. A closer investigation reveals two facts: (i) the PHYS relation mentions that are only correctly predicted by BIDI-RECT involve the long distances between two entity mentions, such as the PHYS relation between "Some" (a person entity) and "desert" (a location entity) in the following sentence: "Some of the 40,000 British troops are kicking up a lot of dust in the Iraqi desert making sure that nothing is left behind them that could hurt them.", and (ii) the ART, ORG-AFF, GEN-AFF relation mentions only correctly predicted by CNN contains the patterns between the two entity mentions that are short but meaningful enough to decide the relation classes, such as "The Iraqi unit in possession of those guns" (the ART relation between "unit" and "guns"), or "the al Qaeda chief operations officer" (the ORG-AFF relation between "al Qaeda" and "officer"). The failure of CNN on the PHYS relation mentions with long distances originates from its mechanism to model short and consecutive k-grams (up to length 5 in our case), causing the difficulty to capture the long and/or unconsecutive patterns. BIDIRECT, on the other hand, fails to predict the short (but expressive enough) patterns for ART, ORG-AFF, GEN-AFF because it involves the hidden vectors that only model the context words outside the short patterns, potentially introducing unnecessary and noisy information into the max-pooling scores for prediction. Eventually, the combination of RNNs and CNNs helps to compensate the drawbacks of each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Starting from the invention of the distributed representations for words <ref type="bibr">(Bengio et al., 2003;</ref><ref type="bibr">Mnih and Hinton, 2008;</ref><ref type="bibr">Collobert and Weston, 2008;</ref><ref type="bibr">Turian et al., 2010;</ref><ref type="bibr" target="#b5">Mikolov et al., 2013)</ref>, CNNs and RNNs have gained significant successes on various NLP tasks, including sequential labeling <ref type="bibr">(Collobert et al., 2011)</ref>, sentence modeling and classification <ref type="bibr">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b3">Kim, 2014)</ref>, paraphrase identification (Yin and Schütze, 2015), event extraction <ref type="bibr">(Nguyen and Grishman, 2015b;</ref><ref type="bibr">Chen et al., 2015)</ref> for CNNs and machine translation <ref type="bibr">(Cho et al., 2014;</ref><ref type="bibr">Bahdanau et al., 2015)</ref> for RNNs, to name a few.</p><p>For relation extraction/classification, most work on neural networks has focused on the relation classification task. In particular, <ref type="bibr">Socher et al. (2012)</ref> and <ref type="bibr">Ebrahimi and Dou (2015)</ref> study the recursive NNs that recur over the tree structures while <ref type="bibr">Xu et al. (2015)</ref> and Zhang and Wang <ref type="formula">(2015)</ref>   <ref type="formula">2015)</ref> work on the feature-rich compositional embedding models. Finally, the only work that combines NN architectures is due to <ref type="bibr" target="#b4">Liu et al. (2015)</ref> but it only focuses on the stacking of the recursive NNs and CNNs for relation classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We investigate different methods to combine CNNs, RNNs as well as the hybrid models to integrate the log-linear model into the NNs. The experimental results demonstrate that the simple majority voting between CNNs, RNNs and their corresponding hybrid models is the best combination method. We achieve the state-of-the-art performance for both relation extraction and relation classification. In the future, we plan to further evaluate the proposed methods on the other tasks such as event extraction and slot filling in the KBP evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>investigate recurrent NNs. Regarding CNNs, Zeng et al. (2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al. (2015) propose dynamic pooling and multi-instance learning. For RE, Yu et al. (2015) and Gormley et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>presents the results.</figDesc><table><row><cell>Systems</cell><cell>DEP</cell><cell>SEQ</cell></row><row><cell></cell><cell cols="2">⇀ ↽ 60.78 63.22</cell></row><row><cell cols="3">HEAD → 55.55 60.05</cell></row><row><cell>FF</cell><cell cols="2">← 57.69 58.54</cell></row><row><cell></cell><cell cols="2">⇀ ↽ 50.00 51.22</cell></row><row><cell>MAX</cell><cell cols="2">→ 52.08 53.96</cell></row><row><cell></cell><cell cols="2">← 45.07 33.50</cell></row><row><cell></cell><cell cols="2">⇀ ↽ 63.32 63.23</cell></row><row><cell cols="3">HEAD → 63.69 62.77</cell></row><row><cell>GRU</cell><cell cols="2">← 61.57 62.55</cell></row><row><cell></cell><cell cols="2">⇀ ↽ 60.96 64.24</cell></row><row><cell>MAX</cell><cell cols="2">→ 61.97 64.59</cell></row><row><cell></cell><cell cols="2">← 61.56 64.30</cell></row><row><cell cols="3">Table 1: Performance (F1 scores) of RNNs on the dev set</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of the Combination Methods</figDesc><table><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>BIDIRECT</cell><cell cols="3">69.16 59.97 64.24</cell></row><row><cell>FORWARD</cell><cell cols="3">69.33 60.45 64.59</cell></row><row><cell>BACKWARD</cell><cell cols="3">65.60 63.05 64.30</cell></row><row><cell>CNN</cell><cell cols="3">68.35 59.16 63.42</cell></row><row><cell>Ensembling</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN-BIDIRECT</cell><cell cols="3">71.22 54.13 61.51</cell></row><row><cell>CNN-FORWARD</cell><cell cols="3">66.19 59.64 62.75</cell></row><row><cell cols="4">CNN-BACKWARD 65.09 60.13 62.51</cell></row><row><cell>Stacking</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN-BIDIRECT</cell><cell cols="3">66.55 59.97 63.09</cell></row><row><cell>CNN-FORWARD</cell><cell cols="3">69.46 63.05 66.10</cell></row><row><cell cols="4">CNN-BACKWARD 72.58 58.35 64.69</cell></row><row><cell>BIDIRECT-CNN</cell><cell cols="3">65.63 61.59 63.55</cell></row><row><cell>FORWARD-CNN</cell><cell cols="3">73.13 58.67 65.11</cell></row><row><cell cols="4">BACKWARD-CNN 67.60 58.51 62.73</cell></row><row><cell>Voting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN-BIDIRECT</cell><cell cols="3">71.08 60.94 65.62</cell></row><row><cell>CNN-FORWARD</cell><cell cols="3">70.38 59.32 64.38</cell></row><row><cell cols="4">CNN-BACKWARD 69.78 61.75 65.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>59.16 63.42 66.44 64.51 65.46 69.07 63.70 66.27 BIDIRECT 69.16 59.97 64.24 68.04 59.00 63.19 71.13 60.29 65.26 FORWARD 69.33 60.45 64.59 66.11 63.86 64.96 72.69 61.26 66.49 BACKWARD 65.60 63.05 64.30 66.03 62.07 63.99 71.56 63.21 67.13 Combined Models VOTE-BIDIRECT 71.08 60.94 65.62 69.24 62.40 65.64 71.30 62.40 66.55 STACK-FORWARD 69.46 63.05 66.10 65.93 68.07 66.99 69.32 66.29 67.77 VOTE-BACKWARD 69.78 61.75 65.52 67.30 63.05 65.10 70.79 64.02 67.23</figDesc><table><row><cell>Model</cell><cell cols="3">Neural Networks</cell><cell></cell><cell cols="2">Hybrid Models</cell><cell cols="3">Hybrid-Voting Models</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>CNN</cell><cell>68.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :57.27 † 60.60</head><label>3</label><figDesc>Performance of the Hybrid Models 66.48 66.11 † 63.58 51.72 57.04 † 56.35 57.22 56.78 † 59.98 VOTE-BACKWARD 69.57 63.28 66.28 † 65.91 52.21 58.26 † 58.81 55.81</figDesc><table><row><cell>System</cell><cell></cell><cell>bc</cell><cell></cell><cell></cell><cell>cts</cell><cell></cell><cell></cell><cell>wl</cell><cell></cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Ave</cell></row><row><cell>FCM</cell><cell cols="2">66.56 57.86</cell><cell>61.9</cell><cell cols="7">65.62 44.35 52.93 57.80 44.62 50.36 55.06</cell></row><row><cell>Hybrid FCM</cell><cell cols="10">74.39 55.35 63.48 74.53 45.01 56.12 65.63 47.59 55.17 58.26</cell></row><row><cell>Separate Systems</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Log-Linear</cell><cell cols="10">68.44 50.07 57.83 73.62 41.57 53.14 60.40 47.31 53.06 54.68</cell></row><row><cell>CNN</cell><cell cols="10">65.62 61.06 63.26 65.92 48.12 55.63 54.14 53.68 53.91 57.60</cell></row><row><cell>BIDIRECT</cell><cell cols="10">65.23 61.06 63.07 66.15 49.26 56.47 55.91 51.56 53.65 57.73</cell></row><row><cell>FORWARD</cell><cell cols="10">63.64 59.39 61.44 60.12 50.57 54.93 55.54 54.67 55.10 57.16</cell></row><row><cell>BACKWARD</cell><cell cols="2">60.44 61.2</cell><cell cols="8">60.82 58.20 54.01 56.03 51.03 52.55 51.78 56.21</cell></row><row><cell cols="2">Hybrid-Voting Systems</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VOTE-BIDIRECT</cell><cell cols="10">70.40 63.84 66.96 † 66.74 49.92 57.12 † 59.24 54.96 57.02 † 60.37</cell></row><row><cell>STACK-FORWARD</cell><cell>65.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison to the State-of-the-art. The cells marked with †designates the models that are significantly better than the other neural network models (ρ &lt; 0.05) on the corresponding domains.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance of Relation Classification Systems. The †refers to special treatment to the Other class.</figDesc><table><row><cell>3.8 Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">In order to better understand the reason helping the</cell></row><row><cell cols="7">combination of CNNs and RNNs outperform the</cell></row><row><cell cols="7">individual networks, we evaluate the performance</cell></row><row><cell cols="7">breakdown per relation for the CNN and BIDIRECT</cell></row><row><cell cols="7">models. The results on the development set of the</cell></row><row><cell cols="6">ACE 2005 dataset are provided in Tabel 6.</cell></row><row><cell>Relation Class</cell><cell></cell><cell>CNN</cell><cell></cell><cell></cell><cell>BIDIRECT</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>F</cell><cell>F1</cell></row><row><cell>PHYS</cell><cell cols="6">66.7 34.7 45.7 57.4 50.9 54.0</cell></row><row><cell cols="7">PART-WHOLE 68.6 67.8 68.2 74.4 70.1 72.2</cell></row><row><cell>ART</cell><cell cols="6">64.2 51.2 57.0 68.6 41.7 51.9</cell></row><row><cell>ORG-AFF</cell><cell cols="6">70.2 83.0 76.0 79.3 76.1 77.7</cell></row><row><cell>PER-SOC</cell><cell cols="6">71.1 59.3 64.6 69.6 59.3 64.0</cell></row><row><cell>GEN-AFF</cell><cell cols="6">65.9 55.1 60.0 59.0 46.9 52.3</cell></row><row><cell>all</cell><cell cols="6">68.4 59.2 63.4 69.2 60.0 64.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The Performance Breakdown per Relation for CNN and BIDIRECT on the development set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">It was an error in Gormley et al. (2015) that reported 43,518 total relations in the training set. The authors acknowledged this error.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank Matthew Gormley and Mo Yu for providing the dataset. Thank you to Kyunghyun Cho and Yifan He for valuable suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head> <ref type="bibr">[Bahdanau et al.2015</ref></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graves</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Neural Computation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A systematic exploration of the feature space for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>NAACL-HLT. [Kalchbrenner et al.2014] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>A convolutional neural network for modelling sentences</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A dependency-based neural network for relation classification</title>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Employing word representations and regularization for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>ACL. [Nguyen and Grishman2015a] Thien Huu Nguyen and Ralph Grishman</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Efficient estimation of word representations in vector space. Relation extraction: Perspective from convolutional neural networks. In The</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
