<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast, Accurate and Lightweight Super-Resolution with Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
							<email>chuxiangxiang@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
							<email>zhangbo11@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Ma</surname></persName>
							<email>mahailong@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
							<email>xuruijun@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyuan</forename><surname>Li</surname></persName>
							<email>liqingyuan@xiaomi.com</email>
							<affiliation key="aff1">
								<orgName type="department">Xiaomi IoT</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast, Accurate and Lightweight Super-Resolution with Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural networks demonstrate impressive results in the super-resolution domain. A series of studies concentrate on improving peak signal noise ratio (PSNR) by using much deeper layers, which are not friendly to constrained resources. Pursuing a trade-off between the restoration capacity and the simplicity of models is still non-trivial. Recent contributions are struggling to manually maximize this balance, while our work achieves the same goal automatically with neural architecture search. Specifically, we handle super-resolution with a multi-objective approach. We also propose an elastic search tactic at both micro and macro level, based on a hybrid controller that profits from evolutionary computation and reinforcement learning. Quantitative experiments help us to draw a conclusion that our generated models dominate most of the state-of-the-art methods with respect to the individual FLOPS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I. INTRODUCTION AND RELATED WORK As a classical task in computer vision, single image superresolution (SISR) is aimed to restore a high-resolution image from a degraded low-resolution one, which is known as an illposed inverse procedure. Most of the recent works on SISR have shifted their approaches to deep learning, and they have surpassed other SISR algorithms with big margins <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Nonetheless, these human-designed models are tenuous to fine-tune or to compress. Meantime, neural architecture search has produced dominating models in classification tasks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Following this trend, a novel work by <ref type="bibr" target="#b6">[7]</ref> has shed light on the SISR task with a reinforced evolutionary search method, which has achieved results outperforming some notable networks including VDSR <ref type="bibr" target="#b1">[2]</ref>. We are distinct to <ref type="bibr" target="#b6">[7]</ref> by stepping forward to design a dense search space which allows searching in both macro and micro level, which has led to significantly better visual results.</p><p>In this paper, we dive deeper into the SISR task with elastic neural architecture search, hitting a record comparable to CARN and CARN-M [4] 1 . Our main contributions can be summarized in the following four aspects,</p><p>• releasing several fast, accurate and lightweight superresolution architectures and models (FALSR-A being the best regarding visual effects), which are highly competitive with recent state-of-the-art methods, • performing elastic search by combining micro and macro space on the cell-level to boost capacity, <ref type="bibr" target="#b0">1</ref> Our models are released at https://github.com/falsr/FALSR.</p><p>• building super-resolution as a constrained multi-objective optimization problem and applying a hybrid model generation method to balance exploration and exploitation, • producing high-quality models that can meet various requirements under given constraints within a single run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PIPELINE ARCHITECTURE</head><p>Like most of Neural Architecture Search (NAS) approaches, our pipeline contains three principle ingredients: an elastic search space, a hybrid model generator and a model evaluator based on incomplete training. It is explained in detail in the following sections.</p><p>Similar to <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b6">[7]</ref>, we also apply NSGA-II <ref type="bibr" target="#b8">[9]</ref> to solve the multi-objective problem. Our work differs from them by using a hybrid controller and a cell-based elastic search space that enables both macro and micro search.</p><p>We take three objectives into account for the superresolution task,</p><p>• quantitative metric to reflect the performance of models (PSNR), • quantitative metric to evaluate the computational cost of each model (mult-adds), • number of parameters. In addition, we consider the following constraints,</p><p>• minimal PSNR for practical visual perception, • maximal mult-adds regarding resource limits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ELASTIC SEARCH SPACE</head><p>Our search space is designed to perform both micro and macro search. The former is used to choose promising cells within each cell block, which can be viewed as a feature extraction selector. In contrast, the latter is aimed to search backbone connections for different cell blocks, which plays a role of combining features at selected levels. In addition, we use one cell block as our minimum search element for two reasons: design flexibility, and broad representational capacity.</p><p>Typically, the super-resolution task can be divided into three sub-procedures: feature extraction, nonlinear mapping, and restoration. Since most of the deep learning approaches concentrate on the second part, we design our search space to describe the mapping while fixing others. <ref type="figure" target="#fig_0">Figure 1</ref>  contains a predefined feature extractor (a 2D convolution with 32 3 × 3 filters), n cell blocks drawn from the micro search space which are joined by the connections from macro search space, and subpixel-based upsampling and restoration 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cell-Level Micro Search Space</head><p>For simplicity, all cell blocks share the same cell search space S. In specific, the micro search space comprises the following elements:</p><p>• convolutions: 2D convolution, grouped convolution with groups in {2, 4}, inverted bottleneck block with an expansion rate of 2, • channels: {16, 32, 48, 64}, • kernels: {1, 3}, • in-cell residual connections:{True, False}, • repeated blocks:{1, 2, 4}. Therefore, the size of micro space for n cell blocks is 192 n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Intercell Macro Search Space</head><p>The macro search space defines the connections among different cell blocks. Specifically, for the i-th cell block CB i , there are n + 1 − i choices of connections to build the information flow from the input of CB i to its following cell blocks <ref type="bibr" target="#b2">3</ref> . Furthermore, we use c j i to represent the path from input of CB i to CB j . We set c j i = 1 if there is a connection path between them, otherwise 0. Therefore, the size of macro space for n cell blocks is 2 n(n+1)/2 . In summary, the size of the total space is 192 n × 2 n(n+1)/2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODEL GENERATOR</head><p>Our model generator is a hybrid controller involving both reinforcement learning (RL) and an evolutionary algorithm (EA). The EA part handles the iteration process and RL is used to bring exploitation. To be specific, the iteration is controlled by NSGA-II <ref type="bibr" target="#b8">[9]</ref>, which contains four sub-procedures: population initialization, selection, crossover, and mutation. To avoid verbosity, we only cover our variations to NSGA-II.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Meta Encoding</head><p>One model is denoted by two parts: forward-connected cells and their information connections. We use the indices of operators from the operator set to encode the cells, and a nested list to depict the connections. Namely, given a model M with n cells, its corresponding chromosome can be depicted by (M mic , M mac ), where M mic and M mac are defined as follows,</p><formula xml:id="formula_0">M mic = (x 1 , x 2 , ..., x n ) (1) M mac = (c 1:n 1 , c 2:n 2 , ..., c n n ) c i:n i = (c i i , c i+1 i , ..c n i )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Initialization</head><p>We begin with N populations and we emphasize the diversities of cells. In effect, to generate a model, we randomly sample a cell from S and repeat it for n times. In case N is larger than the size of S, models are arbitrarily sampled without repeating cells.</p><p>As for connections, we sample from a categorical distribution. While in each category, we pick uniformly, i.e. p ∼ U(0, 1). To formalize, the connections are built based on the following rules,    random connections 0 ≤ p &lt; p r dense connections p r ≤ p &lt; p r + p den no connnections p r + p den ≤ p &lt; 1</p><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Tournament Selection</head><p>We calculate the crowding distance as noted in <ref type="bibr" target="#b9">[10]</ref> to render a uniform distribution of our models, and we apply tournament selection (k = 2) to control the evolution pressure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Crossover</head><p>To encourage exploration, single-point crossovers are performed simultaneously in both micro and macro space. Given two models A (M mic(A) , M mac(A) ) and B (M mic(B) , M mac(B) ), a new chromosome C can be generated as,</p><formula xml:id="formula_1">M mic(C) = (x 1A , x 2A , ..., x iB , ..., x nA )</formula><p>M mac(C) = (c 1:n 1A , c 2:n 2A , ..., c j:n jB , ..., c n nA )</p><p>where i and j are chosen positions respectively for micro and macro genes. Informally, the crossover procedure contributes more to exploitation than to exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Mutation</head><p>We again apply a categorical distribution to balance exploration and exploitation.</p><p>1) Exploration: To encourage exploration, we combine random mutation with roulette wheel selection (RWS). Since we treat super-resolution as a multi-objective problem, FLOPS and the number of parameters are two objectives that can be evaluated soon after meta encodings are available. In particular, we also sample from a categorical distribution to determine mutation strategies, i.e. random mutation (with an upper-bound probability p mr ) or mutated by roulette wheel selection to handle FLOPS (lower than p mf ) or parameters. Formally,</p><formula xml:id="formula_3">   random mutation 0 ≤ p &lt; p mr RWS for FLOPS p mr ≤ p &lt; p mf RWS for params p mf ≤ p &lt; 1<label>(5)</label></formula><p>Whenever we need to mutate a model M by RWS, we keep M mac unchanged. Since each cell shares the same operator set S, we perform RWS on S for n times to generate M mic . Strictly speaking, given M mac , it's intractable to execute a complete RWS (involving 192 n models). Instead, it can be approximated based on S (involving 192 basic operators). Besides, we scale FLOPS and the number of parameters logarithmically before RWS.</p><p>2) Exploitation: To enhance exploitation, we apply a reinforcement driven mutation.</p><p>We use a neural controller to mutate, which is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Specifically, the embedding features for M mic are concatenated, and then are injected into 3 fully-connected layers to generate M mac . The last layer has n(n + 1)/2 neutrons to represent connections, with its output denoted as O mac . The network parameters can be partitioned into two groups, θ mic and θ mac . The probability of selecting S i for cell j is p(cell i = S i |θ mic ) and for the connection c j i = 1, we have p(c j i = 1|θ mac ) = O mac (i−1) * (n+1−0.5 * i)+j . Thus, the gradient g(θ) can be calculated as follows:</p><formula xml:id="formula_4">g(θ) = −∇ θ [ n i=1 log p(cell i = S i |θ mic ) * R i + n(n+1)/2 j=1 c j log O mac j * R j + (1 − c j ) log(1 − O mac j ) * R j ].<label>(6)</label></formula><p>In Equation 6, R i and R j are the discounted accumulated rewards. Here, we set the discount parameter γ = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATOR</head><p>The evaluator calculates the scores of the models generated by the controller. In the beginning, we attempted to train an RNN regressor to predict the performances of models, with data collected in previous pipeline execution. However, its validation error is too high to continue. Instead, each model is trained for a relatively short time (see the 'incomplete training' part in Section VI-A) to roughly differentiate various models. At the end of the incomplete training, we evaluate mean square errors on test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Setup</head><p>In our experiment, about 10k models are generated in total, where the population for each iteration is 64. The Pareto-front of all the models is shown in <ref type="figure">Fig. 6</ref>. It takes less than 3 days on a Tesla-V100 with 8 GPUs to execute the pipeline once. We use DIV2K as our training set.</p><p>During an incomplete training, each model is trained with a batch size of 16 for 200 epochs. In addition, we apply Adam optimizer (β 1 = 0.9, β 2 = 0.999) to minimize the L 1 loss between the generated high-resolution images and its ground truth. The learning rate is initialized as 10 −4 and kept unchanged at this stage.</p><p>As for the full train, we choose 4 models with a large crowding distance in the Pareto front between mean squared error and mult-adds, which was generated at the incomplete training stage. These models are trained based on DIV2K dataset for 24000 epochs with a batch-size of 16 and it takes less than 1.5 days. Moreover, the standard deviation of weights w is initialized as 0.02 and the bias 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparisons with State-of-the-Art Super-Resolution Methods</head><p>After being fully trained, our model are compared with the state-of-the-art methods on the commonly used test dataset for super-resolution (See <ref type="table" target="#tab_0">Table I</ref> and <ref type="figure">Figure 5</ref>). To be fair, we only consider the models with comparable FLOPS. Therefore, too deep and large models such as RDN <ref type="bibr" target="#b16">[17]</ref>, RCAN <ref type="bibr" target="#b17">[18]</ref> are excluded here. We choose PSNR and SSIM as metrics by convention <ref type="bibr" target="#b18">[19]</ref>. The comparisons are made on the ×2 task. Note that all mult-adds are measured based on a 480 × 480 input.  At a comparable level of FLOPS, our model called FALSR-A ( <ref type="figure" target="#fig_2">Figure 3</ref>) outperforms CARN <ref type="bibr" target="#b3">[4]</ref> with higher scores. In addition, it dominates DRCN <ref type="bibr" target="#b11">[12]</ref> and MoreMNAS-A <ref type="bibr" target="#b6">[7]</ref> over three objectives on four datasets. Moreover, it achieves higher PSNR and SSIM with fewer FLOPS than VDSR <ref type="bibr" target="#b1">[2]</ref>, DRRN <ref type="bibr" target="#b13">[14]</ref> and many others.</p><p>For a more lightweight version, one model called FALSR-B <ref type="figure" target="#fig_3">(Figure 4</ref> or higher than CARN-M. Besides, its architecture is attractive and the complexity of connections lies in between residual and dense connections. This means a dense connection is not always the optimal way to transmit information. Useless features from lower layers could make trouble for high layers to restore super-resolution results.</p><p>Another lightweight model called FALSR-C (not drawn because of space) also outperforms CARN-M. This model uses relatively sparse connections (8 in total). We conclude that this sparse flow works well with the selected cells. <ref type="figure" target="#fig_5">Figure 7</ref> shows the qualitative results against other methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Cell Diversity:</head><p>Our experiments show that a good cell diversity also helps to achieve better results for superresolution, same for classification tasks <ref type="bibr" target="#b19">[20]</ref>. In fact, we have trained several models with repeated blocks, however, they underperform the models with diverse cells. We speculate that different types of cells can handle input features more effectively than monotonous ones.</p><p>2) Optimal Information Flow: Perhaps under given current technologies, dense connections are not optimal in most cases. In principle, a dense connection has the capacity to cover other non-dense configurations, however, it's usually difficult to train a model to ignore useless information.</p><p>3) Good Assumption?: Super-resolution is different from feature extraction domains such as classification, where more details need to be restored at pixel level. Therefore, it rarely applies downsampling operations to reduce the feature dimensions and it is more time-consuming than classification tasks like on CIFAR-10.</p><p>Regarding the time, we use incomplete training to differentiate various models. This strategy works well under an implicit assumption: models that perform better when fully trained also behave well with a large probability under an incomplete training. Luckily, most of deep learning tasks share this good feature. For the rest, we must train models as fully as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>To sum up, we presented a novel elastic method for NAS that incorporates both micro and macro search, dealing with neural architectures in multi-granularity. The result is exciting as our generated models dominate the newest state-of-theart SR methods. Different from human-designed and singleobjective NAS models, our methods can generate different tastes of models by one run, ranging from fast and lightweight to relatively large and more accurate. Therefore, it offers a feasible way for engineers to compress existing popular humandesigned models or to design various levels of architectures accordingly for constrained devices.</p><p>Our future work will focus on training a model regressor, which estimates the performance of models, to speed up the pipeline.</p><p>(1) Ground Truth </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>depicts our main flow for super-resolution. Thus, a complete model Neural Architecture of Super-Resolution (the arrows denote skip connections).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The controller network to generate cells and connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The model FALSR-A (the one with best visual effects) comparable to CARN. Note for instance, 'conv f64 k3 b4 isskip' represents a block of 4 convolution layers, each with a filter size of 64 and a kernel size of 3×3, including a skip connection to form residual structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>) dominates CARN-M, which means with fewer FLOPS and a smaller number of parameters it scores equally to The model FALSR-B comparable to CARN-M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>FALSR A, B, C (shown in salmon) vs. others (light blue) The Pareto-front of of all the models during the evolution, paired every two objectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative results on images from Urban100 (image ids in rows from top to bottom: 011, 062, 066, 078).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparisons with the state-of-the-art methods based on ×2 super-resolution task.</figDesc><table><row><cell>Model</cell><cell>Mult-Adds</cell><cell>Params</cell><cell>SET5</cell><cell>SET14</cell><cell>B100</cell><cell>Urban100</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PSNR/SSIM</cell><cell>PSNR/SSIM</cell><cell>PSNR/SSIM</cell><cell>PSNR/SSIM</cell></row><row><cell>SRCNN [1]</cell><cell>52.7G</cell><cell cols="5">57K 36.66/0.9542 32.42/0.9063 31.36/0.8879 29.50/0.8946</cell></row><row><cell>FSRCNN [11]</cell><cell>6.0G</cell><cell cols="5">12K 37.00/0.9558 32.63/0.9088 31.53/0.8920 29.88/0.9020</cell></row><row><cell>VDSR [2]</cell><cell>612.6G</cell><cell cols="5">665K 37.53/0.9587 33.03/0.9124 31.90/0.8960 30.76/0.9140</cell></row><row><cell>DRCN [12]</cell><cell cols="6">17,974.3G 1,774K 37.63/0.9588 33.04/0.9118 31.85/0.8942 30.75/0.9133</cell></row><row><cell>LapSRN [13]</cell><cell>29.9G</cell><cell cols="5">813K 37.52/0.9590 33.08/0.9130 31.80/0.8950 30.41/0.9100</cell></row><row><cell>DRRN [14]</cell><cell>6,796.9G</cell><cell cols="5">297K 37.74/0.9591 33.23/0.9136 32.05/0.8973 31.23/0.9188</cell></row><row><cell>SelNet [15]</cell><cell>225.7G</cell><cell cols="4">974K 37.89/0.9598 33.61/0.9160 32.08/0.8984</cell><cell>-</cell></row><row><cell>CARN [4]</cell><cell cols="6">222.8G 1,592K 37.76/0.9590 33.52/0.9166 32.09/0.8978 31.92/0.9256</cell></row><row><cell>CARN-M [4]</cell><cell>91.2G</cell><cell cols="5">412K 37.53/0.9583 33.26/0.9141 31.92/0.8960 31.23/0.9194</cell></row><row><cell>MoreMNAS-A [7]</cell><cell cols="6">238.6G 1,039K 37.63/0.9584 33.23/0.9138 31.95/0.8961 31.24/0.9187</cell></row><row><cell>AWSRN-M [16]</cell><cell cols="6">244.1G 1,063K 38.04/0.9605 33.66/0.9181 32.21/0.9000 32.23/0.9294</cell></row><row><cell>FALSR-A (ours)</cell><cell cols="6">234.7G 1,021K 37.82/0.9595 33.55/0.9168 32.12/0.8987 31.93/0.9256</cell></row><row><cell>FALSR-B (ours)</cell><cell>74.7G</cell><cell cols="5">326k 37.61/0.9585 33.29/0.9143 31.97/0.8967 31.28/0.9191</cell></row><row><cell>FALSR-C (ours)</cell><cell>93.7G</cell><cell cols="5">408k 37.66/0.9586 33.26/0.9140 31.96/0.8965 31.24/0.9187</cell></row><row><cell>feature extraction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv f64 k3 b4 isskip</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv f48 k1 b1 isskip</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv f64 k3 b4 isskip</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv f64 k3 b4 isskip</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv f64 k3 b4 isskip</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv f64 k1 b4 noskip</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv f64 k3 b4 isskip</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sub-pixel upsampling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our upsampling contains a 2D convolution with 32 3×3 filters, followed by a 3 × 3 convolution with one filter of unit stride.<ref type="bibr" target="#b2">3</ref> Here, i starts with 1.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast, accurate, and, lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08664</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-objective reinforced evolution in mobile neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01074</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Nsga-net: A multi-objective genetic algorithm for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03522</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast and elitist multiobjective genetic algorithm: Nsga-ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved crowding distance for nsga-ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12667</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network with selection units for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1150" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lightweight image super-resolution with adaptive weighted learning network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02358</idno>
		<ptr target="https://arxiv.org/abs/1904.02358" />
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image superresolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image quality metrics: Psnr vs. ssim</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern recognition (icpr), 2010 20th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2366" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Monas: Multi-objective neural architecture search using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10332</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
