<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProtTrans: Towards Cracking the Language of Life&apos;s Code Through Self-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20211">AUGUST 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Dallago</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghalia</forename><surname>Rehawi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gibbs</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Feher</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Angerer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debsindhu</forename><surname>Bhowmik</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
						</author>
						<title level="a" type="main">ProtTrans: Towards Cracking the Language of Life&apos;s Code Through Self-Supervised Learning</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANS PATTERN ANALYSIS &amp; MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20211">AUGUST 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Computational Biology</term>
					<term>High Performance Computing</term>
					<term>Machine Learning</term>
					<term>Language Modeling</term>
					<term>Deep Learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>extract some information learned by the LMs, referred to as embeddings. Transfer-learning refers to the idea of using such embeddings as input for subsequently trained supervised models. These two steps outsource the computationally demanding LM pre-training to the HPC infrastructure, leaving the computationally less demanding inference to commodity hardware.</p><p>Proteins are the machinery of life, built from 20 different basic chemical building blocks (called amino acids). Like beads, those amino acids are strung up in one-dimensional (1D) sequences (the beads are referred to as residues once connected). These 1D sequences adopt unique three-dimensional (3D) shapes (referred to as protein 3D structure) <ref type="bibr">[13]</ref>, and the 3D structures perform specific function(s) (often simplified as sequence determines structure determines function). We know many orders of magnitude more protein amino acid sequences than experimental protein structures (sequence-structure gap) <ref type="bibr">[14]</ref>. Knowing protein structure helps to understand function. Closing, more generally, the sequence-annotation gap through prediction methods based on artificial intelligence (AI) is one of the crucial challenges for computational biology and bioinformatics. Tapping into the vast wealth of unlabeled data through transfer-learning may become crucial to bridging these gaps.</p><p>Top prediction methods in computational biology <ref type="bibr">[15]</ref>, <ref type="bibr">[16]</ref>, <ref type="bibr">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> combine machine learning (ML) and evolutionary information (EI), first established as the winning strategy to predict protein secondary structure <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> in two steps. First, search for a family of related proteins summarized as multiple sequence alignment (MSA) and extract the evolutionary information contained in this alignment. Second, feed the EI into the ML through supervised learning implicit structural or functional constraints. When predicting for proteins without experimental annotations, such methods only use ex-perimental information implicitly captured in the trained model. Since all other information originates from the knowledge of sequences, such methods need no additional information as input other than the EI which is amply available giving the exploding databases of bio-sequences <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. However, there are several prices to pay for EI. Firstly, when predicting for entire proteomes (all proteins in an organism), compiling the EI for all is computationally expensive <ref type="bibr" target="#b24">[25]</ref>. Secondly, EI is not available for all proteins (intrinsically disordered proteins <ref type="bibr" target="#b25">[26]</ref> or dark proteome <ref type="bibr" target="#b26">[27]</ref>). Thirdly, the improvement is best when the EI is most diverse <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Fourthly, predictions based on EI somehow average over an entire family, possibly falling short of distinguishing differences between two different proteins in the same family. The latest, and arguably largest leaps ever in terms of protein structure prediction, namely AlphaFold2, roots on an advanced combination of EI and ML <ref type="bibr" target="#b29">[30]</ref>. Although that method predicts protein 3D structures at unprecedented levels of accuracy, AlphaFold2 models are many order of magnitude more computationally expensive than the creation of EI.</p><p>The leap of NLP through advanced LMs has been successfully generalized toward understanding the language of life through advanced LMs trained on proteins <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In analogy to NLP, these approaches interpret an entire protein sequence as a sentence and its constituents -amino acids -as single words. Protein sequences are constrained to adopt particular 3D structures optimized for accomplishing particular functions. These constraints mirror the rules of grammar and meaning in NLP. Since LMs extract features directly from single protein sequences, they might reach performance of the SOA without using EI.</p><p>In this project, dubbed ProtTrans, we pursued two objectives. Firstly, we explored the limits of up-scaling language models trained on proteins as well as protein sequence databases used for training. Secondly, we compared the effects of autoregressive and auto-encoding pre-training upon the success of the subsequent supervised training, and compared all LMs trained here to existing state-of-the-art (SOA) solutions using evolutionary information (EI) <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data for protein Language Models (LMs)</head><p>In this work, we assessed the impact of database size on performance through three data sets ( <ref type="table" target="#tab_1">Table 1</ref>, SOM <ref type="figure" target="#fig_8">Fig. 10</ref> ): Uniref50 <ref type="bibr" target="#b40">[41]</ref>, UniRef100 <ref type="bibr" target="#b40">[41]</ref>, and BFD <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b41">[42]</ref>. The latter merged UniProt <ref type="bibr" target="#b22">[23]</ref> and proteins translated from multiple metagenomic sequencing projects, making it the largest collection of protein sequences available at the time of writing even after removal of duplicates from the original BFD. Overall, BFD was about eight times larger than the largest data sets used previously for protein LMs <ref type="bibr" target="#b33">[34]</ref>. Despite the 8-fold increase in data, the number of tokens increased only five-fold <ref type="table" target="#tab_1">(Table 1)</ref>, because UniRef100 sequences were longer than those in BFD (1.6-fold). Without a clear mapping for LMs from NLP to proteins, i.e., the concept of words can be related to single amino acids, a window of amino acids (k-mer motifs <ref type="bibr" target="#b42">[43]</ref>) or functional units (domains <ref type="bibr" target="#b43">[44]</ref>), we decided to interpret single amino acids as input tokens/words. Thereby, protein databases contain several orders of magnitude more tokens than corpora used in NLP, e.g., Google's Billion Word data set <ref type="bibr" target="#b44">[45]</ref> is one of the biggest for NLP with about 829 million tokens (words), i.e. about 500times fewer than BFD with 393 billion tokens. Interpreting domains as words, would cut the number of tokens in BFD roughly by a factor of 100 (average domain length <ref type="bibr" target="#b45">[46]</ref>) still leaving 5-times more tokens in BFD than the Billion Word corpus. Uniref50, UniRef100 and BFD were tokenized with a single space (indicating word-boundaries) between each token. Each protein sequence was stored on a separate line, with lines/proteins representing the equivalent of "sentences". Additionally, an empty line was inserted between each protein sequence in order to indicate the "end of a document"; however, this is only essential for models with auxiliary task (Bert and Albert). Non-generic or unresolved amino acids ([BOUZ]) were mapped to unknown (X). For training ProtTXL and ProtT5, the data was transformed to pytorch and tensorflow tensors, respectively on the fly. For ProtBert, ProtAlbert, ProtXLNet and ProtElectra, the data was pre-processed and stored as tensorflow records. Given tensorflow records with terabytes, data sets had to be chunked into 6000 files for thousands of parallel workers.   <ref type="bibr" target="#b40">[41]</ref>; BFD combines UniProt with metagenomic data keeping only one copy for duplicates <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Units: number of proteins in millions (m), of amino acids in billions (b), and of disk space in GB (uncompressed storage as text).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Embeddings for supervised training</head><p>We extracted the information learned by the protein LMs through embeddings, i.e., vector representations from the last hidden state of the protein LM ( <ref type="figure">Fig. 1</ref>). In the transfer-learning step these embeddings served as input to subsequent supervised training. Although we mostly relied on previously published data sets to ease comparisons to other methods, for the supervised training, we also added a novel test set to refine the evaluation. Per-residue prediction/single tokens: to predict properties of single tokens (here: single amino acids, dubbed residues when joined in proteins), we used the training set published with NetSurfP-2.0 <ref type="bibr">[15]</ref> describing secondary structure in 3-and 8-states (class distribution for all data sets in SOM <ref type="bibr">Tables 6,</ref><ref type="bibr">5)</ref>. We also included other public test data sets, namely CB513 <ref type="bibr" target="#b46">[47]</ref>), TS115 <ref type="bibr" target="#b47">[48]</ref>, and CASP12 <ref type="bibr" target="#b48">[49]</ref>. Each of those has severe limitations (CASP12: too small, CB513 and TS115 redundant and outdated). Therefore, we added a new test set using only proteins published after the release of NetSurfP-2.0 (after Jan 1, 2019). We included proteins from the PDB <ref type="bibr" target="#b49">[50]</ref> with resolutions ≤ 2 5Å and ≥ 20 residues. MMSeqs2 <ref type="bibr" target="#b50">[51]</ref> with highest sensitivity (-s 7.5) removed proteins with &gt;20% PIDE to either the training set or to itself. On top, PISCES <ref type="bibr" target="#b51">[52]</ref> removed any protein considered by its procedure to have &gt;20% PIDE. These filters reduced the number of new proteins (chains) from 18k to 364 (dubbed set NEW364).</p><p>Per-protein prediction/embedding pooling: For the prediction of features for entire proteins (analogous to the classification of whole sentences in NLP), the DeepLoc <ref type="bibr">[16]</ref> data set was used to classify proteins into (i) membrane-bound vs. watersoluble and (ii) ten classes of subcellular localization (also referred to as cellular compartments). <ref type="figure">Fig. 1</ref>: Feature extraction overview -We give a general overview on how ProtTrans models can be used to derive features (embeddings) for arbitrary protein sequences either on the level of single amino acids or whole proteins and how they can be used for classification tasks on both levels. First, an example protein sequence "SEQ" is tokenized and positional encoding is added. The resulting vectors are passed through any of our ProtTrans models to create contextaware embeddings for each input token, i.e. each amino acid. Here, we used the last hidden state of the Transformer's attention stack as input for downstream prediction methods. Those embeddings can either be used directly as input for prediction tasks on the level of individual tokens, e.g. a CNN can be used to predict an amino acid's secondary structure. Alternatively, those embeddings can be concatenated and pooled along the length-dimension to get fixedsize embedding irrespective of the input length, i.e., global average pooling is applied. The resulting protein-level embedding can be used as input for predicting aspects of proteins, e.g., a FNN can be used to predict a protein's cellular localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data for unsupervised evaluation of embeddings</head><p>We also assessed the information captured by the embeddings extracted from the protein LMs by projecting the highdimensional representations down to two dimensions (2D) using t-SNE <ref type="bibr" target="#b52">[53]</ref>. Toward this end, we took annotations from several sources. First, a non-redundant (PIDE&lt;40%) version of the SCOPe database <ref type="bibr" target="#b53">[54]</ref> (release 2.07 with 14,323 proteins). Second, we mapped proteins into the three major domains of life (archaea, bacteria, or eukarya) or to viruses (removing all proteins with missing classifications). The number of iterations for the t-SNE projections was set to 3,000 and the perplexity to 30 for all plots with the exception of the amino acid plot for which we used a perplexity of 5. All visualizations used the same random seed (42).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Step 1: Protein LMs extract embeddings</head><p>We trained six successful LMs in NLP (T5 <ref type="bibr" target="#b54">[55]</ref>, Electra <ref type="bibr" target="#b55">[56]</ref>, BERT <ref type="bibr" target="#b56">[57]</ref>, Albert <ref type="bibr" target="#b57">[58]</ref>, Transformer-XL <ref type="bibr" target="#b58">[59]</ref> and XLNet <ref type="bibr">[11]</ref>) on protein sequences. BERT was the first bidirectional model in NLP which tried to reconstruct corrupted tokens, and is considered the de-facto standard for transfer learning in NLP. Albert reduced BERT's complexity by hard parameter sharing between its attention layers which allows to increase the number of attention heads (64 chosen here). Electra tries to improve the sampling-efficiency of the pre-training task by training two networks, a generator and a discriminator. Instead of only reconstructing corrupted input tokens, the generator (BERT) reconstructs masked tokens, potentially creating plausible alternatives, and the discriminator (Electra) detects which tokens were masked. This enriches the training signal as the loss can be computed over all tokens instead of the subset of corrupted tokens (usually only 15%). T5 uses the original transformer architecture proposed for sequence translation, which consists of an encoder that projects a source language to an embedding space and a decoder that generates a translation to a target language based on the encoder's embedding. Only later, models used either the encoder (BERT, Albert, Electra) or the decoder (TransformerXL, XLNet), but T5 showed that this simplification might come at a certain prize as it reaches state-of-the-art results in multiple NLP benchmarks. Additionally, it provides the flexibility to apply different training methodologies and different masking strategies, e.g., T5 allows to reconstruct spans of tokens instead of single tokens.</p><p>As self-attention is a set-operation and thus orderindependent, Transformers require explicit positional encoding. Models trained with sinusoidal position signal like BERT, Albert or Electra, can process only sequences shorter or equal to the length of the positional encoding which has to be set before training. Due to the huge memory requirement of Transformer-models, this parameter is usually set to a value lower than the longest proteins, e.g., Titin with 33k residues. Here, we trained models that were affected by this limitations (ProtBERT, ProtAlbert, ProtElectra) first on proteins of length ≤ 512, then on proteins ≤ 1024. Only setting the length of the positional encoding to 40k after pre-training allowed the models to process protein sequences up to a length of 40k. In contrast to this, TransformerXL introduced a memory that allows it to process sequences of arbitrary length. Still, the model cuts sequences into fragments but allows for flow of information between fragments for longer proteins by re-using hidden states of fragments which have already been processed. While its memory is uni-directional as fragments are processed sequentially, TransformerXL captures only uni-directional context within one memory fragment (auto-regressive) while XLNet, which uses a similar memory mechanism to process sequences of arbitrary length, allows to gather bidirectional context within one memory fragment.</p><p>In contrast to this, T5 learns a positional encoding for each attention head that is shared across all layers. This way, the model learned to combine the relative offset between residue pairs of lower layers, enabling the model to make predictions beyond the actual length of the positional encoding. No auxiliary tasks like BERT's next-sentence prediction were used for any model described here.</p><p>ProtTXL, ProtBert, ProtXLNet, ProtAlbert and ProtElectra were trained on UniRef100, ProtT5 on UniRef50, and ProtTXL, ProtBert &amp; ProtT5, on BFD <ref type="table" target="#tab_4">(Table 2)</ref>. Largely, we transferred configurations successfully from NLP to protein sequences <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b59">[60]</ref>, with the exception of the number of layers that was increased to optimize memory utilization.</p><p>ProtTXL: The Transformer-XL 1 was trained using both UniRef100 and BFD-100 datasets (referred to as ProtTXL and ProtTXL-BFD, respectively; <ref type="table" target="#tab_4">Table 2</ref>). Both models used a dropout rate of 15%, a memory length of 512 tokens and using mixed precision. The number of layers, number of heads, batch  <ref type="table" target="#tab_1">Number of Layers  32  30  30  30  12  30  24  24  Hidden Layers Size  1024  1024  1024  4096  1024  1024  1024  Hidden Layers Intermediate Size  4096  4096  4096  16384  4096  16384  65536  Number of Heads  14  16  16  16  64  16  32  128  Positional Encoding Limits  -40K  -40K  40K  --Dropout  0.</ref>  size, learning rate, weight decay, training steps and warmup steps were adjusted according to training set size as well as GPU utilization. The number of warm-up steps was set to cover at least one epoch for each data set. We tested initial learning rates between 0.001 and 0.005 which were increased linearly at every training step over the warm-up period. To avoid model divergence during training, the learning rate had to be (i) reduced along with the warm-up steps (for BFD), or (ii) increased for both (for Uniref100). Even after increasing the warm-up steps to two epochs, the maximum learning rate remained at 0.0025 for both data sets. Beyond this point, the training diverged. Using weight decay to regularize the network increased the GPU memory usage as it required to compute the norm of all weight vectors on our models, thus reducing the batch size. ProtTXL-BFD was trained for 40k steps in total, with 13.6k warm-up steps using a learning rate of 0.0005, while ProtTXL was trained for 31k steps with 5k warm-up steps using a learning rate of 0.002. The Lamb optimizer was able to handle the resulting batch sizes of 44k and 22k for ProtTXL-BFD and ProtTXL, respectively, without divergence. ProtBert: BERT 2 was trained using both UniRef100 and BFD-100 datasets (referred to as ProtBert and ProtBert-BFD, respectively; <ref type="table" target="#tab_4">Table 2</ref>). Compared to the original BERT publication, the number of layers was increased. Unlike Transformer-XL which was trained on Nvidia GPUs, mixed-precision was not used to train other models because those were trained on TPUs. Similar to the BERT version trained in the Lamb paper <ref type="bibr" target="#b60">[61]</ref>, ProtBert was first trained for 300k steps on sequences with a maximum length of 512 and then for another 100k steps on sequences with a length of a maximum length of 2k. While ProtBert-BFD was trained for 800k steps, then for another 200k steps for sequences with maximum length of 512 and 2k, respectively. This allows the model to first extract useful features from shorter sequences while using a larger batch size, rendering training on longer sequences more efficient.</p><p>ProtAlbert: We trained Albert 3 on UniRef100 (ProtAlbert; <ref type="table" target="#tab_4">Table 2</ref>). We used the configuration from the official GitHub 2. https://github.com/google-research/bert 3. https://github.com/google-research/albert repository for Albert (version: xxlarge v2). For Albert the number of layers is increased through the number of times, Albert stacks its single layer. Compared to the original publication, we achieved increasing the global batch size from 4096 to 10752 on the same hardware. The reason for this counterintuitive effect is the reduced vocabulary size in proteins: the entire diversity of the protein universe is realized by 20 different amino acids, compared to tens of thousands of different words. Similar to ProtBert, ProtAlbert was first trained for 150k steps on sequences with a maximum length of 512 and then for another 150k steps on sequences with a maximum length of 2k.</p><p>ProtXLNet: XLNet 4 was trained on UniRef100 (ProtXL-Net) using the original NLP configuration <ref type="bibr">[11]</ref>  <ref type="table" target="#tab_4">(Table 2</ref>) except for the number of layers that was increased to 30 layers which reduced the global batch size to 1024. Due to the relatively small batch-size, we used the original optimizer: Adam with a learning rate of 0.00001. The model was trained through more steps, i.e. 20k warm-up and 847k steps to compensate for the smaller batch-size of this model. ProtElectra: Electra 5 consists of two models, a generator and discriminator (same number of layers, generator 25% of the discriminator's hidden layer size, hidden layer intermediate size, and number of heads). We copied Electra's NLP configuration with two changes: increasing the number of layers to 30 and using Lamb optimizer. Again, we split the training into two phases: the first for proteins ≤ 512 residues (400k steps at 9k global batch size), the second for proteins ≤ 1024 (400k steps at 3.5k global batch size). While ProtTXL, ProtBert, ProtAlbert and ProtXLNet relied on pre-computed tensorflow records as input, Electra allowed to mask sequences on the fly, allowing the model to see different masking patterns during each epoch.</p><p>ProtT5: Unlike the previous LMs, T5 <ref type="bibr">6</ref> uses an encoder and decoder <ref type="bibr">[10]</ref>. We trained two model sizes, one with 3B (T5-XL) and one with 11B parameters (T5-XXL). T5-XL was trained using 8-way model parallelism, while T5-XXL was trained using 32-way model parallelism. First, T5-XL and T5-4. https://github.com/zihangdai/xlnet 5. https://github.com/google-research/electra 6. https://github.com/google-research/text-to-text-transfer-transformer XXL were trained on BFD for 1.2M and 920k steps respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second step, ProtT5-XL-BFD and ProtT5-XXL-BFD were fine-tuned on UniRef50 for 991k and 343k steps respectively (ProtT5-XL-U50, ProtT5-XXL-U50). Contrary to the original T5 model which masks spans of multiple tokens, we adopted BERT's denoising objective to corrupt and reconstruct single tokens using a masking probability of 15%. All T5 models used the AdaFactor optimizer with inverse square root learning rate schedule for pre-training. Like ProtElectra, T5 masks each sequence on the fly. In our hands, the encoder outperformed the decoder on all benchmarks significantly and running the model in half-precision during inference instead of full-precision had no effect on performance but allowed to run the model on on a single Nvidia TitanV (12GB vRAM). Thus, we dropped the decoder from further analysis which cuts model size by half during inference. For completeness, we made weights for encoder and decoder publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Step 2: Transfer learning of supervised models</head><p>To best analyze the impact of transfer learning, we deliberately kept the supervised models using the embeddings from the protein LMs as input minimal. In particular, compared to SOA solutions such as NetSurfP-2.0, all our experiments used the pre-trained LMs as feature extractors without fine-tuning, i.e. without gradient back-propagating to the LMs. Throughout, we extracted the embeddings from the last hidden state of the pretrained LMs as described in detail elsewhere <ref type="bibr" target="#b31">[32]</ref>. To briefly summarize ( <ref type="figure">Fig. 1</ref>): we applied tasks on two different levels, namely on the level of single tokens (per-residue) and whole sentences through pooling (per-protein) predictions. For the per-residue prediction, we input the embeddings into a twolayer convolutional neural network (CNN). The first CCN layer compressed the embeddings to 32 dimensions using a window size of 7. The compressed representation was fed into two different CNNs (each with window size 7). One learned to predict secondary structure in 3-states, the other in 8-states. The network was trained on both outputs simultaneously by adding their losses (multi-task learning). For ProtBERT-BFD embeddings we additionally trained three other models: logistic regression, FNN and LSTM. Similar to the CNN, the two-layer FNN first compressed the output of the language model down to 32 dimensions which the second FNN-layer used to predict 3-and 8-states simultaneously. The bi-directional LSTM compressed the embeddings down to 16 dimensions. Concatenating both directions, the resulting 32 dimensional representation was used by a FNN layer to predict 3-or 8-states. As the CNN performed best (SOM <ref type="table" target="#tab_1">Table 10</ref>), we used CNNs throughout. For the per-protein prediction, we also extracted the embeddings from the last layer of the protein LMs. However, then we pooled the representations over the length-dimension resulting in a fixed-size representation for all proteins. Using ProtBERT-BFD embeddings, we compared alternative pooling strategies (SOM <ref type="table" target="#tab_1">Table 10</ref>) and chose mean-pooling for all further experiments. The resulting vector was used as an input to a single feed forward layer with 32 neurons which compressed information before making the final predictions for both per-protein tasks, i.e., the prediction of subcellular localization and the differentiation between membrane-bound and water-soluble proteins, simultaneously (multi-task learning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Hardware</head><p>HPC hardware is advancing both through infrastructure of supercomputers, such as Fugaku <ref type="bibr" target="#b61">[62]</ref>, Summit <ref type="bibr">[1]</ref> or the SuperMUC-NG <ref type="bibr" target="#b62">[63]</ref>, and through its components, such as TPU pods <ref type="bibr">[2]</ref>, specifically designed to ease large scale neural network training for users. Concurrent software improvements in form of more efficient libraries such as Horovod <ref type="bibr">[6]</ref> allow executing general purpose code on large distributed clusters with minor code changes. In this section we give details on the hard-and software used for training language models on large protein sequence databases.</p><p>ORNL Summit &amp; Rhea: The Oak Ridge National Laboratory (ORNL) provides several clusters for researchers who need computational resources not provided by research facilities such as universities. Here, we used Summit and Rhea. Summit was used to train the deep learning models, while Rhea was used for the pre-processing of data sets including the distributed generation of tensorflow records.</p><p>Summit is the world's second fastest computer, consisting of approximately 4618 nodes. Each node has two IBM POWER9 processors and six NVIDIA Volta V100 with 16GB of memory each <ref type="bibr">[1]</ref>. Every POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in both directions. A single node has 0.5 TB of DDR4 main memory and 1.6TB of non-volatile memory that can be used as a burst buffer. Summit is divided into racks with each rack having 18 nodes. In all of our experiments we reserved 936 nodes for training. As having nodes on the same rack decreases the communication overhead, we reserved entire racks.</p><p>The smaller cluster (Rhea) contains two partitions: Rhea and GPU. The Rhea partition has 512 node, each with 128 GB of memory and two Intel® Xeon® E5-2650. The GPU partition has only 9 nodes, each with 1 TB of memory and two Intel® Xeon® E5-2695. Reha reduced the time needed for creating tensorflow records for the BFD dataset from 7.5 months (!) to fewer than two days, by converting the original sequential script to distributed processing using MPI. The generation script used two nodes of the GPU partition, with a total of 112 parallel threads.</p><p>Google TPU Pod: In 2016, Google introduced tensor processing unit (TPU) as its application-specific integrated circuit optimized for training neural networks. TPUs can be accessed through Google Cloud. Training the protein LMs used both older TPU generation (V2) with 256 cores, and the latest TPU generation (V3) with 512 and 1024 cores. These cores are divided into hosts with each host having access to 8 cores. Consequently, we had access to 32, 64 and 128 hosts for V2/V3-256, V3-512 and V3-1024, and each core had 8 GiB and 16 GiB of high-bandwidth memory for V2 and V3. Training on the TPUs required access to a virtual machine on Google Cloud and storage on Google Bucket [64].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Software</head><p>Summit integrates several pre-configured modules which include the most popular libraries and tools required for simulation, deep learning, distributed training and other purposes. We used the IBM Watson Machine Learning module versions 1.6.0 and 1.6.2 for our deep learning training. In contrast to this, the Google Cloud server, which we used for the TPU Pod training, had to be configured manually because only the operating system was installed. Pytorch was used to train ProtTXL, tensorflow to train ProtBert, ProtAlbert, ProtXLNet, ProtElectra and ProtT5. Both libraries used the Horovod framework <ref type="bibr">[6]</ref> to train the models on distributed clusters such as Summit. Horovod supports distributed GPU training with minimal change in the code. It supports different backends including MPI, NCCL and IBM PowerAI distributed deep learning (DDL). We tested all three backends and found DDL to be the fastest for our training purpose on Summit. The time needed to finish a single batch with ProtTXL-BFD increased from one to two nodes due to the communication overhead ( <ref type="figure" target="#fig_0">Fig. 2</ref>). After two nodes the communication overhead plateaued, even when scaling up to 936 nodes with 5616 GPUs. Summit has integrated DDL in their Watson Machine Learning module which comes with most DDL libraries including pytorch, tensorflow, apex, DDL and horovod. However, Summit has only a license for using DDL up to 954 nodes. Contrary to Summit, training on TPU Pods did not require any changes in the Tensorflow code to use either a single TPU host or to distribute workload among multiple TPU hosts.</p><p>Mixed precision allows to fit larger models and batch sizes into GPU memory by using 16-bit precision only or a mix of 16bit and 32-bit precision. Nvidia's APEX library <ref type="bibr" target="#b64">[65]</ref> was used for mixed precision training of ProtTXL, because APEX supports pytorch. APEX supports four types of mixed precision and model weights storing: 1) Pure 32-bit precision; this is the regular training without using mixed precision. 2) Pure 16-bit precision, all the model weights will be stored in 16-bit rather than 32bit. 3) Mixed Precision, for different layer types depends on previously tested whitelist/blacklist by Nvidia; some weights will be stored in 32-bit while others in 16-bit format. 4) Almost FP16, storing all model weights at 16 Bit precision; exception: batch-normalization layers, while keeping a master copy of the model's weights in 32-Bit. Using pure 16-bit training leads to a big part of activation gradient values becoming zeros, leading to divergence during training. This problem is solved using Almost FP16 because there is a master copy of the model's weights in 32-Bit. As ProtTXL training became instable when training with 16 Bit precision, we switched to almost half precision training. We did not use mixed-precision for models trained on TPUs.</p><p>Another optimization technique/library crucial for our training on Summit was IBM's large model support (LMS) <ref type="bibr" target="#b65">[66]</ref>. Similar to gradient checkpointing <ref type="bibr" target="#b66">[67]</ref>, LMS virtually extends the GPU memory by outsourcing parts of the model from GPU to main memory. This allows training models larger than the GPU memory. The obvious drawback of LMS is the increase in training time due to shuttling data between CPU and GPU and back. However, the reduced memory consumption of the model allows to increase the batch size, potentially compensating for the communication overhead. Compared to gradient checkpointing, LMS provides easier integration into existing code by operating directly on a computational graph defined by users and automatically adds swap-in and swap-out nodes for transferring tensors from GPU memory to main memory and vice versa. We have tested LMS on ProtTXL as well as ProtBert ( <ref type="figure" target="#fig_0">Figure 2</ref>). As Pytorch and tensorflow have different strategies to integrate LMS, we also compared the effect of LMS on batch-size, model size and training time using the two different libraries. ProtTXL was used to evaluate the effect of Pytorch's implementation of LMS while ProtBert was trained for a few steps BFD using Summit to evaluate tensorflow's implementation of LMS. Training ProtBert for a few steps was sufficient to assess the effect of LMS on batch-size, model size as well as an estimate of training time. In the end, we used LMS only for ProtTXL to strike a balance between model size and training time. The number of LM parameters could be increased by about 15.6% for ProtTXL-BFD and to 6.6% for ProtBert (3a). Additionally, we could increase the batch size by 700% for ProtTXL-BFD <ref type="figure" target="#fig_1">(Figures 3b and 3c</ref>). The NV-Link between CPU and GPU on Summit-nodes, reduced the training time for ProtTXL by 60%while it increased by 72% for ProtBert ( <ref type="figure" target="#fig_1">Figure  3d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Step 1: Unsupervised protein LMs informative</head><p>Embeddings extract constraints about protein function and structure learned by the protein LMs in the first self-supervised step of pre-training on raw protein sequences. Using t-SNE <ref type="bibr" target="#b52">[53]</ref>, we visualized this information by projecting the embeddings onto 2D and annotated structural, functional or evolutionary features. Using attention maps, we analyzed the DNA-binding zinc-finger motif well conserved in evolution.</p><p>Capturing biophysical features of amino acids. Applying t-SNE to the uncontextualized token embedding layer visualized information extracted by the LMs for individual amino acids independent of their context (residues next to it). As previously established for another protein LM <ref type="bibr" target="#b38">[39]</ref>, the t-SNE projections (e.g. ProtT5-XL-U50 SOM <ref type="figure" target="#fig_2">Fig. 14A</ref> or ProtBert-BFD SOM <ref type="figure" target="#fig_3">Fig. 15A</ref>) suggested that all LMs captured essential biophysical amino acid features, including charge, polarity, size, hydrophobicity, even to the level of aliphatic ([AILMV]) vs. aromatic ([WFY]).</p><p>We compared the embedding projection with a randomly initialized model of identical architecture to ascertain that the observed effects did not originate from coincidental signals originating from projecting high-dimensional data <ref type="figure" target="#fig_2">(Fig. 4A</ref>) or some inductive bias of neural networks <ref type="bibr" target="#b67">[68]</ref>. The random projection clearly did not carry biophysical information, while the embeddings projection did.  Capturing protein structure classes. To assess which aspects of protein structure were captured by the unsupervised LMs, we averaged over the length-dimension of the representations derived from the last layer of each model (see <ref type="figure">Fig. 1</ref> for a sketch) to derive fixed size representations for each protein in the database. We annotated structural class through the SCOPe database <ref type="bibr" target="#b53">[54]</ref> (Methods). ProtBert-BFD and especially ProtT5-XL-U50 embeddings visually separated the proteins best (details in SOM Figs. <ref type="bibr">[14]</ref><ref type="bibr">[15]</ref><ref type="bibr">[16]</ref><ref type="bibr">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. Although sequence length was not explicitly encoded and our pooling squeezed proteins into a fixed vector size, all models separated small from long proteins (brown, e.g. ProtT5-XL-U50 SOM <ref type="figure" target="#fig_2">Fig. 14D</ref>). All models also distinguished between water-soluble and transmembrane proteins (light blue, e.g. ProtT5-XL-U50 SOM <ref type="figure" target="#fig_2">Fig. 14D</ref>) and, to some extent, between proteins according to their secondary structure composition(e.g. all-alpha (dark blue) vs. all-beta (dark green) ProtT5-XL-U50 <ref type="figure" target="#fig_2">Fig. 14D</ref>). While having much higher entropy, even the random clustered small proteins from long proteins (brown, <ref type="figure" target="#fig_2">Fig. 4B</ref>).</p><p>Capturing domains of life and viruses. The analysis distinguished three domains of life: archaea, bacteria, and eukarya, along with viruses typically not considered as life. We used the same proteins and per-protein pooling as for the SCOPe analysis. All protein LMs captured some organismspecific aspects (e.g. ProtT5-XL-U50 SOM <ref type="figure" target="#fig_2">Fig. 14E</ref>). Eukarya and bacteria clustered better than viruses and archaea. Comparing different LMs revealed the same trend as for protein structure classes: ProtTXL (SOM 19E) and ProtBert (SOM 16E) produced higher entropy clusters while ProtAlbert (SOM 17E), ProtXLNet (SOM 18E), ProtBERT-BFD (SOM <ref type="figure" target="#fig_3">Fig. 15E)</ref> and ProtT5-XL-U50 (SOM <ref type="figure" target="#fig_2">Fig. 14E</ref>) produce visually easier separable clusters.</p><p>Capturing protein function in conserved motifs. A similar overall per-protein analysis as for structural classes and domains of life also suggested some clustering according to protein function as proxied by enzymatic activity (EC-numbers <ref type="bibr" target="#b68">[69]</ref> and subcellular localization (SOM -1.2 Protein LMs unsupervised). We focused in more detail on the attention mechanism <ref type="bibr" target="#b69">[70]</ref> at the core of each Transformer model <ref type="bibr">[10]</ref> providing some limited understanding of the AI <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>. We visualized <ref type="bibr" target="#b72">[73]</ref> the attention weights of ProtAlbert to analyze the structural motif of a zinc-finger binding domain (SOM <ref type="figure">Fig. 11</ref>) crucial for DNA-and RNA-binding and conserved across diverse organisms. The right part of ProtAlbert' attention heads (SOM <ref type="figure">Fig. 11</ref>; line thickness resembles attention weight) learned to focus mostly on the four residues involved in zinc-binding (residues highlighted in the left part of SOM <ref type="figure">Fig. 11</ref>) which is essential for function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Step 2: Embeddings good input to predict</head><p>The acid test for proving that the embeddings from protein LMs extracted important constraints is to exclusively use embeddings as input to supervised training of features reflecting structure and function. We proxied this through predictions on two different level, namely on the per-residue or token level (secondary structure) and on the per-protein or sentence level through pooling over entire proteins (localization, and classification into membrane/non-membrane proteins). Protein LMs remained unchanged, i.e. both approaches (per-residue/perprotein) used only embeddings derived from the hidden state of the last attention layer of each protein LM ( <ref type="figure">Fig. 1</ref>) without gradient backpropagation to the LM, i.e., LMs were only used as static feature extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Per-residue secondary structure prediction</head><p>To ease comparability, we evaluated all models on standard performance measures (Q3/Q8: three/eight-state per-residue accuracy, i.e. percentage of residues predicted correctly in either of the 3/8 secondary structure states) and on standard data sets (CASP12, TS115, CB513). To increase the validity of the comparisons, we added a novel, non-redundant test set (dubbed NEW364, Methods). For simplicity, we only presented values for Q3 on CASP12 and NEW364 (TS115 and CB513 contain substantial redundancy; Q8 results brought little novelty; all details in SOM <ref type="table" target="#tab_17">Tables 9, 8</ref>). As error estimates failed to capture the performance variation between NEW364 and CASP12, we used CASP12 as lower-and NEW364 as upper-limit.</p><p>Comparing supervised architectures: We input embeddings from ProtBERT-BFD into four different models for supervised training (Methods): logistic regression (LogReg), FNN, CNN and LSTM. LogReg provided an advanced baseline (Q3(LogReg)=74.3-79.3, where the spread is from the lower level for the set CASP12 and the upper for the set NEW364; SOM <ref type="table">Table 7</ref>). LSTMs and CNNs performed alike and better than LogReg (Q3(CNN)=76.1-81.1% vs. Q3(LSTM)76.1-80.9%). As CNNs are computationally more efficient, we focused on those in the following.</p><p>Comparing protein LMs: Trained on UniRef100 <ref type="table" target="#tab_1">(Table 1)</ref>, ProtBert outperformed other models trained on the same corpus <ref type="table" target="#tab_9">(Tables 3, 8</ref>). For ProtTXL and ProtBert we could analyze the influence of database size upon performance: 10-times larger   BFD <ref type="table" target="#tab_1">(Table 1)</ref> helped ProtBert slightly (∆Q3: +1.1%) but made ProtTXL worse (∆Q3: -0.6%; <ref type="table" target="#tab_9">Table 3 and SOM Tables  9, 8</ref>). The gain was larger when fine-tuning the two ProtT5 versions (XL and XXL) by first training on BFD and then refining on UniRef50. Consistently, all models fine-tuned on UniRef50 outperformed the versions trained only on BFD ( <ref type="figure" target="#fig_3">Fig. 5</ref>, <ref type="table" target="#tab_9">Table  3</ref>, SOM <ref type="table" target="#tab_17">Table 8</ref>). Although these gains were consistently numerically higher, the statistical significance remained within the 68% confidence interval (maximal difference: 1.1% compared to one standard error of ±0.5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-domain Membrane, cell surface Small proteins</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oxidoreductases Transferases Hydrolases Lyases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Isomerases Ligases Translocases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eukaryota Bacteria</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Archaea Viruses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soluble</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Membrane-bound</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-domain Membrane, cell surface Small proteins</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oxidoreductases Transferases Hydrolases Lyases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Isomerases Ligases Translocases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eukaryota Bacteria</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Archaea Viruses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soluble Membrane-bound</head><p>Embeddings reach state-of-the-art (SOA): All models (Prot-TXL, ProtBert, ProtAlbert, ProtXLNet, ProtElectra, ProtT5) and all databases (BFD, UniRef50/UniRef100) tested improved significantly over context-free feature extractors such as word2vecbased approaches (DeepProtVec in <ref type="figure" target="#fig_3">Fig. 5</ref> and SOM <ref type="table" target="#tab_17">Table  8</ref>). Both ProtTXL versions fell short compared to an existing ELMo/LSTM-based solution (DeepSeqVec <ref type="bibr" target="#b31">[32]</ref>) while all other Transformer-models outperformed DeepSeqVec. Embeddings extracted from another large Transformer (ESMB-1b <ref type="bibr" target="#b71">[72]</ref>), improved over all our non-ProtT5 models (Figs. 5 and SOM <ref type="table" target="#tab_17">Table 8</ref>)). Most solutions using only embeddings as input were outperformed by the state-of-the-art method NetSurfP-2.0 <ref type="bibr">[15]</ref> using evolutionary information ( <ref type="figure" target="#fig_3">Fig. 5</ref> and SOM <ref type="table" target="#tab_17">Tables 9, 8</ref>).</p><p>However, ProtT5-XL-U50 reached nearly identical performance without ever using multiple sequence alignments (MSA). Analyzing the average Q3 per protein of both models for set NEW364 in more detail (SOM <ref type="figure" target="#fig_0">Fig. 12</ref>), revealed that 57% of the proteins were predicted with higher Q3 by ProtT5-XL-U50 (CASP12 was too small for such a differential analysis).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LMs shine for small families:</head><p>The size of protein families follows the expected power-law/Zipf-distribution (few families have many members, most have fewer <ref type="bibr" target="#b79">[80]</ref>). To simplify: families with fewer members carry less evolutionary information (EI) than those with more. One proxy for this is the number of effective sequences (Neff), i.e., the number of sequences in an MSA clustered at 62% PIDE <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b80">[81]</ref>. We analyzed the effect of Neff by comparing NetSurfP-2.0 (using MSAs/EI) to ProtT5-XL-U50 (not using MSAs) using four subsets of NEW364 using different Neff cutoffs, i.e., the subset of proteins without any hit (Neff=1, 12 proteins), less than 10 hits (Neff&lt;=10, 49 proteins) and Neff&gt;10 (314 proteins) ( <ref type="figure" target="#fig_4">Fig. 6</ref>). Details on the MSA generation are given in SOM. ProtT5XL-U50 improved most over NetSurfP-2.0 for the smallest families (Neff=1).  here exhibited a similar trend: correlating performance and the number of samples presented during the first step training of the protein LMs (pre-training). Toward this end, we computed the number of samples as the product of the number of steps and the global batch size ( <ref type="figure" target="#fig_5">Fig. 7</ref>; Spearman's ρ=0.62). In particular, comparing the two largest models trained by us (ProtT5-XL and ProtT5-XXL) suggested that seeing more samples during pretraining might be more beneficial than increasing model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More samples better performance of protein</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Per-protein localization &amp; membrane prediction</head><p>To investigate per-protein (sentence-level) predictions of protein function, we trained FNNs on sub-cellular localization (also referred to as cellular compartment) in ten classes and on the binary classification of membrane vs. non-membrane (also referred to as globular) proteins. Levels of ten-state (Q10 for localization) and two-state (Q2 for membrane/globular) measured performance. Toward this end, we derived per-residue embeddings from the last hidden layer of the protein LM and pooled over the length-dimension/entire protein ( <ref type="figure">Fig. 1</ref>).</p><p>Mean-pooling performed best: Using ProtBERT-BFD embeddings we compared four different pooling strategies for collapsing per-residue (token-level) embeddings, the dimensions of which differ for proteins of different length, into representations of fixed length. These were min-, max-, and mean-pooling, as well as, the concatenation of those three (concat). The first two (min/max) performed almost fourteen percentage points worse for localization (Q10) and about three for membrane/other (Q2) compared to the others (mean/concat, <ref type="table" target="#tab_1">Table 10</ref>). While meanpooling and concat performed similarly for the classifcation task (membrane/other), mean-pooling outperformed concat for localization by about ten percentage points <ref type="table" target="#tab_1">(Table 10</ref>). In the Additions of BFD mark pre-training on the largest database BFD, U50 mark pre-training with BFD and refining with UniRef50. We included protein LMs described elsewhere (marked as: protein LMs others, namely ESM-1b <ref type="bibr" target="#b71">[72]</ref>, DeepProtVec and DeepSeqVec <ref type="bibr" target="#b31">[32]</ref>). All embeddings were input to the same CNN architecture. Two approaches used amino acids instead of embeddings as input (marked as: no LMs: DeepOneHot [32] -one-hot encoding -and DeepBLOSUM62 <ref type="bibr" target="#b31">[32]</ref> -input BLOSUM62 <ref type="bibr" target="#b73">[74]</ref> substitution matrix), as well as, to the current state-of-the-art (SOA) method NetSurfP-2.0 <ref type="bibr">[15]</ref>, and Jpred4 <ref type="bibr" target="#b74">[75]</ref>, RaptorX <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>, Spider3 <ref type="bibr" target="#b77">[78]</ref>. The rightmost four methods use MSA as input (marked as: MSA input evolutionary information). While only rotT5-XL-U50 reached the SOA without using MSAs, several protein LMs outperformed other methods using MSA. All protein LMs other than the context-free DeepProtVec improved significantly over methods using only amino acid information as input. One interpretation of the difference between the two data sets is that CASP12 provided a lower and NEW364 an upper limit. The top row shows the complete range from 0-100, while the lower row zooms into the range of differences relevant here.</p><p>following, we used only mean-pooling to benchmark the perprotein/sentence-level predictions.</p><p>Comparison of LMs: the per-protein prediction of localization in 10 states largely confirmed the trend observed for per-residue secondary structure prediction (Q3/Q8) in several ways: All LMs introduced in this work (marked by * in <ref type="table" target="#tab_11">Table  4</ref>) clearly outperformed the un-contextualized word2vec-based approaches (DeepProtVec; <ref type="figure" target="#fig_6">Fig. 8</ref>, <ref type="table" target="#tab_11">Table 4</ref>). Except for ProtTXL and ProtXLNet, all transformers trained here outperformed the previous ELMo/LSTM-based solution (DeepSeqVec). Increasing the corpus for pre-training the protein LMs 10-fold appeared to have little effect (Prot* vs. Prot*-BFD in <ref type="figure" target="#fig_6">Fig. 8</ref> and <ref type="table" target="#tab_11">Table 4</ref>). In contrast, fine-tuning ProtT5 models already trained on BFD using UniRef50 improved (Prot*/Prot*-BFD vs. Prot*-U50 in <ref type="figure" target="#fig_6">Fig. 8</ref> and <ref type="table" target="#tab_11">Table 4</ref>). Although most embedding-based approaches were outperformed by the state-of-the-art method (DeepLoc) which uses multiple sequence alignments (MSAs) as input, both ProtT5 models trained on UniRef50 outperformed DeepLoc without using MSAs: Q10, <ref type="figure" target="#fig_6">Fig. 8</ref> and <ref type="table" target="#tab_11">Table 4</ref>.</p><p>Similar for membrane/other: Results for the classification into membrane/other (Q2; <ref type="table" target="#tab_11">Table 4</ref>), largely confirmed those obtained for localization (Q10) and secondary structure (Q3/Q8): : Effect of MSA size. We used our new test set (NEW364) to analyze the effect of the size of an MSA upon secondary structure prediction (Q3) for the two top methods (both reaching Q3=84.3%): NetSurfP-2.0 (using MSA) and ProtT5-XL-U50 (not using MSA). As proxy for MSA size served Neff, the number of effective sequences <ref type="bibr" target="#b78">[79]</ref> (clustered at 62% PIDE): leftmost bars: MSAs with Neff=1, middle: N &amp; ≤ 10, right: Neff&gt;10. As expected ProtT5-XL-U50 tended to reach higher levels than NetSurfP-2.0 for smaller families. Less expected was the almost on par performance for larger families. (1) ProtT5 LMs fine-tuned on UniRef50 performed best without MSAs, (2) the 10-fold larger pre-training BFD had no noticeable effect, (3) our best protein LMs outperformed existing transformer LMs (ESM-1b) ( <ref type="figure" target="#fig_6">Fig. 8)</ref>. In contrast to localization and secondary structure, the fine-tuning appeared not to increase performance <ref type="table" target="#tab_11">(Table 4</ref>) and both ProtT5 remained 1-2 percentage points below DeepLoc.  <ref type="table">Table 5</ref> except for one method using neither LMs nor MSA (no LM no MSA: DeepLoc-BLOSUM62 <ref type="bibr">[16]</ref>), and two methods using MSAs (MSA input evolutionary information): the current state-of-the-art (SOA) method (performance marked by horizontal thin lines in magenta and cyan) DeepLoc <ref type="bibr">[16]</ref>, and LocTree2 <ref type="bibr" target="#b81">[82]</ref>. Almost all LMs outperformed LocTree2 and a version of DeepLoc not using MSAs (DeepLoc-BLOSUM62). Only, ProtT5-XXL-U50 and ProtT5-XL-U50 outperformed the SOA. A recent method optimized localization prediction from embeddings (ProtT5) through a light-attention mechanism; it clearly outperformed the SOA without using MSAs (LA_ProtT5 &amp; LA_ProtT5-U50 <ref type="bibr" target="#b82">[83]</ref>). The top row shows the complete range from 0-100, while the lower row zooms into the range of differences relevant here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fast and reliable predictions from embeddings</head><p>We compared the time needed to generate representations for EI-based prediction methods and protein language models by generating MSAs/embeddings for each protein in the human proteome <ref type="bibr" target="#b19">(20,</ref><ref type="bibr">353)</ref> proteins with a median sequence length of 415 residues). We used the fastest method available, namely MMseqs2 <ref type="bibr" target="#b50">[51]</ref>, with parameters established by NetSurfP-2.0 to generate MSAs from two databases (UniRef90 with 113M and UniRef100 with 216M proteins; see SOM for more technical details). MMseqs2 was about 16 to 28-times slower than the fastest LMs (ProtElectra and ProtBert), and about 4 to 6times slower than our best model (ProtT5-XL; <ref type="figure">Fig. 9</ref>). ProtT5-XL, required on average 0.12 seconds to generate embeddings for a human protein, completing the entire human proteome (all proteins in an organism) in only 40 minutes. We also investigated the cross-effect of sequence length and batch-size (SOM <ref type="table" target="#tab_1">Table 11</ref>) on the inference speed of different protein LMs. When using a single Nvidia Quadro RTX 8000 with half precision on varying batch-sizes <ref type="bibr">(1,</ref><ref type="bibr">16,</ref><ref type="bibr" target="#b31">32)</ref> as well as sequence lengths (128, 256, 512), ProtBert and ProtElectra provided the fastest inference with an average of 0.007 seconds per protein when using a batch size of 32, followed by ProtT5-XL and ProtAlbert (0.025s). The batch-size of most models could   <ref type="bibr">[16]</ref>. Highest values in each column marked in bold-face.</p><p>have been increased on the same hardware but was limited to allow a direct comparison between all models, due to large memory requirements for ProtT5-XXL. The script for running this benchmark is freely available 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Substantial computing resources needed to cope</head><p>HPC Supercomputers such as Summit <ref type="bibr">[1]</ref> and Google's cloud TPU Pod <ref type="bibr">[2]</ref>, combined with optimized libraries such as IBM DDL <ref type="bibr">[7]</ref> and Horovod <ref type="bibr">[6]</ref> set the stage for training LMs with billions of free parameters on corpora with terabytes of data in hours or days. Increasing model size improves performance for some NLP applications <ref type="bibr">[12]</ref>, although the massive data challenges the communication between thousands of nodes and divergence between large batches during training. Here, we presented some solutions to overcome these challenges for training protein LMs by fully utilizing 20% of Summit for TransformerXL <ref type="bibr" target="#b58">[59]</ref>, as well as, by using one TPU Pod V3-512 for Bert <ref type="bibr" target="#b56">[57]</ref>, Electra <ref type="bibr" target="#b55">[56]</ref>, Albert <ref type="bibr" target="#b57">[58]</ref> and XLNet <ref type="bibr">[11]</ref>, and a mix of TPU Pod V3-256, V3-512, and V3-1024 for ProtT5-XL and ProtT5-XXL <ref type="bibr" target="#b54">[55]</ref>. This translated into the parallel use of 5616 GPUs on Summit or 256/512/1024 TPU cores on a TPU Pod, while avoiding training divergence with specialized optimizers such as LAMB <ref type="bibr" target="#b60">[61]</ref> up to a global batch size of 44K samples (here: protein sequences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training protein LMs longer most important</head><p>Better protein LM means higher performance when using it as input. For most of our work, we proxied the degree to which the pre-trained protein language models (LMs) extracted EI LM <ref type="figure">Fig. 9</ref>: Inference Speed Comparison: The time required to generate protein representations for the human proteome (20.353 proteins) is compared using either our protein LMs or mmseqs2 (protein sequence search tool <ref type="bibr" target="#b50">[51]</ref> used to generate evolutionary information; NetSurfP-2.0 <ref type="bibr">[15]</ref> parameters are used). Here, we used mmseqs2 (red bar) to search each protein in the human proteome against two large protein database (UniRef90 and UniRef100 with 113M and 216M proteins, respectively). Only embedding or search time is reported, i.e. no preprocessing or pre-training was measured. mmseqs2 was run on an Intel Skylake Gold 6248 processor with 40 threads, SSD and 377GB main memory, while protein LMs were run on a single Nvidia Quadro RTX 8000 with 48GB memory using half precision and dynamic batch size depending on sequence length (blue bar).</p><p>information through the performance of the second step supervised tasks. Consequently, we considered that the protein LM improved (or was better) when the supervised task using this LM as input reached higher performance.</p><p>BFD: Largest database for pre-training: We trained our protein LMs on the largest protein database ever used for this purpose, namely BFD <ref type="bibr" target="#b41">[42]</ref>, more than an order of magnitude larger than UniProt <ref type="bibr" target="#b22">[23]</ref>, the standard in the field. Although bigger did not equate better in terms of prediction performance on supervised tasks, some of the protein LMs appeared to improve through pre-training on more data (UniRef100 vs BFD, <ref type="table" target="#tab_1">Table 1</ref>). Nevertheless, the top performance increase appeared somehow limited given the 10-fold larger data set (e.g. ∆Q3(ProtBert-BFD/UniProt)=1.3%). Instead, the pre-training fine-tuning protocol by which we first trained on the larger but more noisy (more mistakes in protein sequences) and redundant BFD and then continued pre-training using the smaller, less redundant UniRef50 improved performance significantly for both ProtT5 versions (∆Q3(ProtT5-XL-BFD/U50)=2.8% and ∆Q3(ProtT5-XXL-BFD/U50)=1.4%). Possibly, the refined models were less biased toward large protein families over-represented in redundant databases. The improvement through refined pre-training for ProtT5-XL (3B parameters) exceeded that for ProtT5-XXL (11B parameters), presumably, because it saw more samples when continuing pre-training for a similar amount of time (limited by resources).</p><p>This highlighted a remarkable trend common across an immense diversity of protein LMs and corpus: the performance of supervised downstream tasks using the embeddings from pretrained protein LMs as input increased with the number of samples presented during the LM pre-training ( <ref type="figure" target="#fig_5">Fig. 7</ref>; Spearman's ρ=0.62). We could not observe a similarly consistent trend for model size. However, this might be attributed to some trade-off between both: training for more steps might also require a model with sufficient capacity to absorb the information of the corpus. For instance, while ProtBERT-BFD (420M parameters) saw around 27B proteins during pre-training, it fell short compared to ProtT5-XL-BFD (3B parameters) which saw only around 5B proteins (Figs. 5, <ref type="table" target="#tab_9">Table 3</ref>, and SOM <ref type="table" target="#tab_17">Table 8</ref>). This finding appeared to confirm results from NLP suggesting that larger models absorb information faster and need less training time to achieve similar performance <ref type="bibr" target="#b83">[84]</ref>. However, the comparison between, e.g., ProtT5-XL and ProtT5-XXL suggested a possible cap to this trend as larger models see fewer samples in the same amount of computing power. The clear correlation between performance and samples seen during pre-training combined with the need for sufficient model size spotlighted the crucial role of substantial computing resources (HPC, TPUs, and GPUs): big data needs large models need loads of computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Protein LMs learned global constraints</head><p>Some rudimentary information about how proteins are formed, shaped, and function has been learned by the protein LMs during pre-training because all models (ProtT5, ProtBert, Pro-tElectra, ProtAlbert, ProtTXL, ProtXLNet) extracted valuable information as revealed by visualizing embeddings without further supervised training on labeled data. The comparison to a random LMs highlighted two important aspect. Firstly, how easy it is to mis-interpret patterns when projecting from highdimensional spaces upon 2D: although the randomly initialized protein LMs contained no information, some annotations might have suggested the opposite (top row in <ref type="figure" target="#fig_2">Fig. 4</ref> learned NO information). Secondly, the protein LMs did extract important global constraints relevant for protein structure and function (lower row in <ref type="figure" target="#fig_2">Fig. 4</ref>). This span from the most local (individual token level) biophysical features of the amino acid building blocks (e.g. hydrophobicity, charge, and size, <ref type="figure" target="#fig_2">Fig. 4A</ref>), over global classifications of proteins according to structural classes <ref type="figure" target="#fig_2">(Fig. 4B)</ref>, to the macroscopic level of the domains of life <ref type="figure" target="#fig_2">(Fig. 4C</ref>). Global structural properties (e.g. overall secondary structure content, <ref type="figure" target="#fig_2">Fig. 4B</ref>) and global biochemical properties (e.g. membrane-boundness, SOM <ref type="figure" target="#fig_2">Fig. 14B</ref>) appeared most distinctive. In contrast, local features relying on short motifs were less separated (EC-numbers: <ref type="figure" target="#fig_2">Fig. 14F</ref>, localization: <ref type="figure" target="#fig_2">Fig. 14C</ref>) but still clustered, e.g., for secreted (extracellular) proteins or hydrolases.</p><p>On a more fine-grained level, the visual analysis of the attention mechanism at the core of each of the transformer models trained here, confirmed that the protein LMs even picked up more subtle signals of short functional motifs. Specifically, one of the attention heads of ProtAlbert zoomed mostly into the four residues most important for the coordination of zinc-binding (SOM <ref type="figure">Fig. 11</ref>). Although limited in scope <ref type="bibr" target="#b70">[71]</ref>, such an analysis provides some explanation about the inner workings of the Transformer models without needing large sets of experimental annotations (labels). On top, the resulting interpretations of the AI/ML might be less biased than experimental annotations. For instance, databases with annotations of protein function such as Swiss-Prot <ref type="bibr" target="#b84">[85]</ref> and of protein structure such as PDB <ref type="bibr" target="#b49">[50]</ref> are extremely biased by what today's experimental techniques can handle <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Protein LMs top without MSAs</head><p>The t-SNE and UMAP analyses suggested that the protein LMs had extracted some level of understanding of the language of life. However, prediction is the acid test for understanding. To pass this test, we extracted the embeddings learned by the protein LMs directly as input to predict aspects of protein structure (per-residue/token-level prediction of secondary structure) and protein function (per-protein/sentence-level prediction of localization and membrane/other). Overall, the results obtained from the second step of using embeddings from LMs as input to supervised models confirmed <ref type="bibr" target="#b31">[32]</ref> that evolutionary information (EI, i.e. methods using multiple sequence alignments MSAs) scientifically and statistically significantly outperformed most LMs not using such information except for ProtT5-XL (on all perresidue and per-protein tasks, Figs. 5, 8 and SOM <ref type="table" target="#tab_17">Tables 9, 8</ref>). ProtT5-XL eliminated this gap from embeddings-only input: on some tasks/data sets, it outperformed the current state-of-theart MSA-based method, on others it remained slightly behind. Newer protein LMs using context improved over both previous LM-based approaches <ref type="bibr" target="#b31">[32]</ref> (8-9 percentage points in Q3), other transformers <ref type="bibr" target="#b71">[72]</ref> (2-4 percentage points in Q3), and over noncontextualized word2vec-type approaches <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref> (18-22 percentage points in Q3). The performance ranges for using two different data sets (CASP12 and NEW364) highlight a different problem. While it is clear that we need to redundancyreduce evaluations sets with respect to themselves and all data used for development, it is less clear how to exactly do this. In focusing on CASP12 and NEW364, we approached two different assumption. CASP12 is best described as measuring how well predictions will be for proteins with very different structures. A comprehensive rigorous realization of data sets following this perspective has recently been published <ref type="bibr" target="#b90">[91]</ref>. NEW364, on the other hand, builds on the assumption that the maximal redundancy is defined by sequence similar to protein in the PDB. In this sense, we interpreted results for CASP12 as a lower and those for NEW364 as an upper limit. Either way, the most important task is to constantly create up-to-date sets with enough non-redundant proteins never used for development by any of the methods assessed.</p><p>Protein LMs so powerful that even simple baseline are effective: While pre-training is computationally demanding, training supervised models using embeddings as input requires much fewer resources. For instance, the logistic regression trained on top of ProtBERT-BFD was already competitive with substantially more complex CNNs or LSTMs in predicting secondary structure (SOM <ref type="table">Table 7</ref>). In another example, a parameterfree nearest neighbor lookup using distances from protein LM embeddings sufficed to outperform homology based inference for predicting protein function <ref type="bibr" target="#b91">[92]</ref>. This suggested that protein LMs are particularly suitable when the experimental annotations (labels) available are very limited and hinder training of large supervised networks. In fact, none of the supervised solutions presented here that reached the SOA came anywhere near in complexity (number of free parameters) to that of the EI-based methods they reached. Here, we focused on the development of protein LMs and used performance on supervised tasks primarily as a proof of principle without optimizing particular supervised solutions. Others have already begun beating EIbased methods not using protein LMs by custom-designing such solutions <ref type="bibr" target="#b82">[83]</ref> possibly even through end-to-end systems <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Combinations of evolutionary information and embeddings might bring the most accurate methods. However, such a merger would sacrifice the advantage of protein LMs relying only on single protein sequences which is crucial for large scale analysis as well as certain tasks like single amino acid variant (SAV) effect prediction.</p><p>Bi-directionality crucial for protein LMs: In NLP unidirectional (auto-regressive) and bi-directional (auto-encoding) models perform on par <ref type="bibr">[12]</ref>, <ref type="bibr" target="#b92">[93]</ref>. In contrast, the bi-directional context appeared crucial to model aspects of the language of life. While auto-encoding models such as Albert <ref type="bibr" target="#b57">[58]</ref> utilize context to both sides during loss calculation, auto-regressive models such as TransformerXL <ref type="bibr" target="#b58">[59]</ref> consider only context to one side. Performance increased substantially from uni-directional ProtTXL to bi-directional ProtXLNet <ref type="figure" target="#fig_3">(Fig. 5</ref>, <ref type="table" target="#tab_9">Table 3</ref>, and SOM <ref type="table" target="#tab_17">Table 8</ref>). This might be compensated for by first pre-training on sequences and their reverse and then concatenating the output of uni-directional LMs applied on both directions. While this does not allow the LM to use bi-directional context during training, it allows supervised networks to combine context derived independently from both sides. For instance, ELMo <ref type="bibr">[8]</ref> concatenates the embeddings derived from a forward and a backward LSTM. The protein LM version of ELMo (SeqVec) outperformed the uni-directional ProtTXL but not the bi-directional ProtXLNet. The difference in model size (SeqVec=93M vs. ProtXLNet=409M) and in pre-training data (SeqVec=30M vs. ProtAlbert=224M) might explain some of this effect. Nevertheless, pure uni-directionality as used in TransformerXL appeared detrimental for modeling protein sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Have protein LMs reached a ceiling?</head><p>Applying techniques from NLP to proteins opens new opportunities to extract information from proteins in a self-supervised, data-driven way. New protein representations may complement existing solutions, most successful when combining evolutionary information <ref type="bibr">8</ref> and machine learning <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b93">[94]</ref>. Here we showed for the first time that embeddings from protein LMs input to relatively simple supervised learning models can reach similar levels of performance without using EI and without optimizing the supervised training pipeline much. However, the gain in inference speed for protein LMs compared to traditional models using evolutionary information is so significant that large-scale predictions become, for the first time since 30 years, feasible on commodity hardware. For instance, the bestperforming model ProtT5-XL-U50 can run on a Nvidia TitanV with 12GB vRAM (see Methods for details). Nevertheless, given the experiments described here and in previous work <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>, we might expect an upper limit for what protein LMs can learn when using masked language modeling (or auto-regressive pre-training) exclusively. Although this work explicitly addressed the possibility of reaching such a limit, we could not conclusively provide an answer. We could establish three findings. (1) Less noisy and less redundant corpora (e.g. UniRef50) improved over larger but more noisy and redundant corpora (e.g. BFD). <ref type="bibr">(2)</ref> In our perspective of limited resources, it was most important to use the resources for long-enough training because the number of samples seen <ref type="bibr">8</ref>. Throughout this work, we used evolutionary information (EI) as synonymous for using multiple sequence alignments (MSAs). Whether protein LMs do not implicitly extract EI will have to be proven in separate publications. during pre-training correlated with the prediction performance of downstream tasks. Ultimately, this seemed to originate from a trade-off between sufficient model size and sample throughput.</p><p>(3) The bi-directional outperformed the uni-directional models tested. However, given the advances of protein LMs over the course of the reviewing of this work, we have seen no evidence for having reached a limit for protein LMs, yet.</p><p>Many open questions: Answers to the following questions might advance the status-quo. (1) Would the addition of auxiliary tasks such as next-sentence or sentence-order prediction offered by BERT or Albert suit protein sequences? A suggestion might be the usage of structure information <ref type="bibr" target="#b94">[95]</ref> or evolutionary relationship <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b95">[96]</ref>. (2) Might the efficiency of transformers protein LM training improve through sparse transformers <ref type="bibr" target="#b96">[97]</ref> or attention optimized with locality-sensitive hashing (LSH) <ref type="bibr" target="#b97">[98]</ref> as introduced recently by the Reformer model <ref type="bibr" target="#b98">[99]</ref> or more recent work of linear transformers <ref type="bibr" target="#b99">[100]</ref>? (3) Which data set prepossessing, reduction and training batch sampling should optimally used for better results? (4) How much will it improve to tailor the supervised training pipeline to particular tasks? We treated secondary structure or localization prediction more as proxies to showcase the success of protein LMs than as an independent end. (5) Will the combination of EI and AI <ref type="bibr" target="#b95">[96]</ref> bring the best protein predictions of the future, or will the advantages of single-protein predictions (speed, precision) win out? In fact, single-protein predictions also have the advantage of being more precise in that they do not provide some implicit average over a protein family.</p><p>Overall, our results established that the combination of HPC solutions for training protein LMs and subsequent training of supervised prediction methods scaled up to the largest data sets ever used in the field. Only the combination of these different domains allowed us to demonstrate that protein LMs can reach up to the same performance of the state-of-the-art of methods combining EI and AI without ever exploiting multiple sequence alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Here, we introduced many novel protein language models (LMs) and proved that embeddings extracted from the last LM layers captured constraints relevant for protein structure and function. Although neither the usage of the largest ever database for a protein LMs (BFD), nor that of very large models generated the most informative embeddings, pre-training sufficiently long on considerable diversity made a difference, and more recent LMs performed best. Using embeddings as exclusive input to relatively small-size CNN/FNN models without much optimization yielded methods that appeared competitive in predicting secondary structure, localization and in classifying proteins into membrane/other. In fact, for the first time, new small-size supervised solutions based on LMs embedding input reached levels of performance challenging the state-of-the-art (SOA) methods based on multiple sequence alignment (MSA) input. In contrast, the models presented here never used MSAs. This could save immense expenses when routinely applying embedding-based protein predictions to large data sets, but it also opens a path toward protein-specific rather than family-averaged predictions. Ultimately, joining the strengths of three different, yet complementary, fields (HPC, NLP and computational biology) affected the advance. Self-supervised pre-training combined with transfer-learning tapped into the gold-mines of unlabeled data opening the door for completely novel perspectives (and solutions) on existing problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">AVAILABILITY</head><p>We made all protein LMs trained here publicly available at our ProtTrans repository "https://github.com/agemagician/ ProtTrans/". This repository also holds jupyter python notebooks with various tutorials, e.g., on how to extract embeddings or visualize attention using freely available online resources <ref type="bibr">(Google Colab</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY ONLINE MATERIAL (SOM) -1.1 Datasets</head><p>Language model corpora. Here, we show more details on the differences between the different corpora used for protein LM pre-training. Towards this end, we compare the number of sequences, residues as well as the amino acid distribution in UniRef50, UniRef100 and BFD. We also compare the storage size required after converting the protein sequences in each of the databases to tensorflow records (SOM <ref type="figure" target="#fig_8">Fig. 10</ref>). Secondary structure data sets. We also give a detailed overview of the three-and eight-state secondary structure class distribution for the NetSurfP-2.0 <ref type="bibr">[15]</ref> training set (Train), the three test sets used in NetSurfP-2.0 (CASP12 <ref type="bibr" target="#b48">[49]</ref>, TS115 <ref type="bibr" target="#b47">[48]</ref>, CB513 <ref type="bibr" target="#b46">[47]</ref>) as well as our new test set (NEW364) in SOM Tables 6, 5. functional classification we used only the subset of experimentally annotated EC (Enzyme Commission <ref type="bibr">[5]</ref>) numbers removing all proteins that were not mappable. Protein function was also analyzed from an orthogonal angle using a different, 30%PIDE non-redundant, protein set <ref type="bibr">[16]</ref> that allowed to analyze whether or not the embeddings captured aspects of the cellular compartment; dubbed Localization in SOM <ref type="figure" target="#fig_2">Fig14-19</ref> and membraneassociation(dubbed Membrane vs Soluble). Additionally, we visualized the attention mechanism for a single protein to highlight the different ways and scales that Transformers offer to analyze proteins.</p><p>Visual embedding space analysis -Using t-SNE projections, the information content stored within the novel embeddings was qualitatively assessed on various levels, ranging from bio-physical and bio-chemical properties of single amino acids over different aspects of protein function (E.C. numbers, subcellular localization and membrane-boundness) to the level of kingdoms of life, i.e. Eukaryota, Bacteria and Archaea (for completeness here also including Viruses). We have visualized those different protein modalities for a subset of our language models, i.e. for ProtT5-U50 (SOM <ref type="figure" target="#fig_2">Fig. 14D</ref>), ProtBERT-BFD (SOM <ref type="figure" target="#fig_3">Fig. 15D</ref>), ProtBERT (SOM <ref type="figure" target="#fig_4">Fig. 16D</ref>), ProtAlbert (SOM <ref type="figure" target="#fig_5">Fig. 17D</ref>), ProtXLNet (SOM <ref type="figure" target="#fig_6">Fig. 18D</ref>) and ProtTXL (SOM <ref type="figure">Fig.  19D</ref>). When interpreting visual entropy in those figures as a proxy for the information content learnt by the models, we can observe a similar trend than for the supervised comparison, i.e. ProtT5 seems to learn on all levels a better clustering than ProtBERT.</p><p>Attention mechanism visualization -In line with the idea of explainable AI which tries to move away from the blackbox stereotype of neural networks towards understanding why a model made a certain prediction, the analysis of the attention mechanism <ref type="bibr" target="#b69">[70]</ref> that is at the core of each Transformer model <ref type="bibr">[10]</ref> allows, to a certain extent <ref type="bibr" target="#b70">[71]</ref>, to draw first conclusions about the inner workings and the resulting predictions of Transformers. Applied to protein sequences, it was shown that the attention mechanism of Transformers can be used to predict contacts between residue pairs that are close in 3D space but far apart in sequence space <ref type="bibr" target="#b71">[72]</ref>. Here, we visualize the attention weights <ref type="bibr" target="#b72">[73]</ref> of one of the Transformers trained here (ProtAlbert) to analyze the structural motif of a zinc-binding domain (SOM <ref type="figure">Fig. 11</ref>). This structural motif is crucial for DNA and RNA binding across a multitude of organisms. In order to coordinate the overall fold of zinc-fingers, the binding of four specific residues, usually two Cysteines and two Histidines, to a zinc-ion is crucial. Due to their importance for the correct functioning of this protein, these residues are well conserved across most organisms with zinc-finger domains. The visual analysis of the attention triggered by the first 33 residues of an exemplary zinc-finger (PDB: 1A1L <ref type="bibr" target="#b100">[101]</ref>) confirms that one of the attention heads in the fifth layer of ProtAlbert mostly attends to the four residues involved in coordinating the zincbinding which indicates that the model could have learnt to pick up the signal of this, relatively frequently occurring, structural motif. Such an analysis could allow for a cheap and fast analysis of single proteins without a) needing large labeled datasets for supervised training and b) being less influenced by the experimental bias in today's labeled databases which focus mostly on model organisms with applications to biotechnology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-1.3 Supervised Learning</head><p>Supervised architecture comparison. In this section we compared a) different choices for the supervised network that we use to evaluate the secondary structure prediction performance of the language models trained here <ref type="table">(Table 7)</ref>, b) performance of our ProtTrans models on established datasets to simplify comparability to existing prediction methods (SOM <ref type="table" target="#tab_17">Tables 9, 8</ref>) and c) different choices for pooling embeddings of variable length protein sequence embeddings to a fixed-size representation that can be used for classifying whole protein sequences <ref type="table" target="#tab_1">(Table 10)</ref> and</p><p>Supervised architecture comparison. Different architecture choices for the supervised network used to make predictions on the token-level are possible , i.e., various networks can be used to predict secondary structure for each amino acid in a protein.</p><p>Towards this end, we used embeddings derived from ProtBERT-BFD to evaluate one linear classifier (logistic regression) and three non-linear classifiers (FNN, CNN and LSTM) on secondary structure prediction performance in three-and eightstates using the two hardest test sets, i.e., CASP12 and our new test set (SOM <ref type="table">Table 7</ref>). This analysis revealed that even a logistic regression achieves competitive performance, indicating that secondary structure information is readily available from ProtBERT embeddings. Adding non-linearity without considering neighboring token embeddings (FNN) improves prediction performance and allowing the supervised network to harness local neighboring information (CNN) improves results further. However, allowing the supervised network to learn more longrange information (LSTM) only lead to an insignificant improvement for one out of four benchmarks (Q8(CASP12)) while adding more computational complexity due to the sequential nature of LSTMs. This shows that a) Transformer models already capture most of the long-range information removing the necessity to apply LSTMs, b) applying CNNs instead of FNNs improves performance slightly, potentially due to the inductive local bias of CNNs that fits well to the local nature of secondary structure. Therefore, we used a CNN architecture to evaluate the secondary structure prediction performance of all language models trained here.</p><p>Per-residue (token level) prediction of secondary structure. All models were evaluated using standard measures for performance (Q3/Q8: three/eight-state per-residue accuracy, i.e. percentage of residues predicted correctly in either of the 3/8 states) on standard datasets (CASP12, TS115, CB513) and a novel, highly non-redundant test set (NEW364). While TS115 and CB513 might overestimate performance because they allowed for more redundancy, we added Q3 on TS115 and CB513 to SOM <ref type="table" target="#tab_19">Table 9</ref> to ease comparability to existing Model Q3(CASP12) Q3(NEW364) Q8(CASP12) Q8(NEW364) CNN 76.   <ref type="table">7</ref>: Architecture choice -We used one of our language models (ProtBERT-BFD) to compare different choices for the supervised network that we train to make predictions on the level of single amino acids, i.e, we compared a CNN, FNN, LSTM, and a logistic regression on 3-and 8-state secondary structure prediction on two different test sets (CASP12 and our new test set). Overall, the CNN provides the best performance while being computationally more efficient than the LSTM which reaches a similar performance. Despite its lower expressive power, even the logistic regression achieves competitive performance highlighting that the language models introduced here already learnt aspects of secondary structure during pre-training.</p><p>approaches. As Q8 largely confirmed the trend observed for Q3, we added Q8 for all sets to SOM <ref type="table" target="#tab_17">Table 8</ref>.  -A detailed overview of the accuracy for correctly predicting secondary structure in 8-states is given here. We compare the performance of all language models trained here (marked with star), one word2vec-based approach (DeepProtVec), one LSTMbased (DeepSeqVec), one Transformer-based (ESM-1b) and one of the current state-of-the-art approaches that utilizes evolutionary information (NetSurfP-2.0) on three existing validation datasets (CASP12, TS115, CB513) and our new test set (NEW364). Standard errors were computed using bootstrapping: CASP12=±1.9, TS115=±1.0,CB513=±0.6,NEW364=±0.7.</p><p>Additionally, we added a detailed comparison of the 3-state secondary structure prediction performance (Q3) on NEW364 between NetSurfP-2.0 and our best performing protein LM (ProtT5-XL-U50; SOM <ref type="figure" target="#fig_0">Fig. 12</ref>). Each dot in the scatter plot reflects the Q3 achieved by NetSurfP2.0 and ProtT5-XL-U50 for the same protein. While using only single protein sequences, ProtT5-XL-U50 outperforms NetSurfP-2.0 for 57% (208 out of 364) proteins.</p><p>MSA generation for Neff analysis. The number of effective sequences (Neff) was computed using MSAs generated via MMSeqs2 <ref type="bibr" target="#b50">[51]</ref>. Toward this end, we searched all proteins in NEW364 against UniRef50 <ref type="bibr" target="#b40">[41]</ref> using 3 iterations (-num_iterations 3). The UniRef50 hits were expanded by their UniRef100 cluster members (expandaln), effectively searching UniRef100 while reaching approximately the speed of searching UniRef50. The resulting MSA was clustered at 62% PIDE (filterresult -max-seq-id 0.62) to compute Neff.</p><p>Pooling comparison. When classifying a whole protein sequence instead of single tokens (protein-level predictions), <ref type="figure">Fig. 11</ref>: Attention visualization -Here, we show how the inner workings of the Transformer's attention heads can be used to analyze single proteins in more detail. Each attention head performs internally an all-against-all comparison to compute weighted sums for each token over all other tokens in the sequence, i.e., in our case all residues in a protein are compared to each other. High scores indicate that the model learnt to put more weight on certain residue-pairs. Those scores can be indicative of a more fundamental biological truth, e.g. it was shown that some of these scores correlate with residue contacts <ref type="bibr" target="#b71">[72]</ref>. Here, we have used bioviz <ref type="bibr" target="#b72">[73]</ref> to visualize a protein structural motif that is crucial for DNA or RNA binding, i.e., the structure of the first 33 residues of a zinc-finger binding domain (PDB: 1A1L <ref type="bibr" target="#b100">[101]</ref>) is shown on the left. The four residues that coordinate the zinc-binding in order to stabilize the fold are highlighted (C107, C112, H125, H129). On the right, we show a subset of the attention scores of one of our language models (ProtAlbert) for the same sequence. When visualizing the attention weights for C112, we observed that some of the attention heads in the fifth layer of ProtAlbert learnt to detect the Zinc-finger motif that is separate in sequence space but close in structure space. The attention weight is given by line thickness which indicates that the attention of residue C112 mostly attends to the three other residues (C107, H125, H129) involved in zinc-binding coordination. This shows exemplary how pre-trained Transformer models can be used for cheap and fast analysis that might open the door for novel ways of hypothesis generation.</p><p>one option is to pool token-level embeddings to derive a fixedlength protein representation from variable-length proteins. See <ref type="figure">Fig. 1</ref> for an illustration of this process. Here, we compared four parameter-free pooling choices, i.e., min-, max-, mean-pooling (also called global average pooling) as well as a concatenation of those three. Classification accuracy when using those representations as an input for a FNN to differentiate between a) ten different subcellular localizations and b) membrane-bound and water soluble proteins is reported in <ref type="table" target="#tab_1">Table 10</ref>. Min-and max-pooling perform significantly worse than mean-pooling. The concatenation of the three pooling strategies reaches the same performance as mean-pooling for differentiating between membrane-bound and water soluble proteins but falls significantly short on the classification of subcellular localization. One possible explanation for the latter observation is that the magnitudes of the concatenated vectors differed too much (no normaliziation was applied) while the dataset is too small for the network to learn to ignore the less informative inputs. Therefor, we stick to the mean-pooling strategy when comparing the language models trained here on classifying whole protein sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-1.4 Protein LM inference speed</head><p>In this section we compare the effect of sequence length on the time needed to extract features from the differen protein LMs trained here (SOM <ref type="figure" target="#fig_1">Fig. 13</ref>). Additionally, we analyse the cross-effect of sequence length and batch-size (SOM <ref type="table" target="#tab_1">Table  11</ref>). Further, we add technical details to the human proteome benchmark shown in <ref type="figure">Fig. 9</ref>.</p><p>Sequence length effect.  The three-state accuracy (Q3) for the per-residue/tokenlevel secondary structure prediction (percentage of residues/tokens correctly predicted in either of three states: helix, strand, or other) for all protein language models (LMs) trained for this work (marked by star) along with other LMs, namely one word2vec-based approach (DeepProtVec), one LSTM (DeepSeqVec), one transformer (ESM-1b) and one of the current state-of-the-art methods (NetSurfP-2.0) that uses evolutionary information (EI)/multiple sequence alignments (MSAs). Values were compiled for two datasets: one because it is a standard in the field (CASP12, results for two other standard data sets -TS115 and CB513 -in <ref type="table" target="#tab_13">Table 6</ref>), the other because it is larger and less redundant (dubbed NEW364 introduced here). Standard errors were computed using bootstrapping: CASP12=±1.6%, NEW364=±0.5%. Highest values in each column marked in boldface. <ref type="figure" target="#fig_0">Fig. 12</ref>: Detailed Q3 comparison -We compared the 3-state secondary structure prediction performance (Q3) between one state-ofthe-art method using evolutionary information (NetSurfP-2.0) and the best performing protein LM trained here (ProtT5-XL-U50) using our new test set (NEW364, see Methods). Dots resemble the Q3 achieved by each method for the same protein. Dots above the line indicate that the performance of our LM was better while dots below the diagonal show that NetSurfP-2.0 achieved higher performance. In total more than half of the proteins (57% or 208 out of 364) were predicted with higher accuracy by our method while using only single protein sequences. <ref type="table" target="#tab_1">Min  59  86  Max  60  85  Mean  74  89  Concat  64  89   TABLE 10</ref>: Pooling choice -One of our language models (ProtBERT-BFD) was used to compare different choices for pooling variablelength, token-level embeddings of a protein to a single, fixed-size protein-level representation, i.e., we compared the classification accuracy for differentiating 10 subcellular localizations and whether a protein is membrane-bound or water soluble. Global average pooling (Mean) outperforms min-or max-pooling and also the concatenation of all three pooling strategies (Concat) falls significantly short when classifying ten subcellular localizations while reaching the same performance for the classification of membrane-bound proteins.  <ref type="figure" target="#fig_1">Fig. 13</ref>: Inference speed depends on sequence length: The effect of protein sequence length on the inference time of the protein LMs trained here and a previously published LM (SeqVec) were compared using a Nvidia Quadro RTX 8000 with 48GB memory using half precision (batch-size=1). Longer proteins take disproportionate long to embed for all language models. In particular, SeqVec was affected due to the sequential nature of the LSTMs used this LM, followed by ProtT5-XXL due to it is large number of parameters (5.5B for the encoder). In general, Transformer-based models require more inference time for longer for proteins because the attention maps that need to be computed square with sequence length. However, in contrast to LSTMs, the computation of attention can be parallelized, resulting in overall lower inference time for long proteins when using transformer-based LMs. on the inference time of the protein LMs trained here is reported in table 11. The effect of sequence length on different LM architectures (LSTM-based SeqVec and Transformer-based ProtTrans models) was visualized in <ref type="figure" target="#fig_1">figure 13</ref>. The x-axis refers to different sequence length from 128 up to 4096, while the yaxis represents the time of inference in ms for a single protein with a batch size of 1 on a Nvidia Quadro RTX8000 with 48GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling Strategy Localization (Q10) Membrane (Q2)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-1.5 Fast, proteome-wide feature-extraction.</head><p>We compared the time required to generate features between established methods (EI/MSA-based) and the proposed embeddings derived from the protein LMs trained here. Towards this end, we created embeddings for each protein in the human proteome (20,353) proteins with a median sequence length of  415 residues) using a) our protein LMs to generate embeddings and b) the fastest method available, namely MMseqs2 <ref type="bibr" target="#b50">[51]</ref>, to generate MSAs. In light of wanting to compare to the stateof-the-art secondary structure prediction method, NetSurfP-2.0 <ref type="bibr">[15]</ref>, we used the same parameters for the MMseqs2 search used by that method (-num_iterations 2 -diff 2000) and compared two databases (UniRef90 with 113M and UniRef100 with 216M proteins). All comparisons used an IntelR© XeonR© Scalable Processor "Skylake" Gold 6248 with 40 threads, SSD and 377GB main memory, while protein LMs were computed on a single NVIDIA Quadro RTX 8000 with 48GB memory using half precision and dynamic batch size based on variable protein sequence lengths. MMseqs2 was about 16 to 28-times slower than the fastest LMs (ProtElectra and ProtBert), and about 4 to 6-times slower than our best model (ProtT5) <ref type="figure">(Fig. 9)</ref>. The best performing model, ProtT5-XL-U50, required on average 0.12 seconds to create embeddings for a human protein, completing the entire human proteome (all proteins in an organism) in 40 minutes. We noticed that SeqVec was the slowest model for long proteins (11 seconds for protein with 4096 residues) while ProtBert was the fastest (0.32s) for those.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-1.6 Additional Resources</head><p>For long-term storage the repository is also backed up through zenodo <ref type="bibr">9</ref> . Additionally, all models are in the transformer library of huggingface <ref type="bibr">[15]</ref> 10 . Furthermore, secondary structure predictions for ProtBERT-BFD are available through the Predict-Protein <ref type="bibr" target="#b24">[25]</ref> webserver <ref type="bibr">11</ref> . Furthermore, the bio_embeddings <ref type="bibr">[17]</ref> package <ref type="bibr">12</ref> package could be used for simplified accessibility and analysis of protein LMs and predictions of the supervised models.    The high-dimensional embeddings were projected to 2D using t-SNE. ProtT5-XL captured protein information on different levels: ranging from structural features as annotated in the main classes in SCOPe, over functional aspects as defined by in the Enzyme Commission (E.C.) numbers or the cellular compartment to the branch of the protein within the tree of life, without ever having been explicitly trained on any of these features. Comparing different features for the same datasets revealed that potentially heterogeneous clusters are only formed due to the multi-modal nature of proteins, e.g. the eukaryotic proteins are well separated from bacterial proteins (Panel E) but form internally multiple sub-clusters in structure space (Panel D).         <ref type="figure">Fig. 19</ref>: Unsupervised training captures various features of proteins: We used t-SNE projections to assess which features the LMs trained here learnt to extract from proteins. Exemplarily for ProtTXL, we showed that the protein language models trained here captured biophysical-and biochemical properties of single amino acids (Panel A). A redundancy reduced version (30%) of the DeepLoc ( <ref type="bibr">[16]</ref>) dataset was used to assess whether ProtTXL learnt to classify proteins into membrane-bound or water-soluble (Panel B) or according to the cellular compartment they appear in (Panel C). Not all proteins in the set had annotations for both features, making Panels B and C not directly comparable. Further, a redundancy reduced version (40%) of the Structural Classification of Proteins -extended (SCOPe) database was used to assess whether ProtTXL captured structural (Panel D), functional (Panel F) or lineage-specific (Panel E) features of proteins without any labels. Towards this end, contextualized, fixed-size representations were generated for all proteins in both datasets by mean-pooling over the representations extracted from the last layer of ProtTXL (average over the length of the protein). The high-dimensional embeddings were projected to 2D using t-SNE. While generally forming the least dense clusters compared to other LMs trained here, ProtTXL captured certain aspects about protein function (e.g. Transferases, dark green Panel F) that other LMs trained here did not capture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-domain Membrane, cell surface Small proteins</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oxidoreductases Transferases Hydrolases Lyases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Isomerases Ligases Translocases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-domain Membrane, cell surface Small proteins</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oxidoreductases Transferases Hydrolases Lyases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Isomerases Ligases Translocases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-domain Membrane, cell surface Small proteins</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oxidoreductases Transferases Hydrolases Lyases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Isomerases Ligases Translocases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eukaryota Bacteria</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Archaea Viruses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soluble</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-domain Membrane, cell surface Small proteins</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oxidoreductases Transferases Hydrolases Lyases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Isomerases</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Large Scale Dataset Training: The figure shows the overhead of increasing the number of nodes/gpus for both ProtTXL (blue; low) and ProtBert (red; high). The overhead increases slightly from 1 to 2 nodes but remains constant even when scaling up to 936 nodes with a total of 5616 GPUs. Having a low overhead means the model has a near-linear scale-up across thousands of GPUs, upper-bounded by the theoretical scale-up.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Large Scale Deep Learning Training: The figures show the effect of enabling (red bars) or disabling (blue bars) large model support (LMS) on both, model size as well as batch size, when we tested ProtTXL or ProtBert on Nvidia V-100 16GB GPUs. It highlights the difference between applying LMS using PyTorch (ProtTXL) or tensorflow (ProtBert). Panel (a) shows the effect of using LMS on the maximum model size that can fit in a single V-100 memory. Panels (b,c) compare the effect of LMS on the maximum local (b) and global batch size (c) that can fit in the GPU. The number of hours required to finish a single epoch using 936 nodes, each with 6 GPUs when LMS being enabled is shown in (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Protein LMs learned constraints. t-SNE projections visualized information extracted by the unsupervised protein LMs (here bestperforming ProtT5-U50; upper row: before training (Random), and lower row: after pre-training on BFD &amp; UniRef50. (A) The left-most column highlights single amino acids by biophysical features. (B) The middle column annotates protein structural class (taken from SCOPe).(C) The right-most column distinguishes proteins according to the kingdom of life in which it is native. Although the random projections on top may suggest some adequacy of the clusters, the trained models shown on the lower row clearly stood out. Incidentally, the comparison of the two also highlighted the potential pitfalls of using t-SNE projections from many dimensions onto 2D: although random, the human might see some correct patterns even in the top row. Most impressive might be the fine-grained distinctions of biophysical features of amino acids (A), however, more surprising are the classifications of entire proteins according to structural class (B) and organism (C). For these, we created embeddings through global average pooling over the representations extracted from the last layer of ProtT5-U50 (average over protein length, i.e. per-protein embeddings;Fig. 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Per-residue (token-level) performance for secondary structure prediction: CASP12 (red) and NEW364 (blue) constitute two test sets. Protein LMs trained here are shown in the left panel of thefigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6: Effect of MSA size. We used our new test set (NEW364) to analyze the effect of the size of an MSA upon secondary structure prediction (Q3) for the two top methods (both reaching Q3=84.3%): NetSurfP-2.0 (using MSA) and ProtT5-XL-U50 (not using MSA). As proxy for MSA size served Neff, the number of effective sequences [79] (clustered at 62% PIDE): leftmost bars: MSAs with Neff=1, middle: N &amp; ≤ 10, right: Neff&gt;10. As expected ProtT5-XL-U50 tended to reach higher levels than NetSurfP-2.0 for smaller families. Less expected was the almost on par performance for larger families.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Number of samples seen during pre-training correlates with performance -We compared 3-state secondary structure prediction performance (Q3) on NEW364 for all LMs trained here against the number of samples seen during pre-training (training steps times global batch-size in K). For simplicity, we dropped the prefix "Prot" from the models in this plot. Despite the peculiarities of each of the Transformers trained here, Spearman's ρ of 0.62 indicates that there might be a common trend towards seeing more samples during pretraining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Per-protein (sentence-level) performance: The prediction of localization in 10 states (lower bars in cyan: Q10: percentage of proteins with 1 of 10 classes correctly predicted) and the classification of membrane/other (higher bars in magenta: Q2: percentage of proteins correctly classified in either of two classes). Embeddings were derived from protein LMs by mean-pooling, i.e. averaging over the length of the entire protein (Fig. 1). Abbreviations as in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>7 .</head><label>7</label><figDesc>https://github.com/agemagician/ProtTrans/tree/master/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Large Scale Dataset Training: here we compare the three datasets that were used in this study for language modelling (UniRef50, UniRef100, BFD). a) shows the number of sequences in each dataset in millions. (b) shows the number of residues/tokens in each dataset in billions. (c) shows size of each dataset raw text files as well as after converting to tensors in terabytes. (d) shows the frequency of each amino-acid/token in the each dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>F</head><label></label><figDesc>Function: E.C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 :</head><label>14</label><figDesc>Unsupervised training captures various features of proteins: We used t-SNE projections to assess which features the LMs trained here learnt to extract from proteins. Exemplarily for ProtT5-XL-U50, the best-performing model on supervised tasks, we showed that the protein LMs trained here captured biophysical-and biochemical properties of single amino acids during pre-training (Panel A). A redundancy reduced version (30%) of the DeepLoc[16]  dataset was used to assess whether the LM learnt to classify proteins into membrane-bound and water-soluble (Panel B) or according to their cellular compartment (Panel C). Not all proteins in the set had annotations for both features, making Panels B and C not directly comparable. Further, a redundancy reduced version (40%) of the Structural Classification of Proteinsextended (SCOPe) database was used to assess whether ProtT5-XL captured structural (Panel D), functional (Panel F) or lineage-specific (Panel E) features of proteins without any labels. Towards this end, contextualized, fixed-size representations were generated for all proteins in both datasets by mean-pooling over the representations extracted from the last layer of ProtT5-XL (average over the length of the protein).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>F</head><label></label><figDesc>Function: E.C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 :</head><label>15</label><figDesc>Unsupervised training captures various features of proteins: We used t-SNE projections to assess which features the LMs trained here learnt to extract from proteins. Exemplarily for ProtBert-BFD, we showed that the protein LMs trained here captured biophysical-and biochemical properties of single amino acids during pre-training (Panel A). A redundancy reduced version (30%) of the DeepLoc[16]  dataset was used to assess whether the LM learnt to classify proteins into membrane-bound and water-soluble (Panel B) or according to their cellular compartment (Panel C). Not all proteins in the set had annotations for both features, making Panels B and C not directly comparable. Further, a redundancy reduced version (40%) of the Structural Classification of Proteins -extended (SCOPe) database was used to assess whether ProtBert-BFD captured structural (Panel D), functional (Panel F) or lineage-specific (Panel E) features of proteins without any labels. Towards this end, contextualized, fixed-size representations were generated for all proteins in both datasets by mean-pooling over the representations extracted from the last layer of ProtBert-BFD (average over the length of the protein). The high-dimensional embeddings were projected to 2D using t-SNE. ProtBert-BFD captured protein information on different levels: ranging from structural features as annotated in the main classes in SCOPe, over functional aspects as defined by in the Enzyme Commission (E.C.) numbers or the cellular compartment to the branch of the protein within the tree of life, without ever having been explicitly trained on any of these features. Comparing different features for the same datasets revealed that potentially heterogeneous clusters are only formed due to the multi-modal nature of proteins, e.g. the eukaryotic proteins are well separated from bacterial proteins (Panel E) but form internally multiple sub-clusters in structure space (Panel D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>F</head><label></label><figDesc>Function: E.C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 18 :</head><label>18</label><figDesc>Unsupervised training captures various features of proteins: We used t-SNE projections to assess which features the LMs trained here learnt to extract from proteins. Exemplarily for ProtXLNet, we showed that the protein language models trained here captured biophysicaland biochemical properties of single amino acids (Panel A). A redundancy reduced version (30%) of the DeepLoc ([16]) dataset was used to assess whether ProtXLNet learnt to classify proteins into membrane-bound or water-soluble (Panel B) or according to the cellular compartment they appear in (Panel C). Not all proteins in the set had annotations for both features, making Panels B and C not directly comparable. Further, a redundancy reduced version (40%) of the Structural Classification of Proteins -extended (SCOPe) database was used to assess whether ProtXLNet captured structural (Panel D), functional (Panel F) or lineage-specific (Panel E) features of proteins without any labels. Towards this end, contextualized, fixed-size representations were generated for all proteins in both datasets by mean-pooling over the representations extracted from the last layer of ProtXLNet (average over the length of the protein). Compared to other protein LMs trained here, ProtXLNet learnt small coherent clusters that are scattered among the t-SNE projection. Only comparing different features for the same datasets reveals that potentially heterogenous clusters are only formed due to the mulit-modal nature of proteins, e.g. the eukaryotic proteins are well separated from bacterial proteins (Panel E but form multiple sub-clusters in structure space (Panel D). Compared to other protein LMs trained here (e.g. ProtTXL 19, ProtXLNet learnt small coherent clusters that are scattered among the t-SNE projection. Simlar to ProtBert-BFD, some of the small scattered clusters form homogeneous clusters when focusing on other aspects of proteins, e.g. some of the proteins in the heterogeneous cluster in the lower left part showing subcellular localization (Panel C) can be explained by proteins bound to the membrane (red in Panel B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>F</head><label></label><figDesc>Function: E.C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Data Protein LM -UniRef50 and UniRef100 cluster the UniProt database at 50% and 100% pairwise sequence identity (100% implying that duplicates are removed)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table /><note>Large-scale Deep Learning: the table shows the configurations for pre-training the protein LMs introduced here (ProtTXL, ProtBert, ProtXLNet, ProtAlbert, ProtElectra, ProtT5) using either Summit, a TPU Pod v2 or a TPU Pod v3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Hydrophobic (aromatic)</cell><cell>Special cases</cell></row><row><cell></cell><cell cols="3">Hydrophobic (aliphatic)</cell><cell>Small (&lt;130 Dalton)</cell></row><row><cell></cell><cell>Positive</cell><cell></cell><cell>Medium</cell></row><row><cell></cell><cell>Negative</cell><cell></cell><cell>Big (&gt;150 Dalton)</cell></row><row><cell></cell><cell>Polar neutral</cell><cell></cell></row><row><cell></cell><cell>All alpha</cell><cell></cell><cell>Multi-domain</cell></row><row><cell></cell><cell>All beta</cell><cell></cell><cell>Membrane, cell surface</cell></row><row><cell></cell><cell cols="2">Alpha &amp; beta (a|b)</cell><cell>Small proteins</cell></row><row><cell></cell><cell cols="2">Alpha &amp; beta (a+b)</cell></row><row><cell></cell><cell cols="2">Oxidoreductases</cell><cell>Isomerases</cell></row><row><cell></cell><cell>Transferases</cell><cell></cell><cell>Ligases</cell></row><row><cell></cell><cell>Hydrolases</cell><cell></cell><cell>Translocases</cell></row><row><cell></cell><cell>Lyases</cell><cell></cell></row><row><cell></cell><cell>Eukaryota</cell><cell cols="2">Archaea</cell></row><row><cell></cell><cell>Bacteria</cell><cell cols="2">Viruses</cell></row><row><cell>B Structure: SCOPe</cell><cell cols="2">Soluble Membrane-bound</cell><cell>Nucleus Cytoplasm</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mitochondrion</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Extracellular</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Cell membrane</cell></row></table><note>C Lineage: Kingdoms</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>LMs: despite substantial differences in training corpus, hyperparameter choices and transformer model peculiarities, the LMs trained</figDesc><table><row><cell>Dataset</cell><cell cols="2">CASP12 NEW364</cell></row><row><cell>DeepProtVec</cell><cell>62.9</cell><cell>64.7</cell></row><row><cell>ProtTXL*</cell><cell>71.5</cell><cell>72.8</cell></row><row><cell>ProtTXL-BFD*</cell><cell>71.7</cell><cell>72.2</cell></row><row><cell>DeepSeqVec</cell><cell>73.0</cell><cell>76.0</cell></row><row><cell>ProtXLNet*</cell><cell>73.7</cell><cell>77.3</cell></row><row><cell>ProtElectra*</cell><cell>73,9</cell><cell>78,1</cell></row><row><cell>ProtAlbert*</cell><cell>74.6</cell><cell>78.5</cell></row><row><cell>ProtBert*</cell><cell>75.0</cell><cell>80.1</cell></row><row><cell>ProtBert-BFD*</cell><cell>75.8</cell><cell>81.1</cell></row><row><cell>ESM-1b</cell><cell>76.9</cell><cell>82.6</cell></row><row><cell cols="2">ProtT5-XXL-BFD* 77.7</cell><cell>81.6</cell></row><row><cell>ProtT5-XL-BFD*</cell><cell>77.5</cell><cell>82.0</cell></row><row><cell>ProtT5-XXL-U50*</cell><cell>79.2</cell><cell>83.3</cell></row><row><cell>ProtT5-XL-U50*</cell><cell>81.4</cell><cell>84.8</cell></row><row><cell>NetSurfP-2.0</cell><cell>82.0</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 3 :</head><label>3</label><figDesc>MSAs). Values were compiled for two datasets: one because it is a standard in the field (CASP12, results for two other standard data sets -TS115 and CB513 -inTable 9), the other because it is larger and less redundant (dubbed NEW364 introduced here). Standard errors were computed using bootstrapping: CASP12=±1.6%, NEW364=±0.5%. Highest values in each column marked in bold-face.</figDesc><table><row><cell>The three-state accuracy (Q3) for the per-residue/token-</cell></row><row><cell>level secondary structure prediction (percentage of residues correctly</cell></row><row><cell>predicted in either of 3 states: helix, strand, or other) for all protein</cell></row><row><cell>LMs trained here (marked by star) along with other LMs, namely</cell></row><row><cell>one word2vec-based approach (DeepProtVec), one LSTM (DeepSe-</cell></row><row><cell>qVec), one transformer (ESM-1b) and one of the current state-of-</cell></row><row><cell>the-art methods (NetSurfP-2.0) that uses evolutionary information</cell></row><row><cell>(EI)/multiple sequence alignments (</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 4 :</head><label>4</label><figDesc>Per-protein prediction of protein function: Given is the performance for two tasks that proxy the prediction of aspects of protein function, namely the prediction of subcellular localization</figDesc><table /><note>(Localization) in ten states (Q10) and the classification of proteins into membrane-bound/other (Membrane/other) in two states (Q2). Values mark all protein LMs introduced here (marked by star) along with other LMs, namely one word2vec-based approach (Deep- ProtVec), one LSTM-based (DeepSeqVec), one transformer-based (ESM-1b) and the current state-of-the-art method (DeepLoc) that, unlike all other methods shown, used multiple sequence alignments (MSAs)/evolutionary information (EI) for the values shown. All values based on a standard, public data set</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>).Yu Wang studied AI at Katholieke Universiteit Leuven in Belgium. He later moved to Munich, Germany to join MIPS, Helmholtz Zentrum München, where he got his Ph.D. in genomics and bioinformatics from Technical University Munich in 2011. He is currently CTO of Med AI Technology (Wu Xi) Ltd., working on transforming healthcare with AI in China. Jones is a senior software engineer at Google and has been at Google for over 9 years. Started as a YouTube engineer before moving into machine learning. Was on the original team of researchers who developed the now popular Transformer model where he worked on the initial code base and on the attention visualizations. Supercomputing Business Unit. His primary focus areas are AI for Science, the Convergence of Simulation and Experiment, Quantum Computing and Classical Simulation of High Energy Physics. He has over 40 years of experience in large scale simulation and modeling with an emphasis on grand challenge science problems. Feher is an AI developer technology engineer at NVIDIA. His work is focused on accelerating deep learning and machine learning workloads on GPUs. Tamas holds a PhD from the University of Greifswald. Angerer is a senior manager within the Autonomous Driving team at NVIDIA. Christoph's team is concerned with designing, implementing, and optimizing AI-based solutions for advanced learning and automation. Christoph holds a PhD from the ETH Zurich, Switzerland.Burkhard Rost chairsComp Biol &amp; Bioinformatics at TUM Munich. He rooted the leap through combining evolutionary information and machine learning and the launch of PredictProtein as first Internet prediction server. Over 30 years, the Rostlab contributed influential methods for protein prediction, headed the International Society for Computational Biology (ISCB) and has been dedicated to teaching and raising diversity and gender balance.</figDesc><table><row><cell>Ghalia Rehawi is a PhD candidate at Helmholtz Zentrum München. She completed her Master of Science (Msc), in the field of informatics, from the Technical University of Munich. She is inter-ested in the application of machine and deep learning techniques in genome and transcrip-tome analysis. Llion Tom Gibbs manages Developer Relations for NVIDIA's Tamas Christoph Martin Steinegger is an Assistant Professor in the biology department at the Seoul National University. His group develops novel computa-tional methods that combine big data algorithms and machine learning to gain insights into unex-plored microbial communities. Debsindhu Bhowmik is a Computational Sci-entist in the Computational Sciences &amp; Engi-neering Division and Health Data Sciences In-stitute at Oak Ridge National Laboratory. His current focus is in understanding complex bi-ological and genetic phenomena and studying disordered systems by implementing new gen-eration large scale simulation blended with Deep learning and scattering techniques.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 6 :</head><label>6</label><figDesc>Class distribution 3-state secondary structure -An overview of the class distribution for 3-state secondary structure data sets used in this work is given. We compare the original NetSurfP-2.0 training set (Train), the corresponding validation datasets (CASP12, TS115, CB513) and our new test set (NEW364).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 8 :</head><label>8</label><figDesc></figDesc><table /><note>8-state secondary structure prediction performance (Q8)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>The effect of varying sequence lengths (128, 256, 512) and different batch sizes(1, 16,<ref type="bibr" target="#b31">32)</ref> </figDesc><table><row><cell>Dataset</cell><cell>TS115</cell><cell>CB513</cell></row><row><cell>DeepProtVec</cell><cell>66.5</cell><cell>63.7</cell></row><row><cell>ProtTXL*</cell><cell>75.3</cell><cell>73.7</cell></row><row><cell>ProtTXL-BFD*</cell><cell>75.3</cell><cell>73.5</cell></row><row><cell>DeepSeqVec</cell><cell>79.0</cell><cell>77.0</cell></row><row><cell>ProtXLNet*</cell><cell>80.5</cell><cell>77.9</cell></row><row><cell>ProtElectra*</cell><cell>81.2</cell><cell>79.2</cell></row><row><cell>ProtAlbert*</cell><cell>81.9</cell><cell>79.3</cell></row><row><cell>ProtBert*</cell><cell>82.9</cell><cell>80.9</cell></row><row><cell>ProtBert-BFD*</cell><cell>83.8</cell><cell>82.5</cell></row><row><cell>ESM-1b</cell><cell>84.8</cell><cell>83.9</cell></row><row><cell>ProtT5-XXL-BFD*</cell><cell>84.6</cell><cell>83.2</cell></row><row><cell>ProtT5-XL-BFD*</cell><cell>84.8</cell><cell>83.9</cell></row><row><cell>ProtT5-XXL-U50*</cell><cell>85.6</cell><cell>84.6</cell></row><row><cell>ProtT5-XL-U50*</cell><cell>86.9</cell><cell>86.2</cell></row><row><cell>NetSurfP-2.0</cell><cell>85.7</cell><cell>85.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 9 :</head><label>9</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 11 :</head><label>11</label><figDesc>Comparison of inference speed: The analysis distinguished proteins of different length, as well as different batch sizes (numbers of proteins processed: 1, 16 and 32. For simplicity, no proteins longer than 512 is shown . Each test was repeated 100 times and the average time per protein was reported. The experiment was conducted using a single redNvidia Quadro RTX8000 with 48GB memory.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Vacuole Peroxisome B Membrane vs Soluble Positive Negative Polar neutral Medium Big (&gt;150 Dalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome C Localization Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Positive</cell><cell>Medium</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Negative</cell><cell>Big (&gt;150 Dalton)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Polar neutral</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>All alpha</cell><cell>Multi-domain</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>All beta</cell><cell>Membrane, cell surface</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Alpha &amp; beta (a|b)</cell><cell>Small proteins</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Alpha &amp; beta (a+b)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Oxidoreductases</cell><cell>Isomerases</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Transferases</cell><cell>Ligases</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hydrolases</cell><cell>Translocases</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lyases</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Eukaryota</cell><cell>Archaea</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bacteria</cell><cell>Viruses</cell></row><row><cell cols="3">Hydrophobic (aromatic)</cell><cell>Special cases</cell></row><row><cell cols="3">Hydrophobic (aliphatic)</cell><cell>Small (&lt;130 Dalton)</cell><cell>Nucleus</cell><cell>Endoplasmic reticulum</cell></row><row><cell>Positive</cell><cell></cell><cell></cell><cell>Medium</cell><cell>Cytoplasm</cell><cell>Plastid</cell></row><row><cell>Negative</cell><cell></cell><cell></cell><cell>Big (&gt;150 Dalton)</cell><cell>Mitochondrion</cell><cell>Golgi apparatus</cell></row><row><cell>Polar neutral</cell><cell></cell><cell></cell><cell></cell><cell>Extracellular</cell><cell>Lysosome/</cell></row><row><cell cols="4">A Amino acids</cell><cell>Cell membrane</cell></row><row><cell>All alpha</cell><cell></cell><cell cols="2">Multi-domain</cell></row><row><cell>All beta</cell><cell></cell><cell cols="2">Membrane, cell surface</cell></row><row><cell cols="2">Alpha &amp; beta (a|b)</cell><cell cols="2">Small proteins</cell></row><row><cell cols="2">Alpha &amp; beta (a+b)</cell><cell></cell><cell></cell></row><row><cell cols="2">Oxidoreductases</cell><cell cols="2">Isomerases</cell></row><row><cell>Transferases</cell><cell></cell><cell>Ligases</cell><cell></cell></row><row><cell>Hydrolases</cell><cell></cell><cell cols="2">Translocases</cell></row><row><cell>Lyases</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Eukaryota</cell><cell cols="2">Archaea</cell><cell></cell></row><row><cell>Bacteria</cell><cell cols="2">Viruses</cell><cell></cell></row><row><cell cols="2">Soluble Membrane-bound</cell><cell></cell><cell>Nucleus Cytoplasm</cell><cell>Endoplasmic reticulum Plastid</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mitochondrion</cell><cell>Golgi apparatus</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Extracellular</cell><cell>Lysosome/Vacuole</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Cell membrane</cell><cell>Peroxisome</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Soluble Membrane-bound</cell></row></table><note>9. https://zenodo.org/record/4633482 10. https://huggingface.co/Rostlab 11. https://predictprotein.org/ 12. https://github.com/sacdallago/bio_embeddings/</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome D Structure: SCOPe Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome E Lineage: Kingdoms</head><label></label><figDesc></figDesc><table><row><cell>Hydrophobic (aromatic)</cell><cell>Special cases</cell></row><row><cell>Hydrophobic (aliphatic)</cell><cell>Small (&lt;130 Dalton)</cell></row><row><cell>Positive</cell><cell>Medium</cell></row><row><cell>Negative</cell><cell>Big (&gt;150 Dalton)</cell></row><row><cell>Polar neutral</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Dalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea VirusesDalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Virusessurface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Virusessurface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Virusessurface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome A Amino acidssurface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome B Membrane vs Solublesurface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome C Localization</head><label></label><figDesc>Fig. 16: Unsupervised training captures various features of proteins: We used t-SNE projections to assess which features the LMs trained here learnt to extract from proteins. Exemplarily for ProtBert, we showed that the protein language models trained here captured biophysical-and biochemical properties of single amino acids (Panel A). A redundancy reduced version (30%) of the DeepLoc ([16]) dataset was used to assess whether ProtBert learnt to classify proteins into membrane-bound or water-soluble (Panel B) or according to the cellular compartment they appear in (Panel C). Not all proteins in the set had annotations for both features, making Panels B and C not directly comparable. Further, a redundancy reduced version (40%) of the Structural Classification of Proteins -extended (SCOPe) database was used to assess whether ProtBert captured structural (Panel D), functional (Panel F) or lineage-specific (Panel E) features of proteins without any labels. Towards this end, contextualized, fixed-size representations were generated for all proteins in both datasets by mean-pooling over the representations extracted from the last layer of ProtBert (average over the length of the protein). The high-dimensional embeddings were projected to 2D using t-SNE. ProtBert formed less dense clusters compared to the same model trained on a larger dataset (ProtBert-BFDFig. 15).Fig. 17: Unsupervised training captures various features of proteins: We used t-SNE projections to assess which features the LMs trained here learnt to extract from proteins. Exemplarily for ProtAlbert, we showed that the protein language models trained here captured biophysicaland biochemical properties of single amino acids (Panel A). A redundancy reduced version (30%) of the DeepLoc ( [16]) dataset was used to assess whether ProtAlbert learnt to classify proteins into membrane-bound or water-soluble (Panel B) or according to the cellular compartment they appear in (Panel C). Not all proteins in the set had annotations for both features, making Panels B and C not directly comparable. Further, a redundancy reduced version (40%) of the Structural Classification of Proteins -extended (SCOPe) database was used to assess whether ProtAlbert captured structural (Panel D), functional (Panel F) or lineage-specific (Panel E) features of proteins without any labels. Towards this end, contextualized, fixed-size representations were generated for all proteins in both datasets by mean-pooling over the representations extracted from the last layer of ProtAlbert (average over the length of the protein). The high-dimensional embeddings were projected to 2D using t-SNE. Compared to the some of the other protein LMs trained here (ProtTXL 19 and ProtBert 16, ProtAlbert formed more dense clusters, especially for the projections based on the SCOPe dataset (Panels D, E and F).</figDesc><table><row><cell cols="3">Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane A Amino acids Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Soluble Membrane-bound Nucleus D Structure: SCOPe Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane A Amino acids Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Soluble Membrane-bound Nucleus Cytoplasm D Structure: SCOPe Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell Positive Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane B Membrane vs Soluble Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane C Localization Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome Endoplasmic reticulum Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome E Lineage: Kingdoms Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm F Function: E.C. Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome B Membrane vs Soluble Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Medium Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome C Localization Plastid Mitochondrion Endoplasmic reticulum Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Multi-domain Membrane, cell Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome E Lineage: Kingdoms Hydrophobic (aromatic) Hydrophobic (aliphatic) Positive Negative Polar neutral Special cases Small (&lt;130 Dalton) Medium Big (&gt;150 Dalton) All alpha Negative Polar neutral Big (&gt;150 Dalton) All alpha Multi-domain Membrane, cell Positive Negative Polar neutral Medium Big (&gt;150 Dalton) All alpha Multi-domain All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Membrane, cell Hydrophobic (aromatic) Special cases Multi-domain Hydrophobic (aliphatic) Small (&lt;130 Dalton) All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Cytoplasm Membrane-bound Nucleus F Function: E.C. Positive Polar neutral Medium Membrane, cell Soluble Negative Big (&gt;150 Dalton)</cell></row><row><cell></cell><cell></cell><cell>Mitochondrion</cell></row><row><cell>Cytoplasm Mitochondrion Extracellular Mitochondrion Extracellular Cell membrane</cell><cell>Plastid Golgi apparatus Lysosome/Vacuole Golgi apparatus Lysosome/Vacuole Peroxisome</cell><cell>Extracellular Cell membrane Extracellular Cell membrane</cell></row><row><cell>Cell membrane</cell><cell>Peroxisome</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>surface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Virusessurface Small proteins Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases Translocases Eukaryota Bacteria Archaea Viruses Soluble Membrane-bound Nucleus Cytoplasm Mitochondrion Extracellular Cell membrane Endoplasmic reticulum Plastid Golgi apparatus Lysosome/Vacuole Peroxisome B Membrane vs Soluble</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Hydrophobic (aromatic) Hydrophobic (aliphatic)</cell><cell>Special cases Small (&lt;130 Dalton)</cell></row><row><cell></cell><cell></cell><cell>Positive</cell><cell>Medium</cell></row><row><cell></cell><cell></cell><cell>Negative</cell><cell>Big (&gt;150 Dalton)</cell></row><row><cell></cell><cell></cell><cell>Polar neutral</cell></row><row><cell></cell><cell></cell><cell>Multi-domain</cell></row><row><cell></cell><cell></cell><cell>Membrane, cell</cell></row><row><cell></cell><cell></cell><cell>All alpha</cell></row><row><cell></cell><cell></cell><cell>All beta</cell></row><row><cell></cell><cell></cell><cell>Alpha &amp; beta (a|b)</cell></row><row><cell></cell><cell></cell><cell>Alpha &amp; beta (a+b)</cell></row><row><cell cols="2">Hydrophobic (aromatic)</cell><cell>Special cases</cell></row><row><cell cols="2">Hydrophobic (aliphatic)</cell><cell>Small (&lt;130 Dalton)</cell></row><row><cell>Positive</cell><cell></cell><cell>Medium</cell></row><row><cell>Negative</cell><cell></cell><cell>Big (&gt;150 Dalton)</cell></row><row><cell>Polar neutral</cell><cell></cell></row><row><cell cols="3">A Amino acids</cell></row><row><cell>All alpha</cell><cell cols="2">Multi-domain</cell></row><row><cell cols="3">All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b) Membrane, cell Soluble Nucleus Membrane-bound Cytoplasm</cell><cell>Endoplasmic reticulum Plastid</cell></row><row><cell></cell><cell></cell><cell>Mitochondrion</cell><cell>Golgi apparatus</cell></row><row><cell></cell><cell></cell><cell>Extracellular</cell><cell>Lysosome/Vacuole</cell></row><row><cell></cell><cell></cell><cell>Cell membrane</cell><cell>Peroxisome</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank primarily Tim Karl (TUM) and Jian Kong (TUM) for invaluable help with hard-and software; Inga Weise and Aline Schmidt (both TUM) for support with many other</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ahmed Elnaggar is a PhD candidate at the Technical University of Munich. His main focus of research is self-supervised learning on various modalities (Text, Protein, Source code, Images and speech) using high performance computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Michael</head><p>Heinzinger is a PhD candidate in the Rostlab at TUM in Munich/Garching. His recent research focuses on learning, evaluating and understanding representations for protein sequences from unlabeled data with the goal to empower peers with the computational tools necessary to unravel more fundamental biological truths.</p><p>Christian Dallago performs research at the interface of Biology, Machine Learning and Software Engineering with the goal of improving human health through intelligent machines. <ref type="table">CASP12  1989  1416  1400  668  633  215  62  37  TS115  10434  5085  5395  2210  2875  1033  295  174  CB513  25559  17585  17713  8211  9711  3074  1105  469  NEW364  26182  16563  14911  6233  7923  2732  797  15  Train  888175  559370  504272  209840  282561  99799</ref> 26420 14178  <ref type="table">Irregular  CASP12  21  1478  2241  2701  TS115  115  5380  11641  10480  CB513  511  18690  29102  35635  NEW364  363  17360  28929  29067  Train  10796  585790 1002152  996673</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Announcing Supercomputer Summit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06" />
			<pubPlace>Oak Ridge, TN (United States</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
	<note>Oak Ridge National Lab</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">In-Datacenter Performance Analysis of a Tensor Processing Unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture, ser. ISCA &apos;17</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture, ser. ISCA &apos;17<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017-06" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle et al.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NVIDIA cuda software and gpu parallel computing architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Symposium on Memory Management, ser. ISMM &apos;07</title>
		<meeting>the 6th International Symposium on Memory Management, ser. ISMM &apos;07<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2007-10" />
			<biblScope unit="page" from="103" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Horovod: Fast and easy distributed deep learning in TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Del</forename><surname>Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Finkler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02188</idno>
	</analytic>
	<monogr>
		<title level="j">PowerAI DDL</title>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<imprint>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</title>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Studies on the reduction and reformation of protein disulfide bonds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Anfinsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biological Chemistry</title>
		<imprint>
			<biblScope unit="volume">236</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1361" to="1363" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bridging the protein sequence-structure gap by structure predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Biophysics and Biomolecular Structure</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="113" to="136" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Klausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Jespersen</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25674</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/prot.25674" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DeepLoc: Prediction of protein subcellular localization using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Armenteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="3387" to="3395" />
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using predicted interresidue orientations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1496" to="1503" />
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pred-MutHTP: Prediction of disease-causing and neutral mutations in human transmembrane proteins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulandaisamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zaucha</surname></persName>
		</author>
		<idno type="DOI">10.1002/humu.23961</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/humu.23961" />
	</analytic>
	<monogr>
		<title level="j">Human Mutation</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="590" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evolutionary couplings and sequence variation effect predict protein binding sites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25585</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/prot.25585" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TMSEG: Novel prediction of transmembrane helices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kloppmann</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25155</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/prot.25155" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1706" to="1716" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved prediction of protein secondary structure by use of sequence profiles and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="7558" to="7562" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prediction of protein secondary structure at better than 70% accuracy</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">232</biblScope>
			<biblScope unit="page" from="584" to="599" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">UniProt: A worldwide hub of protein knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">U</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="506" to="515" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="603" to="606" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predictprotein-predicting protein structure and function for 29 years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Protein flexibility and intrinsic disorder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Radivojac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Obradovic</surname></persName>
		</author>
		<idno type="DOI">10.1110/ps.03128904</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1110/ps.03128904" />
	</analytic>
	<monogr>
		<title level="j">Protein Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unexpected features of the dark proteome</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Perdigão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page" from="15" to="898" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Three-dimensional structures of membrane proteins from genomic sequencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1607" to="1621" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pitfalls of protein sequence analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valencia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Biotechnology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="457" to="461" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High accuracy protein structure prediction using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richard</surname></persName>
		</author>
		<ptr target="https://predictioncenter.org/casp14/doc/CASP14_Abstracts.pdf$" />
	</analytic>
	<monogr>
		<title level="m">Fourteenth Critical Assessment of Techniques for Protein Structure Prediction</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Abstract Book</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generative Models for Graph-Based Protein Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle et al.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="820" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling aspects of the language of life through transfer-learning protein sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">723</biblScope>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-based deep representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">ProGen: Language Modeling for Protein Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<idno>bioRxiv, p. 2020.03.07.982272</idno>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pre-Training of Deep Bidirectional Protein Sequence Representations with Structural Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05625</idno>
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
	<note>cs, q-bio, stat</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluating Protein Transfer Learning with TAPE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle et al.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9689" to="9701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Language modelling for biological sequences -curated datasets and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J A</forename><surname>Armenteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Johansen</surname></persName>
		</author>
		<idno>bioRxiv, p. 2020.03.09.983585</idno>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-End Differentiable Learning of Protein Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="292" to="301" />
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page">622803</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining evolutionary information and neural networks to predict protein secondary structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Genetics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="55" to="72" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">UniRef clusters: A comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Clustering huge protein sequence sets in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic variablelength segmentation of protein sequences for discriminative motif discovery (dimotif) and sequence embedding (protvecx)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Mchardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mofrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Enhanced protein domain discovery by using language modeling techniques from speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Coin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4516" to="4520" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<title level="m">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</title>
		<imprint>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hydrophobic forces and the length limit of foldable protein domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Zewail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="9851" to="9856" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sixty-five years of the long march in protein secondary structure prediction: The final stretch?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Briefings in bioinformatics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="482" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Evaluation and improvement of multiple sequence methods for protein secondary structure prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Cuff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="508" to="519" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Assessment of hard target modeling in CASP12 reveals an emerging role of alignment-based contact prediction methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Abriata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Tamò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="97" to="112" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Protein Data Bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Westbrook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="242" />
			<date type="published" when="2000-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1026" to="1028" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">PISCES: A protein sequence culling server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Dunbrack</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1589" to="1591" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SCOPe: Classification of large macromolecular structures in the structural classification of proteins-extended database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Chandonia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="475" to="481" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<title level="m">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Transforming the language of life: Transformer neural networks for protein prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Heflin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Press release announcing Supercomputer Fugaku</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Limited</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RIKEN, Tech. Rep</title>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Extreme Scale-out SuperMUC Phase 2 -lessons learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jamitzky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01507</idno>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
	<note>astro-ph, physics:physics</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tpu</forename><surname>Google</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/tpu/docs/system-architecture" />
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title/>
		<ptr target="https://github.com/NVIDIA/apex" />
	</analytic>
	<monogr>
		<title level="j">Nvidia Apex</title>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Imai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02037</idno>
		<title level="m">TFLMS: Large Model Support in TensorFlow by Graph Rewriting</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Optimal Gradient Checkpoint Search for Arbitrary Computation Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00079</idno>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The ENZYME database in 2000</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bairoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="304" to="305" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Upadhyay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11218</idno>
		<title level="m">Attention interpretability across nlp tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Transformer protein language models are unsupervised structure learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<idno type="DOI">https:/www.biorxiv.org/content/10.1101/2020.12.15.422761v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">bioRxiv</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Amino acid substitution matrices from protein blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Henikoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Henikoff</surname></persName>
		</author>
		<idno type="DOI">http:/www.pnas.org/cgi/doi/10.1073/pnas.89.22.10915</idno>
		<ptr target="http://www.pnas.org/cgi/doi/10.1073/pnas.89.22.10915" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="10" to="915" />
			<date type="published" when="1992-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Jpred4: a protein secondary structure prediction server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drozdetskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">W1</biblScope>
			<biblScope unit="page" from="389" to="394" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Raptorx-property: a web server for protein structure property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">W1</biblScope>
			<biblScope unit="page" from="430" to="435" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Protein secondary structure prediction using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18962</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Capturing non-local interactions by long short-term memory bidirectional recurrent neural networks for improving prediction of protein secondary structure, backbone angles, contact numbers and solvent accessibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2842" to="2849" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Protein 3D Structure Computed from Evolutionary Sequence Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">28766</biblScope>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Psi-2: structural genomics to cover protein domain family space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Dessailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structure</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="869" to="881" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Accurate contact predictions using covariation techniques and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kosciolek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">LocTree2 predicts localization for all domains of life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="458" to="465" />
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Light attention predicts protein location from the language of life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stärk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">bioRxiv</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The swiss-prot protein sequence data bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bairoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boeckmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">2247</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Protein flexibility and intrinsic disorder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Radivojac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Obradovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Dark proteins important for cellular function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schafferhans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>O&amp;apos;donoghue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteomics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1800227</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.4546</idno>
		<title level="m">Distributed Representations of Words and Phrases and their Compositionality</title>
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">FastText.zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Proteinnet: a standardized data set for machine learning of protein structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Embeddings from deep learning transfer go annotations beyond homology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Littmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">ZeRO: Memory Optimization Towards Training A Trillion Parameter Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02054</idno>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">PHD: predicting one-dimensional protein structure by profile based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Enzymology</title>
		<imprint>
			<biblScope unit="volume">266</biblScope>
			<biblScope unit="page" from="525" to="539" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08661</idno>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>cs, q-bio, stat</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Msa transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">bioRxiv</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<title level="m">Generating Long Sequences with Sparse Transformers</title>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: Towards removing the curse of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing</title>
		<meeting>the Thirtieth Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Reformer: The Efficient Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">High-resolution structures of variant zif268-dna complexes: implications for understanding zinc finger-dna recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elrod-Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Pabo</surname></persName>
		</author>
		<ptr target="REFERENCES-SOM-" />
	</analytic>
	<monogr>
		<title level="j">Structure</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="464" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Klausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Jespersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Jurtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O A</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marcatili</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25674</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/prot.25674" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Assessment of hard target modeling in CASP12 reveals an emerging role of alignment-based contact prediction methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Abriata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Tamò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Monastyrskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dal Peraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="97" to="112" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Evaluation and improvement of multiple sequence methods for protein secondary structure prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Cuff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="508" to="519" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Sixty-five years of the long march in protein secondary structure prediction: The final stretch?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="482" to="494" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Enzyme Nomenclature 1992. Recommendations of the Nomenclature committee of the International Union of Biochemistry and Molecular Biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Webb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">DeepLoc: Prediction of protein subcellular localization using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Armenteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="3387" to="3395" />
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11218</idno>
		<title level="m">Attention interpretability across nlp tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Transformer protein language models are unsupervised structure learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<idno type="DOI">https:/www.biorxiv.org/content/10.1101/2020.12.15.422761v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1" />
		<imprint/>
	</monogr>
	<note>bioRxiv, 2020. [Online</note>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">High-resolution structures of variant zif268-dna complexes: implications for understanding zinc finger-dna recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elrod-Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Pabo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structure</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="464" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1026" to="1028" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">UniRef clusters: A comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Predictprotein-predicting protein structure and function for 29 years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Satagopam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Littmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Olenyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schuetze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yachdav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Learned embeddings from deep learning to visualize and predict protein sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Olenyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Littmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Protocols in Bioinformatics</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
