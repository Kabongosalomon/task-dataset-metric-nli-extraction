<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
							<email>tim.dettmers@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
							<email>p.minervini@cs.ucl.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
							<email>p.stenetorp@cs.ucl.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<email>s.riedel@cs.ucl.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Università della Svizzera italiana</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer modelswhich potentially limits performance. In this work we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree -which are common in highlyconnected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test sethowever, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets -deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models, and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across most datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Knowledge graphs are graph-structured knowledge bases, where facts are represented in the form of relationships (edges) between entities (nodes). They have important applications in search, analytics, recommendation, and data integration -however, they tend to suffer from incompleteness, that is, missing links in the graph. For example, in Freebase and DBpedia more than 66% of the person entries are missing a birthplace <ref type="bibr" target="#b6">(Dong et al. 2014;</ref><ref type="bibr" target="#b14">Krompaß, Baier, and Tresp 2015)</ref>. Identifying such missing links is referred to as link prediction. Knowledge graphs can contain millions of facts; as a consequence, link predictors should scale in a manageable way with respect to both the number of parameters and computational costs to be applicable in real-world scenarios.</p><p>For solving such scaling problems, link prediction models are often composed of simple operations, like inner products and matrix multiplications over an embedding space, and use a limited number of parameters . DistMult <ref type="bibr" target="#b30">(Yang et al. 2015</ref>) is such a model, characterised by three-way interactions between embedding parameters, which produce one feature per parameter. Using such simple, fast, shallow models allows one to scale to large knowledge graphs, at the cost of learning less expressive features.</p><p>The only way to increase the number of features in shallow models -and thus their expressiveness -is to increase the embedding size. However, doing so does not scale to larger knowledge graphs, since the total number of embedding parameters is proportional to the the number of entities and relations in the graph. For example, a shallow model like DistMult with an embedding size of 200, applied to Freebase, will need 33 GB of memory for its parameters. To increase the number of features independently of the embedding size requires the use of multiple layers of features. However, previous multi-layer knowledge graph embedding architectures, that feature fully connected layers, are prone to overfit . One way to solve the scaling problem of shallow architectures, and the overfitting problem of fully connected deep architectures, is to use parameter efficient, fast operators which can be composed into deep networks.</p><p>The convolution operator, commonly used in computer vision, has exactly these properties: it is parameter efficient and fast to compute, due to highly optimised GPU implementations. Furthermore, due to its ubiquitous use, robust methodologies have been established to control overfitting when training multi-layer convolutional networks <ref type="bibr" target="#b8">Ioffe and Szegedy 2015;</ref><ref type="bibr" target="#b24">Srivastava et al. 2014;</ref><ref type="bibr" target="#b26">Szegedy et al. 2016)</ref>.</p><p>In this paper we introduce ConvE, a model that uses 2D convolutions over embeddings to predict missing links in knowledge graphs. ConvE is the simplest multi-layer convolutional architecture for link prediction: it is defined by a single convolution layer, a projection layer to the embedding dimension, and an inner product layer.  <ref type="bibr" target="#b4">(Collobert et al. 2011)</ref>. However, most work in NLP uses 1D-convolutions, that is convolutions which operate over a temporal sequence of embeddings, for example a sequence of words in embedding space. In this work, we use 2D-convolutions which operate spatially over embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Interactions for 1D vs 2D Convolutions</head><p>Using 2D rather than 1D convolutions increases the expressiveness of our model through additional points of interaction between embeddings. For example, consider the case where we concatenate two rows of 1D embeddings, a and b with dimension n = 3:</p><formula xml:id="formula_0">([a a a] ; [b b b]) = [a a a b b b] .</formula><p>A padded 1D convolution with filter size k = 3 will be able to model the interactions between these two embeddings around the concatenation point (with a number of interactions proportional to k).</p><p>If we concatenate (i.e. stack) two rows of 2D embeddings with dimension m × n, where m = 2 and n = 3, we obtain the following:</p><formula xml:id="formula_1">a a a a a a ; b b b b b b =    a a a a a a b b b b b b    .</formula><p>A padded 2D convolution with filter size 3 × 3 will be able to model the interactions around the entire concatenation line (with a number of interactions proportional to n and k).</p><p>We can extend this principle to an alternating pattern, such as the following:</p><formula xml:id="formula_2">   a a a b b b a a a b b b    .</formula><p>In this case, a 2D convolution operation is able to model even more interactions between a and b (with a number of interactions proportional to m, n, and k). Thus, 2D convolution is able to extract more feature interactions between two embeddings compared to 1D convolution. The same principle can be extending to higher dimensional convolutions, but we leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>A knowledge graph G = {(s, r, o)} ⊆ E × R × E can be formalised as a set of triples (facts), each consisting of a relationship r ∈ R and two entities s, o ∈ E, referred to as the subject and object of the triple. Each triple (s, r, o) denotes a relationship of type r between the entities s and o.</p><p>The link prediction problem can be formalised as a pointwise learning to rank problem, where the objective is learning a scoring function ψ : E × R × E → R. Given an input triple x = (s, r, o), its score ψ(x) ∈ R is proportional to the likelihood that the fact encoded by x is true.  <ref type="table">Table 1</ref>: Scoring functions ψ r (e s , e o ) from neural link predictors in the literature, their relation-dependent parameters and space complexity; n e and n r respectively denote the number of entities and relation types, i.e. n e = |E| and n r = |R|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Link Predictors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Scoring Function ψ r (e s , e o ) Relation Parameters Space Complexity SE (Bordes et al. <ref type="bibr">2014</ref>)</p><formula xml:id="formula_3">W L r e s − W R r e o p W L r , W R r ∈ R k×k O(n e k + n r k 2 ) TransE (Bordes et al. 2013a) e s + r r − e o p r r ∈ R k O(n e k + n r k) DistMult (Yang et al. 2015) e s , r r , e o r r ∈ R k O(n e k + n r k) ComplEx (Trouillon et al. 2016) e s , r r , e o r r ∈ C k O(n e k + n r k) ConvE f (vec(f ([e s ; r r ] * ω))W)e o r r ∈ R k O(n e k + n r k )</formula><p>the scoring component, the two entity embeddings e s and e o are scored by a function ψ r . The score of a triple (s, r, o) is defined as ψ(s, r, o) = ψ r (e s , e o ) ∈ R.</p><p>In <ref type="table">Table 1</ref> we summarise the scoring function of several link prediction models from the literature. The vectors e s and e o denote the subject and object embedding, where e s , e o ∈ C k in ComplEx and e s , e o ∈ R k in all other models, and x, y, z = i x i y i z i denotes the tri-linear dot product; * denotes the convolution operator; f denotes a non-linear function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional 2D Knowledge Graphs Embeddings</head><p>In this work we propose a neural link prediction model where the interactions between input entities and relationships are modelled by convolutional and fully-connected layers. The main characteristic of our model is that the score is defined by a convolution over 2D shaped embeddings. The architecture is summarised in <ref type="figure" target="#fig_1">Figure 1</ref>; formally, the scoring function is defined as follows:</p><formula xml:id="formula_4">ψ r (e s , e o ) = f (vec(f ([e s ; r r ] * ω))W)e o ,<label>(1)</label></formula><p>where r r ∈ R k is a relation parameter depending on r, e s and r r denote a 2D reshaping of e s and r r , respectively: if e s , r r ∈ R k , then e s , r r ∈ R kw×k h , where k = k w k h . In the feed-forward pass, the model performs a row-vector look-up operation on two embedding matrices, one for entities, denoted E |E|×k and one for relations, denoted R |R|×k , where k and k are the entity and relation embedding dimensions, and |E| and |R| denote the number of entities and relations. The model then concatenates e s and r r , and uses it as an input for a 2D convolutional layer with filters ω. Such a layer returns a feature map tensor T ∈ R c×m×n , where c is the number of 2D feature maps with dimensions m and n. The tensor T is then reshaped into a vector vec(T ) ∈ R cmn , which is then projected into a k-dimensional space using a linear transformation parametrised by the matrix W ∈ R cmn×k and matched with the object embedding e o via an inner product. The parameters of the convolutional filters and the matrix W are independent of the parameters for the entities s and o and the relationship r.</p><p>For training the model parameters, we apply the logistic sigmoid function σ(·) to the scores, that is p = σ(ψ r (e s , e o )), and minimise the following binary cross-entropy loss:</p><formula xml:id="formula_5">L(p, t) = − 1 N i (t i · log(p i ) + (1 − t i ) · log(1 − p i )), (2)</formula><p>where t is the label vector with dimension R 1x1 for 1-1 scoring or R 1xN for 1-N scoring (see the next section for 1-N scoring); the elements of vector t are ones for relationships that exists and zero otherwise.</p><p>We use rectified linear units as the non-linearity f for faster training (Krizhevsky, Sutskever, and Hinton 2012), and batch normalisation after each layer to stabilise, regularise and increase rate of convergence (Ioffe and . We regularise our model by using dropout <ref type="bibr" target="#b24">(Srivastava et al. 2014)</ref> in several stages. In particular, we use dropout on the embeddings, on the feature maps after the convolution operation, and on the hidden units after the fully connected layer. We use Adam as optimiser (Kingma and Ba 2014), and label smoothing to lessen overfitting due to saturation of output non-linearities at the labels (Szegedy et al. <ref type="bibr">2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fast Evaluation for Link Prediction Tasks</head><p>In our architecture convolution consumes about 75-90% of the total computation time, thus it is important to minimise the number of convolution operations to speed up computation as much as possible. For link prediction models, the batch size is usually increased to speed up evaluation <ref type="bibr" target="#b1">(Bordes et al. 2013b)</ref>. However, this is not feasible for convolutional models since the memory requirements quickly outgrow the GPU memory capacity when increasing the batch size.</p><p>Unlike other link prediction models which take an entity pair and a relation as a triple (s, r, o), and score it (1-1 scoring), we take one (s, r) pair and score it against all entities o ∈ E simultaneously (1-N scoring). If we benchmark 1-1 scoring on a high-end GPU with batch size and embedding size 128, then a training pass and an evaluation with a convolution model on FB15k -one of the dataset used in the experiments -takes 2.4 minutes and 3.34 hours. Using 1-N scoring, the respective numbers are 45 and 35 seconds -a considerable improvement of over 300x in terms of evaluation time. Additionally, this approach is scalable to large knowledge graphs and increases convergence speed. For a single forward-backward pass with batch size of 128, going from N = 100, 000 to N = 1, 000, 000 entities only increases the computational time from 64ms to 80ms -in other words, a ten-fold increase in the number of entities only increases the computation time by 25% -which attests the scalability of the approach. If instead of 1-N scoring, we use 1-(0.1N) scoring -that is, scoring against 10% of the entities -we can compute a forward-backward pass 25% faster. However, we converge roughly 230% slower on the training set. Thus 1-N scoring has an additional effect which is akin to batch normalisation (Ioffe and Szegedy 2015) -we trade some computational performance for greatly increased convergence speed and also achieve better performance as shown in Section 7. Do note that the technique in general could by applied to any 1-1 scoring model. This practical trick in speeding up training and evaluation can be applied to any 1-1 scoring model, such as the great majority of link prediction models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Knowledge Graph Datasets</head><p>For evaluating our proposed model, we use a selection of link prediction datasets from the literature.</p><p>WN18 <ref type="bibr" target="#b0">(Bordes et al. 2013a</ref>) is a subset of WordNet which consists of 18 relations and 40,943 entities. Most of the 151,442 triples consist of hyponym and hypernym relations and, for such a reason, WN18 tends to follow a strictly hierarchical structure.</p><p>FB15k <ref type="bibr" target="#b0">(Bordes et al. 2013a</ref>) is a subset of Freebase which contains about 14,951 entities with 1,345 different relations. A large fraction of content in this knowledge graph describes facts about movies, actors, awards, sports, and sport teams.</p><p>YAGO3-10 (Mahdisoltani, Biega, and Suchanek 2015) is a subset of YAGO3 which consists of entities which have a minimum of 10 relations each. It has 123,182 entities and 37 relations. Most of the triples deal with descriptive attributes of people, such as citizenship, gender, and profession.</p><p>Countries <ref type="bibr" target="#b3">(Bouchard, Singh, and Trouillon 2015)</ref> is a benchmark dataset that is useful to evaluate a model's ability to learn long-range dependencies between entities and relations. It consists of three sub-tasks which increase in difficulty in a step-wise fashion, where the minimum pathlength to find a solution increases from 2 to 4.</p><p>It was first noted by <ref type="bibr">Toutanova</ref>   <ref type="formula" target="#formula_4">(2015)</ref> introduced FB15k-237 -a subset of FB15k where inverse relations are removed. However, they did not explicitly investigate the severity of this problem, which might explain why research continues using these datasets for evaluation without addressing this issue (e.g. <ref type="bibr">Trouillon</ref>   <ref type="formula" target="#formula_4">2016)</ref>).</p><p>In the following section, we introduce a simple rule-based model which demonstrates the severity of this bias by achieving state-of-the-art results on both WN18 and FB15k. In order to ensure that we evaluate on datasets that do not have inverse relation test leakage, we apply our simple rule-based model to each dataset. Apart from FB15k, which was corrected by FB15k-237, we also find flaws with WN18. We thus create WN18RR to reclaim WN18 as a dataset, which cannot easily be completed using a single rule -but requires modelling of the complete knowledge graph. WN18RR 1 contains 93,003 triples with 40,943 entities and 11 relations. For future research, we recommend against using FB15k and WN18 and instead recommend FB15k-237, WN18RR, and YAGO3-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>We selected the hyperparameters of our ConvE model via grid search according to the mean reciprocal rank (MRR) on the validation set. Hyperparameter ranges for the grid search were as follows -embedding dropout {0.0, 0.1, 0. Besides the grid search, we investigated modifications of the 2D convolution layer for our models. In particular, we experimented with replacing it with fully connected layers and 1D convolution; however, these modifications consistently reduced the predictive accuracy of the model. We also experimented with different filter sizes, and found that we only receive good results if the first convolutional layer uses small (i.e. 3x3) filters. We found that the following combination of parameters works well on WN18, YAGO3-10 and FB15k: embedding dropout 0.2, feature map dropout 0.2, projection layer dropout 0.3, embedding size 200, batch size 128, learning rate 0.001, and label smoothing 0.1. For the Countries dataset, we increase embedding dropout to 0.3, hidden dropout to 0.5, and set label smoothing to 0. We use early stopping according to the mean reciprocal rank (WN18, FB15k, YAGO3-10) and AUC-PR (Countries) statistics on the validation set, which we evaluate every three epochs. Unlike the other datasets, for Countries the results have a high variance, as such we average 10 runs and produce 95% confidence intervals. For our DistMult and ComplEx results with 1-1 training, we use an embedding size of 100, AdaGrad (Duchi, Hazan, and Singer 2011) for optimisation, and we regularise our model by forcing the entity embeddings to have a L2 norm of 1 after each parameter update. As in <ref type="bibr" target="#b0">Bordes et al. (2013a)</ref>, we use a pairwise margin-based ranking loss.</p><p>The code for our model and experiments is made publicly available, 2 as well as the code for replicating the DistMult results. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverse Model</head><p>It has been noted by , that the training datasets of WN18 and FB15k have 94% and 81% test leakage as inverse relations, that is, 94% and 81% of the triples in these datasets have inverse relations which are linked to the test set. For instance, a test triple (feline, hyponym, cat) can easily be mapped to a training triple (cat, hypernym, feline) if it is known that hyponym is the inverse of hypernym. This is highly problematic, because link predictors that do well on these datasets may simply learn which relations that are the inverse of others, rather than to model the actual knowledge graph.</p><p>To gauge the severity of this problem, we construct a simple, rule-based model that solely models inverse relations. We call this model the inverse model. The model extracts inverse relationships automatically from the training set: given two relation pairs r 1 , r 2 ∈ R, we check whether (s, r 1 , o) implies (o, r 2 , s), or vice-versa.</p><p>We assume that inverse relations are randomly distributed among the training, validation and test sets and, as such, we expect the number of inverse relations to be proportional to the size of the training set compared to the total dataset size. Thus, we detect inverse relations if the presence of (s, r 1 , o) co-occurs with the presence of (o, r 2 , s) with a frequency of at least 0.99 − (f v + f t ), where f v and f t is the fraction of the validation and test set compared to the total size of the dataset. Relations matching this criterion are assumed to be the inverse of each other.</p><p>At test time, we check if the test triple has inverse matches outside the test set: if k matches are found, we sample a permutation of the top k ranks for these matches; if no match is found, we select a random rank for the test triple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Similarly to previous work <ref type="bibr" target="#b30">(Yang et al. 2015;</ref><ref type="bibr" target="#b29">Trouillon et al. 2016;</ref><ref type="bibr" target="#b21">Niepert 2016)</ref>, we report results using a filtered setting, i.e. we rank test triples against all other candidate triples not appearing in the training, validation, or test set <ref type="bibr" target="#b0">(Bordes et al. 2013a)</ref>. Candidates are obtained by permuting either the subject or the object of a test triple with all entities in the knowledge graph. Our results on the standard benchmarks FB15k and WN18 are shown in <ref type="table" target="#tab_3">Table 3</ref>; results on the datasets with inverse relations removed are shown in <ref type="table" target="#tab_4">Table 4</ref>; results on YAGO3-10 and Countries are shown in <ref type="table" target="#tab_5">Table 5</ref>.</p><p>Strikingly, the inverse model achieves state-of-the-art on many different metrics for both FB15k and WN18. However, it fails to pick up on inverse relations for YAGO3-10 and FB15k-237. The procedure used by  to derive FB15k-237 does not remove certain symmetric relationships, for example "similar to". The presence of these relationships explains the good score of our inverse model on WN18RR, which was derived using the same procedure.</p><p>Our proposed model, ConvE, achieves state-of-the-art performance for all metrics on YAGO3-10, for some metrics on FB15k, and it does well on WN18. On Countries, it solves the S1 and S2 tasks, and does well on S3, scoring better than other models like DistMult and ComplEx.</p><p>For FB15k-237, we could not replicate the basic model results from Toutanova et al. <ref type="bibr">(2015)</ref>, where the models in general have better performance than what we can achieve. Compared to Schlichtkrull et al. <ref type="bibr">(2017)</ref>, our results for standard models are a slightly better then theirs, and on-a-par with their R-GCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter efficiency of ConvE</head><p>From <ref type="table" target="#tab_2">Table 2</ref> we can see that ConvE for FB15k-237 with 0.23M parameters performs better than DistMult with 1.89M parameters for 3 metrics out of 5.</p><p>ConvE with 0.46M parameters still achieves state-of-theart results on FB15k-237 with 0.425 Hits@10. Comparing to  Overall, ConvE is more than 17x parameter efficient than R-GCNs, and 8x more parameter efficient than DistMult. For the entirety of Freebase, the size of these models would be more than 82GB for R-GCNs, 21GB for DistMult, compared to 5.2GB for ConvE. <ref type="table" target="#tab_7">Table 7</ref> shows the results from our ablation study where we evaluate different parameter initialisation (n = 2) to calculate confidence intervals. We see that hidden dropout is by far the most important component, which is unsurprising since it is our main regularisation technique. 1-N scoring improves performance, as does input dropout, feature map dropout has a minor effect, while label smoothing seems to be unimportant -as good results can be achieved without it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Indegree and PageRank</head><p>Our main hypothesis for the good performance of our model on datasets like YAGO3-10 and FB15k-237 compared to WN18RR, is that these datasets contain nodes with very high relation-specific indegree. For example the node "United States" with edges "was born in" has an indegree of over 10,000. Many of these 10,000 nodes will be very different from each other (actors, writers, academics, politicians, business people) and successful modelling of such a high indegree nodes requires capturing all these differences. Our hypothesis is that deeper models, that is, models that learn multiple layers of features, like ConvE, have an advantage over shallow models, like DistMult, to capture all these constraints.</p><p>However, deeper models are more difficult to optimise, so we hypothesise that for datasets with low average relationspecific indegree (like WN18RR and WN18), a shallow model like DistMult might suffice for accurately representing the structure of the network.</p><p>To test our two hypotheses, we take two datasets with low (low-WN18) and high (high-FB15k) relation-specific indegree and reverse them into high (high-WN18) and low (low-FB15k) relation-specific indegree datasets by deleting low and high indegree nodes. We hypothesise that, compared to DistMult, ConvE will always do better on the dataset with high relation-specific indegree, and vice-versa.</p><p>Indeed, we find that both hypotheses hold: for low-FB15k we have ConvE 0.586 Hits@10 vs DistMult 0.728 Hits@10; for high-WN18 we have ConvE 0.952 Hits@10 vs DistMult 0.938 Hits@10. This supports our hypothesis that deeper models such as ConvE have an advantage to model more complex graphs (e.g. FB15k and FB15k-237), but that shallow models such as DistMult have an advantage to model less complex graphs (e.g. WN18 WN18RR).</p><p>To investigate this further, we look at PageRank, a measure of centrality of a node. PageRank can also be seen as a measure of the recursive indegree of a node: the PageRank value of a node is proportional to the indegree of this node, its   To test this hypothesis, we calculate the PageRank for each dataset as a measure of centrality. We find that the most central nodes in WN18 have a PageRank value more than one order of magnitude smaller than the most central nodes in YAGO3-10 and Countries, and about 4 times smaller than the most central nodes in FB15k. When we look at the mean PageRank of nodes contained in the test sets, we find that the difference of performance in terms of Hits@10 between DistMult and ConvE is roughly proportional to the mean test set PageRank, that is, the higher the mean PageRank of the test set nodes the better ConvE does compared to DistMult, and vice-versa. See <ref type="table" target="#tab_6">Table 6</ref> for these statistics. The correlation between mean test set PageRank and relative error reduction of ConvE compared to DistMult is strong with r = 0.56. This gives additional evidence that models that are deeper have an advantage when modelling nodes with high (recursive) indegree.</p><p>From this evidence we conclude, that the increased performance of our model compared to a standard link predictor, DistMult, can be partially explained due to our it's ability to model nodes with high indegree with greater precisionwhich is possibly related to its depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>We introduced ConvE, a link prediction model that uses 2D convolution over embeddings and multiple layers of nonlinear features to model knowledge graphs. ConvE uses fewer parameters; it is fast through 1-N scoring; it is expressive through multiple layers of non-linear features; it is robust to overfitting due to batch normalisation and dropout; and achieves state-of-the-art results on several datasets, while still scaling to large knowledge graphs. In our analysis, we show that the performance of ConvE compared to a common link predictor, DistMult, can partially be explained by its ability to model nodes with high (recursive) indegree.</p><p>Test leakage through inverse relations of WN18 and FB15k was first reported by : we investigate the severity of this problem for commonly used datasets by introducing a simple rule-based model, and find that it can achieve state-of-the-art results on WN18 and FB15k. To ensure robust versions of all investigated datasets exists, we derive WN18RR.</p><p>Our model is still shallow compared to convolutional architecture found in computer vision, and future work might deal with convolutional models of increasing depth. Further work might also look at the interpretation of 2D convolution, or how to enforce large-scale structure in embedding space so to increase the number of interactions between embeddings. test triple x i in T , we generate all its possible corruptions C s (x i ) (resp. C o (x i )) -obtained by replacing its subject (resp. object) with any other entity in the Knowledge Graph -to check whether the model assigns an higher score to x i and a lower score to its corruptions. Note that the set of corruptions can also contain several true triples, and it is not a mistake to rank them with an higher score than x i . For such a reason, we remove all triples in the graph from the set of corruptions: this is referred to as the filtered setting in Bordes et al. <ref type="bibr">(2013a)</ref>. The left and right rank of the i-th test tripleeach associated to corrupting either the subject or the objectaccording to a model with scoring function ψ( · ), are defined as follows:</p><formula xml:id="formula_6">rank s i =1 + xi∈C s (xi)\G I [ψ(x i ) &lt; ψ(x i )] , rank o i =1 + xi∈C o (xi)\G I [ψ(x i ) &lt; ψ(x i )] ,</formula><p>where I [P ] is 1 iff the condition P is true, and 0 otherwise. For measuring the quality of the ranking, we use the Mean Reciprocal Rank (MRR) and the Hits@k metrics, which are defined as follows: MRR is the average inverse rank for all test triples: the higher, the better. Hits@k is the percentage of ranks lower than or equal to k: the higher, the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRR</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Neural link prediction models (Nickel et al. 2016) can be seen as multi-layer neural networks consisting of an encoding component and a scoring component. Given an input triple (s, r, o), the encoding component maps entities s, o ∈ E to their distributed embedding representations e s , e o ∈ R k . In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>In the ConvE model, the entity and relation embeddings are first reshaped and concatenated (steps 1, 2); the resulting matrix is then used as input to a convolutional layer (step 3); the resulting feature map tensor is vectorised and projected into a k-dimensional space (step 4) and matched with all candidate object embeddings (step 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>et al. (2016), Nickel, Rosasco, and Poggio (2016), Nguyen et al. (2016), Liu et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2}, feature map dropout {0.0, 0.1, 0.2, 0.3}, projection layer dropout {0.0, 0.1, 0.3, 0.5}, embedding size {100, 200}, batch size {64, 128, 256}, learning rate {0.001, 0.003}, and label smoothing {0.0, 0.1, 0.2, 0.3}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>I</head><label></label><figDesc>[rank s i ≤ k] + I [rank o i ≤ k].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1707.01476v6 [cs.LG] 4 Jul 2018Specifically, our contributions are as follows:• Introducing a simple, competitive 2D convolutional link prediction model, ConvE. • Developing a 1-N scoring procedure that speeds up training three-fold and evaluation by 300x.</figDesc><table><row><cell>• Establishing that our model is highly parameter efficient,</cell></row><row><cell>achieving better scores than DistMult and R-GCNs on</cell></row><row><cell>FB15k-237 with 8x and 17x fewer parameters.</cell></row><row><cell>• Showing that for increasingly complex knowledge graphs,</cell></row><row><cell>as measured by indegree and PageRank, the difference</cell></row><row><cell>in performance between our model and a shallow model</cell></row><row><cell>increases proportionally to the complexity of the graph.</cell></row><row><cell>• Systematically investigating reported inverse relations test</cell></row><row><cell>set leakage across commonly used link prediction datasets,</cell></row><row><cell>introducing robust versions of datasets where necessary, so</cell></row><row><cell>that they cannot be solved using simple rule-based models.</cell></row><row><cell>the first</cell></row><row><cell>neural link prediction model to use 2D convolutional lay-</cell></row><row><cell>ers. Graph Convolutional Networks (GCNs) (Duvenaud</cell></row><row><cell>et al. 2015; Defferrard, Bresson, and Vandergheynst 2016;</cell></row><row><cell>Kipf and Welling 2016) are a related line of research, where</cell></row><row><cell>the convolution operator is generalised to use locality infor-</cell></row><row><cell>mation in graphs. However, the GCN framework is limited</cell></row><row><cell>to undirected graphs, while knowledge graphs are naturally</cell></row><row><cell>directed, and suffers from potentially prohibitive memory</cell></row><row><cell>requirements (Kipf and Welling 2016). Relational GCNs</cell></row><row><cell>(R-GCNs) (Schlichtkrull et al. 2017) are a generalisation</cell></row><row><cell>of GCNs developed for dealing with highly multi-relational</cell></row><row><cell>data such as knowledge graphs -we include them in our</cell></row><row><cell>experimental evaluations.</cell></row><row><cell>Several convolutional models have been proposed in natu-</cell></row><row><cell>ral language processing (NLP) for solving a variety of tasks,</cell></row><row><cell>including semantic parsing (Yih et al. 2011), sentence classi-</cell></row><row><cell>fication (Kim 2014), search query retrieval (Shen et al. 2014),</cell></row><row><cell>sentence modelling (Kalchbrenner, Grefenstette, and Blun-</cell></row><row><cell>som 2014), as well as other NLP tasks</cell></row></table><note>• Evaluating ConvE and several previously proposed models on these robust datasets: our model achieves state-of-the- art Mean Reciprocal Rank across most of them.Related Work Several neural link prediction models have been proposed in the literature, such as the Translating Embeddings model (TransE) (Bordes et al. 2013a), the Bilinear Diagonal model (DistMult) (Yang et al. 2015) and its extension in the complex space (ComplEx) (Trouillon et al. 2016); we refer to Nickel et al. (2016) for a recent survey. The model that is most closely related to this work is most likely the Holographic Embeddings model (HolE) (Nickel, Rosasco, and Poggio 2016), which uses cross-correlation -the inverse of circular convolution -for matching entity embeddings; it is inspired by holographic models of associative memory. However, HolE does not learn multiple layers of non-linear features, and it is thus theoretically less expressive than our model. To the best of our knowledge, our model is</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and Chen (2015) that WN18 and FB15k suffer from test leakage through inverse relations: a large number of test triples can be obtained simply by inverting triples in the training set. For example, the test set frequently contains triples such as (s, hyponym, o) while the training set contains its inverse (o, hypernym, s). To create a dataset without this property, Toutanova and Chen</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Parameter scaling of DistMult vs ConvE.</figDesc><table><row><cell></cell><cell cols="2">Param. Emb.</cell><cell></cell><cell></cell><cell>Hits</cell></row><row><cell>Model</cell><cell>count</cell><cell cols="4">size MRR @10 @3 @1</cell></row><row><cell cols="2">DistMult 1.89M</cell><cell>128</cell><cell>.23</cell><cell>.41</cell><cell>.25 .15</cell></row><row><cell cols="2">DistMult 0.95M</cell><cell>64</cell><cell>.22</cell><cell>.39</cell><cell>.25 .14</cell></row><row><cell cols="2">DistMult 0.23M</cell><cell>16</cell><cell>.16</cell><cell>.31</cell><cell>.17 .09</cell></row><row><cell>ConvE</cell><cell>5.05M</cell><cell>200</cell><cell>.32</cell><cell>.49</cell><cell>.35 .23</cell></row><row><cell>ConvE</cell><cell>1.89M</cell><cell>96</cell><cell>.32</cell><cell>.49</cell><cell>.35 .23</cell></row><row><cell>ConvE</cell><cell>0.95M</cell><cell>54</cell><cell>.30</cell><cell>.46</cell><cell>.33 .22</cell></row><row><cell>ConvE</cell><cell>0.46M</cell><cell>28</cell><cell>.28</cell><cell>.43</cell><cell>.30 .20</cell></row><row><cell>ConvE</cell><cell>0.23M</cell><cell>14</cell><cell>.26</cell><cell>.40</cell><cell>.28 .19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Link prediction results for WN18 and FB15k</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>WN18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FB15k</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hits</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hits</cell><cell></cell></row><row><cell></cell><cell cols="4">MR MRR @10 @3</cell><cell>@1</cell><cell cols="4">MR MRR @10 @3</cell><cell>@1</cell></row><row><cell>DistMult (Yang et al. 2015)</cell><cell cols="2">902 .822</cell><cell cols="3">.936 .914 .728</cell><cell>97</cell><cell>.654</cell><cell cols="3">.824 .733 .546</cell></row><row><cell>ComplEx (Trouillon et al. 2016)</cell><cell>-</cell><cell>.941</cell><cell cols="3">.947 .936 .936</cell><cell>-</cell><cell>.692</cell><cell cols="3">.840 .759 .599</cell></row><row><cell>Gaifman (Niepert 2016)</cell><cell>352</cell><cell>-</cell><cell>.939</cell><cell>-</cell><cell>.761</cell><cell>75</cell><cell>-</cell><cell>.842</cell><cell>-</cell><cell>.692</cell></row><row><cell>ANALOGY (Liu, Wu, and Yang 2017)</cell><cell>-</cell><cell>.942</cell><cell cols="3">.947 .944 .939</cell><cell>-</cell><cell>.725</cell><cell cols="3">.854 .785 .646</cell></row><row><cell>R-GCN (Schlichtkrull et al. 2017)</cell><cell>-</cell><cell>.814</cell><cell cols="3">.964 .929 .697</cell><cell>-</cell><cell>.696</cell><cell cols="3">.842 .760 .601</cell></row><row><cell>ConvE</cell><cell cols="2">374 .943</cell><cell cols="3">.956 .946 .935</cell><cell>51</cell><cell>.657</cell><cell cols="3">.831 .723 .558</cell></row><row><cell>Inverse Model</cell><cell cols="2">740 .963</cell><cell cols="3">.964 .964 .953</cell><cell cols="2">2501 .660</cell><cell cols="3">.660 .659 .658</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="7">: Link prediction results for WN18RR and FB15k-237</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell><cell></cell><cell></cell><cell cols="2">FB15k-237</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hits</cell><cell></cell><cell></cell><cell></cell><cell>Hits</cell></row><row><cell></cell><cell>MR</cell><cell cols="4">MRR @10 @3 @1</cell><cell cols="3">MR MRR @10 @3</cell><cell>@1</cell></row><row><cell>DistMult (Yang et al. 2015)</cell><cell>5110</cell><cell>.43</cell><cell>.49</cell><cell cols="2">.44 .39</cell><cell>254</cell><cell>.241</cell><cell cols="2">.419 .263 .155</cell></row><row><cell>ComplEx (Trouillon et al. 2016)</cell><cell>5261</cell><cell>.44</cell><cell>.51</cell><cell cols="2">.46 .41</cell><cell>339</cell><cell>.247</cell><cell cols="2">.428 .275 .158</cell></row><row><cell>R-GCN (Schlichtkrull et al. 2017)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>.248</cell><cell cols="2">.417 .258 .153</cell></row><row><cell>ConvE</cell><cell>4187</cell><cell>.43</cell><cell>.52</cell><cell cols="2">.44 .40</cell><cell>244</cell><cell>.325</cell><cell cols="2">.501 .356 .237</cell></row><row><cell>Inverse Model</cell><cell>13526</cell><cell>.35</cell><cell>.35</cell><cell cols="2">.35 .35</cell><cell cols="2">7030 .010</cell><cell cols="2">.014 .011 .007</cell></row><row><cell cols="4">the previous best model, R-GCN (Schlichtkrull et al. 2017),</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">which achieves 0.417 Hits@10 with more than 8M parame-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ters.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Link prediction results for YAGO3-10 and Countries</figDesc><table><row><cell></cell><cell></cell><cell cols="2">YAGO3-10</cell><cell></cell><cell></cell><cell>Countries</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hits</cell><cell></cell><cell>AUC-PR</cell><cell></cell></row><row><cell></cell><cell>MR</cell><cell cols="3">MRR @10 @3 @1</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell></row><row><cell>DistMult (Yang et al. 2015)</cell><cell>5926</cell><cell>.34</cell><cell>.54</cell><cell>.38 .24</cell><cell cols="3">1.00±0.00 0.72±0.12 0.52±0.07</cell></row><row><cell cols="2">ComplEx (Trouillon et al. 2016) 6351</cell><cell>.36</cell><cell>.55</cell><cell>.40 .26</cell><cell cols="3">0.97±0.02 0.57±0.10 0.43±0.07</cell></row><row><cell>ConvE</cell><cell>1676</cell><cell>.44</cell><cell>.62</cell><cell>.49 .35</cell><cell cols="3">1.00±0.00 0.99±0.01 0.86 ±0.05</cell></row><row><cell>Inverse Model</cell><cell>59448</cell><cell>.01</cell><cell>.02</cell><cell>.02 .01</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Mean PageRank ×10 −3 of nodes in the test set vs reduction in error in terms of AUC-PR or Hits@10 of ConvE wrt. DistMult.</figDesc><table><row><cell>Dataset</cell><cell cols="2">PageRank Error Reduction</cell></row><row><cell>WN18RR</cell><cell>0.104</cell><cell>0.06</cell></row><row><cell>WN18</cell><cell>0.125</cell><cell>0.45</cell></row><row><cell>FB15k</cell><cell>0.599</cell><cell>0.04</cell></row><row><cell>FB15-237</cell><cell>0.733</cell><cell>0.16</cell></row><row><cell>YAGO3-10</cell><cell>0.988</cell><cell>0.21</cell></row><row><cell>Countries S3</cell><cell>1.415</cell><cell>2.36</cell></row><row><cell>Countries S1</cell><cell>1.711</cell><cell>0.00</cell></row><row><cell>Countries S2</cell><cell>1.796</cell><cell>17.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation study for FB15k-237.</figDesc><table><row><cell>Ablation</cell><cell>Hits@10</cell></row><row><cell>Full ConvE</cell><cell>0.491</cell></row><row><cell>Hidden dropout</cell><cell>-0.044 ± 0.003</cell></row><row><cell>Input dropout</cell><cell>-0.022 ± 0.000</cell></row><row><cell>1-N scoring</cell><cell>-0.019</cell></row><row><cell cols="2">Feature map dropout -0.013 ± 0.001</cell></row><row><cell>Label smoothing</cell><cell>-0.008 ± 0.000</cell></row><row><cell cols="2">neighbours indegrees, its neighbours-neighbours indegrees</cell></row><row><cell cols="2">and so forth scaled relative to all other nodes in the network.</cell></row><row><cell cols="2">By this line of reasoning, we also expect ConvE to be better</cell></row><row><cell cols="2">than DistMult on datasets with high average PageRank (high</cell></row><row><cell cols="2">connectivity graphs), and vice-versa.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>ConvE link prediction results for UMLS, Nations, and Kinship. Following Bordes et al. (2013a), for the i-th</figDesc><table><row><cell>Hits</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/TimDettmers/ConvE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/TimDettmers/ConvE 3 https://github.com/uclmr/inferbeddings</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Johannes Welbl, Peter Hayes, and Takuma Ebisu for their feedback and helpful discussions related to this work. We thank Takuma Ebisu for pointing out an error in our Inverse Model script -the corrected results are slightly better for WN18 and slightly worse for FB15k. We thank Victoria Lin for helping us to unroot and fix a bug where the exclusion of triples during inference worked incorrectly -the changes did not affect the main results in this work, though some results in the appendix changed (UMLS, Nations). This work was supported by a Marie Curie Career Integration Award, an Allen Distinguished Investigator Award, a Google Europe Scholarship for Students with Disabilities, and the H2020 project SUMMA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data -application to word-sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="259" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On approximate reasoning capabilities of low-rank vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural Language Processing (Almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD 2014</title>
		<meeting>KDD 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2015</title>
		<meeting>NIPS 2015</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
	<note>Convolutional Networks on Graphs for Learning Molecular Fingerprints</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Convolutional Neural Network for Modelling Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2014</title>
		<meeting>ACL 2014</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2012</title>
		<meeting>NIPS 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Type-Constrained Representation Learning in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krompaß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISWC 2015</title>
		<meeting>ISWC 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical random walk inference in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="445" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Analogical Inference for Multi-Relational Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">YAGO3: A Knowledge Base from Multilingual Wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIDR</title>
		<meeting>CIDR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08140</idno>
		<title level="m">Stranse: a novel embedding model of entities and relationships in knowledge bases</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Holographic Embeddings of Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative Gaifman Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2016</title>
		<meeting>NIPS 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3405" to="3413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
		<title level="m">Modeling Relational Data with Graph Convolutional Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Semantic Representations Using Convolutional Neural Networks for Web Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW 2014</title>
		<meeting>WWW 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Observed Versus Latent Features for Knowledge Base and Text Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2015</title>
		<meeting>EMNLP 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2016</title>
		<meeting>ICML 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Discriminative Projections for Text Similarity Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SUPPLEMENTAL MATERIAL Versions • 2018-07-04: -Added new YAGO3-10 results. The new results are worse on most metrics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
	<note>Proceedings of CoNLL. but state-of-the-art results are retained</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">-I was unable to replicate FB15k scores that I initially reported</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">-I update the PageRank table and the reported PageRankerror-reduction correlation to reflect the new scores. -I removed the Nations scores in the appendix. The Nations dataset has a high proportion of inverse relationships and is thus not suitable for the use in research. I do not want to encourage its use</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<idno>2017-07-08</idno>
		<title level="m">Victoria Lin helped us to find and fix issues 5 with triple masks during evaluation. We report new numbers for UMLS and Nations in the appendix. The results were unchanged on other datasets that we tested thus far (Kinship, WN18, WN18RR</title>
		<imprint>
			<biblScope unit="page" from="15" to="237" />
		</imprint>
	</monogr>
	<note>Extended AAAI camera ready. Missing grant in acknowledgements. • 2017-07-05: Original NIPS submission (6/7/6)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Further ConvE results Evaluation Metrics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">We now describe the evaluation metrics used for assessing the quality of the models</title>
		<ptr target="https://github.com/TimDettmers/ConvE/issues/26fordetails" />
	</analytic>
	<monogr>
		<title level="m">x |T | } denote 4 See</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
