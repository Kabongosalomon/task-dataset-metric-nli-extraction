<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-Latency Video Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yule</forename><surname>Li</surname></persName>
							<email>yule.li@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<email>shijianping@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<email>dhlin@ie.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Low-Latency Video Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years have seen remarkable progress in semantic segmentation. Yet, it remains a challenging task to apply segmentation techniques to video-based applications. Specifically, the high throughput of video streams, the sheer cost of running fully convolutional networks, together with the low-latency requirements in many real-world applications, e.g. autonomous driving, present a significant challenge to the design of the video segmentation framework. To tackle this combined challenge, we develop a framework for video semantic segmentation, which incorporates two novel components: (1) a feature propagation module that adaptively fuses features over time via spatially variant convolution, thus reducing the cost of per-frame computation; and (2) an adaptive scheduler that dynamically allocate computation based on accuracy prediction. Both components work together to ensure low latency while maintaining high segmentation quality. On both Cityscapes and CamVid, the proposed framework obtained competitive performance compared to the state of the art, while substantially reducing the latency, from 360 ms to 119 ms. * This work is done when Yule Li is intern at CUHK Multimedia Lab 85 80 75 70 65 60 55 0 100 200 300 400 500 600 700 PSPNet Ours Clockwork PEARL DFF Deeplab NetWarp Latency(ms) Accuracy(mIoU%)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation, a task to divide observed scenes into semantic regions, has been an active research topic in computer vision. In recent years, the advances in deep learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12]</ref> and in particular the development of Fully Convolutional Network (FCN) <ref type="bibr" target="#b18">[19]</ref> have brought the performance of this task to a new level. Yet, many existing methods for semantic segmentation were devised for parsing images <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>. How to extend the success of segmentation techniques to video-based applications (e.g. robotics, autonomous driving, and surveillance) remains a challenging question.</p><p>The challenges of video-based semantic segmentation <ref type="bibr">Figure 1</ref>. Latency and mIoU performance on Cityscapes <ref type="bibr" target="#b6">[7]</ref> dataset. Methods involved are NetWarp <ref type="bibr" target="#b9">[10]</ref>, PSPNet <ref type="bibr" target="#b28">[29]</ref>, Deeplab <ref type="bibr" target="#b5">[6]</ref>, PEARL <ref type="bibr" target="#b13">[14]</ref>, DFF <ref type="bibr" target="#b30">[31]</ref>, Clockework <ref type="bibr" target="#b24">[25]</ref> and Ours. Our method achieves the lowest latency while maintaining competitive performance.</p><p>consist in two aspects. On one hand, videos usually involve significantly larger volume of data compared to images. Particularly, a video typically contains 15 to 30 frames per second. Hence, analyzing videos requires much more computing resources. On the other hand, many real-world systems that need video segmentation, e.g. autonomous driving, have strict requirments on the latency of response, thus making the problem even more challenging. Previous efforts on video semantic segmentation mainly fall into two classes, namely high-level modeling and feature-level propagation. The former <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref> integrates frame-wise analysis via a sequential model. The methods along this line usually add additional levels on top, and therefore are unable to reduce the computing cost. The latter, such as Clockwork Net <ref type="bibr" target="#b24">[25]</ref> and Deep Feature Flow <ref type="bibr" target="#b30">[31]</ref>, instead attempts to reuse the features in preceding frames to accelerate computation. Such methods were designed to reduce the overall cost in an amortized sense, while neglecting the issue of latency. Moreover, from a technical standpoint, existing methods exploit temporal correlations by treating all locations independently and uniformly. It ignores the different characteristics between smooth regions and boundaries, and lacks the flexibility of handling complex variations. <ref type="figure">Fig. 1</ref> compares the performance/latency tradeoffs of various methods. We can see that previous methods fall short in either aspect.</p><p>Our primary goal in this work is to reduce not only the overall cost but also the maximum latency, while maintaining competitive performance in complicated and everchanging scenarios. Towards this goal, we explore a new framework. Here, we adopt the idea of feature sharing, but move beyond the limitations of previous methods in two important aspects. <ref type="bibr" target="#b0">(1)</ref> We introduce an adaptive feature propagation component, which combines features from preceding frames via spatially variant convolution. By adapting the combination weights locally, it results in more effective use of previous features and thus higher segmentation accuracy. <ref type="bibr" target="#b1">(2)</ref> We adaptively allocate the keyframes on demand based on accuracy prediction and incorporate a parallel scheme to coordinate keyframe computation and feature propagation. This way not only leads to more efficient use of computational resources but also reduce the maximum latency. These components are integrated into a network.</p><p>Overall, the contributions of this work lie in three key aspects. <ref type="bibr" target="#b0">(1)</ref> We study the issue of latency, which is often overlooked in previous work, and present a framework that achieves low-latency video segmentation. (2) We introduce two new components: a network using spatially variant convolution to propagate features adaptively and an adaptive scheduler to reduce the overall computing cost and ensure low latency. (3) Experimental results on both Cityscapes and CamVid demonstrate that our method achieve competitive performance as compared to the state of the art with remarkably lower latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image Semantic Segmentation Semantic segmentation predicts per-pixel semantic labels given the input image. The Fully Convolutional Network (FCN) <ref type="bibr" target="#b18">[19]</ref> is a seminal work on this topic, which replaces fully-connected layers in a classification network by convolutions to achieve pixelwise prediction. Extensions to this formulation mainly follow two directions. One is to apply Conditional Random Fields (CRF) or their variants on top of the CNN models to increase localization accuracy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref>. The other direction explores multi-scale architectures to combine both low-level and high-level features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>. Various improved designs were proposed in recent years. Noh et al. <ref type="bibr" target="#b21">[22]</ref> proposed to learn a deconvolution network for segmentation. Badrinarayanan et al. <ref type="bibr" target="#b0">[1]</ref> proposed SegNet, which adopts an encoder-decoder architecture and leverages max pooling indices for upsampling. Paszke et al. <ref type="bibr" target="#b22">[23]</ref> focused on the efficiency and developed ENet, a highly efficient network for segmentation. Zhao et al. <ref type="bibr" target="#b28">[29]</ref> presented the PSPNet, which uses pyramid spatial pooling to combine global and local cues. All these works are purely image-based. Even if applied to videos, they work on a per-frame basis without considering the temporal relations.</p><p>Video Semantic Segmentation Existing video semantic segmentation methods roughly fall in two categories. One category is to improve the accuracy by exploiting temporal continuity. Fayyaz et al. <ref type="bibr" target="#b7">[8]</ref> proposed a spatial-temporal LSTM on per-frame CNN features. Nilsson and Sminchisescu <ref type="bibr" target="#b20">[21]</ref> proposed gated recurrent units to propagate semantic labels. Jin et al. <ref type="bibr" target="#b13">[14]</ref> proposed to learn discriminative features by predicting future frames and combine both the predicted results and current features to parse a frame. Gadde et al. <ref type="bibr" target="#b9">[10]</ref> proposed to combine the features wrapped from previous frames with flows and those from the current frame to predict the segmentation. While these methods improve the segmentation accuracy by exploting across-frame relations, they are built on per-frame feature computation and therefore are not able to reduce the computation.</p><p>Another category focuses instead on reducing the computing cost. Clockwork Net <ref type="bibr" target="#b24">[25]</ref> adapts mutli-stages FCN and directly reuses the second or third stage features of preceding frames to save computation. Whereas the high level features are relatively stable, such simple replication is not the best practice in general, especially when signficant changes occur in the scene. The DFF <ref type="bibr" target="#b30">[31]</ref> propagates the high level feature from the key frame to current frame by optical flow learned in a flow network <ref type="bibr" target="#b8">[9]</ref> and obtains better performance. Nevertheless, the separate flow network increases the computational cost; while the per-pixel location transformation by optical flow may miss the spatial information in the feature field. For both methods, the key frame selection is crucial to the overall performance. However, they simply use fixed-interval schedules <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref> or heuristic thresholding schemes <ref type="bibr" target="#b24">[25]</ref>, without providing a detailed investigation. Moreover, while being able to reduce the overall cost, they do not decrease the maximum latency.</p><p>There are also other video segmentation methods but with different settings. Perazzi et al. <ref type="bibr" target="#b23">[24]</ref> built a large scale video object segmentation dataset, which concerns about segmenting the foreground objects and thus are different from our task, parsing the entire scene. Khoreva et al. <ref type="bibr" target="#b14">[15]</ref> proposed to learn smooth video prediction from static images by combining the results from previous frames. Caelles et al. <ref type="bibr" target="#b2">[3]</ref> tackled this problem in a semi-supervised manner. Mahasseni et al. <ref type="bibr" target="#b19">[20]</ref> developed an offline method, which relies on a Markov decision process to select key frames, and propagate the results on key frames to others via interpolation. This method needs to traverse the video frames back and forth, and therefore is not suitable for the online settings discussed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video Segmentation Framework</head><p>We develop an efficient framework for video semantic segmentation. Our goal is to reduce not only the overall computing cost but also the maximum latency, while maintaining competitive performance. Specifically, we adopt the basic paradigm of the state-of-the-art frameworks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>, namely, propagating features from key frames to others by exploiting the strong correlations between adjacent frames. But we take a significant step further, overcoming the limitations of previous work with new solutions to two key problems: (1) how to select the key frames and (2) how to propagate features across frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework Overview</head><p>Previous works usually select key frames based on fixed intervals <ref type="bibr" target="#b30">[31]</ref> or simple heuristics <ref type="bibr" target="#b24">[25]</ref>, and propagate features based on optical flows that are costly to compute <ref type="bibr" target="#b30">[31]</ref> or CNNs with fixed kernels. Such methods often lack the capability of handling complex variations in videos, e.g. the changes in camera motion or scene structures. In this work, we explore a new idea to tackle these problems -taking advantage of low-level features, i.e. those from lower layers of a CNN. Specifically, low-level features are inexpensive to obtain, yet they provide rich information about the characteristics of the underlying frames. Hence, we may select key frames and propagate features more effectively by exploiting the information contained in the low-level features, while maintaining a relatively low computing cost. <ref type="figure">Figure 2</ref> shows the overall pipeline of our framework. Specifically, we use a deep convolutional network (ResNet-101 <ref type="bibr" target="#b11">[12]</ref> in our implementation) to extract visual features from frames. We divide the network into two parts, the lower part S l and the higher-part S h . The low-level features derived from S l will be used for selecting key frames and controlling how high-level features are propagated.</p><p>At runtime, to initialize the entire procedure, the framework will feed the first frame I 0 through the entire CNN and obtain both low-level and high-level features. At a later time step t, it performs the computation adaptively. In particular, it first feeds the corresponding frame I t to S l , and computes the low-level features F t l . Based on F t l , it decides whether to treat I t as a new key frame, depending on how much it deviates from the previous one. If the decision is "yes", it will continue to feed F t l to S h to compute the high-level features F t h , and then the segmentation map (via pixel-wise classification). Otherwise, it will feed F t l to a kernel predictor, obtain a set of convolution kernels therefrom, and use them to propagate the high-level features from the previous key frame via spatially variant convolution.</p><p>Note that the combined cost of kernel prediction and spatially variant convolution is dramatically lower than com-puting the high-level features from F t l (38 ms vs 299 ms). With our design, such an adaptive propagation scheme can maintain reasonably high accuracy for a certain range (7 frames) from a key frame. Moreover, the process of deciding whether I t is a key frame is also cheap (20 ms). Hence, by selecting key frames smartly and propagating features effectively, we can significantly reduce the overall cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptive Selection of Key Frames</head><p>An important step in our pipeline is to decide which frames are the key frames. A good strategy is to select key frames more frequently when the video is experiencing rapid changes, while reducing the computation when the observed scene is stable. Whereas this has been mentioned in previous literatures, what dominate the practice are still fixed-rate schedulers <ref type="bibr" target="#b30">[31]</ref> or those based on simple thresholding of feature variances <ref type="bibr" target="#b24">[25]</ref>.</p><p>According to the rationale above, a natural criterion for judging whether a frame should be chosen as a new key frame is the deviation of its segmentation map from that of the previous key frame. This can be formally defined as the fraction of pixels at which the semantic labels differ. Intuitively, a large deviation implies significant changes and therefore it would be the time to set a new key frame.</p><p>However, computing the deviation as defined above requires the segmentation map of the current frame, which is expensive to obtain. Our approach to this problem is to leverage the low-level features to predict its value. Specifically, we conducted an empirical study on both Cityscapes and Camvid datasets, and found that there exists strong correlation between the difference in low-level features and the deviation values. Greater differences in the low-level features usually indicate larger deviations.</p><p>Motivated by this observation, we devise a small neural network to make the prediction. Let k and t be the indexes of two frames, this network takes the differences between their low-level features, i.e. (F t l −F k l ), as input, and predicts the segmentation deviation, denoted by dev S (k, t). Specifically, our design of this prediction network comprises two convolutional kernels with 256 channels, a global pooling and a fully-connected layer that follows. In runtime, at time step t, we use this network to predict the deviation from the previous key frame, after the low-level features are extracted. As shown in <ref type="figure">Figure 3</ref>, we observed that the predicted deviation would generally increases over time. If the predicted deviation goes beyond a pre-defined threshold, we set the current frame as a key frame, and computes its highlevel features with S h , the higher part of the CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Feature Propagation</head><p>As mentioned, for a frame I t that is not a key frame, its high-level features will be derived by propagating from the previous key frame, which we denote by I k . The ques- <ref type="figure">Figure 2</ref>. The overall pipeline. At each time step t, the lower-part of the CNN S l first computes the low-level features F t l . Based on both F k l (the low-level features of the previous key frame) and F t l , the framework will decide whether to set I t as a new key frame. If yes, the high-level features F t h will be computed based on the expensive higher-part S h ; otherwise, they will be derived by propagating from F k h using spatially variant convolution. The high-level features, obtained in either way, will be used in predicting semantic labels.  <ref type="figure">Figure 3</ref>. Adaptive key frame selection. As we proceed further away from the key frame, the predicted deviation of the segmentation map, depicted with blue dots, gradually increases. In our scheme, when the deviation goes beyond a pre-defined threshold, the current frame will be selected as a new key frame.</p><formula xml:id="formula_0">! " ! " ! " ! # 1 0 0 1 ! " ! # 0 ! " $ " % &amp; ' &amp; ' +1 &amp; ' +2 $ " % ( $ " %)* $ " %)+ $ " % ( )* &amp; &amp; + 1 &amp; + 2 $ # % $ # %)+ $ # %)* $ # % ( 0 ! " $ " % ( )+ $ # % ( )+ $ # % ( )* 0 ! " ! " &amp; ' +3 &amp; '' $ " % ( )0 $ " % (( $ # % ( )0 $ # % (( ! # 1 0 ! " $ " % (( )+ $ # % (( )+ 0 ! " $ " % (( )* $ # % (( )* &amp; '' + 1 &amp; '' + 2</formula><p>tion of how to propagate features effectively and efficiently is nontrivial. Existing works often adopt either of the following two approaches. 1) Follow optical flows <ref type="bibr" target="#b30">[31]</ref>. While sounding reasonable, this way has two drawbacks: (a) The optical flows are expensive to compute. (b) Point-to-point mapping is often too restrictive. For high-level feature maps, where each feature actually captures the visual patterns over a neighborhood instead of a single site, a linear combination may provide greater latitude to express the propagation more accurately. 2) Use translation-invariant convolution <ref type="bibr" target="#b19">[20]</ref>. While convolution is generally less expensive and offers greater flexibility, using a fixed set of convolution kernels uniformly across the map is problematic. Different parts of the scene have different motion patterns, e.g. they may move towards different directions, therefore they need different weights to propagate. Motivated by this analysis, we propose to propagate the features by spatially variant convolution, that is, using convolution to express linear combinations of neighbors, with the kernels varying across sites. Let the size of the kernels be H K × H K , then the propagation from the high-level features of the previous key frame (F k h ) to that of the current frame (F t h ) can be expressed as</p><formula xml:id="formula_1">F t h (l, i, j) = ∆ u=−∆ ∆ v=−∆ W (k,t) ij (u, v) · F k h (l, i − u, j − v). (1) Here, ∆ = H K /2 , F t h (l, i, j) is the feature value at (i, j) of the l-th channel in F t h , W (k,t) ij</formula><p>is an H × H kernel used to compute the feature at (i, j) when propagating from F k h to F t h . Note that the kernel values are to assign weights to different neighbors, which are dependent on the feature location (i, j) but shared across all channels.</p><p>There remains a question -how to obtain the spatially variant kernels W (k,t) ij . Again, we leverage low-level features to solve the problem. In particular, we devise a kernel weight predictor, which is a small network that takes the low-level features of both frames, F k l and F t l , as input, and produces the kernels at all locations altogether. This network comprises three convolutional layers interlaced with ReLU layers. The output of the last convolutional layer is of size H 2 K ×H ×W , where H ×W is the spatial size of the high-level feature map. This means that it outputs an H 2 Kchannel vector at each location, which is then repurposed to be a kernel of size H K × H K for that location. This output is then converted to normalized weights via a softmax layer <ref type="figure">Figure 4</ref>. Adaptive feature propagation. Let t refer to the current frame and k the previous key frame. When the low-level features F t l are computed, the kernel weight predictor will take both F k l and F t l as input, and yield a series of convolution kernels, each for a different location. Then the high-level features in the previous key frame F k h will be propagated to the current time step via spatially variant convolution using the predicted kernels. Finally, the high-level features will be used in pixel-wise label prediction.</p><p>to ensure that the weights of each kernel sum to one. With the weights decided on the low-level features, we allow the kernels to be adapted to not only the locations but also the frame contents, thus obtaining great expressive power.</p><p>To increase the robustness against scene changes, we fuse the low-level features F t l with the propagated highlevel feature F t h for predicting the labels. Unlike in original Clockwork Net <ref type="bibr" target="#b24">[25]</ref>, where low-and high-level features are simply concatenated, we introduce a low-cost AdaptNet to adapt the low-level features before it is used for prediction. The AdaptNet consists of three convolution layers and each layer has a small number of channels. The AdaptNet is jointly learned with the weight predictor in model training. In this way, the framework can learn to exploit the complementary natures of both components more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Low-Latency Scheduling</head><p>Low latency is very important in many real-world applications, e.g. surveillance and autonomous driving. Yet, it has not received much attention in previous works. While some existing designs <ref type="bibr" target="#b24">[25]</ref> can reduce the overall cost in an amortized sense, the maximum latency is not decreased in this design due to the heavy computation at key frames.</p><p>Based on the framework presented above, we devise a new scheduling scheme that can substantially reduce the maximum latency. The key of our approach is to introduce a "fast track" at key frames. Specifically, when a frame I t is decided to be a key frame, this scheme will compute the segmentation of this frame through the "fast track", i.e. via feature propagation. The high-level features resulted from the fast track are temporarily treated as the new key frame feature and placed in the cache. In the mean time, a background process is launched to compute the more accurate version of F t h via the "slow track" S h , without blocking the main procedure. When the computation is done, this version will replace the cached features.</p><p>Our experiments show that this design can significantly reduce the maximum latency (from 360 ms to 119 ms), only causing minor drop in accuracy (from 76.84% to 75.89%). Hence, it is a very effective scheme for real-world applications, especially those with stringent latency constraints. Note that the low-latency scheme cannot work in isolation. This good balance between performance and latency can only be achieved when the low-latency scheme is working with the adaptive key-frame selection and the adaptive feature propagation modules presented above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head><p>Our basic network is a ResNet-101 <ref type="bibr" target="#b11">[12]</ref> pretrained on ImageNet. We choose conv4 3 as the split point between the lower and higher parts of the network. The low-level features F t l derived from this layer has 1024 channels. The lower part consumes about 1/6 of the total inference time.</p><p>In general, the model can achieve higher accuracy with a heavier low-level part, but at the expense of higher computing cost. We chose conv4 3 as it attains a good tradeoff between accuracy and speed on the validation sets.</p><p>The adaptive key frame selector takes the low-level features of the current frame F t l and that of the previous key frame F k l as input. This module first reduces the input features to 256 channels with a convolution layer with 3 × 3 kernels and then compute their differences, which are subsequently fed to another convolution layer with 256 channels and 3 × 3 kernels. Finally, a global pooling and a fullyconnected layer are used to predict the deviation.</p><p>The kernel weight predictor in the adaptive feature propagation module (see <ref type="figure">Fig. 4</ref>) has a similar structure, except that the input features are concatenated after being reduced to 256 channels, the global pooling is removed, and the fully-connected layer is replaced by a convolution layer with 1 × 1 kernels and 81 channels. The AdaptNet also reduces the low-level features to 256 channels by a convolution layer with 3 × 3 kernels, and then pass them to two two convolution layers with 256 channels and 3 × 3 kernels. For non-key frames, the adapted low-level features, i.e. the output of the AdaptNet are fused with the propagated high-level features. The fusion process takes the concatenation of both features as input, and sends it through a convolution layer with 3 × 3 kernels and 256 channels.</p><p>During training, we first trained basic network with respective ground-truths and fixed it as the feature extractor. Then, we finetuned the adaptive propagation module and the adaptive schedule module, both of which can take a pair of frames that are l steps apart as input (l is randomly chosen in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>). Here, we choose the pairs such that in each pair the first frame is treated as the key frame, and the second one comes with annotation. For the adaptive propagation module, the kernel predictor and the Adapt-Net are integrated into a network. This integrated network can produce a segmentation map for the second frame in each pair through keyframe computation and propagation. This combined network is trained to minimize the loss between the predicted segmentation (on the second frame of each pair) and the annotated groundtruth. For training the adaptive schedule module, we generated the segmentation maps of all unlabelled frames as auxiliary labels based on the trained basic network and computed the deviation of the segmentation maps between key frame and current frame as the regression target. The training images were randomly cropped to 713 × 713 pixels. No other data argumentation methods were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>We evaluated our framework on two challenging datasets, Cityscapes <ref type="bibr" target="#b6">[7]</ref> and CamVid <ref type="bibr" target="#b1">[2]</ref>, and compared it with state-of-the-art approaches. Our method, with lowest latency, outperforms previous methods significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Metrics</head><p>Cityscapes <ref type="bibr" target="#b6">[7]</ref> is set up for urban scene understanding and autonomous driving. It contains snippets of street scenes collected from 50 different cities, at a frame rate of 17 fps. The training, validation, and test sets respectively contain 2975, 500, and 1525 snippets. Each snippet has 30 frames, where the 20-th frame is annotated with pixellevel ground-truth labels for semantic segmentation with 19 categories. The segmentation accuracy is measured by the pixel-level mean Intersection-over-Union (mIoU) scores.</p><p>Camvid <ref type="bibr" target="#b1">[2]</ref> contains 701 color images with annotations of 11 semantic classes. These images are extracted from driving videos captured at daytime and dusk. Each video contains 5000 frames on average, with a resolution of 720× 960 pixels. Totally, there are about 40K frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation on Cityscapes Dataset</head><p>We compared our low-latency video semantic segmentation framework with recent state-of-the-art methods, following their evaluation protocol. <ref type="table">Table 1</ref> shows the quantitative comparison. The baseline is per-frame segmentation Method mIOU Avg RT Latency Clockwork Net <ref type="bibr" target="#b24">[25]</ref> 67.7% 141ms 360ms Deep Fea. Flow <ref type="bibr" target="#b30">[31]</ref> 70.1% 273ms 654ms GRFP(5) <ref type="bibr" target="#b20">[21]</ref> 69.4% 470ms 470ms baseline 80.2% 360ms 360ms AFP + fix schedule 75.26% 151ms 360ms AFP + AKS 76.84% 171ms 380ms AFP + AKS + LLS 75.89% 119ms 119ms <ref type="table">Table 1</ref>. Comparison with state-of-the-art on Cityscapes dataset. "Avg RT" means average runtime per frame. "AFP" means adaptive feature propopagation. "fix schedule" means key frame is selected very 5 frame. "AKS" means adaptive key frame selection. "LLS" means low-latency scheduling scheme. Clockwork Net is implmented with the same settings as our method with fix schedule: (a) the same backbone network (ResNet-101), (b) the same split between the low-level and high-level stages, (c) the same AdaptNet following the low-level stage, and (d) the same interval between keyframes (l = 5).</p><p>with the full ResNet. Our adaptive feature propagation with fixed-interval schedule speeds up the pipeline, reducing the per-frame runtime from 360 ms to 151 ms while decreasing the performance by 4.9%. Using adaptive key frame selection, the performance is boosted by 1.6%. While these schemes reduce the overall runtime, they are not able to reduce the maximum latency, due to the heavy computation on key frames. The low-latency scheduler effectively tackles this issue, which substantially decreases the latency to 119 ms (about 1/3 of the baseline latency) while maintaining a comparable performance (with just a minor drop).</p><p>From the results, we also see that other methods proposed recently fall short in certain aspects. In particular, Clockwork Net <ref type="bibr" target="#b24">[25]</ref> has the same latency, but at the cost of significantly dropped performance. DFF <ref type="bibr" target="#b30">[31]</ref> maintains a better performance, but at the expense of considerably increased computing cost and dramatically larger latency. Our final result, with low latency schedule for video segmentation, outperforms previous methods by a large margin. This validates the effectiveness of our entire design.</p><p>In addition to the quantitative comparison, we also shows some visual results in <ref type="figure">Fig. 6</ref>. Our method can successfully learn the video segmentation even when the frames vary significantly. In what follows, we will investigate the design for each module respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on Feature Propagation</head><p>We begin by evaluating the effectiveness of our proposed adaptive propagation module. To evaluate the specific performance gain on this module, we fix the scheduling scheme as previous methods, namely selecting a key-frame per 5 frames. <ref type="table">Table 2</ref> shows the quantitative comparison. We compared our method with a globally learned unified propagation, for which the result is not satisfactory. It reveals that the spatially variant weight is quite important in our solu-Method mIOU Clockwork Propagation <ref type="bibr" target="#b24">[25]</ref> 56.53% Optical Flow Propagation <ref type="bibr" target="#b30">[31]</ref> 69.2% Unified Propagation 58.73% Weight By Image Difference 60.12% Adaptive Propagation Module (no fuse) 68.41% Adaptive Propagation Module (with fuse) 75.26% <ref type="table">Table 2</ref>. Comparison of different feature propagation modules. tion. We also compared to a baseline, where the weights are directly set by the differences of the input pixel values, our learned propagation weights also perform better. Our experiment also shows that the fuse with the low-level features from the current frame by AdaptNet achieves significant performance improvements (from 68.41% to 75.26%), which may be ascribed to the strong complementary information learned by the fuse model.</p><p>Compared to recently proposed methods for feature propagation, our adaptive propagation module still performs significantly better. Particularly, Clockwork directly replaces parts of the current features with previous ones, and therefore the derived representation is not adapted to current changes, thus resulting in poor performance. The optical flow based method DFF <ref type="bibr" target="#b30">[31]</ref> relies on the optical flows to propagate features and shows substantially better accuracies as compared to Clockwork, due to its better adaptivity. Yet, its performance is sensitive to the quality of the Flownet, and it ignores the spatial relationship on the feature space. These factors limit its performance gain, and hence it is still inferior to the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on Scheduling</head><p>We also studied the performance of the adaptive scheduling module for key frame selection, given our feature propagation module. Note that a good schedule not only reduces the frequency of key frames, but also increases the performance given a fixed number of key frames. Hence, we conducted several experiments to compare different schedule methods across different key-frame intervals.</p><p>Specifically, we compared fixed rate schedule, threshold schedule, and also our adaptive schedule module. <ref type="figure" target="#fig_0">Fig. 5</ref> shows the results. Under each key-frame interval, our adaptive schedule always outperforms the other two. We believe the reason why the proposed method outperforms the threshold schedule is that the intermediate feature maps is not specially optimized for key-frame selection. They may contain noises and perhaps many other irrelevant factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost and Latency Analysis</head><p>We analyzed the computation cost of each component in our pipeline, and then the overall latency of our method. The results are in <ref type="table" target="#tab_1">Table 3</ref>.</p><p>With the low-latency scheduling scheme, the total latency of our pipeline is equal to the sum of the lower part of the network S l , adaptive key-frame selection, adaptive feature propagation, which is 0.119s (33% of the basic network).</p><p>On the contrary, all previous methods fail to reduce latency in their system design, despite that they may reduce the overall cost in the amortized sense. Hence, they are not able to meet the latency requirements in real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Pixel Accuracy CA SuperParsing <ref type="bibr" target="#b27">[28]</ref> 83.9% 62.5% DAG-RNN <ref type="bibr" target="#b25">[26]</ref> 91.6% 78.1% MPF-RNN <ref type="bibr" target="#b12">[13]</ref> 92.8% 82.3% RTDF <ref type="bibr" target="#b16">[17]</ref> 89.9% 80.5% PEARL <ref type="bibr" target="#b13">[14]</ref> 94.2% 82.5% Ours 94.6% 82.9% <ref type="table">Table 4</ref>. Result comparison for CamVid dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">CamVid Dataset</head><p>We also evaluated our method on CamVid, another video segmentation dataset, and compard it with multiple previous methods that have reported performances on CamVid, which range from traditional methods to various CNN or RNN based methods. In <ref type="table">Table 4</ref>, we report the pixel accuracy and average per-Class Accuracy(CA), which can reduce the dominated effect on majority classes. We can see our framework still performs the best. The consistently good performances on both datasets show that our adaptive framework for video segmentation method is in general Input DFF Clockwork Ours <ref type="figure">Figure 6</ref>. Visual Results for Cityscapes Dataset: our method can achieve significant improvement shown as the red rectangle.</p><p>Input DFF Clockwork Ours <ref type="figure">Figure 7</ref>. Visual Results for Camvid Dataset: our method can achieve significant improvement shown as the red rectangle.</p><p>beneficial to video perception. Qualitative results on this dataset are shown in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented an efficient video semantic segmentation framework with two key components: adaptive feature propagation and adaptive key-frame schedule. Particularly, our specially designed schedule scheme can achieve low latency in an online setting. The results on both Cityscapes and CamVid showed that our method can yield a substantially better tradeoff between accuracy in latency, compared to previous methods. In future, we will explore more model compression approaches which can further reduce the overall computation cost and latency for a practical system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 .</head><label>5</label><figDesc>Comparison of different scheduling schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Latency analysis for each module of our network. The second column (R) shows the ratio of the latency to the time spent by the basic network.</figDesc><table><row><cell>Modules</cell><cell>Time (ms)</cell><cell>R</cell></row><row><cell>Lower part of network S l</cell><cell>61</cell><cell>16.9%</cell></row><row><cell>Higher part of network S h</cell><cell>299</cell><cell>83.1%</cell></row><row><cell>Basic Network</cell><cell>360</cell><cell>100%</cell></row><row><cell>Adaptive Schedule Module</cell><cell>20</cell><cell>5.5%</cell></row><row><cell>Adaptive Propagation Module</cell><cell>38</cell><cell>10.5%</cell></row><row><cell>Non-Key Frame</cell><cell>119</cell><cell>33%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05198</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1511.03339</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stfcn: Spatio-temporal fcn for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05971</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06852</idno>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03088</idno>
		<title level="m">Semantic video cnns through representation warping</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-path feedback recurrent neural network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07706</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Video scene parsing with predictive feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00119</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02646</idno>
		<title level="m">Learning video object segmentation from static images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent temporal deep field for semantic video labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="302" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Budget-aware deep semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semantic video segmentation by gated recurrent flow propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08871</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno>abs/1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clockwork convnets for video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dag-recurrent neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3620" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="352" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01105</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
