<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cloud Transformers: A Universal Approach To Point Cloud Processing Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Mazur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center Moscow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center Moscow</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cloud Transformers: A Universal Approach To Point Cloud Processing Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new versatile building block for deep point cloud processing architectures that is equally suited for diverse tasks. This building block combines the ideas of spatial transformers and multi-view convolutional networks with the efficiency of standard convolutional layers in two and three-dimensional dense grids. The new block operates via multiple parallel heads, whereas each head differentiably rasterizes feature representations of individual points into a low-dimensional space, and then uses dense convolution to propagate information across points. The results of the processing of individual heads are then combined together resulting in the update of point features. Using the new block, we build architectures for both discriminative (point cloud segmentation, point cloud classification) and generative (point cloud inpainting and image-based point cloud reconstruction) tasks. The resulting architectures achieve state-of-the-art performance for these tasks, demonstrating the versatility and universality of the new block for point cloud processing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (ConvNets) <ref type="bibr" target="#b18">[24]</ref> and Transformers <ref type="bibr" target="#b49">[55]</ref> have emerged as the most successful data processing architectures across a variety of data domains. ConvNets are naturally suited for high-dimensional data that are sampled on low-dimensional grids and have spatial, temporal or spatial-temporal nature (e.g. images or sound). At the same time, Transformers scale less well to high-dimensional data but excel at handling less structured data such as phrases of a natural language.</p><p>In this work, we focus on point clouds, which are data of relatively high dimensionality but lacking the regularity of images. Despite the lack of the regularity and due to high dimensionality and spatial nature of point clouds, most stateof-the-art architectures for point cloud processing are derived from ConvNets. These ConvNet adaptations are based on direct rasterization of point clouds onto regular grids followed by convolutional pipelines <ref type="bibr" target="#b37">[43,</ref><ref type="bibr" target="#b8">9]</ref>, as well as on generalizations of the convolutional operators to irregularly sampled data <ref type="bibr" target="#b23">[29,</ref><ref type="bibr" target="#b51">57]</ref> or non-rectangular grids <ref type="bibr" target="#b16">[22,</ref><ref type="bibr" target="#b13">19]</ref>.</p><p>Here, we propose a new building block (a cloud transform block) for point cloud processing architectures that combines the ideas of ConvNets and Transformers <ref type="figure">(Figure 3</ref>). Similarly to the (self)-attention layers within transformers cloud transform blocks take unordered sets of vectors as an input, and process such input using multiple parallel heads. For an input set element, each head computes two-or three-dimensional key and a higher dimensional value, and then uses the computed keys to rasterize the respective values onto a regular grid. A two-or threedimensional convolution is then used to propagate the information across elements. The results of parallel heads are then probed at key locations and are recombined together, producing an update to element features.</p><p>We show that multiple cloud transform blocks can be stacked sequentially and trained end-to-end, as long as special care is taken when implementing forward and backward pass through the rasterization operations. We then design cloud transformer architectures that concatenate multiple cloud transform blocks together with task-specific 3D convolutional layers. Specifically, we design a cloud transformer for semantic segmentation (which we evaluate on the S3DIS benchmark <ref type="bibr" target="#b0">[1]</ref>), classification (which we evaluate on the ScanObjectNN benchmark <ref type="bibr" target="#b48">[54]</ref>), point cloud inpainting (which we evaluate on ShapeNet-based benchmark <ref type="bibr" target="#b59">[65]</ref>), and a cloud transformer for image-based geometric reconstruction (which we evaluate on a recently introduced ShapeNet-based benchmark <ref type="bibr" target="#b42">[48]</ref>). In the eval-uation, the designed cloud transformers achieve state-ofthe-art accuracy for semantic segmentation and point cloud completion tasks and considerably outperform state-of-theart for image-based reconstruction and point cloud classification ( <ref type="figure" target="#fig_0">Figure 1</ref>). We note that such versatility is rare among previously introduced point cloud processing architectures, which can handle either recognition tasks (such as semantic segmentation, classification) or generation tasks (such as inpainting and image-based reconstruction) but usually not both.</p><p>To sum up, our key contributions and novelty are:</p><p>• We propose a new approach to point cloud processing based on repeated learnable projection, rasterization and de-rasterization operations. We investgate how to make rasterizations and de-rasterizations repeatable sequentially within the same architecture through the gradient balancing trick. Additionally, we show that aggregating rasterizations via element-wise maximum performs better than additive accumulation at least in the context of our approach.</p><p>• We propose and validate an idea of multi-head selfattention for point clouds that performs parallel processing by rasterization and de-rasterization to separate low-dimensional grids. Additionally, we propose an idea of using both two-dimensional and threedimensional grids in parallel with each other.</p><p>• Based on the two ideas above, we propose architectures for semantic segmentation, classification, point cloud inpainting, and image-based reconstruction. The proposed architectures are all based on the same Cloud Transform blocks and achieve state-of-the-art performance on standard benchmarks in each case despite the diversity of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Point cloud processing with deep architectures has grown to a large field of study. The ideas implemented within cloud transform blocks are closely related to a large body of prior works. Below, we review only the most related ones.</p><p>A number of work use rasterizations of the point cloud over regular 3D grids <ref type="bibr" target="#b25">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">32,</ref><ref type="bibr" target="#b24">30]</ref>, where each point is rasterized at its original position within the point cloud.</p><p>Multi-view ConvNets <ref type="bibr" target="#b38">[44]</ref> project point clouds to multiple predefined 2D views. The approaches that use splat convolutions on permutohedral grid convolutions <ref type="bibr" target="#b14">[20,</ref><ref type="bibr" target="#b36">42]</ref> are perhaps most similar to ours (and have been an inspiration to us), as they also interleave rasterization (splatting), (permutohedral) convolution, and probing (slicing). In contrast to all above-mentioned works, which use initial positions or data-independent projections of points for rasterizations, our architectures learn a variety of different and data-dependent projections (one projection per head in each block).</p><p>In <ref type="bibr" target="#b52">[58]</ref>, a dynamic graph ConvNet (DGCNN) architecture based on graph convolutions is presented. The graph is computed from spatial positions of the points that are modified in a data-dependent way within the architecture. In their case, the loss can not be backpropagated through the graph node position estimation since the spatial graph construction is non-differentiable. In contrast, our approach is based on regular grid convolutions and includes the backpropagation through position estimation (key computation). We also note that differentiable point cloud projection onto a 2D grid (from 3D space) has been used in [16] though in a different way and for a different purpose than in our case.</p><p>Our approach is also strongly related to the seminal work on spatial transformers <ref type="bibr" target="#b12">[18]</ref>, which introduced blocks that warp signals on regular grids through data-dependent parametric warping and bilinear sampling. Our blocks also use bilinear sampling in the end of each head processing. Inspired by spatial transformers, <ref type="bibr" target="#b50">[56]</ref> investigate how dataindependent and data-dependent deformations of the original point clouds can be used to boost the performance of several recognition architectures including DGCNN <ref type="bibr" target="#b34">[40]</ref>, SplatNet <ref type="bibr" target="#b36">[42]</ref>, and VoxelNet <ref type="bibr" target="#b62">[68]</ref>. Similarly to <ref type="bibr" target="#b52">[58]</ref> and unlike <ref type="bibr" target="#b12">[18]</ref>, <ref type="bibr" target="#b50">[56]</ref> do not propagate the loss fully through deformation computation (in the case of data-dependent deformations). Compared to <ref type="bibr" target="#b50">[56]</ref>, our architectures employ regular 2D and 3D convolutions, can handle both recognition and generative tasks (the latter not considered in <ref type="bibr" target="#b50">[56]</ref>), and are trained with gradient propagation through key position computation.</p><p>Our work is also related to <ref type="bibr" target="#b22">[28,</ref><ref type="bibr" target="#b40">46]</ref>, as our method is also based on rasterizations and de-rasterizations. However, these methods are not applicable for data-dependent transformations of rasterizaton positions and, therefore, cannot handle generative tasks. Additionally, we employ a different method for rasterizations via element-wise maximum, instead of averaging.</p><p>Transformer's quadratic complexity have been recently addressed by sparse transformers <ref type="bibr" target="#b3">[4]</ref> that alleviate the quadratic complexity of the original transformers in the set size, by restricting the interaction between elements in the set to predefined sparse subsets. Our mechanism based on rasterization and convolution can be seen as an alternative to sparse transformers that restricts interaction to elements that have been projected into adjacent grid cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Overview. We describe our approach in a bottom-up way. In Section 3.1, we introduce the basic building operation of our processing pipeline that we call cloud transform. In Section 3.2, we discuss how cloud transforms can be assembled in a parallel fashion into blocks (which we call multi- <ref type="figure">Figure 2</ref>: The cloud transform consists of rasterization (left) and de-rasterization(right) steps, with the convolutional part in between. It projects the high-dimensional point cloud onto lowdimensional (two-dimensional in this case) grid, applies convolutional processing, and lifts the result back to the high-dimensional space. Electronic zoom-in recommended.</p><formula xml:id="formula_0">… − − CNN 0 w -1 0 h -1 − − − − … − − − − − 0 w -1 0 h -1 − − ∈ ℝ ℎ× × ∈ ℝ ℎ× × − − − − , max( )</formula><p>head processing blocks). In Section 3.3, we discuss sequential stacking of multi-head processing blocks into bigger blocks (called cascaded multi-head processing blocks) that cycle through different spatial resolution and different numbers of feature channels. Finally, in Section 3.4, we introduce the architectures (that we call cloud transformers) build from cascaded multi-head processing blocks for four different point cloud processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cloud Transform</head><p>The cloud transform contains three steps: rasterization, convolution, and de-rasterization.</p><p>Rasterization step. To rasterize each point x i , we predict the value v i ∈ R cin and the key k i ∈ [0, 1] 2 . These two vectors stand for what to rasterize and where to rasterize respectively.</p><p>The cloud transform ( <ref type="figure">Figure 2</ref>) takes as an input an unordered set (to which we further refer as point cloud) X = {x 1 , . . . , x N | x i ∈ R f }, whose elements are vectors x i ∈ R f of a potentially high dimension f . The Cloud Transform T (X) maps such input into a new gdimensional point cloud Y ∈ R N ×g of the same size N . Additionally, our layer uses an input point cloud positions</p><formula xml:id="formula_1">P = {p 1 , . . . , p N | p i ∈ R 3 }.</formula><p>The cloud transform first applies a learnable projection P 2 (further called rasterization), which generates a twodimensional feature map with c in channels, i.e. P 2 : X → I ∈ R w×w×cin . In a volumetric setting, the cloud transform starts with a learnable projection P 3 , which generates a three-dimensional volumetric feature map, i.e. P 3 : X → I ∈ R w×w×w×cin . In both cases, w stands for the spatial resolution of the grid, while c in stands for the number of input channels.</p><p>Once an irregular point cloud X is projected onto a regular feature map, the cloud transform applies a single convolution or a more complex combination of convolutional operations. We denote the result of these convolutional layers asĨ ∈ R w×w×cout (Ĩ ∈ R w×w×w×cout in the volumetric case). Note that we expectĨ to be of the same spatial size as I. However, the channel dimension ofĨ might be changed from c in to c out .</p><p>The last step of our Cloud Transform operation is derasterization (also called slicing)P :Ĩ → V from the processed feature mapĨ into a new transformed values V ∈ R N ×cout . Note, that cloud transform passes information from x i to x j as long as these two points have been projected to sufficiently close positions. Thus, the cloud transform can be seen as a variant of self-attention layer with adaptive sparse attention mechanism. Below, for the sake of simplicity, we detail the steps of the cloud transform for a two-dimensional feature map case. The volumetric case is completely analogous.</p><p>Our method allows to directly predict keys via linear layer and stack these layers into deep architectures. However, in our experiments transforming positions via matrix multiplication (i.e predicting it with a point-wise MLP from x i ) results in suboptimal performance. A better solution predicts deep residuals d(x i ) to the input positions p i with a single layer peceptron d and apply a learnable transformation T ∈ SE(3) afterwards (the transform T becomes the parameter of the layer). The keys are thus computed as:</p><formula xml:id="formula_2">k i = T (p i + d(x i ))<label>(1)</label></formula><p>In the case of two-dimensional heads, we project k i to the z = 0 plane (omitting the third coordinate). Finally, we apply the sigmoid activation to the keys k i ensuring they are positioned between zero and one. As for the values v i prediction, we use a single affine layer with the output dimension equal to c in , followed by the normalization layer. Depending on the architecture, the normalization layer can be batch normalization <ref type="bibr" target="#b11">[17]</ref>, instance normalization <ref type="bibr" target="#b47">[53]</ref> or adaptive instance normalization <ref type="bibr">[14]</ref>.</p><p>We then rasterize the value v i ∈ R c in onto the grid I = R w×w×cin using the predicted key k i as a position.</p><p>Specifically, k i = (k 0 i , k 1 i ) ∈ [0, 1] 2 may be interpreted as a relative coordinate inside the spatial grid of I.</p><p>Thus, the position defined by k i falls into the enclosing integer cell (h 0 , w 0 ),</p><formula xml:id="formula_3">(h 0 , w 1 ), (h 1 , w 0 ), (h 1 , w 1 ), where h 0 = (w−1)·k 0 i , h 1 = (w−1)·k 0 i ,w 0 = (w−1)·k 1 i , w 1 = (w−1)·k 1 i . The value v i is then rasterized into four neighbouring feature map pixels I[h 0 , w 0 ], I[h 0 , w 1 ], I[h 1 , w 0 ], I[h 1 , w 1 ] ∈ R cin via bilinear assignment. In more detail, we compute bilinear weights b i = (b 00 i , b 01 i , b 10 i , b 11 i )</formula><p>of the key k i with respect to the cell it falls to. The bilinear weights are then used to update the feature map I at corresponding locations:</p><formula xml:id="formula_4">I[h 0 , w 0 ] ← max(I[h 0 , w 0 ], b 00 i v i ) I[h 0 , w 1 ] ← max(I[h 0 , w 1 ], b 01 i v i ) I[h 1 , w 1 ] ← max(I[h 1 , w 0 ], b 10 i v i ) I[h 1 , w 1 ] ← max(I[h 1 , w 1 ], b 11 i v i )<label>(2)</label></formula><p>The feature map I is initialized with zeros, and the rasterization is repeated for every x i ∈ X, i ∈ 1..N , aggregating rasterized results via element-wise maximum at respective cells of the feature map I. While the choice of the maximum aggregator may seem unnatural compared to average or sum, we have found that it boosts the performance substantially. Convolution step. As discussed above, after rasterization, we transform the feature map I intoĨ with any convolutional architecture that preserves the spatial resolution. In practice, we use a single convolutional layer that keeps the number of channels unchanged. De-rasterization step. As the last step, we perform the derasterization transformP 2 :Ĩ →Ṽ produces the transformed feature cloud Y using standard bilinear grid sampling operation. Thus, the transformed valuesĨ[h 0 , w 0 ],</p><formula xml:id="formula_5">I[h 0 , w 1 ],Ĩ[h 1 , w 0 ],Ĩ[h 1 , w 1 ] ∈ R cout of the feature map are combined with bilinear weights b i into the transformed value vectorṽ i .</formula><p>We apply the normalization layer, and the ReLU nonlinearity to the result of de-rasterization step, and further map each value from c out dimensions back to g dimensions (g=512 unless noted otherwise) using a learnable affine transform.</p><p>Backpropagation through Cloud Transform. We have found that learning architectures with multiple sequentiallystacked Cloud Transform blocks via back-propagation <ref type="bibr" target="#b32">[38]</ref> is highly unstable, as the gradients explode during the backward step. An ideal assumption on gradient variance during the back-propagation is to preserve its scale throughout the network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">12]</ref>. In the supplementary we demonstrate that a naïve verison of Cloud Transform block could not satisfy this assumption and suggest a gradient balancing trick to solve this issue. In our case, the instability can be tracked to the gradient of the bilinear weights b w.r.t. the key k at the rasterization and de-rasterization steps. According to the chain rule, the gradients' variance is multiplied by w during backpropagation through the keys, which results in the exponential resulting gradient variance (w.r.t. depth).</p><p>Gradient balancing trick Based on observation above, during back-propagation of L through keys, we simply divide the partial derivatives w.r.t. both coordinates of k i by w, i.e. we apply:</p><formula xml:id="formula_6">∂L ∂k i ← 1 w ∂L ∂k i .<label>(3)</label></formula><p>We have found that this gradient balancing trick is sufficient to enable the learning of deep architectures containing multiple layers with cloud transforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Headed Cloud Transform block</head><p>The rasterization and de-rasterization operation may lead to the information loss due to the limited number of nodes  <ref type="figure">Figure 3</ref>: Our building block has several planar heads and several volumetric heads operating in parallel. Each head is a cloud transform, using a two-dimensional or a three-dimensional grid for rasterization, followed by convolutional operations, and derasterization (differentiable sampling). Electronic zoom-in recommended.</p><p>in two dimensional and three dimensional lattices (we use up to w=128 for two-dimensional grids and up to w=32 for three-dimensional grids). We therefore build our architectures from blocks that combine multiple cloud transforms operating in parallel. This is reminiscent of both the multiple self-attention head in the Transformer architecture <ref type="bibr" target="#b49">[55]</ref> and the multi-view convolutional networks <ref type="bibr" target="#b38">[44]</ref>. Following <ref type="bibr" target="#b49">[55]</ref>, we call each of the parallel cloud transform modules a head and thus consider a multi-head architecture. Each head predicts keys and values independently, and may use its own spatial resolution w. In fact, twodimensional and three-dimensional heads can operate in parallel. We note that the use of one-dimensional or higherdimensional (e.g. four-dimensional) heads is also possible. One-dimensional grids however inevitably results in very strong conflation of the data, while higher-dimensional grids are computationally heavy and convolutions on them are not well supported. We therefore focus on two-and three-dimensional heads.</p><p>The results of the parallel heads for each point i are summed together, so that the resulting multi-head cloud transform (MHCT) block ( <ref type="figure">Figure 3</ref>) still maps each input vector x i to a g-dimensional vector y i . We add another normalization layer and ReLU nonlinearity after the results of the heads are summed, and complete the block with the residual skip connection from the start to the end. <ref type="bibr">[12]</ref>. We note that the multi-head cloud transform block also resembles the Inception block <ref type="bibr" target="#b39">[45]</ref>, which uses heterogeneous parallel convolutions, as well as the blocks of the ResNeXt networks <ref type="bibr" target="#b55">[61]</ref>, which use grouped convolutions with small number of channels in each group.</p><p>In this work we use MHCT blocks with 16 twodimensional heads and 16 three-dimensional heads. CMHCT CMHCT <ref type="figure">Figure 4</ref>: The architecture used for semantic segmentation is based on (1) a single point-wise layer, (2) four standard cascaded multiheaded cloud transform blocks, followed by (3) a final point-wise MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cascaded Multi-Headed Cloud Transform Block</head><p>Our network does not use pooling and upsampling operations directly on points, as is done in most of the point cloud processing networks (e.g <ref type="bibr" target="#b46">[52]</ref>, <ref type="bibr" target="#b21">[27]</ref>). Instead, we employ feature maps of different spatial sizes as a way of increasing or decreasing the receptive field. Specifically, following the pattern introduced in <ref type="bibr" target="#b35">[41]</ref>, we propose to stack three MHCT blocks sequentially into Cascaded Multi-Headed Cloud Transform Block (CMHCT), decreasing spatial dimension, while increasing the number of channels. In practice, we set the spatial and channels dimensions as in the <ref type="figure">Figure 4</ref>, yellow blocks.</p><p>The order of these three MHCT blocks in CMHCT block is as on the figure. The CMHCT blocks can then be stacked sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Cloud Transformers</head><p>We now discuss the architectures that can be constructed from CMHCT blocks for specific point cloud processing tasks. We note that while the different nature of tasks requires different architectures, we strove to keep these architectures as similar as possible. Most importantly, all proposed architectures are build from CMHCT blocks that are built out of MHCT blocks that are based on Cloud Transforms.</p><p>Semantic segmentation. The semantic segmentation cloud transformer ( <ref type="figure">Figure 4)</ref> consists of an initial one-layer perceptron, which is applied to each point independently and transforms its 3D coordinates and 3D color features to an f -dimensional vector (f =512). Afterwards, we apply four cascaded multi-headed cloud transform layers with default setting. And then conclude the architecture with a two-layer shared perceptron that maps the features of each point to the logits of segmentation classes. All normalization layers in the architectures are BatchNorm layers <ref type="bibr" target="#b11">[17]</ref>. The architecture has 9.6M parameters and is trained with the cross-entropy loss.</p><p>Point cloud generation. To create the architecture that generates point cloud, we stack four CMHCT blocks sequentially, followed by a point-wise two-layer perceptron.  <ref type="figure">Figure 6</ref>: Cloud Transformer designed for classification. Primarily, the input point cloud is processed with a Cloud Transformer backbone, as in the segmentation setting. Afterward, we apply a Multi-headed Cloud Pooling (yellow), followed by a fullyconnected layer to produce a classification vector (red). We also use a separate branch (bottom) for the background mask prediction as is common for the ScanObjectNN benchmark.</p><p>Finally, we add a tanh non-linearity to produce 3D points coordinates. The input point cloud is sampled from a uniform 3D distribution on the unit sphere S 2 and then passed through point-wise linear layer, mapping each feature to f =512 dimensions. To solve the image-based geometry reconstruction task (recovering point clouds from images), we use adaptive instance normalization (AdaIN) layers [15] in the MHCT blocks. We create image encoder with ResNet-50 architecture [12] (pretrained on ImageNet <ref type="bibr" target="#b33">[39]</ref>). The output of the encoder is a 512-dimensional vector, which is transformed into AdaIN coefficients via affine layer <ref type="figure" target="#fig_2">(Figure 5</ref>). The architecture is trained with approximate earth mover distance (EMD) loss <ref type="bibr" target="#b20">[26]</ref>. Note, our generator architecture highly resembles our segmenter, apart from the normalization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point cloud classification</head><p>As in the segmentation model, we apply a "backbone" of a leading linear layer and four CMHCT blocks first <ref type="figure">(Figure 2</ref>). To solve a classification task, we introduce a multi-headed pooing layer. Similarly to the regular MHCT layer, this layer performs multiple rasterizations onto 2D and 3D feature maps of spatial size 8 and 16 respectively. The channel dimension of each head is 32 for three-dimensional heads and 16 for two-dimensional heads. Afterward, the resulting feature maps are processed with three standard convolutional residual blocks, each interchanged with a max-pooling (see the supplementary material for the exact architecture). The resulting vectors are aggregated across the heads via concatenation and pro- cessed with a dense layer to form a final classification vector k class of dimension 1024. Following the original paper <ref type="bibr" target="#b48">[54]</ref>, we also predict an object's instance mask, using the point-wise features predicted with from the features extracted by the backbone and the class vector k class . We train our architecture as in <ref type="bibr" target="#b48">[54]</ref> with the two crossentropy (CE) loss terms. The first one is a CE loss λ class on object classification and the latter one is a point-wise CE loss λ seg on foreground segmentation. The final loss is set to be λ full : = 0.5 · λ class + 0.5 · λ seg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point cloud inpainting</head><p>We also present a model for point cloud inpainting (completion). Given a partial point cloud, the goal is to infer the full shape. Our architecture for this task strongly resembles the one used for image-based reconstruction, apart from the encoder and the generator input. Namely, we apply a point cloud encoder to obtain a style vector. Our encoder is the Cloud Transformer model introduced above for classification. It extracts a vector of dimension 512.</p><p>To account for the input point cloud geometry, we use two sets of points in the generator branch input. First, we put in the input point cloud, thus giving the architecture opportunity to transfer its points to the output. Second, to account for the novel parts, we again sample random points from a unit sphere S 2 . We augment the feature of each input point with a binary variable indicating whether is is sampled from the incomplete scan or from the sphere. As in the case of the image-based reconstruction, the generator branch is conditioned on the encoded vector via AdaIN connections. The model is trained with a sum of approximate EMD <ref type="bibr" target="#b20">[26]</ref> and Chamfer Distance <ref type="bibr" target="#b6">[7]</ref> losses. Afterwards, we fine-tine it on Chamfer Distance loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We compare the performance of our approach to the state-of-the-art on the four considered tasks, using estab-lished benchmarks and metrics associated with those benchmarks. We then perform a short ablation study. In the supplementary materials, we provide visualizations of the cloud transformer operations and results, and the reviewers are encouraged to check the material to gain better intuition about cloud transformers.</p><p>Point Cloud Classification. We evaluate our Cloud Transformer for classification on a real-world dataset ScanOb-jectNN <ref type="bibr" target="#b48">[54]</ref>. The dataset consists of 2902 unique objects obtained from ScanNet <ref type="bibr" target="#b5">[6]</ref> scenes. The objects are categorized into 15 classes. Each object is obtained via cutting it from the scene using the ground truth bounding box. Note that the resulting cut may include background points as well. In the variant we use, each bounding box is randomly shifted and rotated to emulate real-world detection boxes. This process is repeated five times with different bounding box perturbations for each object. This process results in 15k objects in total. An object is represented as a cloud of 2048 3D spatial points. Additionally, a binary pointwise mask is provided for training, indicating whether a point belongs to the background or to the object. We use the hardest variant of the dataset (PB T50 RS), with the highest rate of bounding box perturbations. We adopt the original <ref type="bibr" target="#b48">[54]</ref> train/test split.</p><p>We provide results in <ref type="table">Table 1</ref> and show that our method outperforms the existing ones both in overall and meanclass accuracy. Note, our method outperforms the current state-of-the-art in terms of the overall accuracy by a large margin (+3.7%). We also evaluate a CT variant with a learnable anisotropic scaling s applied after the transformation T in the equation 1. The resulting model outperforms the state-of-the-art by 3.5% in the mean class accuracy.</p><p>For completeness, we have also evaluated our approach on the ModelNet40 benchmark <ref type="bibr" target="#b53">[59]</ref>, which has been saturated. Following the PointNet++ protocol <ref type="bibr" target="#b28">[34]</ref>, our approach (CT+scales) achieves 93.1 in Overall Accuracy (OA), and 90.8 in mean Class Accuracy (mAcc), which is on par or better than other methods that work with point clouds (rather than CAD meshes).</p><p>Single-View Object Reconstruction. In our generation experiments we follow the recently introduced benchmark [48] on 3D object reconstruction. The benchmark is based on ShapeNet <ref type="bibr" target="#b2">[3]</ref> renderings. Unlike previous ShapeNetbased benchmarks for image-based reconstruction that used canonical coordinate frames, the new one argues that the reconstructions should be evaluated in the viewer-based coordinate frame, where the task is more challenging and more realistic. The work <ref type="bibr" target="#b42">[48]</ref> also provides evaluations of several recent methods on image-based reconstruction, as well as the retrieval-based oracle. The dataset consists of ShapeNet <ref type="bibr" target="#b2">[3]</ref>   five random view points. We employ the same train/val/test split as in <ref type="bibr" target="#b42">[48]</ref>.</p><p>In the benchmark, objects were rendered to 224 × 224 pixel images, which we resize to 128 × 128 pixels and then fed to our model. The ground truth is represented as a cloud of 10.000 points in the viewer-aligned coordinate system.</p><p>Our model outputs 8196 points to represent a reconstructed object. Since the protocol requires to predict exactly 10.000 points, we perform the reconstruction twice with different sphere noise and the same style vector z extracted by the convolutional encoder ( <ref type="figure" target="#fig_2">Figure 5</ref>). This results in 16.392 points total from which we randomly select 10.000 points. We note that the ability to sample arbitrarily large number of points is an attractive property of our architecture.</p><p>The main evaluation metric proposed in <ref type="bibr" target="#b42">[48]</ref> is the Fscore computed at a 1% volume distance threshold. The methods are compared with macro-averaged F -score @1% and by the number of classes, in which a method has the highest mean F -score @1%. Our quantitative results are summarized in <ref type="table" target="#tab_2">Table 2</ref>. In is evident that our method outperforms all methods evaluated in <ref type="bibr" target="#b42">[48]</ref> including the retrieval-based oracle very significantly.  <ref type="table">Table 3</ref>: Semantic segmentation intersection-over-union scores on S3DIS Area-5 split. The methods in the left column use the standard protocol with chunking of the scene into blocks, methods marked with † employ KPConv's <ref type="bibr" target="#b46">[52]</ref> protocol. Other protocols are labeled with * . Cloud transformers outperform state-of-the-art in both popular protocols (standard and KPConv's).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indoor</head><p>tion benchmark that consists of large 3D point cloud scenes captured at three different buildings annotated with 13 semantic labels at the point level. The dataset comes with six splits. For the sake of fair comparison, we evaluate on S3DIS using a conventional protocol, established by <ref type="bibr" target="#b27">[33]</ref>, which chunks rooms into 1m×1m blocks. Each block consists of 4096 points and each point is represented with its 3D coordinates and its RGB color, which results in a sixdimensional input vector. In this setting, the average inference time is 0.5 sec on a Tesla P40 GPU card, with only 200 MB memory per chunk used. Following many previous works, we evaluate on the 'Area 5' split and train on the remaining five splits, as <ref type="bibr" target="#b44">[50]</ref> advocate this fold as representative in measuring generalization ability due to being shot in a separate building.</p><p>Since the current state-of-the-art-method JSENet [13], uses a different protocol <ref type="bibr" target="#b46">[52]</ref>), we also evaluate our model using 'KPConv' protocol. In it, at each step an input point cloud is dynamically sampled from a sphere of radius 2m. Each point cloud contains up to 8192 points. During evaluation, the same data strategy is applied together with voting.</p><p>In the standard 1m × 1m protocol xyz spatial coordinates are augmented with random rotation, anisotropic scale, jitter and shifts. For color, on thhe other hand, we use chromatic autocontrast, jitter and translation (following <ref type="bibr" target="#b4">[5]</ref>). As for the 'KPConv' protocol, we follow the original paper and augment point clouds with anisotropic random scaling, spatial gaussian jittering, random rotations around the z-axis and random color dropping. <ref type="table">Table 3</ref> shows that for the semantic segmentation task our method outperforms state-of-the-art in both considered protocols. Point Cloud Completion. Finally, we evaluate Cloud Transformer Inpainter on the ShapeNet-based benchmark <ref type="bibr" target="#b59">[65]</ref> for high-resolution point cloud completion. The benchmark is composed of the eight largest ShapeNet categories (airplane, cabinet, car, chair, lamp, sofa, table, vessel). This makes 30974 unique objects in total. In each category 100 of unique objects is reserved for validation and 150 for testing. The dataset consists of (P part , P gt ) pairs, Methods mAvg. F-Score@1% mAvg. CD AtlasNet <ref type="bibr">[11]</ref> 0.616 4.523 PCN <ref type="bibr" target="#b59">[65]</ref> 0.695 4.016 FoldingNet <ref type="bibr" target="#b57">[63]</ref> 0.322 7.142 TopNet <ref type="bibr" target="#b45">[51]</ref> 0.503 5.154 MSN <ref type="bibr" target="#b20">[26]</ref> 0.705 4.758 GRNet <ref type="bibr" target="#b54">[60]</ref> 0.708 2.723 CT (ours) 0.752 3.392 where P part is a partial point cloud and P gt is a complete point cloud. Partial point clouds are obtained via 2.5D depth image back-projection, taken from a random view. There are eight random views generated per each training object. The partial cloud consists of no more than 2048 points, while the complete point cloud consists of 16.384 points.</p><p>In contrast to the image-based reconstruction, both partial and complete point clouds are provided in the object-based coordinate system. We predict a high-resolution reconstruction with 16.384 points in a single pass of our Cloud Transformer Inpainter network. Our model produces detailed reconstructions with complex geometries (see <ref type="figure" target="#fig_0">Figure 1</ref>). Regarding quantitative evaluation of our method, we report both F-Score@1% and CD (Chamfer Distance), both of them computed with the ground truth 16.384 point clouds. Following <ref type="bibr" target="#b42">[48]</ref>, we argue that the F-Score@1% should be regarded as a primal metric of the shape prediction quality, and in this metric our method again beats state-of-the-art considerably ( <ref type="table" target="#tab_4">Table 4</ref>).</p><p>Ablation Study. We also perform an ablation study to justify our architecture choices. We consider the following ablations:</p><p>• Linear key prediction: We replace key prediction procedure with a linear point-wise layer d, followed by BatchNormalization. Using the notations form 3,</p><formula xml:id="formula_7">k i = d(x i ).</formula><p>• Mean agg. and Sum agg.: The aggregation method in the rasterization step is replaced with element-wise mean and sum correspondingly. Note, in the latter case (sum) it makes our operation similar to the splatting, used in SplatNet <ref type="bibr" target="#b36">[42]</ref>.</p><p>• Non-learnable keys: In this ablation we use different non-learnable projections of the input positions as keys. More precisely,</p><formula xml:id="formula_8">k i = T (p i ), where T ∈ SO(3)</formula><p>is a fixed random transformation and no deep residual predicted. While this variant performs on par with linear key prediction for segmentation, it performs marginally better on classification.</p><p>• We also train an architecture without planar heads to see if using only volumetric heads might be sufficient.</p><p>• In the Coarser feature maps experiment the spatial dimensions of feature maps are halved.</p><p>• No multihead: We ablate our multi-headed architecture by replacing 16 headed CMHCT blocks with a single-headed block, where we increase the channel dimension 8 times to keep the model's capacity intact.</p><p>• Finally, we consider 2x shallower and architectures, replacing four CMHCT blocks with two CMHCT blocks.  Observing the advantage of the full architecture over the shallower architecture in the S3DIS case, we have also evaluated a 2x deeper architecture with eight CMHCT blocks, achieving 64.1 mIOU score.</p><p>We have also evaluated the importance of the gradient balancing trick. On the S3DIS an ablation without gradient balancing trick achieved 62.8 mIOU. More importantly, when we tried to run the linear key prediction variant without the gradient balancing, the learning diverged for all reasonable learning rates, revealing the importance of gradient balancing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a new block for neural architectures that process point clouds. Our block extends the ideas of Spatial Transformers, Transformers, and Multi-View CNNs on neural point cloud processing. Based on the new block, we have presented architectures for point cloud semantic segmentation, point cloud classification, point cloud completion and single-image based geometry reconstruction that achieve state-of-the-art results. The code of our approach will be released prior to the publication. <ref type="figure">Figure 8</ref>: Results of image-based reconstruction. Our network performs the reconstruction by sampling points from the unit sphere S 2 (top) and then "folding" this set into a point cloud corresponding to the input image. Here, we show the point clouds, with points colored according to their initial positions on the sphere (red, green, and blue values are used to color-code each Cartesian coordinate). The color-coding reveals shape correspondences within and across classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional experimental results</head><p>In <ref type="figure">Figure 8</ref>, we show the "foldings" of the input sphere in our Single-View reconstruction experiments. The resulting point colors represent spatial coordinates of their input positions on the sphere. The shape correspondences learned implicitly during training are revealed. Additionally, we provide per-class results for our experiments in <ref type="table">Table 7</ref>, <ref type="table" target="#tab_9">Table 8</ref>, <ref type="table" target="#tab_10">Table 9</ref>, <ref type="table" target="#tab_11">Table 10 and Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental details</head><p>We train our models using the standard ADAM optimizer <ref type="bibr" target="#b15">[21]</ref> with the learning rate 1e−4 for generative tasks (image-based reconstruction and point cloud completion) and with the learning rate 1e−3 for recognition tasks (semantic segmentation and classification). We halve the learning rate every 100k iterations for image-based reconstruction and point cloud inpainting experiments. For classification and semantic segmentation experiments, the learning rate is decayed by 0.7 and every 25k iterations.</p><formula xml:id="formula_9">3D CNN 2D CNN Res3D(in=32, out=64) Res2D(in=16, out=32) MaxPool(2) MaxPool(2) Res3D(in=64, out=64) Res2D(in=32, out=64) MaxPool(2) MaxPool(2) Res3D(in=64, out=64) Res2D(in=64, out=64)</formula><p>AvgPool AvgPool For classification and segmentation experiments we use batch size 8 per GPU and 2 GPUs in total. For image-based reconstruction and point cloud completion, the batch size 4 and 2 are used respectively. In both cases, we train the models on eight GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-Headed Cloud Pooling</head><p>Here we provide more details on our multi-headed cloud pooling layer used in our classification model. To solve the classification task, we introduce a multiheaded pooling layer <ref type="figure" target="#fig_4">(Figure 9</ref>). This is needed, because unlike other tasks, the output of the classification process is a global vector of probabilities. We devise this layer to be as similar to our standard layers as possible.</p><p>Thus, similarly to the regular MHCT layer, the new layer performs multiple rasterizations onto 2D and 3D feature maps of spatial size 8 and 16 respectively. The channel dimension of each head is 32 for three-dimensional heads and 16 for two-dimensional heads. Afterward, the resulting feature maps are processed with three standard convolutional residual blocks, each interchanged with a max-pooling. The architectures of these 2D and 3D ConvNets are shown in <ref type="table" target="#tab_7">Table 6</ref>. The residual path of Res(in, out) consists of two convolutional layers Conv(in, out) and Conv(out, out) with a 3 × 3 (or 3 × 3 × 3) spatial filters, each followed by a BatchNormalization and ReLU activation.</p><p>The resulting vectors are aggregated across the heads via concatenation and processed with a dense layer to form a final classification vector k class .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Gradient Balancing</head><p>Below we provide a more thorough discussion of our gradient balancing trick introduced in the main paper and its necessity.</p><p>Problem discussion Ultimately, the problem can be pinpointed to the fact that as the key k i moves from the top-left to the bottom-right of a certain grid cell thus traversing only 1/w-th of its variation range, the assignment weight of v i to the bottom-right corner changes from 0 to 1 (i.e traverses the full variation range). This means that gradients w.r.t. keys k i in our architecture will always be roughly w times stronger than those w.r.t. values v i . We justify this trick by an exact derivation. Further we denote a target loss function as L and assume the spatial feature dimension to be w−1.</p><p>Lemma 1. Let k = (k 0 , k 1 ) ∈ [0, 1] 2 be a key, which is typically an output of the network's key prediction branch in our Cloud Transform block. Let b be a vector of bilinear weights of k inside the enclosing cell, as in equation <ref type="formula" target="#formula_2">(1)</ref> (main paper). Then:</p><formula xml:id="formula_10">∂b ∂k = w ·     (w · k1− w · k1 ) (w · k0− w · k0 ) −(w · k1− w · k1 ) −(w · k0− w · k0 ) −(w · k1− w · k1 ) −(w · k0− w · k0 ) (w · k1− w · k1 ) (w · k0− w · k0 )     (4)</formula><p>The derivation is straightforward, given the formula for the bilinear weights b = b 00 , b 01 , b 10 , b 11 :</p><formula xml:id="formula_11">b 00 = (w·k 0 i − h 1 )(w·k 1 i − w 1 ) b 01 = −(w·k 0 i − h 1 )(w·k 1 i − w 0 ) b 10 = −(w·k 0 i − h 0 )(w·k 1 i − w 1 ) b 11 = (w·k 0 i − h 0 )(w·k 1 i − w 0 )<label>(5)</label></formula><p>To ease the notations, we denote the matrix above as D, i.e ∂b ∂k = w · D.</p><p>Lemma 2. Let X = ∂L ∂bi and Var (X) be its variance. Then the following inequality on matrix spectral norms holds:</p><formula xml:id="formula_12">||D Var (X) D T || 2 ≥ 1 2 || Var (X) || 2<label>(6)</label></formula><p>Proof.</p><formula xml:id="formula_13">2||D Var (X) D T || 2 ≥ tr D Var (X) D T trace cyclic prop. ≥ tr DD T Var (X) ≥ ||DD T Var (X) || 2 ≥ σ n (DD T )|| Var (X) || 2<label>(7)</label></formula><p>, where σ n (DD T ) is the smallest non-zero singular value of D T D.</p><p>Note that D has a special form, given −1 ≤ a, b ≤ 0: By a straightforward derivation, one can find two nonzero singular values σ 2 = 1 and σ 1 = (2a + 1) 2 + (2b + 1) 2 + 1 of DD T . And σ 1 &gt; σ 1 , therefore:</p><formula xml:id="formula_14">D =     a b −(1 + a) −b −a −(1 + b) 1 + a 1 + b    <label>(8)</label></formula><formula xml:id="formula_15">||D Var (X) D T || 2 ≥ 1 2 || Var (X) || 2<label>(9)</label></formula><p>This, informally, means that D does not decrease gradient variance Var (X) too much. In the case of 3D rasterization operation, the computation is analogous.</p><p>To sum up the results above, the back-propagation from bilinear weights b i to k i has a form:</p><formula xml:id="formula_16">∂L ∂k i = ∂b i ∂k i T · ∂L ∂b i = w · D T · ∂L ∂b i<label>(10)</label></formula><p>From the lemma 2 we know that multiplication by D does not decrease variance's Var (X) spread more than by two times, while w takes typical vales between 16 and 128.</p><p>Intuitively, the gradients' variance is scaled up by w at each layer during the backpropagation through the keys. Therefore, given a network with d cloud transform layers, the gradient variance would "explode" as w d . We have observed such explosions experimentally.  <ref type="table">Table 7</ref>: Semantic segmentation intersection-over-union scores on S3DIS Area-5 split. The models without any marks use the standard protocol with chunking of the scene into blocks, while the models with † employ KPConv's <ref type="bibr" target="#b46">[52]</ref> protocol. The rest protocols are labeled with * . Per-class results are provided.    <ref type="table">Table 11</ref>: F-score evaluation (@1%) in the viewer-centered mode, per-class results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sample outputs of cloud transformers across four diverse cloud processing tasks including recognition tasks (left) and generation tasks (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The architecture used for image-based reconstruction consists of a convolutional style encoder (1) and a generator<ref type="bibr" target="#b1">(2)</ref>. The generator is built of a linear layer, followed by four cascaded multi-headed cloud transform blocks (green), conditioned via adaptive instance normalizations on the style vector produced by the encoder (orange). The input to the generator is sampled from a uniform sphere S 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Our Cloud Transformer Inpainter differs from our singleview reconstruction model in two ways. First, the encoder part is taken from the classifier model, producing a style vector for adaptive normalization layers of the generator. Secondly, the input to the generator consists of the partial point cloud as wekk as of the points sampled from the unit sphere S 2 for "unknown" points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>The Multi-Headed Cloud Pooling layer operates as a Multi-Headed Cloud Transform layer except for the de-rasterization step. Instead of de-rasterization, a compact 2D or 3D ConvNet is applied producing a single vector as an output of each head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>models, where each model belongs to one of 55 classes.</figDesc><table><row><cell></cell><cell cols="2">overall acc. mean class acc.</cell></row><row><cell>3DmFV [2]</cell><cell>63</cell><cell>58.1</cell></row><row><cell>PointNet [33]</cell><cell>68.2</cell><cell>63.4</cell></row><row><cell>SpiderCNN [62]</cell><cell>73.7</cell><cell>69.8</cell></row><row><cell>PointNet++ [34]</cell><cell>77.9</cell><cell>75.4</cell></row><row><cell>DGCNN [58]</cell><cell>78.1</cell><cell>73.6</cell></row><row><cell>PointCNN [25]</cell><cell>78.5</cell><cell>75.1</cell></row><row><cell>DRNet [36]</cell><cell>80.3</cell><cell>-</cell></row><row><cell>GFNet [35]</cell><cell>80.5</cell><cell>77.8</cell></row><row><cell>DI-PointCNN [66]</cell><cell>81.3</cell><cell>79.6</cell></row><row><cell>CT (ours)</cell><cell>85.0</cell><cell>80.7</cell></row><row><cell>CT (ours) + scales</cell><cell>85.5</cell><cell>83.1</cell></row><row><cell cols="3">Table 1: Classification results on ScanObjectNN. Our method out-</cell></row><row><cell cols="3">performs the others both in terms of overall accuracy and mean</cell></row><row><cell>class accuracy.</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">mAvg. F -score@1% Top-1 cat.</cell></row><row><cell>AtlasNet [11]</cell><cell>0.252</cell><cell>2</cell></row><row><cell>Matryoshka [37]</cell><cell>0.217</cell><cell>0</cell></row><row><cell>OGN [47]</cell><cell>0.264</cell><cell>1</cell></row><row><cell>Retrieval</cell><cell>0.236</cell><cell>0</cell></row><row><cell>Retrieval (oracle)</cell><cell>0.290</cell><cell>3</cell></row><row><cell>CT (ours)</cell><cell>0.359</cell><cell>49</cell></row><row><cell></cell><cell></cell><cell>Each object has been rendered with ShapeNet-Viewer from</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>F-score evaluation (@1%) of 3D shape reconstruction in the viewer-based coordinate frame, averaged by categories. The cloud transformer outperforms other methods including the re- trieval based oracle considerably.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Point completion results on ShapeNet compared using F-Score@1% and CD. Note that both of them are computed on 16,384 points and macro-averaged.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Ablation study on S3DIS semantic segmentation and ScanObjectNN classification. See text for discussion.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Our mini-CNN architectures, in the 2D and 3D cases.</figDesc><table><row><cell>Note that a different ConvNet applied per each head indepen-</cell></row><row><cell>dently.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>The balancing trick discussed above successfully fixes this problem and allows training deep architectures built out of Cloud Transformer blocks.</figDesc><table><row><cell>Method</cell><cell cols="2">ceil. floor wall beam</cell><cell>col.</cell><cell cols="6">wind. door chair table book. sofa board clut.</cell></row><row><cell>Pointnet</cell><cell>88.8 97.3 69.8</cell><cell>0.1</cell><cell>3.9</cell><cell>46.3</cell><cell>10.8 52.6 58.9</cell><cell>40.3</cell><cell>5.9</cell><cell>26.4</cell><cell>33.2</cell></row><row><cell>SegCloud*</cell><cell>90.1 96.1 69.9</cell><cell>0.0</cell><cell>18.4</cell><cell>38.4</cell><cell>23.1 75.9 70.4</cell><cell>58.4</cell><cell>40.9</cell><cell>13.0</cell><cell>41.6</cell></row><row><cell>Eff. 3D Conv</cell><cell>79.8 93.9 69.0</cell><cell>0.2</cell><cell>28.3</cell><cell>38.5</cell><cell>48.3 71.1 73.6</cell><cell>48.7</cell><cell>59.2</cell><cell>29.3</cell><cell>33.1</cell></row><row><cell>TangentConv</cell><cell>90.5 97.7 74.0</cell><cell>0.0</cell><cell>20.7</cell><cell>39.0</cell><cell>31.3 69.4 77.5</cell><cell>38.5</cell><cell>57.3</cell><cell>48.8</cell><cell>39.8</cell></row><row><cell>RNN Fusion</cell><cell>92.3 98.2 79.4</cell><cell>0.0</cell><cell>17.6</cell><cell>22.8</cell><cell>62.1 74.4 80.6</cell><cell>31.7</cell><cell>66.7</cell><cell>62.1</cell><cell>56.7</cell></row><row><cell>SPGraph*</cell><cell>89.4 96.9 78.1</cell><cell>0.0</cell><cell>42.8</cell><cell>48.9</cell><cell>61.6 84.7 75.4</cell><cell>69.8</cell><cell>52.6</cell><cell>2.1</cell><cell>52.2</cell></row><row><cell>ParamConv</cell><cell>92.3 96.2 75.9</cell><cell>0.3</cell><cell>6.0</cell><cell>69.5</cell><cell>63.5 66.9 65.6</cell><cell>47.3</cell><cell>68.9</cell><cell>59.1</cell><cell>46.2</cell></row><row><cell>PointCNN</cell><cell>92.3 98.2 79.4</cell><cell>0.0</cell><cell>17.6</cell><cell>22.7</cell><cell>62.1 74.4 80.6</cell><cell>31.7</cell><cell>66.7</cell><cell>62.1</cell><cell>56.7</cell></row><row><cell cols="2">CT (ours) std. prot. 94.4 98.2 85.4</cell><cell>0.0</cell><cell>23.6</cell><cell>47.4</cell><cell>71.0 87.3 77.5</cell><cell>66.0</cell><cell>49.3</cell><cell>71.1</cell><cell>57.8</cell></row><row><cell>Minkowski32*</cell><cell>91.7 98.7 86.1</cell><cell>0.0</cell><cell>34.0</cell><cell>48.9</cell><cell>62.4 89.8 81.5</cell><cell>74.8</cell><cell>47.2</cell><cell>74.4</cell><cell>58.5</cell></row><row><cell>KPConv †</cell><cell>92.8 97.3 82.4</cell><cell>0.0</cell><cell>23.9</cell><cell>58.0</cell><cell>69.0 91.0 81.5</cell><cell>75.3</cell><cell>75.4</cell><cell>66.7</cell><cell>58.9</cell></row><row><cell>JSENet †</cell><cell>93.8 97.0 83.0</cell><cell>0.0</cell><cell>23.2</cell><cell>61.3</cell><cell>71.6 89.9 79.8</cell><cell>75.6</cell><cell>72.3</cell><cell>72.7</cell><cell>60.4</cell></row><row><cell>CT † (ours)</cell><cell>94.2 97.7 82.7</cell><cell>0.0</cell><cell>34.4</cell><cell>62.8</cell><cell>68.4 89.8 80.4</cell><cell>78.2</cell><cell>61.4</cell><cell>67.7</cell><cell>64.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Per-class classification results on ScanObjectNN.</figDesc><table><row><cell></cell><cell>bag</cell><cell>bin</cell><cell cols="7">box cabinet chair desk display door shelf table bed pillow sink sofa toilet</cell></row><row><cell>3DmFV</cell><cell cols="3">39.8 62.8 15.0</cell><cell>65.1</cell><cell cols="2">84.4 36.0</cell><cell>62.3</cell><cell>85.2 60.6 66.7 51.8</cell><cell>61.9</cell><cell>46.7 72.4 61.2</cell></row><row><cell>PointNet</cell><cell cols="3">36.1 69.8 10.5</cell><cell>62.6</cell><cell cols="2">89.0 50.0</cell><cell>73.0</cell><cell>93.8 72.6 67.8 61.8</cell><cell>67.6</cell><cell>64.2 76.7 55.3</cell></row><row><cell>SpiderCNN</cell><cell cols="3">43.4 75.9 12.8</cell><cell>74.2</cell><cell cols="2">89.0 65.3</cell><cell>74.5</cell><cell>91.4 78.0 65.9 69.1</cell><cell>80.0</cell><cell>65.8 90.5 70.6</cell></row><row><cell>PointNet++</cell><cell cols="3">49.4 84.4 31.6</cell><cell>77.4</cell><cell cols="2">91.3 74.0</cell><cell>79.4</cell><cell>85.2 72.6 72.6 75.5</cell><cell>81.0</cell><cell>80.8 90.5 85.9</cell></row><row><cell>DGCNN</cell><cell cols="3">49.4 82.4 33.1</cell><cell>83.9</cell><cell cols="2">91.8 63.3</cell><cell>77.0</cell><cell>89.0 79.3 77.4 64.5</cell><cell>77.1</cell><cell>75.0 91.4 69.4</cell></row><row><cell>PointCNN</cell><cell cols="3">57.8 82.9 33.1</cell><cell>83.6</cell><cell cols="2">92.6 65.3</cell><cell>78.4</cell><cell>84.8 84.2 67.4 80.0</cell><cell>80.0</cell><cell>72.5 91.9 71.8</cell></row><row><cell>GFNet</cell><cell cols="3">59.0 84.4 44.4</cell><cell>78.2</cell><cell>92.1</cell><cell>66</cell><cell>91.2</cell><cell>91.0 86.7 70.4 82.7</cell><cell>78.1</cell><cell>72.5 92.4 77.6</cell></row><row><cell>DI-PointCNN</cell><cell cols="3">65.1 80.9 62.4</cell><cell>80.4</cell><cell cols="2">90.5 78.0</cell><cell>86.8</cell><cell>88.1 84.6 67.0 84.5</cell><cell>82.8</cell><cell>73.3 95.2 74.1</cell></row><row><cell>CT (ours)</cell><cell cols="3">50.6 86.9 52.6</cell><cell>87.4</cell><cell cols="2">96.2 77.3</cell><cell>85.8</cell><cell>93.8 88.4 80.0 76.4</cell><cell>87.6</cell><cell>81.7 94.3 89.4</cell></row><row><cell cols="4">CT (ours) + scales 57.8 89.9 45.9</cell><cell>87.4</cell><cell cols="2">95.6 84.7</cell><cell>84.8</cell><cell>92.9 88.4 77.4 82.7</cell><cell>85.7</cell><cell>85.8 92.9 95.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Per-class point completion results on ShapeNet compared using F-Score@1%. Note that the F-Score@1% is computed on 16,384 points. Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan Russell, and Mathieu Aubry. AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2018. [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. Proc. ICCV, pages 1026-1034, 2015. [13] Zeyu Hu, Mingmin Zhen, Xuyang Bai, Hongbo Fu, and Chiew-lan Tai. Jsenet: Joint semantic segmentation and edge detection network for 3d point clouds. In ECCV, 2020. [14] Xun Huang and Serge J. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. Proc. ICCV, pages 1510-1519, 2017. [15] Xun Huang and Serge J. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. Proc. ICCV, pages 1510-1519, 2017. [16] Eldar Insafutdinov and Alexey Dosovitskiy. Unsupervised learning of shape and pose with differentiable point clouds.</figDesc><table><row><cell>Methods</cell><cell cols="2">Airplane Cabinet</cell><cell>Car</cell><cell cols="2">Chair Lamp Sofa Table Watercraft</cell></row><row><cell>AtlasNet</cell><cell>0.845</cell><cell>0.552</cell><cell cols="2">0.630 0.552 0.565 0.500 0.660</cell><cell>0.624</cell></row><row><cell>PCN</cell><cell>0.881</cell><cell>0.651</cell><cell cols="2">0.725 0.625 0.638 0.581 0.765</cell><cell>0.697</cell></row><row><cell>FoldingNet</cell><cell>0.642</cell><cell>0.237</cell><cell cols="2">0.382 0.236 0.219 0.197 0.361</cell><cell>0.299</cell></row><row><cell>TopNet</cell><cell>0.771</cell><cell>0.404</cell><cell cols="2">0.544 0.413 0.408 0.350 0.572</cell><cell>0.560</cell></row><row><cell>MSN</cell><cell>0.885</cell><cell>0.644</cell><cell cols="2">0.665 0.657 0.699 0.604 0.782</cell><cell>0.708</cell></row><row><cell>GRNet</cell><cell>0.843</cell><cell>0.618</cell><cell cols="2">0.682 0.673 0.761 0.605 0.751</cell><cell>0.750</cell></row><row><cell>CT (ours)</cell><cell>0.921</cell><cell>0.652</cell><cell cols="2">0.733 0.710 0.774 0.628 0.811</cell><cell>0.789</cell></row><row><cell>[11]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Per-class point completion results on ShapeNet compared using Chamfer Distance (CD) with L2 norm computed on 16.384 points and multiplied by 10 4 .</figDesc><table><row><cell>Methods</cell><cell cols="2">Airplane Cabinet</cell><cell>Car</cell><cell cols="2">Chair Lamp Sofa Table Watercraft</cell></row><row><cell>AtlasNet</cell><cell>1.753</cell><cell>5.101</cell><cell cols="2">3.237 5.226 6.342 5.990 4.359</cell><cell>4.177</cell></row><row><cell>PCN</cell><cell>1.400</cell><cell>4.450</cell><cell cols="2">2.445 4.838 6.238 5.129 3.569</cell><cell>4.06</cell></row><row><cell>FoldingNet</cell><cell>3.151</cell><cell>7.943</cell><cell cols="2">4.676 9.225 9.234 8.895 6.691</cell><cell>7.32</cell></row><row><cell>TopNet</cell><cell>2.152</cell><cell>5.623</cell><cell cols="2">3.513 6.346 7.502 6.949 4.784</cell><cell>4.359</cell></row><row><cell>MSN</cell><cell>1.543</cell><cell>7.249</cell><cell cols="2">4.711 4.539 6.479 5.894 3.797</cell><cell>3.853</cell></row><row><cell>GRNet</cell><cell>1.531</cell><cell>3.620</cell><cell cols="2">2.752 2.945 2.649 3.613 2.552</cell><cell>2.122</cell></row><row><cell>CT (ours)</cell><cell>1.059</cell><cell>4.592</cell><cell cols="2">2.581 4.163 3.294 5.816 3.360</cell><cell>2.274</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhak</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anath</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3145" to="3152" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Christopher Bongsoo Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3070" to="3079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haoqiang Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2463" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIS-TATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2802" to="2812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Permutohedral lattice cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop Track</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on xtransformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Morphing and sampling network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pointvoxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1578" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">A</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IROS</title>
		<meeting>IROS</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Geometric backprojection network for point cloud classification. arXiv: Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">S</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dense-resolution network for point cloud classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matryoshka networks: Predicting 3d geometry via nested shape layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">What do single-view 3d reconstruction networks learn?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyne</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">Bongsoo</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segcloud</surname></persName>
		</author>
		<title level="m">Semantic segmentation of 3d point clouds. International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Topnet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6410" to="6419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4105" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<editor>Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Spatial transformer for 3d points. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Grnet: Gridding residual network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">3d recurrent neural networks with context fusion for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Point cloud classification model based on a dual-input deep network framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="55991" to="55999" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Efficient convolutions for real-time semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
