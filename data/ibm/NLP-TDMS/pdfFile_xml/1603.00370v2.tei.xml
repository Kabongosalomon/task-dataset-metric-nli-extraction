<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Metric Learning via Weighted Approximate Rank Component Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cijo</forename><surname>Jose</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Instituté Ecole Polytechnique Fédérale de Lausanne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Fleuret</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Instituté Ecole Polytechnique Fédérale de Lausanne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Metric Learning via Weighted Approximate Rank Component Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We are interested in the large-scale learning of Mahalanobis distances, with a particular focus on person re-identification. We propose a metric learning formulation called Weighted Approximate Rank Component Analysis (WARCA). WARCA optimizes the precision at top ranks by combining the WARP loss with a regularizer that favors orthonormal linear mappings, and avoids rank-deficient embeddings. Using this new regularizer allows us to adapt the large-scale WSABIE procedure and to leverage the Adam stochastic optimization algorithm, which results in an algorithm that scales gracefully to very large data-sets. Also, we derive a kernelized version which allows to take advantage of state-of-the-art features for re-identification when data-set size permits kernel computation. Benchmarks on recent and standard re-identification data-sets show that our method beats existing state-of-the-art techniques both in term of accuracy and speed. We also provide experimental analysis to shade lights on the properties of the regularizer we use, and how it improves performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Metric learning methods aim at learning a parametrized distance function from a labeled set of samples, so that under the learned distance, samples with the same labels are nearby and samples with different labels are far apart <ref type="bibr" target="#b0">[1]</ref>. Many fundamental questions in computer vision such as "How to compare two images? and for what information?" boil down to this problem. Among them, person re-identification is the problem of recognizing individuals at different physical locations and times, on images captured by different devices.</p><p>It is a challenging problem which recently received a lot of attention because of its importance in various application domains such as video surveillance, biometrics and behavior analysis <ref type="bibr">[2]</ref>.</p><p>The performance of person re-identification systems relies mainly on the image feature representation and the distance measure used to compare them. Hence the research in the field has focused either on designing features <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b3">5]</ref> or on learning a distance function from a labeled set of images <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b8">10]</ref>.</p><p>It is difficult to analytically design features that are invariant to the various nonlinear transformations that an image undergoes such as illumination, viewpoint, pose changes and occlusion. Furthermore, even if such features were provided, the standard arXiv:1603.00370v2 [cs.CV] 23 Mar 2016</p><p>Euclidean metric would not be adequate as it does not take into account dependencies on the feature representation. This motivates the use of metric learning for person reidentification.</p><p>Re-identification models are commonly evaluated by the cumulative match characteristic (CMC) curve. This measure indicates how the matching performance of the algorithm improves as the number of returned image increases. Given a matching algorithm and a labeled test set, each image is compared against all the others, and the position of the first correct match is recorded. The CMC curve indicates for each rank the fraction of test samples which had that rank or better. A perfect CMC curve would reach the value 1 for rank #1, that is the best match is always of the correct identity.</p><p>In this paper we are interested in learning a Mahalanobis distance by minimizing a weighted rank loss such that the precision at the top rank positions of the CMC curve is maximized. When learning the metric, we directly learn the low-rank projection matrix instead of the PSD matrix because of the computational efficiency and the scalability to high dimensional datasets (see § 3.1). But naively learning the low-rank projection matrix suffers from the problem of matrix rank degeneration and non-isolated minima <ref type="bibr" target="#b9">[11]</ref>. We address this problem by using a simple regularizer which approximately enforces the orthonormality of the learned matrix very efficiently (see § 3.2). We extend the WARP loss <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b9">11]</ref> and combine it with our approximate orthonormal regularizer to derive a metric learning algorithm which approximately minimizes a weighted rank loss efficiently using stochastic gradient descent (see <ref type="bibr">§ 3.3)</ref>.</p><p>We extend our model to kernel space to handle distance measures which are more natural for the features we are dealing with (see <ref type="bibr">§ 3.4)</ref>. We also show that in kernel space SGD can be carried out more efficiently by using preconditioning <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b4">6]</ref>.</p><p>We validate our approach on nine challenging person re-identification datasets: Market-1501 <ref type="bibr" target="#b13">[15]</ref>, CUHK03 <ref type="bibr" target="#b14">[16]</ref>, OpeReid <ref type="bibr" target="#b15">[17]</ref>, CUHK01 <ref type="bibr" target="#b16">[18]</ref>, VIPeR <ref type="bibr" target="#b17">[19]</ref>, CAVIAR <ref type="bibr" target="#b1">[3]</ref>, 3DPeS <ref type="bibr" target="#b18">[20]</ref>, iLIDS <ref type="bibr" target="#b19">[21]</ref> and PRI450s <ref type="bibr" target="#b20">[22]</ref>, where we outperform other metric learning methods proposed in the literature, both in speed and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Metric learning is a well studied research problem <ref type="bibr" target="#b21">[23]</ref>. Most of the existing approaches have been developed in the context of the Mahalanobis distance learning paradigm <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7]</ref>. This consists in learning distances of the form:</p><formula xml:id="formula_0">D 2 M (x i , x j ) = (x i − x j ) T M (x i − x j ),<label>(1)</label></formula><p>where M is a positive semi-definite matrix. Based on the way the problem is formulated the algorithms for learning such distances involve either optimization in the space of positive semi-definite (PSD) matrices, or learning the projection matrix W , in which case M = W T W . Large margin nearest neighbors <ref type="bibr" target="#b0">[1]</ref> (LMNN) is a metric learning algorithm designed to maximize the performance of k-nearest neighbor classification in a large margin framework. Information theoretic metric learning <ref type="bibr" target="#b23">[25]</ref> (ITML) exploits the relationship between the Mahalanobis distance and Gaussian distributions to learn a metric by minimizing the KL-divergence with a metric prior. Many researchers have applied LMNN and ITML to re-identification problem with varying degree of success <ref type="bibr" target="#b20">[22]</ref>.</p><p>Pairwise Constrained Component Analysis (PCCA) <ref type="bibr" target="#b4">[6]</ref> is a metric learning method that learns the low rank projection matrix W in the kernel space from sparse pairwise constraints. Xiong et al. <ref type="bibr" target="#b7">[9]</ref> extended PCCA with a L 2 regularization term and showed that it further improves the performance.</p><p>Köstinger et al. <ref type="bibr" target="#b5">[7]</ref> proposed the KISS ("Keep It Simple and Straight forward") metric learning abbreviated as KISSME. Their method enjoys very fast training and they show good empirical performance and scaling properties along the number samples. However this method suffers from of the Gaussian assumptions on the model.</p><p>Li et al. <ref type="bibr" target="#b6">[8]</ref> consider learning a local thresholding rule for metric learning. This method is computationally expensive to train, even with as few as 100 dimensions. Their paper discusses solving the problem in dual to decouple the dependency on the dimension but in practice it is solved in primal form with off-the-shelf solvers.</p><p>The performance of many kernel-based metric learning-based methods for person re-identification was evaluated in <ref type="bibr" target="#b7">[9]</ref>. In particular the authors evaluated PCCA <ref type="bibr" target="#b4">[6]</ref>, variants of kernel Fisher discriminant analysis (KFDA) and reported that the KFDA variants consistently out-perform all other methods. The KFDA variants they investigated were Local Fisher Discriminant Analysis (LFDA) and Marginal Fisher Discriminant Analysis(MFA).</p><p>Recently few metric learning algorithms have been proposed for person re-identification. Chen et al. <ref type="bibr" target="#b24">[26]</ref> attempt to learn a metric in the polynomial feature map exploiting the relationship between Mahalanobis metric and the polynomial features. Ahmed et al. <ref type="bibr" target="#b25">[27]</ref> propose a deep learning model which learns the features as well as the metric jointly. Liao et al. <ref type="bibr" target="#b3">[5]</ref> propose XQDA exploiting the benefits of Fisher discriminant analysis and KISSME to learn a metric. However like FDA and KISSME, XQDA's modeling power is limited because of the Gaussian assumptions on the data. In another work Liao et al. <ref type="bibr" target="#b8">[10]</ref> apply accelerated proximal gradient descent (APGD) to a Mahalanobis metric under a logistic loss similar to the loss of PCCA <ref type="bibr" target="#b4">[6]</ref>. The proximal step is the projection on to the PSD cone. They also use asymmetric sample weighting to exploit the imbalance between the positive and negative pairs. The application of APGD makes this model converge fast compared to existing batch metric learning algorithms but still it suffers from scalability issues because all the pairs are required to take one gradient step and the projection step on to the PSD cone is computationally expensive.</p><p>None of the above mentioned techniques explicitly models the objective that we are looking for in person re-identification, that is to optimize a weighted rank measure. We show that modeling this in the metric learning objective improves the performance. We address scalability through stochastic gradient descent and our model naturally eliminates the need for asymmetric sample weighting as we use triplet based loss function.</p><p>There is an extensive body of work on optimizing ranking measures such as AUC, precision at k, F 1 score, etc. Most of this work focuses on learning a linear decision boundary in the original input space, or in the feature space for ranking a list of items based on the chosen performance measure. A well known such model is the structural SVM <ref type="bibr" target="#b26">[28]</ref>. In contrast here we are interested in ranking pairs of items by learning a metric. A related work by McFee et al. <ref type="bibr" target="#b27">[29]</ref> studies metric learning with different rank measures in the structural SVM framework. Wu et al. <ref type="bibr" target="#b28">[30]</ref> used this framework to do person re-identification by optimizing the mean reciprocal rank criterion. Outside the direct scope of metric learning from a single feature representation, Paisitkriangkrai et al. <ref type="bibr" target="#b29">[31]</ref> developed an ensemble algorithm to combine different base metrics in the structural SVM framework which leads to excellent performance for re-identification. Such an approach is complementary to ours, as combining heterogeneous feature representations requires a separate additional level of normalization or the combination with a voting scheme.</p><p>We use the WARP loss from WSABIE <ref type="bibr" target="#b11">[13]</ref>, initially proposed for large-scale image annotation problem, that is a multi-label classification problem. WSABIE simultaneously optimizing the precision in the top rank positions and learn a low dimensional joint embedding for both images and annotations. This work reports excellent empirical results in terms of accuracy, computational efficiency, and memory footprint.</p><p>The work that is closely related to us is FRML <ref type="bibr" target="#b9">[11]</ref> where they learn a Mahalanobis metric by optimizing the WARP loss function with stochastic gradient descent. However there are some key differences with our approach. FRML is a linear method using L 2 or LMNN regularizer, and relies on an expensive projection step in the SGD. Beside, this projection requires to keep a record all the gradients in the mini-batch, which results in high memory footprint. The rationale for the projection step is to accelerate the SGD because directly optimizing low rank matrix may result in rank deficient matrix and thus results in non-isolated minimizers which might generalizes poorly to unseen samples. We propose a computationally cheap solution to this problem by using a regularizer which approximately enforces the rank of the learned matrix very efficiently.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Weighted Approximate Rank Component Analysis</head><p>This section presents our metric learning algorithm, Weighted Approximate Rank Component Analysis (WARCA). <ref type="table" target="#tab_0">Table 1</ref> summarizes some important notations that we use in the paper.</p><p>Let us consider a training set of data point / label pairs:</p><formula xml:id="formula_1">(x n , y n ) ∈ R D × {1, .</formula><p>. . , Q}, n = 1, . . . , N.</p><p>(2) and let S be the set of pairs of indices of samples of same labels:</p><formula xml:id="formula_2">S = (i, j) ∈ {1, . . . , N } 2 , y i = y j .<label>(3)</label></formula><p>For each label y we define the set T y of indices of samples of a class different from y:</p><formula xml:id="formula_3">T y = {k ∈ {1, . . . , N }, y k = y} .<label>(4)</label></formula><p>In particular, to each (i, j) ∈ S corresponds a set T yi . Let W be a linear transformation that maps the data points from R D to R D , with D ≤ D. For the ease of notation, we do not distinguish between matrices and their corresponding linear mappings. The distance function under the linear map W is given by:</p><formula xml:id="formula_4">F W (x i , x j ) = W (x i − x j ) 2 .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>For a pair of points (i, j) of same label y i = y j , we define a ranking error function:</p><formula xml:id="formula_5">∀(i, j) ∈ S, err(F W , i, j) = L (rank i,j (F W ))<label>(6)</label></formula><p>where:</p><formula xml:id="formula_6">rank i,j (F W ) = yi =y k I (F W (x i , x k ) ≤ F W (x i , x j ))<label>(7)</label></formula><p>is the number of samples x k of different labels which are closer to x i than x j is. Formulating our objective that way, following closely the formalism of <ref type="bibr" target="#b11">[13]</ref>, shows how training a multi-class predictor shares similarities with our metric-learning problem. The former aims at avoiding, for any given sample to have incorrect classes with responses higher than the correct one, while the latter aims at avoiding, for any pair of samples (x i , x j ) of the same label, to have samples x k of other classes in between them.</p><p>Minimizing directly the rank treats all the rank positions equally, and usually in many problems including person re-identification we are interested in maximizing the correct match within the top few rank positions. This can be achieved by a weighting function L(·) which penalizes more a drop in the rank at the top positions than at the bottom positions. In particular we use the rank weighting function proposed by Usunier et al. <ref type="bibr" target="#b10">[12]</ref>, of the form:</p><formula xml:id="formula_7">L(r) = r s=1 α s , α 1 ≥ α 2 ≥ ... ≥ 0.<label>(8)</label></formula><p>For example, using α 1 = α 2 = ... = α m will treat all rank positions equally, and using higher values of αs in top few rank positions will weight top rank positions more. We use the harmonic weighting, which has such a profile and was also used in <ref type="bibr" target="#b11">[13]</ref> as it yielded state-of-the-art results on their application. Finally, we would like to solve the following optimization problem:</p><formula xml:id="formula_8">argmin W 1 |S| (i,j)∈S L (rank i,j (F W )) .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approximate OrthoNormal (AON) Regularizer</head><p>The optimization problem of Equation 9 is made more difficult by training sets of small or medium sizes, which may lead to a severe over-fitting. Regularizing penalty terms are central in re-identification for that reason. The standard way of regularizing a low-rank metric learning objective function is by using a L 2 penalty, such as the Frobenius norm <ref type="bibr" target="#b9">[11]</ref>. However, such a regularizer tends to push toward rank-deficient linear mappings, which we observe in practice (see § 4.4, and in particular <ref type="figure">Figure 2a</ref>).</p><p>Lim et al. <ref type="bibr" target="#b9">[11]</ref> in their FRML algorithm, addresses this problem by using a Riemannian manifold update step in their SGD algorithm, which is computationally expensive and induces a high memory footprint. We propose an alternative approach that maintains the rank of the matrix by pushing toward orthonormal matrices. This is achieved by using as a penalty term the L 2 divergence of W W T from the identity matrix I :</p><formula xml:id="formula_9">W W T − I 2 .<label>(10)</label></formula><p>This orthonormal regularizer can also be seen as a strategy to mimic the behavior of approaches such as PCA or FDA, which ensure that the learned linear transformation is orthonormal. For such methods, this property emerges from the strong Gaussian prior over the data, which is beneficial on small data-sets but degrades performance on large ones where it leads to under-fitting. Controlling the orthonormality of the learned mapping through a regularizer weighted by a meta-parameter λ allows us to adapt it on each data-set individually through cross-validation.</p><p>Finally, with this regularizer the optimization problem of Equation 9 becomes:</p><formula xml:id="formula_10">argmin W λ 2 W W T − I 2 + 1 |S| (i,j)∈S L (rank i,j (F W )) .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Max-Margin Reformulation</head><p>The metric learning problem on Equation 11 aims at minimizing the 0-1 loss, which is a difficult optimization problem. Applying the reasoning behind the WARP loss to make it tractable, we upper-bound this loss with the hinge one with margin γ. This is equivalent to minimizing the following loss function:</p><formula xml:id="formula_11">L(W ) = λ 2 W W T − I 2 + 1 |S| (i,j)∈S k∈Ty i L(rank γ i,j (F W )) |γ + ξ ijk | + rank γ i,j (F W ) ,<label>(12)</label></formula><p>where:</p><formula xml:id="formula_12">ξ ijk = F W (x i , x j ) − F W (x i , x k )<label>(13)</label></formula><p>and rank γ i,j (F W ) is the margin penalized rank:</p><formula xml:id="formula_13">rank γ i,j (F W ) = k∈Ty i 1 γ+ξ ijk &gt;0 .<label>(14)</label></formula><p>The loss function in <ref type="figure">Equation 12</ref> is the WARP loss <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b9">11]</ref>. It was shown by Weston et al. <ref type="bibr" target="#b11">[13]</ref> that the WARP loss can be efficiently solved by using stochastic gradient descent and we follow the same approach:</p><p>1. Sample (i, j) uniformly at random from S. 2. For the selected (i, j) uniformly sample k in {k ∈ T yi : γ + ξ ijk &gt; 0}, i.e. from the set of incorrect matches scored higher than the correct match x j .</p><p>The sampled triplet (i, j, k) has a contribution of L(rank γ i,j (F W ))|γ + ξ ijk | + because the probability of drawing a k in step 2 from the violating set is</p><formula xml:id="formula_14">1 rank γ i,j (F W ) .</formula><p>We use the above sampling procedure to solve WARCA efficiently using mini-batch stochastic gradient descent (SGD). We use Adam SGD algorithm <ref type="bibr" target="#b30">[32]</ref>, which is found to converge faster empirically compared to vanilla SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Kernelization</head><p>Most commonly used features in person re-identification are histogram-based such as LBP, SIFT BOW, RGB histograms to name a few. The most natural distance measure for histogram-based features is the χ 2 distance. Most of the standard metric learning methods work on the Euclidean distance with PCCA being a notable exception. To plug any arbitrary metric which is suitable for the features, such as χ 2 , one has to resort to explicit feature maps that approximates the χ 2 metric. However, it blows up the dimension and the computational cost. Another way to deal with this problem is to do metric learning in the kernel space, which is the approach we follow.</p><p>Let W be spanned by the samples:</p><formula xml:id="formula_15">W = AX T = A   x T 1 . . . x T N   .<label>(15)</label></formula><p>which leads to:</p><formula xml:id="formula_16">F A (x i , x j ) = AX T (x i − x j ) 2 ,<label>(16)</label></formula><formula xml:id="formula_17">= A(κ i − κ j ) 2 .<label>(17)</label></formula><p>Where κ i is the i th column of the kernel matrix K = X T X. Then the loss function in Equation 12 becomes:</p><formula xml:id="formula_18">L(A) = λ 2 AKA T − I 2 + 1 |S| (i,j)∈S k∈Ty i L(rank γ i,j (F A )) |γ + ξ ijk | + rank γ i,j (F A ) ,<label>(18)</label></formula><p>with:</p><formula xml:id="formula_19">ξ ijk = F A (x i , x j ) − F A (x i , x k ).<label>(19)</label></formula><p>Apart from being able to do non-linear metric learning, kernelized WARCA can be solved efficiently again by using stochastic sub-gradient descent. If we use the inverse of the kernel matrix as the pre-conditioner of the stochastic sub-gradient, the computation of the update equation, as well the parameter update, can be carried out efficiently. Mignon et al. <ref type="bibr" target="#b4">[6]</ref> used the same technique to solve their PCCA, and showed that it converges faster than vanilla gradient descent. We use the same technique to derive an efficient update rule for our kernelized WARCA. A stochastic sub-gradient of Equation 18 with the sampling procedure described in the previous section is given as:</p><formula xml:id="formula_20">∇L(A) = 2λ(AKA T − I)AK + 2L(rank γ i,j (F A ))A1 γ+ξ ijk &gt;0 G ijk ,<label>(20)</label></formula><p>where:</p><formula xml:id="formula_21">G ijk = (κ i − κ j )(κ i − κ j ) T d ij − (κ i − κ k )(κ i − κ k ) T d ik ,<label>(21)</label></formula><p>and:</p><formula xml:id="formula_22">d ij = F A (x i , x j ), d ik = F A (x i , x k ).<label>(22)</label></formula><p>Multiplying the right hand side of Equation 20 by K −1 :</p><formula xml:id="formula_23">∇L(A)K −1 = 2λ(AKA T − I)A + 2L(rank γ i,j (F A ))AK1 γ+ξ ijk &gt;0 E ijk .<label>(23)</label></formula><p>with:</p><formula xml:id="formula_24">E ijk = K −1 G ijk K −1 = (e i −e j )(e i −e j ) T d ij − (e i −e k )(e i −e k ) T d ik .<label>(24)</label></formula><p>where e l is the l th column of the canonical basis that is the vector whose l th component is one and all others are zero. In the preconditioned stochastic sub-gradient descent we use the updates of the form:</p><formula xml:id="formula_25">A t+1 = (I − 2λη(A t KA T t − I))A t − 2ηL(rank γ i,j (F A ))A t K1 γ+ξ ijk &gt;0 E ijk .<label>(25)</label></formula><p>Please note that E ijk is a very sparse matrix with only nine non-zero entries. This makes the update extremely fast. Preconditioning also enjoys faster convergence rates since it exploits second order information through the preconditioning operator, here the inverse of the kernel matrix <ref type="bibr" target="#b12">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our proposed algorithm on nine standard person re-identification datasets. We first describe the datasets and baseline algorithms and then present our results. The source-code of our experimental framework, including our very efficient implementation of WARCA will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Baselines</head><p>The largest dataset we experimented with is the Market-1501 dataset <ref type="bibr" target="#b13">[15]</ref>  We compare our method against the current state-of-the-art baselines MLAPG, rPCCA, SVMML, FRML, LFDA and KISSME. A brief overview of these methods is given in section 2. rPCCA, MLAPG, SVMML, FRML are iterative methods whereas LFDA and KISSME are spectral methods on the second order statistics of the data. Since WARCA, rPCCA and LFDA are kernel methods we used both the χ 2 kernel and the linear kernel with them to benchmark the performance. Marginal Fisher discriminant analysis (MFA) is proven to give similar result as that of LFDA so we do not use them as the baseline.</p><p>We did not compare against other ranking based metric learning methods such as LORETA <ref type="bibr" target="#b32">[34]</ref>, OASIS <ref type="bibr" target="#b33">[35]</ref> and MLR <ref type="bibr" target="#b27">[29]</ref> because all of them are linear methods. Infact we derived a kernelized OASIS but the results were not as good as ours or rPCCA. We also do not compare against LMNN and ITML because many researchers have evaluated them before <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b6">8]</ref> and found out that they do not perform as well as other methods considered here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Technical Details</head><p>For the Market-1501 dataset we used the experimental protocol and features described in <ref type="bibr" target="#b13">[15]</ref>. We used their baseline code and features. As Market-1501 is quite large for kernel methods we do not evaluate them. We also do not evaluate the linear methods such as Linear rPCCA and SVMML because their optimization algorithms were found to be very slow.</p><p>All other evaluations where carried out in the single-shot experiment setting [2] and our experimental settings are very similar to the one adopted by Xiong et al. <ref type="bibr" target="#b7">[9]</ref>. Except for Market-1501, we randomly divided all the other datasets into two subsets such that there are p individuals in the test set. We created 10 such random splits. In each partition one image of each person was randomly selected as a probe image, and the rest of the images were used as gallery images and this was repeated 10 times. The position of the correct match was processed to generate the CMC curve. We followed the standard François Fleuret  train-validation-test splits for all the other datasets and P was chosen to be 100, 119, 486, 316, 225, 36, 95 and 60 for CUHK03, OpeReid, CUHK01, VIPeR, PRID450s, CAVIAR, 3DPeS and iLIDS respectively. We used the same set of features for all the datasets except for the Market-1501 and all the features are essentially histogram based. First all the datasets were re-scaled to 128×48 resolution and then 16 bin color histograms on RGB, YUV, and HSV channels, as well as texture histogram based on Local Binary Patterns (LBP) were extracted on 6 non-overlapping horizontal patches. All the histograms are normalized per patch to have unit L 1 norm and concatenated into a single vector of dimension 2,580 <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b7">9]</ref>.</p><p>The source codes for LFDA, KISSME and SVMML are available from their respective authors website, and we used those to reproduce the baseline results <ref type="bibr" target="#b7">[9]</ref>. The code for PCCA is not released publicly. A version from Xiong et al. <ref type="bibr" target="#b7">[9]</ref> is available publicly but the memory footprint of that implementation is very high making it impossible to use with large datasets (e.g. it requires 17GB of RAM to run on the CAVIAR dataset). Therefore to reproduce the results in <ref type="bibr" target="#b7">[9]</ref> we wrote our own implementation, which uses 30 times less memory and can scale to much larger datasets. We also ran LFDA-χ 2 LFDA-L KISSME <ref type="figure">Fig. 1</ref>: CMC curves comparing WARCA against state-of-the-art methods on nine reidentification datasets sanity checks to make sure that it behaves the same as that of the baseline code. All the implementations were done in Matlab with mex functions for the acceleration of the critical components.</p><p>In order to fairly evaluate the algorithms, we set the dimensionality of the projected space to be same for WARCA, rPCCA and LFDA. For the Market-1501 dataset the dimensionality used is 200 and for VIPeR it is 100 and all the other datasets it is 40. We choose the regularization parameter and the learning rate through cross-validation across the data splits using grid search in (λ, η) ∈ {10 −8 , . . . , 1} × {10 −3 , . . . , 1}. Margin γ is fixed to 1. Since the size of the parameter matrix scales in O(D 2 ) for SVMML and KISSME we first reduced the dimension of the original features using PCA keeping 95% of the original variance and then applied these algorithms. In our tables and figures WARCA−χ 2 , WARCA-L, rPCCA−χ 2 , rPCCA-L, LFDA−χ 2 and LFDA-L denote WARCA with χ 2 kernel, WARCA with linear kernel, rPCCA with χ 2 kernel, rPCCA with linear kernel, and LFDA with χ 2 kernel, LFDA with linear kernel respectively.</p><p>For all experiments with WARCA we used harmonic weighting for the rank weighting function of Equation <ref type="bibr" target="#b6">8</ref>. That is weighting of the form L(M ) = M m=1 1 m . We also tried uniform weighting which gave poor results compared to the harmonic weighting for a given computational budget. For all the datasets we used a mini-batch size of 512 in the SGD algorithm and we ran the SGD for 2000 iterations (A parameter update using the mini-batch is considered as 1 iteration).</p><p>Tables 2a and 2b summarize respectively the rank-1 and rank-5 performance of all the methods, and <ref type="table" target="#tab_3">Table 2c</ref> summarizes the Area Under the Curve (AUC) performance score. <ref type="figure">Figure 1</ref> reports the CMC curves comparing WARCA against the baselines on all the nine datasets. The circle and the star markers denote linear and kernel methods respectively.</p><p>WARCA improves over all other methods on all the datasets. On VIPeR, 3DPeS, PRID450s and iLIDS datasets LFDA come very close to the performance of WARCA. The reason for this is that these datasets are too small and consequently simple methods such as LFDA which exploits strong prior assumptions on the data distribution work nearly as well as WARCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison against State-of-the-art</head><p>Dataset WARCA(Ours) MLAPG <ref type="bibr" target="#b8">[10]</ref> MLPOLY <ref type="bibr" target="#b24">[26]</ref> IDEEP <ref type="bibr" target="#b25">[27]</ref> rank=1 rank=5 rank=10 rank=20 rank=1 rank=5 rank=10 rank=20 rank=1 rank=5 rank=10 rank=20 rank=1 rank=5 rank=10 rank=20 VIPeR 40. <ref type="bibr" target="#b20">22</ref>   <ref type="table">Table 3</ref>: Comparison of WARCA against state-of-the-art results for person reidentification.</p><p>We also compare against the state-of-the-art results reported using recent algorithms such as MLAPG on LOMO features <ref type="bibr" target="#b8">[10]</ref>, MLPOLY <ref type="bibr" target="#b24">[26]</ref> and IDEEP <ref type="bibr" target="#b25">[27]</ref> on VIPeR, CUHK01 and CUHK03 datasets. The reason for not including these comparisons in the main results is because apart from MLAPG the code for other methods is not available, or the features are different which makes a fair comparison difficult. Our goal is to evaluate experimentally that, given a set of features, which is the best off-the-shelf metric learning algorithm for re-identification.</p><p>In this set of experiments we used the state-of-the-art LOMO features <ref type="bibr" target="#b3">[5]</ref> with WARCA for VIPeR and CUHK01 datasets. The results are summarized in the <ref type="table">Table 3</ref>. We improve the rank1 performance by 21% on CUHK03 by 1.40% on CUHK01 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of the AON regularizer</head><p>Here we present an empirical analysis of the AON regularizer against the standard Frobenius norm regularizer. We used the VIPeR dataset with LOMO features for all these experiments. With very low regularization strength AON and Frobenius behaves the same. As the regularization strength increases Frobenous norm results in rank deficient mappings <ref type="figure">(Figure 2a</ref>), which is less discriminant and performs poorly on the test Step Size  <ref type="figure">Fig. 2</ref>: Comparison of the Approximate OrthoNormal (AON) regularizer we use in our algorithm to the standard Frobenius norm (L 2 ) regularizer. Graph (a) shows the condition number (ratio between the two extreme eigenvalues of the learned mapping) vs. the weight λ of the regularization term. As expected, the AON regularizer pushes this value to one, as it eventually forces the learning to chose an orthonormal transformation, while the Frobenius regularizer eventually kills the smallest eigenvalues to zero, making the ratio extremely large. Graph (b) shows the Rank-1 performance vs. the regularizer weight λ, graph (c) the Rank-1 performance vs. the SGD step size η, and finally graph (d) CMC curve with the two regularizers.</p><p>set <ref type="figure">(Figure 2b</ref>). On the contrary, the AON regularizer pushes towards orthonormal mappings and results in an embedding well conditioned, which generalizes well to the test set. It is also worth noting that training with the AON regularizer is robust over wide range of regularization parameter, which is not the case the Frobenius norm. Finally, the AON regularizer was found to be very robust to the choice of the SGD step size η <ref type="figure">(Figure 2c</ref>) which is a crucial parameter in large-scale learning. A similar behaviour was observed by Lim et al. <ref type="bibr" target="#b9">[11]</ref> with their orthonormal Riemannian gradient update step in the SGD but it is computationally very expensive and cannot be used combined with modern SGD algorithms such as Adam <ref type="bibr" target="#b30">[32]</ref>, and Nesterov's momentum <ref type="bibr" target="#b34">[36]</ref>.  <ref type="figure">Fig. 3</ref>: WARCA performs significantly better than the state-of-the-art rPCCA on large datasets for a given training time budget. <ref type="figure">Figure 3</ref> illustrates how the test set performance of WARCA and rPCCA increase as a function of training time on 3 datasets. We implemented both the algorithms entirely in C++ with BLAS and OpenMP to have a fair comparison on their running times. In this set of experiments we used number of test identites to be 730 for CUHK03 dataset to have a quick evaluation. Other datasets follow the same experimental protocol described above. Please note that we do not include spectral methods in this plot because its solutions are found analytically. Linear spectral methods are very fast for low dimensional problems but the training time scales quadratically in the data dimension. In case of kernel spectral methods the training time scales quadratically in the number of data points. We also do not include iterative methods, MLAPG and SVMML because they proved to be very slow and not giving good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of the Training Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future work</head><p>We have proposed a simple and scalable approach to metric learning that combines a new and simple regularizer to a proxy for a weighted sum of the precision at different ranks. The later can be used for any weighting of the precision-at-k metrics. Experimental results show that it outperforms state-of-the-art methods on standard person re-identification datasets, and that contrary to most of the current state-of-the-art methods, it allows for large-scale learning.</p><p>The simplicity and efficiency of WARCA call for several future research directions. The first one is to investigate different forms of the regularizer, in particular using the LogDet divergence <ref type="bibr" target="#b35">[37]</ref> instead of the L 2 . Such a form is justifiable under a Gaussian assumption on the data distributions, and it has been used successfully for metric learning before <ref type="bibr" target="#b23">[25]</ref>. The second is to extend WARCA to a more general class of models, in particular non-parametric ones such as forests of decision trees or (deep) multi-layer perceptrons.</p><p>From a more theoretical perspective, we are interested also in looking at the relations between the behavior of the learning with the orthonormal regularizer, and the recent residual networks <ref type="bibr" target="#b36">[38]</ref>. In both case, strong regularization pushes toward full-rank mappings instead of null transformations, as standard L 2 penalty does, which appears to be a very reasonable behavior to expect in general.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>N</head><label></label><figDesc>Number of training samples D Dimension of training samples Q Number of classes (xi, yi) ∈ R D × {1, . . . , Q} i-th training sample 1condition is equal to 1 if the condition is true, 0 otherwise S the pairs of indices of samples of same class Ty the indices of samples not of class y FW distance function under the linear map W ranki,j(FW ) for i and j of same class, number of miss-labeled examples closer to i than j is L(W ) the loss we minimize L(r) rank weighting function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>9 CUHK01WARCA-χ 2</head><label>92</label><figDesc>Rank-1 rPCCA-χ 2 Rank-1 WARCAχ 2 Rank-5 rPCCA-χ 2 Rank-5 WARCAχ 2 AUC rPCCA-χ 2 AUC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notation</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>CUHK03 dataset<ref type="bibr" target="#b14">[16]</ref> consists of 13,164 images of 1,360 persons and it has both DPM detected and manually annotated bounding boxes. We use the manually annotated bouding boxes here. OpeReid dataset<ref type="bibr" target="#b15">[17]</ref> consists of 7,413 images of 200 persons. CUHK01 dataset<ref type="bibr" target="#b16">[18]</ref> is composed of 3,884 images of 971 persons, with two pairs of images per person, each pair taken from a different viewpoint. Each image is of resolution 160×60. The most popular and challenging person re-identification dataset is the VIPeR<ref type="bibr" target="#b17">[19]</ref> dataset. It consists of 1,264 images of 632 person, with 2 images per person. The images are of resolution 128x48, captured from horizontal viewpoints but from widely different directions. The</figDesc><table><row><cell>PRID450s dataset [22] consists of 450 image pairs recorded from two different static</cell></row><row><cell>surveillance cameras. The CAVIAR dataset [3] consists of 1,220 images of 72 individu-</cell></row><row><cell>als from 2 cameras in a shopping mall. The number of images per person varies from 10</cell></row><row><cell>to 20 and image resolution also varies significantly from 141×72 to 39×17. The 3DPeS</cell></row><row><cell>dataset [20] has 1,011 images of 192 individuals, with 2 to 6 images per person. The</cell></row><row><cell>dataset is captured from 8 outdoor cameras with horizontal but significantly different</cell></row></table><note>which is composed of 32,668 images of 1,501 persons captured from 6 different view points. It uses DPM [33] detected bounding boxes as annotations.viewpoints. Finally the iLIDS dataset [21] contains 476 images and 119 persons, with 2 to 8 images per individual. It is captured from a horizontal view point at an airport.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>38±2.44 62.12±2.07 76.74±2.06 59.22±2.65 44.90±1.57 53.87±2.31 47.89±2.59 69.94±2.21 46.02±1.55 47.88±1.80 CUHK01 58.34±1.26 39.30±0.76 48.55±1.12 34.73±1.06 22.92±0.94 33.58±0.69 27.96±0.86 54.25±1.04 33.74±0.73 35.74±0.95 OpeReid 57.65±1.60 43.74±1.34 52.89±1.78 43.66±1.45 40.63±1.31 42.27±1.35 30.63±1.51 53.58±1.65 42.84±1.18 41.76±1.36 VIPeR 37.47±1.70 20.86±1.04 22.25±1.91 15.91±1.16 19.49±2.26 18.52±0.78 23.28±1.53 36.77±2.10 20.22±1.85 20.89±1.22 PRID450s 24.58±1.75 10.33±1.20 16.35±1.30 8.34±1.25 2.13±0.59 7.05±1.60 13.08±1.63 24.31±1.44 3.24±0.95 15.24±1.56 CAVIAR 43.44±1.82 39.35±1.98 37.56±2.17 27.26±2.15 36.74±1.96 35.40±2.67 26.82±1.64 41.29±2.25 37.72±2.08 31.99±2.17 3DPeS 51.89±2.27 43.57±2.18 46.42±2.25 33.12±1.58 41.17±2.26 39.03±1.85 29.94±2.10 51.44±1.40 43.24±2.57 37.55±1.80 iLIDS 36.61±2.40 31.77±2.77 26.57±2.60 23.07±3.07 31.13±1.57 25.68±2.25 21.32±2.89 36.23±1.89 32.70±3.12 28.29±3.59 55±1.31 86.03±1.62 94.50±1.29 84.52±1.41 71.80±1.52 80.36±1.22 79.97±2.08 90.15±1.27 65.41±1.66 69.29±2.35 CUHK01 79.76±0.69 61.84±0.98 73.29±1.32 56.67±1.20 48.48±1.49 55.27±0.83 53.11±0.78 74.60±1.00 49.73±0.91 53.34±0.69 OpeReid 80.43±1.71 67.39±1.02 77.95±1.82 67.68±1.25 61.45±1.61 66.08±1.30 60.32±1.31 75.34±1.76 59.70±1.37 61.74±1.55 VIPeR 70.78±2.43 50.29±1.61 53.82±2.32 42.71±2.02 46.49±2.23 46.15±1.62 55.28±1.99 69.30±2.23 45.25±1.90 47.73±2.28 PRID450s 55.52±2.23 31.73±3.08 43.82±2.18 26.89±2.21 11.29±1.66 24.16±3.04 38.38±1.77 54.58±2.06 12.55±1.41 37.22±1.81 CAVIAR 74.06±3.13 68.06±2.44 70.62±2.26 57.44±2.48 65.83±2.73 66.24±3.08 61.53±3.64 69.12±3.02 61.60±2.94 61.17±3.21 3DPeS 75.64±2.80 68.26±1.91 73.54±2.26 58.34±2.31 65.06±1.89 65.20±2.15 59.52±2.62 75.36±1.91 65.64±1.91 60.22±2.05 iLIDS 66.09±2.31 59.27±3.12 57.07±2.93 51.55±3.59 57.31±3.12 53.42±2.17 51.45±4.30 65.20±2.68 59.66±2.51 54.08±3.63 94±0.76 89.67±0.80 93.92±0.81 89.17±0.69 82.30±1.01 86.64±0.65 86.64±1.07 91.66±0.68 74.23±1.51 77.68±1.83 CUHK01 84.99±0.65 71.88±0.67 81.00±0.88 67.56±0.93 62.84±1.51 66.39±0.76 65.73±1.07 80.84±0.80 58.92±1.08 62.36±0.95 OpeReid 86.47±1.08 77.17±0.94 85.25±1.16 77.42±1.01 72.34±1.11 76.51±0.88 73.88±1.04 82.67±1.30 68.96±1.53 71.33±1.14 VIPeR 81.87±1.07 67.00±1.11 71.30±1.50 62.40±1.43 64.71±1.15 64.19±1.39 71.04±1.63 81.34±1.21 62.67±1.35 64.74±1.20 PRID450s 72.13±1.49 50.07±2.25 63.10±2.16 46.19±1.89 30.81±2.19 42.97±2.84 59.54±1.25 71.55±1.70 28.18±1.22 53.83±1.86 CAVIAR 85.76±1.48 83.01±1.44 84.41±1.28 76.57±1.29 81.58±1.50 81.88±1.85 79.38±2.19 81.94±2.32 76.76±1.69 78.85±1.54 3DPeS 83.89±1.53 78.07±1.57 82.84±1.44 72.27±1.96 75.98±1.28 76.89±1.44 73.38±1.70 83.49±0.95 75.87±1.49 72.22±1.31 iLIDS 79.04±1.60 73.42±1.96 74.10±2.04 69.60±2.44 72.45±1.99 71.26±1.55 70.25±2.09 78.98±1.43 74.26±2.02 70.33±2.90</figDesc><table><row><cell>Dataset</cell><cell cols="3">WARCA-χ 2 WARCA-L rPCCA-χ 2</cell><cell>rPCCA-L</cell><cell>MLAPG</cell><cell>FRML</cell><cell>SVMML</cell><cell>LFDA-χ 2</cell><cell>LFDA-L</cell><cell>KISSME</cell></row><row><cell>Market-1501</cell><cell>−</cell><cell>45.16±0.00</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="2">34.65±0.00 42.81±0.00</cell></row><row><cell>CUHK03</cell><cell cols="6">78.(a) Rank 1 accuracy.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">WARCA-χ 2 WARCA-L rPCCA-χ 2</cell><cell>rPCCA-L</cell><cell>MLAPG</cell><cell>FRML</cell><cell>SVMML</cell><cell>LFDA-χ 2</cell><cell>LFDA-L</cell><cell>KISSME</cell></row><row><cell>Market-1501</cell><cell>−</cell><cell>68.23±0.00</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="2">52.76±0.00 62.74±0.00</cell></row><row><cell>CUHK03</cell><cell cols="6">94.(b) Rank 5 accuracy.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">WARCA-χ 2 WARCA-L rPCCA-χ 2</cell><cell>rPCCA-L</cell><cell>MLAPG</cell><cell>FRML</cell><cell>SVMML</cell><cell>LFDA-χ 2</cell><cell>LFDA-L</cell><cell>KISSME</cell></row><row><cell>Market-1501</cell><cell>−</cell><cell>75.41±0.00</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="2">60.53±0.00 70.02±0.00</cell></row><row><cell>CUHK03</cell><cell>93.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(c) AUC score.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Table showing the rank 1, rank 5 and AUC performance measure of our method WARCA against other state-of-the-art methods. Bold fields indicate best performing methods. The dashes indicate computation that could not be run in a realistic setting on Market-1501.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>68.16 80.70 91.14 40.73 69.94 82.34 92.37 36.80 70.40 83.70 91.70 34.81 63.61 75.63 84.49 CUHK01 65.64 85.34 90.48 95.04 64.24 85.41 90.84 94.</figDesc><table><row><cell>92</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.53 71.60 80.25 87.45</cell></row><row><cell>CUHK03 78.38 94.5 97.52 99.11 57.96 87.09 94.74 98.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.74 86.50 94.02 97.02</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Person Re-Identification. Advances in Computer Vision and Pattern Recognition</title>
		<editor>2. Gong, S., Cristani, M., Yan, S., Loy</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pcca: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient psd constrained asymmetric metric learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient learning of mahalanobis metrics for ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14</meeting>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ranking with ordered weighted pairwise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Buffoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual International Conference on Machine Learning (ICML-09)</title>
		<meeting>the 26th annual International Conference on Machine Learning (ICML-09)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">WSABIE: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training a support vector machine in the primal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1155" to="1178" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark. Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.0872</idno>
		<title level="m">Open-set person re-identification</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACCV</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3dpes: 3d people dataset for surveillance and forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 joint ACM workshop on Human gesture and behavior understanding</title>
		<meeting>the 2011 joint ACM workshop on Human gesture and behavior understanding</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Associating groups of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mahalanobis distance learning for person re-identification. In: Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distance metric learning: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Michigan State Universiy</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual International Conference on Machine Learning (ICML-07)</title>
		<meeting>the 24th annual International Conference on Machine Learning (ICML-07)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Similarity learning on an explicit polynomial kernel feature map for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual International Conference on Machine Learning (ICML-04)</title>
		<meeting>the 21st annual International Conference on Machine Learning (ICML-04)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Metric learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th annual International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimizing mean reciprocal rank for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mukunoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funatomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal-Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="408" to="413" />
		</imprint>
	</monogr>
	<note>8th IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to rank in person reidentification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online learning in the embedded manifold of lowrank matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="429" to="458" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">13</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimation with quadratic loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fourth Berkeley symposium on mathematical statistics and probability</meeting>
		<imprint>
			<date type="published" when="1961" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="361" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
