<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CrossWeigh: Training Named Entity Tagger from Imperfect Annotations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
							<email>zihanw2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
							<email>shang7@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihao</forename><surname>Lu</surname></persName>
							<email>lihaolu2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CrossWeigh: Training Named Entity Tagger from Imperfect Annotations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38% test sentences, which is a significant ratio considering that the state-of-the-art test F 1 score is already around 93%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo 1 . * Equal Contributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER), identifying both spans and types of named entities in text, is a fundamental task in the natural language processing pipeline. On one of the widely-adopted NER benchmarks, the CoNLL03 NER dataset <ref type="bibr" target="#b17">(Sang and Meulder, 2003)</ref>, the state-of-the-art NER performance has been pushed to a F 1 score around 93% <ref type="bibr" target="#b1">(Akbik et al., 2019)</ref>, through building end-toend neural models <ref type="bibr" target="#b7">(Lample et al., 2016;</ref><ref type="bibr" target="#b10">Ma and Hovy, 2016)</ref> and introducing language models for contextualized representations <ref type="bibr" target="#b13">(Peters et al., 2017</ref><ref type="bibr" target="#b14">(Peters et al., , 2018</ref><ref type="bibr" target="#b2">Akbik et al., 2018;</ref><ref type="bibr" target="#b8">Liu et al., 2018a)</ref>. Such high performance makes the label mistakes in manually curated "gold standard" data non-negligible. For example, given a sentence "Chicago won game 1 with Derrick Rose scoring 25 points.", this "Chicago", representing the NBA team Chicago Bulls, should be annotated as an organization. However, when annotators are not careful or lack background knowledge, this "Chicago" might be annotated as a location, thus being a label mistake.</p><p>These label mistakes bring up two challenges to NER: (1) mistakes in the test set can interfere the evaluation results and even lead to an inaccurate assessment of model performance; and (2) mistakes in the training set can hurt NER model training. Therefore, in this paper, we conduct empirical studies to understand these mistakes, correct the mistakes in the test set to form a cleaner benchmark, and develop a novel framework to handle the mistakes in the training set.</p><p>We dive deep into the CoNLL03 NER dataset, and find label mistakes in about 5.38% test sentences. Considering that the state-of-the-art F 1 score on this test set is already around 93%, these 5.38% mistakes should be considered as significant. So we hire human experts to correct these label mistakes in the test set. We then re-evaluate recent state-of-the-art NER models on this new, cleaner test set. Compared to the results on the original test set, the re-evaluation results are more accurate and stable. Therefore, we believe this new test set can better reflect the performance of NER models.</p><p>We further propose a novel, general framework, CrossWeigh, to handle the label mistakes during the NER model training stage. <ref type="figure" target="#fig_0">Figure 1</ref> presents an overview of our proposed framework. It contains two modules: (1) mistake estimation: it iden- <ref type="bibr">[Liverpool]</ref>  tifies the potential label mistakes in training data through a cross checking process and (2) mistake re-weighing: it lowers the weights of these instances during the training of the final NER model. The cross checking process is inspired by the k-fold cross validation; differently, in each fold's training data, it removes the data containing any of entities that appeared in this fold. In this way, each sentence will be scored by a NER model trained on a subset of training data not containing any entity in this sentence. Once we know where the potential mistakes are, we lower the weights of these sentences and train the final NER model based on this weighted training set. The final NER model is trained in a mistake-aware way, thus being more accurate. Note that, our proposed framework is general and fits most of, if not all, NER models that accept weighted training data.</p><p>To the best of our knowledge, we are the first to handle the label mistake issues systematically in the NER problem. We conduct extensive experiments on both the original CoNLL03 NER dataset and our corrected dataset. CrossWeigh is able to consistently improve performance when plugging with different NER models. In addition, we verify the effectiveness of CrossWeigh on emerging-entity and low-resource NER datasets. In summary, our major contributions are the following: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CoNLL03 NER Re-Examination</head><p>The CoNLL03 NER dataset is one of the widelyadopted NER benchmark datasets. Its annotation guideline is based on MUC Conventions 3 <ref type="bibr" target="#b17">(Sang and Meulder, 2003)</ref>. Following this guideline, the annotators are asked to mark entities of person (PER), location (LOC), and organization (ORG), while using an extra miscellaneous (MISC) type to deal with entities that do not fall in these categories. This dataset has been split into training, development, and test sets, with 14041, 3250, and 3453 sentences, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test Set Correction</head><p>In order to understand and correct the label mistakes, we have hired 5 human experts as annotators. Before looking at the data, we first train the annotators by carefully going through the aforementioned guideline. During the correction process, we strongly encourage the annotators to use search engines for suspicious token spans. This helps them  have more background knowledge. We also allow annotators to look at the original paragraph containing the sentence. This helps them have a better understanding of the context. For the whole test set, we randomly split the test sentences between each pair combination of 5 annotators. In this way, each sentence in the test set is checked by exactly two annotators. The interannotator agreement is 95.66%. This is a reasonable score, given that the inter-annotator agreement in POS tagging annotations is about 97% (Manning, 2011). After we collected all annotations, we run a final round of verification on each sentence, where the original annotation and the two annotators' are not all the same. In the end, we have corrected label mistakes in 186 sentences, which is about 5.38% of the test set. <ref type="table" target="#tab_3">Table 1</ref> presents some typical examples of our corrections. In the first sentence, as a sport team, "Sporting Gijon" was not annotated completely. In the second sentence, while "JAPAN" is correctly marked as LOC, "China" is wrongly identified as PER instead of LOC. One may notice that they both represent sport teams. However, according to the aforementioned guideline, country names should be marked as LOC even when they are sports teams. More details about this type of labels are discussed in Section 5. In the third sentence, "NZ" is the abbreviation of New Zealand. However, "Nat" and "NZ First" in fact refer to political parties (i.e., New Zealand Young Nationals and New Zealand First). So they should be labelled as ORG. In the forth sentence, looking at its paragraph, our annotators figure out that this is a table about ships and vessels loading items at different locations. Through comparing with other sentences in the context, such as "Algoa Day 21/11/96 6,000 Africa", our annotators identified "Seagramd ace" as a vessel, thus marking it as MISC. We have verified that there is indeed a vessel called "Seagrand Ace" ("Seagramd ace" might be a typo).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CoNLL03</head><p>Re-Evaluation NER Algorithms. We re-evaluate following popu-   <ref type="bibr" target="#b2">(Akbik et al., 2018)</ref> also aims for contextualized representations, utilizing pretrained character level language models. • Pooled-Flair <ref type="bibr" target="#b2">(Akbik et al., 2018)</ref> extends Flair and maintains an embedding pool for each word to bring in dataset-level word embedding. We use the implementation released by the authors for each algorithm and report the performance on original test set and corrected test set averaging 5 runs.</p><p>Results &amp; Discussions. We re-evaluate the performance of the NER algorithms on the corrected test set. Their performance on the original test set is also listed for the reference. From the results in Table 2, one can observe that all models have higher F 1 scores as well as smaller standard deviations on the corrected test set, compared to those on the orig-inal test set. Moreover, LSTM-CRF has a similar performance as LSTM-CNNs-CRF on the original test set, but on average lower performance on the corrected test set. This indicates that the corrected test set may be more discriminative. Therefore, we believe this corrected test set can better reflect the accuracy of NER algorithms in a stable way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Framework: CrossWeigh</head><p>In this section, we introduce our framework. It is worth mentioning that our framework is designed to be general and fits most of, if not all, NER models. The only requirement is the capability to consume weighted training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>As we have seen in the Section 2, human curated NER datasets are by no means perfect. Label mistakes in the training set can directly hurt the model's performance. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, if there are many similar mistakes like wrongly annotating "Chicago" in "Chicago won ..." as LOC instead of ORG, the NER model will likely capture the wrong pattern "LOC won" and make wrong predictions in future.</p><p>Our proposed CrossWeigh framework automates this process. <ref type="figure" target="#fig_0">Figure 1</ref> presents an overview. It contains two modules: (1) mistake estimation: it identifies the potential label mistakes in training data through a cross checking process and (2) mistake re-weighing: it lowers the weights of these instances for the NER model training. The workflow is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preliminary</head><p>We denote the training sentences as {x 1 , x 2 , . . . , x n } where n is the number of sentences. Each sentence x i is formed up of a sequence of words. Correspondingly, the label sequence for each sentence is denoted as {y 1 , y 2 , . . . , y n }. We use D to denote the training set, including both sentences and their labels. We use w i to represent the weight of the i-th sentence. In most NER papers, the weights are uniform, i.e., w i = 1.</p><p>We use f (D, w) to describe the training process of an NER model using the training set D weighted by w. This training process will return an NER model M = f (D, w). During this training, the </p><formula xml:id="formula_0">i = 1 ... n do c i ← 0, w i ← 1 for iter = 1 . . . t do Randomly partition D into k folds. for Each fold D i do Obtain test_entities i . (Eq. 2) Build train_set i . (Eq. 3). Train a NER model M i = f (train_set i , w). for Each x j ∈ D i dô y j ← M i 's prediction on x j . if y j =ŷ j then c i ← c i + 1 for i = 1 . . . n do Compute w i (Eq. 4). Return f (D, w).</formula><p>weighted loss function is as below.</p><formula xml:id="formula_1">J = n i=1 w i · l(M (x i ), y i )<label>(1)</label></formula><p>where l(M (x i ), y i ) is the loss function of prediction M (x i ) against its label sequence y i . Typically, it is the negative log-likelihood of the model's prediction M (x i ) compared to labeling sequence y i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mistake Estimation</head><p>Our mistake estimation module is designed to let an NER model itself decide which sentences contain mistake and which do not. We would like to find sentences with label mistakes as many as possible (i.e. high recall), while keeping away from wrongly identified non-mistake sentences (i.e. high precision). The basic idea of our mistake estimation module is similar to k-fold cross validation, however, in each fold's training data, it further removes the data containing any of entities appearing in this fold. The details are presented as follows.</p><p>We first randomly partition the training data into k folds: D 1 , D 2 , . . . , D k .</p><p>We then train k NER models separately based on these k folds. The i-th (1 ≤ i ≤ k) NER model M i will be evaluated on the sentences in the hold-out fold D i .</p><p>During its training, we avoid any sentence that may lead to "easy prediction" on this hold-out set.  Therefore, we inspect every sentence in D i and get the set of entities as follows.</p><formula xml:id="formula_2">test_entities i = x j ∈D i e j<label>(2)</label></formula><p>where e j is the set of named entities in sentence x j . We only consider the surface name in this entity set. That is, no matter "Chicago" is LOC or ORG, it only counts as its surface name "Chicago". All training sentences that have entities included in test_entities i will be excluded in training process of the model M i . Specifically,</p><formula xml:id="formula_3">train_set i = {x j |e j ∩test_entities i = ∅, 1 ≤ j ≤ n}</formula><p>(3) We call this step as entity disjoint filtering. The intuition behind this step is that we want the model to make prediction of an entity without prior information of the entity itself from training. This will be helpful to detect sentences that are inconsistent.</p><p>We train k models M i by feeding each train_set i into f (·, ·) with default uniform weight, and we use each M i to make predictions for D i and check for each sentence, whether the original label is the same as the model output. In this way, if the trained model M i makes correct predictions on some sentences in D i , they are more likely mistake-free. For those sentences that have labels disagreeing with the model output, we mark them as potentially mistake.</p><p>We run this mistake estimation module multiple iterations (i.e. t iterations) using different random partitions. Then, for each sentence in the training set, we get t estimations for it. We denote c i (0 ≤ c i ≤ t) as the confidence that sentence x i contains label mistakes. c i is defined as the the number of potentially mistake indications among all t estimations.</p><p>The number of folds k plays the role of a tradeoff between the efficiency of the mistake estimation process and the number of training examples that can be used in each M i . When k becomes larger, each fold D i will be smaller, thus leading to a smaller size of test_entities i ; correspondingly, a larger train_set i will be picked. The model can therefore be trained with more examples. However, it also slows down the whole mistake estimation process. On the CoNLL03 NER dataset, we observe that k = 10 leads to effective results, while having a reasonable running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mistake Reweighing</head><p>In the mistake reweighing module, we adjust weight w i for each sentence x i that is marked as potentially mistake in the mistake estimation step. Here, we assign a weight w i to all sentences marked, while the weights of other sentences remain 1. Specifically, we set ∀i(1 ≤ i ≤ n),</p><formula xml:id="formula_4">w i = c i (4)</formula><p>where is a parameter. In practice, it can be chosen according to the quality of mistake estimation module. Particularly, we first estimate the precision of the detected mistakes of a single iteration. Let p be the ratio of the number of true detected label mistakes over the number of detected label mistakes. p can be roughly estimated through a manual check of a random sample from the detected label mistakes. Then, we choose = 1 − p, because 1 − p represents the fraction of these detected label mistakes that might be still useful during the model training. Therefore, for the sentences that are marked as potentially mistake in that iteration, of them are actually correct. With more iterations, the confidence of being correct lowers like a binomial distribution, which is the reason that we chose an exponential decaying weight function in Equation 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct several experiments to show effectiveness of our CrossWeigh framework. We first evaluate the overall performance of CrossWeigh on benchmark NER datasets, by plugging it into three base NER models. Since we have two modules in CrossWeigh, we then dive into each module and explore different variants and ablations. In addition, we further verify the effectiveness of CrossWeigh on two more datasets:</p><p>an emerging-entity NER dataset from WNUT'17 and a low-resource language NER dataset of the Sinhalese language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Dataset. We use both the original and corrected CoNLL03 datasets. We follow the standard train/dev/test splits and use both the train set and dev set for training <ref type="bibr" target="#b13">(Peters et al., 2017;</ref><ref type="bibr" target="#b2">Akbik et al., 2018)</ref>. Entity-wise F 1 score on the test set is the evaluation metric. Base NER Algorithm. We mainly choose Flair as our base NER algorithm. Flair is a strong NER algorithm using external resources (large corpus to train a language model). While Pooled-Flair has even better performance, its computational cost refrains us from doing extensive experiments. Default Parameters in CrossWeigh. For all NER algorithms we experiment with, their default parameters are used. For CrossWeigh parameters, by default, we set k = 10, t = 3, and = 0.7. We decide = 0.7 because among 100 randomly sampled sentences with potentially mistake, we find that 27 of them really contain label mistakes (i.e., the probability of one annotation to be correct is roughly 70%). We use both train and development set to train the models, and report average F 1 and its standard deviation on both original test set and our corrected test set across 5 different runs <ref type="bibr" target="#b13">(Peters et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance</head><p>We pair CrossWeigh with our base algorithm (i.e. Flair) and two best-performing NER algorithms with or without language models in Table 2 (i.e. Pooled-Flair and VanillaNER), and evaluate their performance. As shown in <ref type="table" target="#tab_8">Table 3</ref>, compared with the three algorithms, applying CrossWeigh always leads to a higher F 1 score and a comparable, sometimes even smaller, standard deviation. Therefore, it is clear that CrossWeigh can improve the performance of NER models. The smaller standard deviations also imply that the models trained with CrossWeigh are more stable. All these results illustrate the superiority of training with CrossWeigh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations and Variants</head><p>We pick Flair as the base algorithm to conduct ablation study. Entity Disjoint Filtering. There is an entity disjoint filtering step, when we are collecting training  data train_set i for the NER model M i during the mistake estimation step. To study its importance, we have done a few ablation experiments.</p><p>We have evaluated the following variants: • Flair w/ CrossWeigh -Entity Disjoint: Skip the entity disjoint filtering step. • Flair w/ CrossWeigh + Random Discard: Instead of entity disjoint filtering, randomly discard the same number of sentences from each train_set i as it would do.</p><p>The results are listed in <ref type="table" target="#tab_10">Table 4</ref>. One can easily observe that without the entity disjoint filtering, the F 1 scores are very close to the raw Flair model. This demonstrates that the entity disjoint filtering is critical to reduce the over-fitting risk in the mistake estimation step. Also, our proposed entity disjoint filtering strategy works more effective than random discard. This further confirms the effectiveness of entity disjoint filtering. Variants in Computing c i . There is definitely more than one way to determine c i . Let δ be the number of "potentially mistake"s among the t estimations, we can apply any of the following heuristics:</p><p>• Ratio: c i is the number of "potentially mistake" (i.e. c i = δ). This is the method mentioned in Section 3, and used by default. • At Least One: c i is the indicator of at least one estimation being "potentially mistake" (i.e. c i = t ⇐⇒ δ &gt;= 1). • Majority: c i is the indicator of at least t/2 + 1 estimations being "potentially mistake" (i.e. c i = t ⇐⇒ δ &gt;= t/2 + 1). • All: c i is the indicator of all t estimations being "potentially mistake" (i.e. c i = t ⇐⇒ δ = t). We evaluate the performance of these heuristics when used in CrossWeigh, as shown in <ref type="table" target="#tab_12">Table 5</ref>. There is not much difference across these heuristics, while our default choice "Ratio" is the most stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Label Mistake Identification Results</head><p>Another usage of CrossWeigh is to identify potential label mistakes during label annotation process, thus improving the annotation quality. This could   be also helpful to active learning. Specifically in this experiment, we apply our noise estimation module to the concatenation of training and testing data. As we have manually corrected the label mistakes in the testing set, we are able to report the number of true mistakes among the potential mistakes discovered in the test set.</p><p>The results are presented in <ref type="table" target="#tab_18">Table 9</ref>. The potential mistakes are the total number of mistakes identified by CrossWeigh, and actual mistakes is the true positives among all identifications. From the results, we can see that when the base model is Flair, CrossWeigh is able to spot more than 75% of label mistakes, while maintaining a precision about 25%. It is worth noting that 25% is a reasonably high precision, given that the label mistake ratio is only 5.38%. The 75% recall indicates that CrossWeigh is able to identify most of the label mistakes, which are extremely valuable to improve the annotation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Study</head><p>We study how CrossWeigh performs with different hyper-parameters, i.e., t (the number of iterations that we run mistake estimation), k (the number of folds in mistake estimation), and (the weight scaling factor of identified potential mistakes).</p><p>In principle, a larger t usually gives us a more stable mistake estimation. However, a larger t also requires more computation resources. In our experiments (see <ref type="table" target="#tab_13">Table 6</ref>), we find that t = 3 provides a good enough result.</p><p>Specifically, during mistake estimation, we have to choose the number of folds to partition the data. The more partitions made, the smaller each D i is and the fewer sentences will be filtered, leading to more training data train_set i and better trained   M i . On the other hand, this is at the cost of higher computational expense. As shown in <ref type="table" target="#tab_15">Table 7</ref>, we observe that k = 5, k = 10 are significantly better than k = 2. In fact, when k = 2, each train_set i has only around 5000 sentences and 1500 entities inside. These numbers become 7000 and 4000 when k = 5, and 9000 and 7000 when k = 10.</p><p>As we mentioned before, the value can be chosen by estimating the quality of mistake estimation. <ref type="table" target="#tab_16">Table 8</ref> presents some results when other values are used. = 0.3 leads to the worst performance. Since our estimation does not have high precision, assigning to a low value like 0.3 may not be a good choice. Interestingly = 0.5 performs on par with = 0.7, and even slightly better in the original test set. We hypothesize that this is because there are some ambiguous sentences that we did not count during estimating the quality of mistake estimation, see Section 5, and the actual precision could be higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Other Datasets</head><p>To show the generalizability of our method across domains and languages, we further evaluate CrossWeigh on an emerging-entity NER dataset from WNUT'17 and a Sinhalese NER dataset from LORELEI 4 . Sinhalese is a low-resource, morphology-rich language. For WNUT'17, we use the Flair as our base NER algorithm. For Sinhalese, we use BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> followed by a BiLSTM-CRF as our base NER algorithm. We use the same parameters as used in the previous CoNLL03 experiments, namely k = 10, t = 3, = 0.7.</p><p>The results averaged across 5 runs are reported in   Training with CrossWeigh leads to a significantly higher F1 and a smaller standard deviation. This suggests that CrossWeigh works well in other datasets and languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Case Studies</head><p>Test Set Correction. Despite the label mistakes that we have corrected, we also find some ambiguous but consistent cases. For instances, (1) All NBA/NHL divisions such as "CENTRAL DI-VISION", "WESTERN DIVISION" were annotated as MISC, while all European leagues, such as "SPANISH FIRST DIVISION" and "ENGLISH PREMIER LEAGUE", are not marked as MISC correctly -only "SPANISH" and "ENGLISH" are labelled as MISC. And (2) "Team A at Team B" is a way to say "Team A" as an away team playing with Team B as a home team. However, in almost all cases (only 1 exception out of more than 100), "Team A" was labelled as ORG while "Team B" was labelled as LOC. For example, in "MINNESOTA AT MILWAUKEE", "NEW YORK AT CALIFOR-NIA", and "ORLANDO AT LA LAKERS", the second sports team "MILWAUKEE", "CALIFOR-NIA" and "LA LAKERS" were always labelled as LOC. Because these parts behave consistently and generally follow the annotation guideline, we didn't touch them during the test set correction. CrossWeigh Framework. The mistakes in the training set can harm the generalizability of the trained model. For example, in <ref type="table" target="#tab_3">Table 11</ref>, the original training sentence "Hapoel Haifa 3 Maccabi Tel Aviv 1" contains a label mistake, because "Maccabi Tel Aviv" is a sports team but was not annotated completely. Interestingly, there is a similar sentence in the test set -"Hapoel Jerusalem 0 Maccabi Tel Aviv 4". In all 5 different runs of the original Flair model, they failed to predict correctly that "Maccabi Tel Aviv" in the test sentence as ORG because of the label mistake in the training sentence, even though "ORG number ORG number" is an obvious pattern in the training set. In CrossWeigh, this label mistake in the training set was detected in all t = 3 iterations and therefore assigned a very low weight during training. After that, in all 5 different runs of Flair w/ CrossWeigh, they successfully predict that "Maccabi Tel Aviv" is ORG as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>In this section, we review related works from three aspects, mistake identification, cross validation &amp; boosting, and NER algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Mistake Identification</head><p>Researchers have noticed the label mistakes in sophisticated natural language processing tasks for a while. For example, it is reported that the interannotator agreement is about 97% on the Penn Treebank POS tagging dataset <ref type="bibr" target="#b11">(Manning, 2011;</ref><ref type="bibr" target="#b20">Subramanya et al., 2010)</ref>.</p><p>There are a few attempts towards detecting label mistakes automatically. For example, Nakagawa and Matsumoto (2002) designed a support vector machine-based model to assign weights to examples that were hard to classify in the POS tagging task. <ref type="bibr" target="#b5">Helgadóttir et al. (2014)</ref> further applied previous detection models and manually corrected Icelandic Frequency Dictionary <ref type="bibr" target="#b15">(Pind et al., 1991)</ref> POS tagging dataset. However, these two methods are specifically developed for POS tagging and cannot be directly applied to NER.</p><p>Recently, <ref type="bibr" target="#b16">Rehbein and Ruppenhofer (2017)</ref> extends variational inference with active learning to detect label mistakes in "silver standard" data generated by machines. In this paper, we focus on detecting label mistakes in "gold standard" data, which is a different scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Cross Validation &amp; Boosting</head><p>Our mistake estimation module shares some similarity with cross validation. Applying cross valida-  tion to the training set is the same as our mistake estimation module, except that we have an entity disjoint filtering step. Experiments in <ref type="table" target="#tab_10">Table 4</ref> show that this step is crucial to our performance gain. The choice of ten folds also stems from cross validation <ref type="bibr" target="#b6">(Kohavi, 1995)</ref>. Another similar thread of work is boosting, such as Adaboost <ref type="bibr" target="#b4">(Freund et al., 1999;</ref>. For example, <ref type="bibr" target="#b0">Abney et al. (1999)</ref> has applied Adaboost on the Penn Treebank POS tagging dataset and gained encouraging results on model performance. In boosting algorithms, the training data is assumed to be perfect. Therefore, it trains models using the full training set and then increases the weights of training instances that fails the current model in the next round of learning. In contrast, we decrease the weights of sentences that differ from the model built upon the entity disjoint training set. More importantly, our framework is a better fit for neural models, because they can likely overfit the training data and thus being bad choices as weak classifiers in boosting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">NER Algorithms</head><p>Neural models have been widely used for Named Entity Recognition, and the state-of-the-art models integrate LSTMs, conditional random field and language models <ref type="bibr" target="#b7">(Lample et al., 2016;</ref><ref type="bibr" target="#b10">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b9">Liu et al., 2018b;</ref><ref type="bibr" target="#b14">Peters et al., 2018;</ref><ref type="bibr" target="#b2">Akbik et al., 2018)</ref>. In this paper, we focus on improving the annotation quality for NER, and our method has a big potential to help other methods, especially for noisy datasets <ref type="bibr" target="#b19">(Shang et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion &amp; Future work</head><p>In this paper, we explore and correct the label mistakes in the CoNLL03 NER dataset. Based on the corrected test set, we re-evaluate most of recent NER models. We further propose a novel framework, CrossWeigh, that is able to detect label mistakes in the training set and then train a more robust NER model accordingly. Extensive experiments demonstrate the effectiveness of CrossWeigh on three datasets and also indicate the potentials of using CrossWeigh to improve the annotation quality during the label curation process.</p><p>In future, we plan to extend our framework into an iterative setting, similar to those boosting algorithms. The bottleneck of doing this lies in the efficiency problems of training multiple deep neural models hundreds of times. One solution to overcome it is to apply meta learning. We can first train a meta model and only fine-tune on different training data on each fold. In this way, we can identify label mistakes more accurately and obtain a series of weighted models at the end.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of our proposed CrossWeigh framework. It can better handle label mistakes, identify low quality annotations and conduct learning from a weighted training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We release both the corrected test set and the implementation of CrossWeigh framework 2 .</figDesc><table><row><cell>• We correct label mistakes in the test set of the</cell></row><row><cell>CoNLL03 NER dataset and re-evaluate popular</cell></row><row><cell>NER models. This establishes a more accurate</cell></row><row><cell>NER benchmark.</cell></row></table><note>• We propose a novel framework CrossWeigh to accommodate the mistakes during the model training stage. The proposed framework fits most of, if not all, NER models.• Extensive experiments demonstrate the signifi- cant, robust test F 1 score improvements of plug- ging NER models into our proposed framework on three datasets, not only CoNLL03 but also emerging-entity and low-resource datasets. Reproducibility.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Typical Examples of Our Corrections on the CoNLL03 NER dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>CoNLL03 Re-Evaluation: Test F 1 scores and standard deviations on both original and corrected datasets. The results are based on 5 different runs.lar NER algorithms: • LSTM-CRF (Lample et al., 2016) incorporates long short term memory (LSTM) neural network with conditional random field (CRF). It also uses a word-wise character LSTM. • LSTM-CNNs-CRF (Ma and Hovy, 2016) has a similar structure as LSTM-CRF, but captures character-level information through a convolu- tional neural network (CNN) over the character embedding.• VanillaNER (Liu et al., 2018a) also extends LSTM-CRF and LSTM-CNNs-CRF by using a sentence-wise character LSTM.• ELMo (Peters et al., 2018) extends LSTM-CRF and leverages pre-trained word-level language models for better contextualized representations. • Flair</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Algorithm 1 :</head><label>1</label><figDesc>Our CrossWeigh Framework Input: A NER model f , the training set D = {x 1 , . . . , x n }, {y 1 , . . . , y n } , and hyper-parameters k, t, and . Output: A final NER model for</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Test F 1 scores and its standard deviations of models trained without or with CrossWeigh.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>CrossWeigh 93.19 (±0.09) 94.18 (±0.06) − Entity Disjoint 92.88 (±0.11) 93.84 (±0.08) + Random Discard 93.01 (±0.10) 93.94 (±0.10)</figDesc><table><row><cell>Method</cell><cell>Original</cell><cell>Corrected</cell></row><row><cell>w/o CrossWeigh</cell><cell cols="2">92.87 (±0.08) 93.89 (±0.06)</cell></row><row><cell>w/</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Importance of Entity Disjoint Filtering.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Different c i Estimation Heuristics.</figDesc><table><row><cell>t</cell><cell>Original</cell><cell>Corrected</cell></row><row><cell>1</cell><cell>93.09 (±0.14)</cell><cell>94.07 (±0.09)</cell></row><row><cell>5</cell><cell>93.23 (±0.10)</cell><cell>94.14 (±0.08)</cell></row><row><cell>3</cell><cell>93.19 (±0.09)</cell><cell>94.18 (±0.06)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Different Numbers of Iterations t.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Different Numbers of Folds k.</figDesc><table><row><cell></cell><cell>Original</cell><cell>Corrected</cell></row><row><cell>0.3</cell><cell>92.79 (±0.14)</cell><cell>93.68 (±0.15)</cell></row><row><cell>0.5</cell><cell>93.21 (±0.09)</cell><cell>94.18 (±0.07)</cell></row><row><cell>0.9</cell><cell>93.01 (±0.10)</cell><cell>93.96 (±0.09)</cell></row><row><cell>0.7</cell><cell>93.19 (±0.09)</cell><cell>94.18 (±0.06)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 :</head><label>8</label><figDesc>Different Weight Adjustments .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 .</head><label>10</label><figDesc>One can observe quite similar results as those in the previous CoNLL03 experiments.</figDesc><table><row><cell>4 LDC2018E57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 9 :</head><label>9</label><figDesc>Quality of noise estimation. The number of true mistakes, based on our manual correction, is 186. The potential mistakes are counted based on average of 3 runs.</figDesc><table><row><cell>Dataset</cell><cell>w/o CrossWeigh</cell><cell>w/ CrossWeigh</cell></row><row><cell>WNUT'17</cell><cell>48.96 (±0.97)</cell><cell>50.03 (±0.40)</cell></row><row><cell>Sinhalese</cell><cell>66.34 (±0.34)</cell><cell>67.68 (±0.21)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>Applying CrossWeigh on other datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>Original Annotations [Hapoel Haifa]{ORG}, [Tel Aviv]{ORG} [Hapoel Jerusalem]{ORD}, [Maccabi Tel Aviv]{ORG} Correct Annotations [Hapoel Haifa]{ORG}, [Maccabi Tel Aviv]{ORG} [Hapoel Jerusalem]{ORD}, [Maccabi Tel Aviv]{ORG} Action Result Flair Assumes this sentence is equally reliable as others. [Hapoel Jerusalem]{ORD}, [Tel Aviv]{ORG} Flair w/ CrossWeight Lowers the weight of this sentence as mistakes. [Hapoel Jerusalem]{ORD}, [Maccabi Tel Aviv]{ORG}</figDesc><table><row><cell></cell><cell>Training Set</cell><cell>Test Set</cell></row><row><cell>Text</cell><cell>Hapoel Haifa 3 Maccabi Tel Aviv 1</cell><cell>Hapoel Jerusalem 0 Maccabi Tel Aviv 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>Case Study on the CoNLL03 dataset. Errors are marked with red</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ZihanWangKi/ CrossWeigh 3 https://www-nlpir.nist.gov/related_ projects/muc/proceedings/ne_task.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledge</head><p>We thank all reviewers for valuable comments and suggestions that brought improvements to our final version. Research was sponsored in part by U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), DARPA under Agreement No. W911NF-17-C-0099, National Science Foundation IIS 16-18481, IIS 17-04532, and IIS-17-41317, DTRA HD-TRA11810026, Google Ph.D. Fellowship and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov). Any opinions, findings, and conclusions or recommendations expressed in this document are those of the author(s) and should not be interpreted as the views of any U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon. This research was supported by grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov). This work was supported by Contracts HR0011-15-C-0113 and HR0011-18-2-0052 with the US Defense Advanced Research Projects Agency (DARPA).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Boosting applied to tagging and pp attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pooled contextualized embeddings for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-20" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A short introduction to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal-Japanese Society For Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1612</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Correcting errors in a new gold standard for tagging icelandic text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sigrún</forename><surname>Helgadóttir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrafn</forename><surname>Loftsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiríkur</forename><surname>Rögnvaldsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation, LREC 2014<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05-26" />
			<biblScope unit="page" from="2944" to="2948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A study of cross-validation and bootstrap for accuracy estimation and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="1137" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient contextualized representation: Language model pruning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1215" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">Fangzheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging from 97% to 100%: Is it time for some linguistics?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-19400-9_14</idno>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing -12th International Conference, CI-CLing</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-02-20" />
			<biblScope unit="page" from="171" to="189" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detecting errors in corpora using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuji</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Íslensk orðtíðnibók [the icelandic frequency dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörgen</forename><surname>Pind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friðrik</forename><surname>Magnússon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefán</forename><surname>Briem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<pubPlace>Reykjavik, Iceland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Institute of Lexicography, University of Iceland</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting annotation noise in automatically labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Rehbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Ruppenhofer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1107</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1160" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning</title>
		<meeting>the Seventh Conference on Natural Language Learning<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05-31" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
	<note>cooperation with HLT-NAACL 2003</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved boosting algorithms using confidence-rated predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning named entity tagger using domain-specific dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2054" to="2064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient graph-based semisupervised learning of structured tagging models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
