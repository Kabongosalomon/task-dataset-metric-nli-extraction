<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Biomedical Event Extraction with Hierarchical Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kung-Hsiang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Yang</surname></persName>
							<email>yangmu@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Biomedical Event Extraction with Hierarchical Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Biomedical event extraction is critical in understanding biomolecular interactions described in scientific corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Biomedical event extraction is a task that identifies a set of actions among proteins or genes that are associated with biological processes from natural language texts <ref type="bibr">(Kim et al., 2009</ref><ref type="bibr">(Kim et al., , 2011</ref>. Development of biomedical event extraction tools enables many downstream applications, such as domain-specific text mining <ref type="bibr" target="#b0">(Ananiadou et al., 2015;</ref><ref type="bibr" target="#b9">Spangher et al., 2020)</ref>, semantic search engines <ref type="bibr">(Miyao et al., 2006)</ref> and automatic population and enrichment of database <ref type="bibr">(Hirschman et al., 2012)</ref>.</p><p>A typical event extraction system 1) finds triggers that most clearly demonstrate the presence of events, 2) recognizes the protein participants (arguments), and 3) associates the arguments with the corresponding event triggers. For instance, the Nodes associated with the tokens in the example sentence are boldfaced. Bidirectional edges imply hierarchical relation between concept and semantic nodes. The word "induces" is a trigger of a Positive regulation event, whose trigger role and corresponding argument role cannot be easily determined with only textual input. The KG provides clues for identifying this trigger and its corresponding arguments given the red and blue double line reasoning paths connecting nodes BMP-6, Induce, Phosphorylation, and Positive regulation of biological process. We can infer that: 1) "induces" is an action of biological function, 2) a biological function can be quantified by positive regulation, and 3) positive regulation can result in phosphorylation.</p><p>sentence "Protein A inhibits the expression of Protein B" will be annotated with two nested events: Gene expression(Trigger: expression, Arg-Theme: Protein B) and Negative Regulation(Trigger: inhibits, Arg-Theme: Gene expression(Protein B), Arg-Cause: Protein A). Early attempts on biomedical event extraction adopted hand-crafted features <ref type="bibr">(Björne et al., 2009;</ref><ref type="bibr">Björne and Salakoski, 2011;</ref><ref type="bibr" target="#b6">Riedel and McCallum, 2011;</ref><ref type="bibr" target="#b11">Venugopal et al., 2014a)</ref>. Recent advances have shown improvements using deep neural networks via distributional word representations in the arXiv:2009.09335v3 [cs.CL] 12 Oct 2020 biomedical domain <ref type="bibr">(Moen and Ananiadou, 2013;</ref><ref type="bibr" target="#b4">Rao et al., 2017a;</ref><ref type="bibr">Björne and Salakoski, 2018;</ref><ref type="bibr" target="#b7">ShafieiBavani et al., 2019)</ref>. <ref type="bibr" target="#b14">Li et al. (2019)</ref> further extends the word representations with embeddings of descriptive annotations from a knowledge base and demonstrates the importance of domain knowledge in biomedical event extraction.</p><p>However, encoding knowledge with distributional embeddings does not provide adequate clues for identifying challenging events with nonindicative trigger words and nested structures. These embeddings do not contain structural or relational information about the biomedical entities. To overcome this challenge, we present a framework that incorporates knowledge from hierarchical knowledge graphs with graph neural networks (GNN) on top of a pre-trained language model.</p><p>Our first contribution is a novel representation of knowledge as hierarchical knowledge graphs containing both conceptual and semantic reasoning paths that enable better trigger and word identification based on Unified Medical Language System (UMLS), a biomedical knowledge base. <ref type="figure" target="#fig_0">Fig.  1</ref> shows an example where the Positive Regulation event can be better identified with knowledge graphs and factual relational reasoning. Our second contribution is a new GNN, Graph Edgeconditioned Attention Networks (GEANet), that encodes complex domain knowledge. By integrating edge information into the attention mechanism, GEANet has greater capabilities in reasoning the plausibility of different event structure through factual relational paths in knowledge graphs (KGs).</p><p>Experiments show that our proposed method achieved state-of-the-art results on the BioNLP 2011 event extraction task <ref type="bibr">(Kim et al., 2011)</ref>. 1 2 Background UMLS Knowledge Base. Unified Medical Language System (UMLS) is a knowledge base for biomedical terminology and standards, which includes three knowledge sources: Metathesaurus, Semantic Network, and Specialist Lexicon and Lexical Tools <ref type="bibr">(Bodenreider, 2004)</ref>. We use the former two sources to build hierarchical KGs. The concept network from Metathesaurus contains the relationship between each biomedical concept pairs, while each concept contains one or more semantic types 1 Our code for pre-proecessing, modeling, and evaluation is available at https://github.com/PlusLabNLP/ GEANet-BioMed-Event-Extraction. that can be found in the semantic network. The concept network provides direct definition lookup of the recognized biomedical terms, while the semantic network supports with additional knowledge in the semantic aspect. Example tuples can be found in <ref type="figure" target="#fig_0">Figure 1</ref>. 2 There are 3.35M concepts, 10 concept relations, 182 semantic types, and 49 semantic relations in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>Our event extraction framework builds upon the pre-trained language model, SciBERT <ref type="bibr">(Beltagy et al., 2019)</ref>, and supplement it with a novel graph neural network model, GEANet, that encodes domain knowledge from hierarchical KGs. We will first illustrate each component and discuss how training and inference are done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Knowledge Graph Modeling</head><p>The two knowledge sources discussed in Section 2 are jointly modeled as a hierarchical graph for each sentence, which we refer to as a sentence graph. Each sentence graph construction consists of three steps: concept mapping, concept network construction, and semantic type augmentation. The first step is to map each sentence in the corpus to UMLS biomedical concepts with MetaMap, an entity mapping tool for UMLS concepts <ref type="bibr" target="#b1">(Aronson, 2001</ref>). There are 7903 concepts (entities) being mapped from the corpus, denoted as K. The next step is concept network construction, where a minimum spanning tree (MST) that connects mapped concepts in the previous step is identified, forming concept reasoning paths. This step is NPcomplete. <ref type="bibr">3</ref> We adopt a 2-approximate solution that constructs a global MST for the corpora GE'11 by running breadth-first search, assuming all edges are of unit distance. To prune out less relevant nodes and to improve computation efficiency, concept nodes that are not in K with less than T neighbors in K are removed. <ref type="bibr">4</ref> The spanning tree for each sentence is then obtained by depth-first search on the global MST. Each matched token in the corpus is also included as a token node in the sentence graph, connecting with corresponding concept node. Finally, the semantic types for each concept node are modeled as nodes that are linked with associated concept nodes in the sentence graph. Two semantic type nodes will also be linked if they have known relationships in the semantic network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GEANet</head><p>The majority of existing graph neural networks (GNN) consider only hidden states of nodes and adjacency matrix without modeling edge information. To properly model the hierarchy of the graph, it is essential for the message passing function of a GNN to consider edge features. We propose Graph Edge Conditioned Attention Networks (GEANet) to integrate edge features into the attention mechanism for message propagation. The node embeddings update of GEANet at the l-th layer can be expressed as follows:</p><formula xml:id="formula_0">x (l) i = MLP θ x (l−1) i + j∈N (i) ai,j · x (l−1) j (1) ai,j = exp (MLP ψ (ei,j)) k∈N (i) exp (MLP ψ (e i,k )) (2) where x (l)</formula><p>i denotes the node embeddings at layer l, e i,j denotes the embedding for edge (i, j), and MLP ψ and MLP θ are two multi-layer perceptrons.</p><p>GEANet is inspired by Edge Conditioned Convolution (ECC), where convolution operation depends on edge type <ref type="bibr" target="#b8">(Simonovsky and Komodakis, 2017</ref>),</p><formula xml:id="formula_1">x (l) i = MLP θ x (l−1) i + j∈N (i) x (l−1) j · MLP ψ (ei,j) (3)</formula><p>Compared to ECC, GEANet is able to determine the relative importance of neighboring nodes with attention mechanism.</p><p>Knowledge Incorporation. We build GEANet on top of SciBERT <ref type="bibr" target="#b3">(Peters et al., 2019)</ref> to incorporate domain knowledge into rich contextualized representations. Specifically, we take the contextual embeddings {h 1 , ..., h n } produced by SciB-ERT as inputs and produces knowledge-aware embeddings {ĥ 1 , ...,ĥ n } as outputs. To initialize the embeddings for a sentence graph, for a mapped token, we project its SciBERT contextual embedding to initialize its corresponding node embedding</p><formula xml:id="formula_2">h i,KG = h i W KG + b KG .</formula><p>Other nodes and edges are initialized by pretrained KG embeddings (details in Section 4.1). To accommodate multiple relations between two entities in UMLS, edge embeddings e i,j are initialized by summing the embeddings of each relation between the nodes i and j. Then we apply layers of GEANet to encode the graph h l i,KG = GEANet(h i,KG ). The knowledgeaware representation is obtained by aggregating SciBERT representations and KG representations,</p><formula xml:id="formula_3">h i = h l i,KG W LM + b LM + h i . 5</formula><p>The process is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> GEANet layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Event Extraction</head><p>The entire framework is trained with a multitask learning pipeline consisting of trigger classification and argument classification, following <ref type="bibr">(Han et al., 2019a,b)</ref>. Trigger classification predicts the trigger type for each token. The predicted score of each token is computed asŷ tri i = MLP tri (ĥ i ). In the argument classification stage, each possible pair of gold trigger and gold entity is gathered and labeled with corresponding argument role. <ref type="bibr">6</ref> The argument scores between the i-th token and j-th token are computed asŷ arg i,j = MLP arg (ĥ i ;ĥ j ), where (; ) denotes concatenation. Cross Entropy</p><formula xml:id="formula_4">loss L t = − 1 N t N t i=1 y t i · logŷ t i ,</formula><p>is used for both tasks, where t denotes task, N t denotes the number of training instances of task t, y t i denotes the ground truth label, andŷ t i denotes the predicted label. The multitask learning minimizes the sum of the two losses L = L tri + L arg in the training stage. During inference, unmerging is conducted to combine identified triggers and arguments for multiple arguments events (Björne and Salakoski, 2011). We adopted similar unmerging heuristics. For Regulation events, we use the same heuristics as <ref type="bibr">Björne et al. (2009)</ref>. For Binding events, we subsume all Theme arguments associated with a trigger 5ĥ i = hi for each token i without mapped concept. <ref type="bibr">6</ref> During inference, predicted triggers are used instead.   into one event such that every trigger corresponds to only one single Binding event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Our models are evaluated on BioNLP 11 GENIA event extraction task (GE'11 ). All models were trained on the training set, validated on the dev set, and tested on the test set. A separate evaluation on Regulation events is conducted to validate the effectiveness of our framework on nested events with non-indicative trigger word. Reported results are obtained from the official evaluator under approximate span and recursive criteria.</p><p>In the preprocessing step, the GE'11 corpora were parsed with TEES preprocessing pipeline <ref type="bibr">(Björne and Salakoski, 2018)</ref>. Tokenization is done by the SciBERT tokenizer. Biomedical concepts in each sentence are then recognized with MetaMap and aligned with their corresponding tokens. The best performing model was found by grid search conducted on the dev set. The edge and node representation in KGs were intialized with 300 dimensional pre-trained embeddings using TransE <ref type="bibr" target="#b15">(Wang et al., 2014)</ref>. The entire framework is optimized with BERTAdam optimizer for a maximum of 100 epochs with batch size of 4. Training is stopped if the dev set F 1 does not improve for 5 consecutive epochs (more details see Appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Analysis</head><p>Comparison with existing methods We compare our method with the following prior works: TEES and Stacked Gen. use SVM-based models with token and sentence-level features <ref type="bibr">(Björne and Salakoski, 2011;</ref><ref type="bibr">Majumder et al., 2016)</ref>;   TEES CNN leverages Convolutional Neural Networks and dependency parsing graph (Björne and Salakoski, 2018); KB-driven T-LSTM adopts an external knowledge base with type and sentence embeddings, into a Tree-LSTM model <ref type="bibr" target="#b14">(Li et al., 2019)</ref>. SciBERT-FT is a fine-tuned SciB-ERT without external resources, the knowledgeagnostic counterpart of GEANet-SciBERT. According to <ref type="table" target="#tab_1">Table 1</ref>, SciBERT-FT achieves similar performance to KB-driven T-LSTM, implying that SciBERT may have stored domain knowledge implicitly during pre-training. Similar hypothesis has also been studied in commonsense reasoning <ref type="bibr" target="#b14">(Wang et al., 2019)</ref>. GEANet-SciBERT achieves an absolute improvement of 1.41% in F 1 on the test data compared to the previous state-of-theart method. In terms of Regulation events, <ref type="table" target="#tab_2">Table  2</ref> shows that GEANet-SciBERT outperforms the previous system and fine-tuned SciBERT by 3.19% and 1.39% in F 1.</p><p>Ablation study To better understand the importance of different model components, ablation study is conducted and summarized in <ref type="table" target="#tab_4">Table 3</ref>. GEANet achieves the highest F 1 when compared to two other GNN variants, ECC and GAT <ref type="bibr" target="#b10">(Veličković et al., 2018)</ref>, demonstrating its stronger knowledge incorporation capacity. Hierarchical knowledge graph representation is also shown to be critical. Removing semantic type (STY) nodes from hierarchical KGs leads to performance drop. Impact of amount of training data Model performance on different amount of randomly sampled training data is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. GEANet-SciBERT shows consistent improvement over finetuned SciBERT across different fractions. The performance gain is slightly larger with less training data. This illustrates the robustness of GEANet in integrating domain knowledge and its particular advantage under low-resource setting.</p><p>Error Analysis By comparing the predictions from GEANet-SciBERT and gold events in the dev set, two major failed cases are identified:</p><p>• Adjective Trigger: Most events are associated with a verb or noun trigger. Adjective triggers are scarce in the training set (∼7%), which poses a challenge to identify this type of trigger. Although knowledge-aware methods should be able to resolve these errors theoretically, these adjective triggers often cannot be linked with UMLS concepts. Without proper grounding, it is hard for our model to recognize these triggers.</p><p>• Misleading Trigger: Triggers providing "clues" about incorrect events can be misleading. For instance, Furthermore, expression of an activated PKD1 mutant enhances HPK1-mediated NFkappaB activation.</p><p>Our model predicts expression as a trigger of type Gene expression, while the gold label is Positive regulation. Despite that fact that our model is capable of handling such scenarios sometimes given grounded biomedical concepts and factual reasoning paths, there is still room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>Event Extraction Most existing event extraction systems focus on extracting events in news. Early attempts relied on hand-crafted features and a pipeline architecture <ref type="bibr">(Gupta and Ji, 2009;</ref><ref type="bibr">Li et al., 2013)</ref>. Later studies gained significant improvement from neural architectures, such as convolutional neural networks <ref type="bibr">(Chen et al., 2015;</ref><ref type="bibr">Nguyen and Grishman, 2015)</ref>, and recurrent neural networks <ref type="bibr">(Nguyen et al., 2016)</ref>. More recent studies leverages large pre-trained language models to obtain richer contextual information <ref type="bibr" target="#b13">(Wadden et al., 2019;</ref><ref type="bibr">Lin et al., 2020)</ref>. Another line of works utilized GNN to enhance event extraction performance. <ref type="bibr">Liu et al. (2018)</ref> applied attention-based graph convolution networks on dependency parsing trees. We instead propose a GNN, GEANet, for integrating domain knowledge into contextualized embeddings from pre-trained language models. Biomedial Event Extraction Event extraction for biomedicine is more challenging due to higher demand for domain knowledge. BioNLP 11 GE-NIA event extraction task (GE'11 ) is the major benchmark for measuring the quality of biomedical event extraction system <ref type="bibr">(Kim et al., 2011)</ref>. Similar to event extraction in news domain, initial studies tackle biomedical event extraction with humanengineered features and pipeline approaches <ref type="bibr">(Miwa et al., 2012;</ref><ref type="bibr">Björne and Salakoski, 2011)</ref>. Great portion of recent works observed significant gains from neural models <ref type="bibr" target="#b12">(Venugopal et al., 2014b;</ref><ref type="bibr" target="#b5">Rao et al., 2017b;</ref><ref type="bibr">Jagannatha and Yu, 2016;</ref><ref type="bibr">Björne and Salakoski, 2018)</ref>. Li et al. <ref type="formula">(2019)</ref> incorporated information from Gene Ontology, a biomedical knowledge base, into tree-LSTM models with distributional representations. Instead, our strategy is to model two knowledge graphs from UMLS hierarchically with conceptual and semantic reasoning paths, providing stronger clues for identifying challenging events in biomedical corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed a framework to incorporate domain knowledge for biomedical event extraction. Evaluation results on GE'11 demonstrated the efficacy of GEANet and hierarchical KG representation in improving extraction of non-indicative trigger words associated nested events. We also show that our method is robust when applied to different amount of training data, while being advantageous in low-resource scenarios. Future works include grounding adjective triggers to knowledge bases, better biomedical knowledge representation and extracting biomedical events at document level. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>Our models are implemented in PyTorch <ref type="bibr" target="#b2">(Paszke et al., 2019)</ref>. Hyper-parameters are found by grid search within search range listed in <ref type="table" target="#tab_6">Table 4</ref>. The hyper-parameters of the best performing model are summarized in 5. All experiments are conducted on a 12-CPU machine running CentOS Linux 7 (Core) and NVIDIA RTX 2080 with CUDA 10.1.</p><p>To pre-train KGE, we leverage the TransE implementation from OpenKE <ref type="bibr">(Han et al., 2018)</ref>. All tuples associated with the selected nodes described in Section 3.1 are used for pre-training with margin loss and negative sampling, where γ denotes margin, and d(x, x ) denotes the − 1 distance between x and x . h and t are embeddings of head and tail entities from the gold training sets S with relation . (h , ,t ) denotes a corrupted tuplet with either the head or tail entity replaced by a random entity. TransE is optimized using Adam (Kingma and Ba, 2015) with hyperparameters illustrated in <ref type="table" target="#tab_8">Table 6</ref>. Every 50 epochs, the model checkpoint is saved if the mean reciprocal rank on the development set improve from the last checkpoint; otherwise, training will be stopped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset</head><p>The statistics of GE'11 is shown in 7. The corpus contains 14496 events with 37.2% containing nested structure (Björne and Salakoski, 2011). <ref type="bibr">7</ref> We use the official dataset split for all the results reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameter</head><p>Range Relation MLP dim.</p><p>{300, 500, 700, 1000} Trigger MLP dim.</p><p>{300, 500, 700, 1000} Learning rate { 1 × 10 −5 , 3 × 10 −5 , 5 × 10 −5 }    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of a UMLS-based hierarchical KG assisting event extraction. Circles represent concept nodes and triangles represent semantic nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of knowledge incorporation. Contextualized embeddings for each token are generated by SciBERT. GEANet updates node embeddings for v 1 , v 2 , and v 3 via corresponding sentence graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Performance comparison on the test set w.r.t. different amount of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>,t)∈S (h , ,t ) / ∈S max(0, d(h, , t) − d(h , , t ) + γ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Model comparison on GE'11 test set.</figDesc><table><row><cell>Model</cell><cell>Recall Prec. F1</cell></row><row><cell>KB-driven T-LSTM</cell><cell>41.73 55.73 47.72</cell></row><row><cell>SciBERT-FT</cell><cell>45.39 54.48 49.52</cell></row><row><cell>GEANet-SciBERT</cell><cell>47.23 55.21 50.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on the Regulation events of the test set (including Regulation, Positive Regulation, and Negative Regulation sub-events).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study over different components.</figDesc><table><row><cell>64</cell><cell></cell></row><row><cell>60</cell><cell></cell></row><row><cell>56 F1 (%)</cell><cell></cell></row><row><cell>20 48 52</cell><cell>40 Percentage of Training Data 60 80 GAENet-SciBERT 100 SciBERT-FT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Nanyun Peng. 2019a. Deep structured neural network for event temporal relation extraction. In The 2019 SIGNLL Conference on Computational Natural Language Learning (CoNLL). Long Papers), pages 73-82, Sofia, Bulgaria. Association for Computational Linguistics. Ying Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020. A joint neural model for information extraction with global features. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7999-8009, Online. Association for Computational Linguistics.</figDesc><table><row><cell>the Association for Computational Linguistics (Vol-</cell><cell>Rujun Han, I Hsu, Mu Yang, Aram Galstyan, Ralph</cell></row><row><cell>ume 1: Xiao Liu, Zhunchen Luo, and Heyan Huang. 2018. Jointly multiple events extraction via attention-</cell><cell>Weischedel, and Rujun Han, Qiang Ning, and Nanyun Peng. 2019b. Joint event and temporal relation extraction with shared representations and structured prediction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 434-</cell></row><row><cell>based graph information aggregation. In Proceed-Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-ings of the 2018 Conference on Empirical Methods ERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-in Natural Language Processing, pages 1247-1256, Brussels, Belgium. Association for Computational Linguistics.</cell><cell>444, Hong Kong, China. Association for Computa-tional Linguistics. Xu Han, Shulin Cao, Lv Xin, Yankai Lin, Zhiyuan Liu, Maosong Sun, and Juanzi Li. 2018. Openke: An open toolkit for knowledge embedding. In Proceed-</cell></row><row><cell>guage Processing (EMNLP-IJCNLP), pages 3615-Amit Majumder, Asif Ekbal, and Sudip Kumar Naskar. 3620, Hong Kong, China. Association for Computa-tional Linguistics. Jari Björne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-tracting complex biological events with rich graph-2016. Biomolecular event extraction using a stacked generalization based classifier. In Proceedings of the 13th International Conference on Natural Lan-guage Processing, pages 55-64, Varanasi, India. NLP Association of India.</cell><cell>ings of EMNLP. Lynette Hirschman, Gully AP Burns, Martin Krallinger, Cecilia Arighi, K Bretonnel Cohen, Alfonso Valencia, Cathy H Wu, Andrew Chatr-Aryamontri, Karen G Dowell, Eva Huala, et al. 2012. Text mining for the biocuration workflow.</cell></row><row><cell>based feature sets. In Proceedings of the BioNLP Makoto Miwa, Paul Thompson, and Sophia Ananiadou. 2009 Workshop Companion Volume for Shared Task, 2012. Boosting automatic event extraction from the pages 10-18, Boulder, Colorado. Association for literature using domain adaptation and coreference Computational Linguistics. resolution. Bioinformatics, 28(13):1759-1765.</cell><cell>Database, 2012. Abhyuday N Jagannatha and Hong Yu. 2016. Bidi-rectional RNN for medical event detection in elec-tronic health records. In Proceedings of the 2016</cell></row><row><cell>Jari Björne and Tapio Salakoski. 2011. Generaliz-Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-ing biomedical event extraction. In Proceedings of masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-BioNLP Shared Task 2011 Workshop, pages 183-nomiya, and Jun'ichi Tsujii. 2006. Semantic re-191, Portland, Oregon, USA. Association for Com-trieval for the accurate identification of relational putational Linguistics. concepts in massive textbases. In Proceedings of</cell><cell>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 473-482, San Diego, California. Association for Computational Linguis-tics.</cell></row><row><cell>the 21st International Conference on Computational Jari Björne and Tapio Salakoski. 2018. Biomedi-Linguistics and 44th Annual Meeting of the Asso-cal event extraction using convolutional neural net-ciation for Computational Linguistics, pages 1017-works and dependency parsing. In Proceedings of 1024, Sydney, Australia. Association for Computa-the BioNLP 2018 workshop, pages 98-108, Mel-tional Linguistics. bourne, Australia. Association for Computational Linguistics. Olivier Bodenreider. 2004. The unified medical lan-guage system (umls): integrating biomedical termi-nology. Nucleic acids research, 32(suppl 1):D267-SPFGH Moen and Tapio Salakoski2 Sophia Anani-adou. 2013. Distributional semantics resources for biomedical text processing. Proceedings of LBM, pages 39-44. D270. Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and Thien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-ishman. 2016. Joint event extraction via recurrent</cell><cell>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun'ichi Tsujii. 2009. Overview of BioNLP'09 shared task on event extraction. In Pro-ceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 1-9, Boulder, Col-orado. Association for Computational Linguistics. Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, Ngan Nguyen, and Jun'ichi Tsujii. 2011. Overview of bionlp shared task 2011. In Proceed-ings of the BioNLP shared task 2011 workshop, pages 1-6. Association for Computational Linguis-tics.</cell></row><row><cell>Jun Zhao. 2015. Event extraction via dynamic multi-pooling convolutional neural networks. In Proceed-ings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-neural networks. In Proceedings of the 2016 Con-ference of the North American Chapter of the As-sociation for Computational Linguistics: Human Language Technologies, pages 300-309, San Diego,</cell><cell>Diederick P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR).</cell></row><row><cell>tional Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers), pages 167-176, California. Association for Computational Linguis-tics.</cell><cell>Diya Li, Lifu Huang, Heng Ji, and Jiawei Han. 2019. Biomedical event extraction based on knowledge-</cell></row><row><cell>Beijing, China. Association for Computational Lin-guistics. Prashant Gupta and Heng Ji. 2009. Predicting un-known time arguments based on cross-event prop-agation. In Proceedings of the ACL-IJCNLP 2009 Thien Huu Nguyen and Ralph Grishman. 2015. Event detection and domain adaptation with convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-guistics and the 7th International Joint Conference</cell><cell>driven tree-lstm. In Proceedings of the 2019 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, Volume 1 (Long and Short Pa-pers), pages 1421-1430.</cell></row><row><cell>Conference Short Papers, pages 369-372, Suntec, on Natural Language Processing (Volume 2: Short</cell><cell>Qi Li, Heng Ji, and Liang Huang. 2013. Joint event</cell></row><row><cell>Singapore. Association for Computational Linguis-Papers), pages 365-371, Beijing, China. Associa-</cell><cell>extraction via structured prediction with global fea-</cell></row><row><cell>tics. tion for Computational Linguistics.</cell><cell>tures. In Proceedings of the 51st Annual Meeting of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Hyper-paramter search range for fine-tuning SciBERT. The dataset can be downloaded from http://bionlpst.dbcls.jp/GE/2011/downloads/.</figDesc><table><row><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell>Relation MLP dim.</cell><cell>300</cell></row><row><cell>Trigger MLP dim.</cell><cell>300</cell></row><row><cell>Learning rate</cell><cell>3 × 10 −5</cell></row><row><cell>GEANet node dim.</cell><cell>300</cell></row><row><cell>GEANet edge dim.</cell><cell>300</cell></row><row><cell>GEANet layers</cell><cell>2</cell></row><row><cell>Dropout rate</cell><cell>0.2</cell></row></table><note>7</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyper-paramters of the best performing GEANet-SciBERT model.</figDesc><table><row><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell>Learning rate</cell><cell>0.5</cell></row><row><cell>Margin</cell><cell>3</cell></row><row><cell>Batch size</cell><cell>128</cell></row><row><cell># corrupted tuplets / # gold tuplets</cell><cell>25</cell></row><row><cell># Epochs</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Hyper-paramters for pre-training KGE.</figDesc><table><row><cell>Metric</cell><cell>Number</cell></row><row><cell>events</cell><cell>14496</cell></row><row><cell>sentences</cell><cell>11581</cell></row><row><cell>nested events</cell><cell>37.2%</cell></row><row><cell>intersentence events</cell><cell>6.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>GE'11 dataset statistics</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">There are several bi-directional relations between some concepts. We only show one direction for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Finding a MST on a subset of nodes (K) is known as a Steiner tree problem.4  T is empirically set to be 35.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Rujun Han for helpful advice during the development of our model. We also appreciate insightful feedback from PLUSLab members, and the anonymous reviewers. This research was sponsored by an NIH R01 (LM012592) and the Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007. The views and conclusions of this paper are those of the authors and do not reflect the official policy or position of NIH, IARPA, or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Event-based text mining for biology and functional genomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raheel</forename><surname>Nawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcnaught</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas B</forename><surname>Kell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in functional genomics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="230" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Effective mapping of biomedical text to the umls metathesaurus: the metamap program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alan R Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMIA Symposium</title>
		<meeting>the AMIA Symposium</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Biomedical event extraction using abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Biomedical event extraction using Abstract Meaning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2315</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and robust joint models for biomedical event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elaheh</forename><surname>Shafieibavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">Jimeno</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04822</idno>
		<title level="m">Global locality in event extraction</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enabling low-resource transfer learning across covid-19 corpora by combining event-extraction and co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Spangher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020 Workshop on Natural Language Processing for COVID-19 (NLP-COVID)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relieving the computational bottleneck: Joint inference for event extraction with high-dimensional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Gogate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="831" to="843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relieving the computational bottleneck: Joint inference for event extraction with high-dimensional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Gogate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1090</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="831" to="843" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Does it make sense? and why? a pilot study for sense making and explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuailong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1393</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4020" to="4026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
