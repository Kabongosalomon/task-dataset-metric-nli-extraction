<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-Shot Object Detection with Co-Attention and Co-Excitation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-I</forename><surname>Hsieh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Lo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
							<email>htchen@cs.nthu.edu.tw</email>
							<affiliation key="aff2">
								<orgName type="department">Aeolus Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<country>Taiwan AI Labs</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">One-Shot Object Detection with Co-Attention and Co-Excitation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper aims to tackle the challenging problem of one-shot object detection. Given a query image patch whose class label is not included in the training data, the goal of the task is to detect all instances of the same class in a target image. To this end, we develop a novel co-attention and co-excitation (CoAE) framework that makes contributions in three key technical aspects. First, we propose to use the nonlocal operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation. Second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasize correlated feature channels to help uncover relevant proposals and eventually the target objects. Third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen in training. The resulting model is therefore a two-stage detector that yields a strong baseline on both VOC and MS-COCO under one-shot setting of detecting objects from both seen and never-seen classes. Codes are available at https://github.com/timy90022/One-Shot-Object-Detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability of humans to learn new concepts under limited guidance is remarkable. Take, for example, the task of learning to identify and localize a never-before-seen object in an image based on a given query template. Even without prior knowledge about the object's category, the human visual system has evolved to be able to handle such a task by performing different functionalities that include grouping the pixels of objects as a whole, extracting distinctive cues for comparison, and exhibiting attention or fixation for localization. All these can be done under a wide range of variations in object appearances, viewing angles, lighting conditions, and so on.</p><p>The goal of this work is to address the problem of one-shot object detection by taking account of achieving the aforementioned capability and flexibility of the human visual system when a similar one-shot task of perceptual categorization and localization is performed. We assume that a query image of an object will be provided as an exemplar or a prototype of some unseen class, and the task is to localize the most likely occurrences of the query object in a new target image. Further, we require that the query object must not belong to any seen class at any level in the categorical hierarchies during training. It is also worth noting that the definition of object category may vary over different context <ref type="bibr" target="#b0">[1]</ref>. The contextual information for our one-shot scenario of object detection comes in two forms. First, the target image provides the spatial context, implying the likelihood of observing an object at a specific location with respect to the background and other foreground objects. Second, the query image and the target image jointly provide the categorical context. The exact level in the categorical hierarchies that both the query and the target objects belong to is determined by how they share significant numbers of attributes (such as color, texture, and shape) in common.</p><p>Metric learning is often employed as a key component to solve one-shot classification problems. However, it is not straightforward to apply a learned metric to one-shot object detection. The detector still needs to know which candidate regions in the target image are more likely to contain the objects to be compared with the query object using the learned metric. We propose to extract region proposals from non-local feature maps that incorporate co-attention visual cues of both the query and target images. On the other hand, object tracking can be considered as a special case of one-shot object detection with the temporal consistency assumption. The initial bounding box specified in the first frame can be viewed as the query. The subsequent frames are target images. A key difference between object tracking and our formulation of one-shot detection is that we do not assume that the target image must contain the same instance as the query image. It is allowed to have significant appearance variations between the objects, as long as there exist some common attributes for characterizing them as the same category. We present a new mechanism called squeeze and co-excitation to simultaneously emphasize the features of the query and target images for detecting objects of novel classes. The experiments show that our co-attention and co-excitation (CoAE) framework can better explore the spatial and categorical context information that is jointly embedded in the query and target images, and as a result, yields a strong baseline on one-shot object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Object detection State-of-the-art object detectors unanimously adopt variants of deep convolutional neural networks as their backbones and have been improving the performance on large-scale benchmarks. Two types of pipeline designs are often taken into consideration by recent object detectors: one-stage (proposal-free) <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> and two-stage (proposal-based) <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Two-stage detectors generate a set of region proposals at the first stage, and then classify the proposals as well as refine their locations at the second stage. The two-stage pipeline is first demonstrated by R-CNN <ref type="bibr" target="#b10">[11]</ref> and further improved by Faster R-CNN <ref type="bibr" target="#b12">[13]</ref>, which replaces the grouping-based proposal method with a region proposal network (RPN), making the whole pipeline end-to-end trainable. Subsequently, stateof-the-art two-stage object detectors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> mainly follow Faster R-CNN in the design of architecture. In contrast, one-stage detectors like <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> trade localization performance for fast inference speed by skipping the region-proposal step and directly predicting the bounding boxes and the corresponding class labels with respect to a fixed set of anchors.</p><p>Few-shot classification via metric learning The aim of metric-learning based few-shot classification is to derive a similarity metric that can be directly applied to the inference of unseen classes supported by a set of labeled examples (i.e., support set). The setting of N -way K-shot classification is considered to have a support set containing K labeled examples for each of N classes, where K = 0, K = 1, and K &gt; 1 mean zero-shot, one-shot, and few-shot, respectively. Koch <ref type="bibr" target="#b13">[14]</ref> presents the first principled approach that employs Siamese networks for one-shot image classification. The Siamese networks learn a general similarity metric from pairs of input images to decide whether the two images belong to the same class. Then, during inference, the Siamese networks can be used to match unlabeled images of either seen or unseen classes with the one-shot support set. The prediction is done by assigning the test image with the class label of the most similar example in the support set. Vinyals et al. <ref type="bibr" target="#b14">[15]</ref> propose the matching networks and an episodic training strategy tailored to the few-shot criterion. The matching networks learn a more powerful similarity metric using an attention mechanism <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> over the examples in the support set. Instead of associating unlabeled samples with their nearest support examples, the prototypical networks proposed by Snell et al. <ref type="bibr" target="#b17">[18]</ref> map the unlabeled samples to the nearest 'class prototype' of the support set. Snell et al. also show that the prototypical networks can be applied to zero-shot setting where the class prototype becomes the semantic vector of the class. Sung et al. <ref type="bibr" target="#b18">[19]</ref> present the relation network, which is similar to <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref> but learns the similarity metric fully based on a relational convolutional block instead of the Euclidean or cosine distance.</p><p>Few-shot object detection Similar to few-shot classification, the problem of object detection can also be addressed under a few-shot setting. This problem is relatively new and less explored, and only a few preliminary results are reported from the perspectives of transfer learning <ref type="bibr" target="#b19">[20]</ref>, meta learning <ref type="bibr" target="#b20">[21]</ref>, or metric learning <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. For transfer learning, Chen et al. <ref type="bibr" target="#b19">[20]</ref> present the regularization techniques to address overfitting when directly training on a handful of labeled images of unseen classes. For meta learning, Kang et al. <ref type="bibr" target="#b20">[21]</ref> propose a meta-model that is trained to reweight the features of an input image extracted from a base detection model. The reweighted features can be adapted to the detection of novel objects from a few examples. For metric learning, <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> share a similar framework that replaces the conventional classifier in the object detector with a metric-based classifier akin to <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>The problem formulation of our work is more closely related to <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> than <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. Our formulation is class-agnostic and training-free for unseen novel classes. Once the training process is done, our model can be used to detect objects of unseen classes without either knowing the classes beforehand or the need of fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our method</head><p>Consider the task of object detection over a set of class labels, denoted as C. As our method is designed to deal with the one-shot scenario, we further divide the label set by C = C 0 ∪ C 1 , where the former includes those are available during training and the latter comprises the remaining for the inference of one-shot object detection. We choose to tackle the one-shot detection task in two stages, and develop the proposed network architecture based on Faster R-CNN <ref type="bibr" target="#b12">[13]</ref>. In our implementation we have experimented with using ResNet-50 as the CNN backbone and carried out extensive comparisons with other relevant techniques.</p><p>We formulate the one-shot object detection as follows. Given a query image patch p, depicting an instance of a particular object class from C 1 , the inference task is to uncover all the corresponding instance(s) of the same class in a target image I. Notice that in this work we assume each feasible target image includes at least one object instance with respect to the class label of the one-shot query. We particularly focus on three key issues in training the underlying neural network and introduce new concepts to effectively perform one-shot object detection. We next describe the motivations, the reasoning, and the details of the proposed techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-local object proposals</head><p>We denote the training dataset as D with bounding box information over the class labels from C 0 . In view that we adopt the Faster R-CNN architecture for object detection, the first issue arises essentially to examine whether the region proposals generated by RPN (Region Proposal Network) are suitable for one-shot object detection. Recall that the training of an RPN uses the information of the presence of bounding boxes over all object classes in each image. However, in our setting only those ground-truth boxes corresponding to the labels in C 0 can be accessed in learning the RPN. The constraint implies if a one-shot object class in C 1 is significantly different from any of those in C 0 , the resulting RPN may not yield an expected set of proposals for detecting the corresponding object instances in a target image. To resolve this matter, we enrich the conv feature maps of interest with the non-local operation <ref type="bibr" target="#b24">[25]</ref>. Again let I be the target image and p the query image patch. The conv feature maps used by the conventional RPN to generate the proposals are expressed by φ(I) ∈ R N ×W I ×H I , while φ(p) ∈ R N ×Wp×Hp represents the feature maps of patch p from the same conv layer. Taking φ(p) as the input reference, the non-local operation is applied to φ(I) and results in a non-local block, ψ(I; p) ∈ R N ×W I ×H I . Analogously, we can derive the non-local block ψ(p; I) ∈ R N ×Wp×Hp using φ(I) as the input reference. The mutual non-local operations between I and p can indeed be thought of as performing co-attention. Finally, we can represent the two extended conv feature maps by</p><formula xml:id="formula_0">F (I) = φ(I) ⊕ ψ(I; p) ∈ R N ×W I ×H I for target image I,<label>(1)</label></formula><formula xml:id="formula_1">F (p) = φ(p) ⊕ ψ(p; I) ∈ R N ×Wp×Hp for image patch p,<label>(2)</label></formula><p>where ⊕ is the element-wise sum over the original features maps φ and the non-local block ψ. Since F (I) comprises not only image features from the target image I but also the weighted/attended features between I and the query patch p, designing the RPN based on the extended features would learn to explore more information from the query patch p and generate region proposals of better quality. In other words, the resulting non-local region proposals will be more appropriate for one-shot object detection.</p><p>Squeeze and co-excitation Besides linking the generation of region proposals with the given query patch, the co-attention mechanism realized by the non-local operation elegantly arranges the two sets of feature maps F (I) and F (p) for having the same number (i.e., N ) of channels. The relatedness between the two can be further explored by our proposed squeeze-and-co-excitation (SCE) technique such that the query p can flexibly match a candidate proposal by adaptively re-weighting the importance distribution over the N channels. Specifically, the squeeze step spatially summarizes each feature map with GAP (global average pooling), while the co-excitation functions as a bridge between F (I) and F (p) to simultaneously emphasize those feature channels that play crucial roles in evaluating the similarity measure. In between the squeeze layer and the co-excitation layer, we have two fc/MLP layers as in the design of an SE block <ref type="bibr" target="#b25">[26]</ref>. We depict the SCE operation as follows.</p><formula xml:id="formula_2">SCE(F (p), F (I)) = w, F (p) = w F (p), F (I) = w F (I),<label>(3)</label></formula><p>where F (p) and F (I) are the re-weighted feature maps, w ∈ R N is the co-excitation vector, and denotes the element-wise product. With <ref type="formula" target="#formula_2">(3)</ref>, the query patch p can now be represented by</p><formula xml:id="formula_3">q = w GAP(F (p)) = GAP( F (p)) ∈ R N ,<label>(4)</label></formula><p>while the feature vector, say, r for a region proposal generated by RPN can be analogously computed, i.e., performing spatially GAP over the corresponding cropped region of F (I).</p><p>Proposal ranking Assume that K region proposals by RPN are chosen as possible candidates for object detection with respect to the query image patch p. (K = 128 in all experiments.) We design a two-layer MLP network M, whose ending layer is a two-way softmax. In the training stage, we first annotate each of the K proposals as foreground (label 1) or background (label 0) according to whether their IoU value with respect to the bounding-box ground truth is greater than 0.5. We then consider a margin-based ranking loss to implicitly learn the desirable metric such that the most relevant proposals to the query p would appear in the top portion of the ranking list. To this end, we concatenate for each proposal its feature vector r with the feature vector q from (4) to obtain a combined vector, denoted as x = [r ; q ] ∈ R 2N , whose label y is 1 if r corresponds to a foreground proposal, and 0, otherwise. We choose to construct M with layer dimensions distributed by 2N → 8 → 2. Now let s = M(x) be the foreground probability predicted by M with respect to the query q. We define the margin-based ranking loss by</p><formula xml:id="formula_4">L MR ({x i }) = K i=1 y i × max{m + − s i , 0} + (1 − y i ) × max{s i − m − , 0} + ∆ i ,<label>(5)</label></formula><formula xml:id="formula_5">∆ i = K j=i+1 [y i = y j ] × max{|s i − s j | − m − , 0} + [y i = y j ] × max{m + − |s i − s j |, 0} ,<label>(6)</label></formula><p>where [·] is the Iverson bracket, the margin m + is the expected probability lower bound for predicting a foreground proposal and m − is the expected upper bound for predicting a background proposal. In our implementation, we have set m + = 0.7 and m − = 0.3 for all the experiments.</p><p>Finally, the total loss for learning the neural network architecture shown in <ref type="figure" target="#fig_0">Figure 1</ref> to carry out one-shot object detection can be expressed by</p><formula xml:id="formula_6">L = L CE + L Reg + λL MR ,<label>(7)</label></formula><p>where the first two losses are respectively the cross entropy and regression losses of Faster R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets and hyperparameters Following the previous work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>, we train and evaluate our model on VOC and COCO benchmark datasets. For VOC, our model is trained on the union set of   <ref type="table" target="#tab_0">Table  1</ref> shows the splits of seen and unseen VOC classes, the same setting as <ref type="bibr" target="#b23">[24]</ref>. For COCO, we use the same four splits over the 80 classes as <ref type="bibr" target="#b21">[22]</ref>, alternately taking three splits as seen classes and one split as unseen classes (see <ref type="figure" target="#fig_1">Figure 2</ref>). We train our models using SGD optimizer with momentum 0.9 for ten epochs, with batch size 128 on eight NVIDIA V100 GPUs in parallel. We use a learning rate starting with 0.01, and then decay it by a ratio 0.1 for every four epochs. We use λ = 3 in <ref type="formula" target="#formula_6">(7)</ref> for the margin-based ranking loss.</p><p>Generating target and query pairs The target images are directly chosen from the datasets. To generate a query image for a target image, we adopt different generation procedures for different datasets. For VOC, we simply crop out the ground truth bounding boxes as the query image patches. For COCO, however, such a cropping procedure cannot simply be applied, since the cropped image patches might be either too small or too hard to identify even for humans. Therefore, we adopt a pre-trained Mask-RCNN [10] 1 to filter out the too small or too hard query image patches. Specifically, we only crop out the patches that are enclosed by the predicted bounding boxes of Mask R-CNN. During training, given a target image, we randomly sample a query image patch of a seen class that exists in the target image. During testing, to evaluate each class in a target image, we first randomly shuffle the query image patches of that class with a random seed of target image ID (the image ID is accessible in either VOC or COCO), then sample the first five query image patches, and finally average their AP scores. The shuffle procedure ensures that the sampled five query patches would be random and thus result in stable statistics for evaluation.</p><p>ImageNet pre-training To ensure that our model does not 'foresee' the unseen classes, we pretrain our ResNet-50 backbone on a reduced training set of ImageNet from which we remove all COCO-related ImageNet classes by matching the WordNet synsets of ImageNet classes to COCO classes, resulting in 933, 052 images from the remaining 725 classes, while the original one contains 1, 284, 168 images of 1, 000 classes. Our pre-trained ResNet-50 achieves 75.8% (top-1) accuracy on the reduced ImageNet. Note that, by removing the COCO classes from ImageNet, we are also guaranteed to exclude the VOC classes from ImageNet.</p><p>Baselines We choose the previous work that is closely related to our work as the baseline methods, each evaluated on different datasets: For VOC dataset, SiamFC <ref type="bibr" target="#b26">[27]</ref>, SiamRPN <ref type="bibr" target="#b27">[28]</ref>, and CompNet   <ref type="bibr" target="#b28">[29]</ref> and also replaces the conventional classifier with a metric-based classifier in R-CNN. We compare their model with ours on COCO dataset under the same setting.</p><p>Overall performance For VOC, <ref type="table" target="#tab_0">Table 1</ref> shows that our model using reduced ImageNet pre-trained backbone ('Ours (725)') still achieves better performance on both seen and unseen classes than the baseline methods. Furthermore, the performance significantly improves when we also adopt ImageNet pre-trained backbone with all 1000 classes ('Ours (1k)'). However, the unseen classes have better performance than seen classes due to the high variations in appearance of seen objects such as plant, bottle, and chair. For COCO, <ref type="table" target="#tab_1">Table 2</ref> also shows that our model achieves better performance than Siamese Mask-RCNN on both seen and unseen classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation studies</head><p>Co-attention, co-excitation, and margin-based ranking loss We investigate the contributions of different proposed modules and summarize the results in <ref type="table" target="#tab_2">Table 3</ref>. First, the model without both co-attention (non-local RPN) and co-excitation (SCE) gets the worst performance. However, adding either non-local RPN or SCE significantly boosts the performance with an increase of 4.4/6.3 mAP(%) and 8.2/9.8 AP50(%) on VOC and COCO, respectively. Applying both modules further provides 1.8/0.9 mAP(%) and 1.9/0.3 AP50(%) performance gains. This implies that both coattention and co-excitation are crucial to our method. The margin-based ranking loss also enhances the performance moderately, which means that margin-based ranking loss can still be helpful for learning the desirable metric.</p><p>Visualizing the distribution of non-local object proposals To analyze the behavior of non-local object proposals, we visualize the distribution of proposals as a heatmap. Each pixel associates with a count that indicates how many region proposals cover that pixel. The final heatmap is then produced by normalizing the pixel count to a probability map. As shown in <ref type="figure">Figure 3</ref>, the mutual non-local features enable the RPN to generate proposals that focus more on the regions of both the target's and query's interest, and hence provide a co-attention effect.</p><p>Visualizing the characteristics of co-excitation To analyze whether our proposed co-excitation mechanism learns a different weight distribution for each class, we collect all co-excitation weights for every query image during testing. Therefore, each class associates with a set of query images, and each of the query images associates a set of co-excitation weights. For each class, we average the co-excitation weights to a single vector. The visualization of class-to-class pairwise distances is then carried out by computing the pairwise Euclidean distance of the co-excitation weight vector of each class-to-class pair. <ref type="figure">Figure 4</ref> clearly points out that our 'squeeze and co-excitation' module learns a meaningful weight distribution for each class. For example, the co-excitation weights of animalrelated classes are closer to each other. A similar phenomenon can be observed for vehicle-related classes. On the other hand, the person class is far away from all the other classes, meaning that the person class is hard to share the common attributes either in texture or in shape with the other classes.</p><p>Analyzing the co-excitation mechanism We consider two opposite cases. The first scenario is to use different image patches to query the same target image. <ref type="figure">Figure 5</ref> shows that the cows in p 1 and p 2 share a similar color to the target instance in I, while the other two in p 3 and p 4 are of different colors to the target. A reasonable conclusion is that the former would emphasize the color features and the latter two the shape features so that each query can match to the target instance in I. The observation is supported by that w 2 is closer to w 1 than both w 3 and w 4 . The second case is to use a same query p for different target images. Analogously, in <ref type="figure">Figure 6</ref>, the distances between every two co-excitation vectors are insightful. In particular, the two sets of distance values suggest that the query p to I 1 and I 2 would pay more attention to texture features, rather than the shape features as to I 3 and I 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In designing the proposed CoAE one-shot object detector, we have intentionally cast the learning formulation such that it does not solely rely on the label information of training data. Both the proposed co-attention and co-excitation techniques are to explore the correlated evidence revealed by the query-target pairs. Such information is generic and not heavily biased to the training data. As a result, the proposed method can yield non-local object proposals and uses the co-excitation operation to emphasize important features shared by both the query and the target images. The resulting one-shot object detector achieves state-of-the-art performances on two popular datasets.</p><p>The future work will focus on generalizing our method for any k-shot (k ≥ 0) object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overall neural network architecture of the propose method for one-shot object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The AP50 (%) on different splits of COCO unseen classes. Each split is alternately used as unseen classes for evaluation, with the other three splits as seen classes for training. VOC 2007 train&amp;val sets and VOC 2012 train&amp;val sets, and is evaluated on VOC 2007 test set. For COCO, our model is trained on COCO 'train 2017' set and evaluated on COCO 'val 2017' set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 Figure 3 :Figure 4 :</head><label>234</label><figDesc>further shows the fine-grained performance on each class; the artifact classes are the hardest to detect since they vary in textures and shapes, such as hand bag, book, and tie. Query w/o non-local RPN non-local RPN Non-local RPN is useful for attracting more proposals on the correct targets. o b i k e b o a t b u s c a r m b i k e t r a i n b o t t l e c h a i r t a b l e p l a n t s o f a t v p e r s Visualization of class-to-class pairwise distances based on co-excitation weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 d(w 1 ,IFigure 5 : 4 Figure 6 :</head><label>41546</label><figDesc>w 1 ) = 0 d(w 2 , w 1 ) = 0.467 d(w 3 , w 1 ) = 0.512 d(w 4 , w 1 ) = 0.508 d(w 1 , w 4 ) = 0.508 d(w 2 , w 4 ) = 0.510 d(w 3 , w 4 ) = 0.398 d(w 4 , w 4 ) = 0 Query p i to the same I results in the co-excitation w i . Both w 1 and w 2 should emphasize the color channels to detect the target instance in I, while w 3 and w 4 focus on those related to shape.I a Wa ← −−−−− → co-excitation Query W b ← −−−−− → co-excitation I b d(w a , w 1 ) = 0.084 d(w a , w 2 ) = 0.080 d(w a , w 3 ) = 0.393 d(w a , w 4 ) = 0.381 d(w b , w 1 ) = 0.383 d(w b , w 2 ) = 0.357 d(w b , w 3 ) = 0.069 d(w b , w 4 ) = 0The same query p to each different I x results in a co-excitation vector w x . Observe that d(w a , w 1 ), d(w a , w 2 ) d(w a , w 3 ), d(w a , w 4 ) (favoring texture features) and d(w b , w 1 ), d(w b , w 2 ) d(w b , w 3 ), d(w b , w 4 ) (favoring shape features).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different few-shot detection methods on VOC in AP (%). 'Ours (725)' means our model is pre-trained on a reduced ImageNet dataset to prevent from foreseeing the unseen classes. Note that SiamFC, SiamRPN, and CompNet use all classes in their ImageNet pre-trained backbones. car bottle boat chair person bus train horse bike dog bird mbike table mAP cow sheep cat aero mAP SiamFC 3.2 22.8 5.0 16.7 0.5 8.1 1.2 4.2 22.2 22.6 35.4 14.2 25.8 11.7 19.7 27.8 15.1 6.8 2.28 31.6 12.4 13.3 SiamRPN 1.9 15.7 4.5 12.8 1.0 1.1 6.1 8.7 7.9 6.9 17.4 17.8 20.5 7.2 18.5 5.1 9.6 15.9 15.7 21.7 3.5 14.2 CompNet 28.4 41.5 65.0 66.4 37.1 49.8 16.2 31.7 69.7 73.1 75.6 71.6 61.4 52.3 63.4 39.8 52.7 75.3 60.0 47.9 25.3 52.1 Ours (725) 24.9 50.1 58.8 64.3 32.9 48.9 14.2 53.2 71.5 74.7 74.0 66.3 75.7 61.5 68.5 42.7 55.1 78.0 61.9 72.0 43.5 63.8 Ours (1k) 30.0 54.9 64.1 66.7 40.1 54.1 14.7 60.9 77.5 78.3 77.9 73.2 80.5 70.8 72.4 46.2 60.1 83.9 67.1 75.6 46.2 68.2</figDesc><table><row><cell>Method 10 20 30 40 50 60 70 plant sofa tv Per son Airp lan e Boa t Par kin g me ter Dog Ele pha nt Bac kpa ck Sui tca se Spo rts bal l Ska teb oar d Win e gla ss Spo on San dw ich Hot dog Cha ir Din ing tab le Mo use Mic row ave Ref rige rato r Sci sso rs 0</cell><cell>Seen class Piz za Bow l Ora nge Cup Sur fbo ard Kite Fris bee Um bre lla Bea r Ho rse Ben ch Tra ffic ligh t Bus Bic ycl e 10 20 30 40 50 Cou ch Toi let Rem ote Ov en Boo k Ted dy bea r 0</cell><cell>10 20 30 40 50 60 0</cell><cell>Car Tra in Fire Hyd ran t Bird She ep Zeb ra Han dba g Ski s Bas eba ll bat Ten nis roc ket For k Ban ana Bro cco li Don ut Pot ted pla nt TV Key boa rd Toa ste r Clo ck Hai r drie r</cell><cell>Unseen class Sno wb oar d Bas eba ll glo ve Bot tle Tie Gir affe Cow Cat Sto p sig n Tru ck Mo torc ycle Kni fe App le Car rot Cak e Bed Lap top Cel l pho ne Sin k Vas e Too thb rus h</cell></row><row><cell>(a) split 1</cell><cell>(b) split 2</cell><cell></cell><cell>(c) split 3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on COCO val 2017 with respect to AP50 score (%).</figDesc><table><row><cell>split</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>Average</cell></row><row><cell>SiamMask (seen)</cell><cell cols="4">38.9 37.1 37.8 36.6</cell><cell>37.6</cell></row><row><cell>Ours (seen)</cell><cell cols="4">42.2 40.2 39.9 41.3</cell><cell>40.9</cell></row><row><cell cols="5">SiamMask (unseen) 15.3 17.6 17.4 17.0</cell><cell>16.8</cell></row><row><cell>Ours (unseen)</cell><cell cols="4">23.4 23.6 20.5 20.4</cell><cell>22.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Ablation study on co-attention (non-local object proposals), co-excitation (SCE), and</cell></row><row><cell>margin-based ranking loss.</cell><cell></cell></row><row><cell cols="2">Co-attention Co-excitation Margin loss VOC mAP (%) COCO AP50 (%)</cell></row><row><cell>57.0</cell><cell>23.6</cell></row><row><cell>54.2</cell><cell>21.7</cell></row><row><cell>56.1</cell><cell>23.3</cell></row><row><cell>51.6</cell><cell>22.4</cell></row><row><cell>49.8</cell><cell>13.5</cell></row><row><cell cols="2">[24] are the baseline methods to be compared. CompNet builds on Faster R-CNN and replaces the</cell></row><row><cell cols="2">conventional classifiers with the metric-based classifiers in both RPN and R-CNN, while SiamFC and</cell></row><row><cell cols="2">SiamRPN (outperformed by CompNet) aim to solve the visual tracking problem instead of focusing</cell></row><row><cell cols="2">on one-shot object detection. Note that SiamFC, SiamRPN, and CompNet do not remove unseen</cell></row><row><cell cols="2">classes in their ImageNet pre-trained backbones, while ours removes both seen and unseen classes</cell></row><row><cell cols="2">from the pre-trained backbone. For COCO, SiamMask [22] sets up a baseline performance on COCO</cell></row><row><cell>dataset. SiamMask extends Mask R-CNN [10] with a feature pyramid network</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Mask R-CNN implementation: https://github.com/matterport/Mask_RCNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the Ministry of Science and Technology (MOST), Taiwan under Grants 106-2221-E-007-080-MY3, 107-2218-E-007-047, 107-2634-F-001-002, and 108-2634-F-001-007. We are particularly grateful to the National Center for High-performance Computing (NCHC) for providing computational resources and facilities. The authors also like to thank Songhao Jia for insightful discussions on the implementation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vision science : photons to phenomenology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="765" to="781" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">LSTD: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2836" to="2843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1812.01866</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">One-shot instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Ustyuzhaninov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<idno>abs/1811.11507</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Repmet: Representative-based metric learning for classification and one-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharathchandra</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogério</forename><surname>Schmidt Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno>abs/1806.04728</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Comparison network for one-shot conditional object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional siamese fusion networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaobin</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheolkon</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-07" />
			<biblScope unit="page" from="3718" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
