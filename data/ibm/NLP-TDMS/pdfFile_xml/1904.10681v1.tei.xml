<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019">APRIL 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vol</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>No</surname></persName>
						</author>
						<title level="a" type="main">A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="page">1</biblScope>
							<date type="published" when="2019">APRIL 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Human action recognition</term>
					<term>Varying-view RGB- D action dataset</term>
					<term>Cross-view recognition</term>
					<term>Arbitrary-view recog- nition</term>
					<term>HRI</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current researches of action recognition mainly focus on single-view and multi-view recognition, which can hardly satisfies the requirements of human-robot interaction (HRI) applications to recognize actions from arbitrary views. The lack of datasets also sets up barriers. To provide data for arbitraryview action recognition, we newly collect a large-scale RGB-D action dataset for arbitrary-view action analysis, including RGB videos, depth and skeleton sequences. The dataset includes action samples captured in 8 fixed viewpoints and varying-view sequences which covers the entire 360 • view angles. In total, 118 persons are invited to act 40 action categories, and 25,600 video samples are collected. Our dataset involves more participants, more viewpoints and a large number of samples. More importantly, it is the first dataset containing the entire 360 • varyingview sequences. The dataset provides sufficient data for multiview, cross-view and arbitrary-view action analysis. Besides, we propose a View-guided Skeleton CNN (VS-CNN) to tackle the problem of arbitrary-view action recognition. Experiment results show that the VS-CNN achieves superior performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human action recognition is widely applied in public surveillance, image/video captioning and human-robot interaction <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, etc. Approaches for action recognition has developed from silhouettes <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, local features <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> to depth features <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and skeletons <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Existing researches focus on single-view (mostly in the front viewpoint) and multiview action recognitions <ref type="bibr" target="#b12">[13]</ref>. However, they can hardly satisfy the demand of robots to recognize human actions in arbitrary views for Human-robot interaction (HRI) applications. Taking a service robot at home (shown in <ref type="figure" target="#fig_0">Fig. 1)</ref> as an example, it freely moves to anywhere and interacts with family members. During moving, the robot captures human actions in any viewpoints, and it is certainly expected to understand human behaviors in arbitrary viewpoints. However, the arbitrary-view human action recognition is still a big challenging problem. On the one hand, view changes lead to action occlusions and pose variances. On the other hand, there are few datasets for arbitrary-view action recognition. There have been datasets developed for multi-view action recognition <ref type="bibr" target="#b14">[14]</ref>. RGB information is used for the multiview action recognition <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>. With the development of depth sensors, datasets containing RGB-D information were presented, such as Act4 2 , Multiview 3D Event, Northwestern-UCLA, and UWA3D Multiview, and the NTU RGB+D action dataset <ref type="bibr" target="#b17">[17]</ref>- <ref type="bibr" target="#b21">[21]</ref>. For the arbitrary-view recognition, it is expected to use action samples captured in wide-range views for model training. However, almost all existing datasets were captured in limited viewpoints. Taking advantage of the mocap motion in the CMU action dataset 1 , training datasets including action samples of various viewpoints were generated and used to train classifiers for multi-view action recognition <ref type="bibr" target="#b22">[22]</ref>- <ref type="bibr" target="#b24">[24]</ref>. Nonetheless, the dataset generation suffers from expensive computational cost, and the mocap motion dataset is required to cover a large number of action categories, which is also a difficult problem. To solve the problem of lacking suitable data, we present a large-scale RGB-D action dataset which contains varying-view sequences covering the entire 360 • view angels. The dataset provides sufficient samples for the arbitrary-view action recognition.</p><p>Many research endeavors have been dedicated to solving the problem of multi-view action recognition. Transfer learning methods were adopted to transfer the knowledge from one viewpoint to other viewpoints <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b25">[25]</ref>- <ref type="bibr" target="#b28">[28]</ref>, or to transfer feature knowledge from the benchmark dataset to test datasets <ref type="bibr" target="#b16">[16]</ref>. Since action sequences observed in varying viewpoints easily suffer occlusions, temporal motion is used for view-invariance action representation <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b29">[29]</ref>. A further solution is to learn spatial relationships of joints from 3D poses to construct view-invariant representations <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>. However, most of the existing approaches can only deal with small-range view changes. For action recognition with view changes, one solution is to seek a common representation for actions in different views. Liu et al. <ref type="bibr" target="#b32">[32]</ref> visualized a skeleton sequence into a color image for action representation and presented the SK-CNN approach to recognize actions, which is potent to weaken the difference between different views. Regarding our dataset containing full-circle views, current solutions cannot handle the recognition task. To cover the full-circle view, we separate the full-circle view into four view groups, and propose a View-guided Skeleton-CNN (VS-CNN) approach to recognize actions with large view changes.</p><p>In this paper, we newly collect a large-scale RGB-D action dataset for arbitrary-view action recognition. The dataset contains samples captured in 8 fixed viewpoints and varying-view sequences that cover the entire 360 • view angels. Samples captured in fixed viewpoints provide training data for the arbitrary-view recognition, and also may be used for the multi-view recognition. The dataset contains 40 fitness action categories, and 118 persons are invited to act these actions. In total 83 hours' RGB videos are collected, and depth image sequences, skeleton sequences have similar frame numbers with RGB videos. Moreover, we propose a baseline, termed View-guided Skeleton-CNN (VS-CNN) to tackle these problems. The model consists of a view-group prediction module and four classifiers corresponding to four view groups. The view-group prediction module guides the training of classifiers through separating action samples to four view groups, and driving the training of corresponding classifiers. Finally, a weighted fusion is performed on the four classifiers, and the SoftMax classifier is used to classify fused features to corresponding action categories. Since view groups overlap each other, the VS-CNN learns a common representation of actions in different view groups. In summary, our major contributions include:</p><p>• We present a large-scale RGB-D action dataset for arbitrary-view action recognition, which includes 118 subjects and 8 fixed viewpoints. To the best of our knowledge, this is one of the first datasets covering the entire 360 • varying-view sequences.</p><p>• To tackle the arbitrary-view action recognition problems, we propose the VS-CNN, which overcomes the gap of action recognition in large view ranges.</p><p>• The proposed approach is extensively evaluated on our collected dataset, and the promising performance validates the efficacy of both the approach and dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-view action recognition with 2D features</head><p>The same with general action recognition, the crucial problem of multi-view action recognition is also to learn an effective representation for actions. There had developed many local features for action representation in 2D videos and depth sequences <ref type="bibr" target="#b33">[33]</ref>- <ref type="bibr" target="#b35">[35]</ref>, and they were introduced for multi-view action recognition <ref type="bibr" target="#b36">[36]</ref>- <ref type="bibr" target="#b38">[38]</ref>. To learn effective features, Hu et al. <ref type="bibr" target="#b39">[39]</ref> presented the JOULE model which explored the shared and feature-specific components from multiple feature channels, i.e. RGB, and skeleton features, as heterogeneous features for action recognition. Recent years, Convolution Neuron Networks (CNN) were introduced for 2D feature learning, and a series of effective networks were developed, i.e. ResNeXt <ref type="bibr" target="#b40">[40]</ref>. To include temporal information of action sequences, LRCN (Long-term Recurrent Convolutional Networks) <ref type="bibr" target="#b41">[41]</ref> were presented for action recognition.</p><p>Since view variance leading to human pose change in 2D videos and depth sequences, a series of approaches were proposed to solve the problem. Liu et al. <ref type="bibr" target="#b16">[16]</ref> used the bipartite graph partitioning to cluster vocabularies collected from two independent viewpoints by a bag of visual-words into visual-word clusters, which bridged the semantic gap of actions between different viewpoints. Moreover, Liu et al. <ref type="bibr" target="#b28">[28]</ref> built a transferable dictionary pair for feature transformation between the front view and side view actions, and a common representation was obtained in the two views. Though Local features are insensitive to viewpoint change in a small range, it suffers serious occlusions when a large view change occurs. Therefore, approaches that could be used to solve the problem of view change are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-view action recognition with 3D features</head><p>The 3D information plays important roles in multi-view action recognition <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b42">[42]</ref>. Building bridges between a large collection dataset and test datasets, multi-view action recognition was realized by matching sequences of various viewpoints to data samples of the large collection dataset to reduce the gap between different viewpoints <ref type="bibr" target="#b22">[22]</ref>- <ref type="bibr" target="#b24">[24]</ref>. However, a major limitation is the expensive computational cost for dataset generation, and the mocap motion dataset is also required to have numerous action categories. A solution was to learn spatial relationships of 3D joints for a view-invariant action representation and recognition <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>. Moreover, Shahroudy et al. <ref type="bibr" target="#b21">[21]</ref> presented a Part-aware LSTM model (P-LSTM) which contained multiple parallel memory cells for body-part feature learning and one output gate for information sharing among body parts. The P-LSTM combined bodypart context information, and provided a global representation for action recognition. Graph model was employed to model the 3D geometric relations for multi-view recognition <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>. These high-level representations somewhat produced a common description in different viewpoints.</p><p>Moreover, some approaches transformed the 3D skeleton feature to 2D visual images, and took advantage of feature learning via CNN to achieve higher action recognition results. Kim et al. <ref type="bibr" target="#b43">[43]</ref> collected temporal skeleton trajectories and created frame-wise skeleton features concatenated temporally across the entire video sequence, and the Res-TCN was designed for action recognition. Liu et al. <ref type="bibr" target="#b32">[32]</ref> visualized skeleton motions of an action sequence to an enhanced color image, and a multi-stream CNN fusion model was used to recognize actions (SK-CNN). Yan et al. <ref type="bibr" target="#b44">[44]</ref> modeled spatiotemporal skeletons using a Spatial-Temporal Graph Convolutional Networks (ST-GCN), which learned the importance of skeleton joints and assigned proper weights on graph convolution layers for action representation. Benefiting from effective feature extraction via CNN, these approaches had good performance with small-range view changes. In this paper, we propose a VS-CNN model to deal with action recognition in a large view ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW OF RELATED DATASETS</head><p>Several multi-view human action datasets had been released. Weinland et al. <ref type="bibr" target="#b14">[14]</ref> released the IXMAS dataset containing RGB videos of human actions. The dataset was captured in five fixed viewpoints and contained 11 basic action categories, each performed by 10 actors. With the depth sensor Kinect V1, Cheng et al. <ref type="bibr" target="#b17">[17]</ref> presented the ACT 4 2 action dataset including the RGB and depth information of 14 daily actions. 24 persons were invited to perform each action, and the dataset was captured in 4 fixed viewpoints. Wei et al. <ref type="bibr" target="#b18">[18]</ref> built a multi-view 3D event dataset which included 8 event categories and 11 interacting object classes. RGB-D data of actions were captured using three stationary Kinect sensors. 8 persons were invited as participants in the data capture. Wang et al. <ref type="bibr" target="#b19">[19]</ref> constructed the Northwestern-UCLA Multiview 3D event dataset which contained RGB, depth and skeleton data of 10 daily actions. Each action was performed by 10 participants, and data was captured in 3 fixed viewpoints. Rahmani et al. <ref type="bibr" target="#b30">[30]</ref> collected the UWA3D Multiview Activity Dataset in 4 viewpoints. The dataset contained 30 daily action categories, and each category was performed by 10 persons. Moreover, Shahroudy et al. <ref type="bibr" target="#b21">[21]</ref> presented a large-scale dataset, the NTU RGB+D action dataset. The dataset includes 60 daily actions, and totally 40 persons were invited for the data collection. Using 3 Kinect sensors, the dataset was captured in major 5 viewpoints. By changing camera-to-subject distances and camera heights, action data of 80 camera views were recorded.</p><p>Almost all existing datasets captured actions in limited viewpoints. It can hardly support the research of arbitrary-view action recognition for HRI applications. In addition, there are rare datasets including action samples captured in a very wide range of view angles and even continuously varying views. To provide data for the arbitrary-view recognition, we simulate the HRI scenario and newly collect an action dataset which contains both action samples captured in fixed viewpoints and continuously varying-view action sequences. The varying-view sequence particularly covers the entire 360 • view angles, that is significantly different with existing datasets and beneficial for the evaluation of arbitrary-view action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. VARYING-VIEW RGB-D ACTION DATASET A. Database Description</head><p>The action dataset is collected using Microsoft Kinect v2 sensors. We use the sensor to capture 3 modality action data, i.e., RGB videos, Depth images, and skeleton sequences. For RGB videos, we record image frames with the resolution of 960×540 pixels. Depth images retain the maximum resolution of Kinect v2 sensors, 512 × 424 pixels, and 16-bit pixel values. Human skeletons contain 25 body joints per frame, and each joint is recorded as a 3D coordinate (x, y, z) in the 3D space centered on the Kinect sensor. We show dataset capture settings in the <ref type="figure" target="#fig_1">Figure 2</ref>. Camera positions indicate the 8 fixed viewpoints, and red arrows show the moving trajectory of sensors when we capture varying-view action sequences. During the data collection, subjects always face the Kinect sensor in the front viewpoint. <ref type="bibr" target="#b1">2</ref> We collect fixed-viewpoint data to train classifiers for the arbitrary-view recognition because it is difficult to train a robust classifier using varying-view sequences due to fast varying views.</p><p>Viewpoints For the arbitrary-view action recognition, the dataset is designed to have more viewpoints and covers a wider range of view angles. Our dataset has 8 fixed viewpoints which averagely distribute around subjects, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In order to simulate a real scenario concerning the HRI, we design three capture settings. The setting A in <ref type="figure" target="#fig_1">Figure 2</ref>(a) is a circle with a radius of 2.5m, and there are two settings having the rectangle shape in <ref type="figure" target="#fig_1">Figure 2</ref>(b). The size of setting B and C are shown with black dashes and green dashes, respectively. In addition, we capture varying-view action sequences by moving a sensor around the subject along blue paths in the <ref type="figure" target="#fig_1">Figure 2</ref>. It captures actions in continuously varying views covering 360 • full-circle view angles. We set the height of all Kinect sensors to 1.2m. The capture in three settings involves various camerato-subject distances. Varying-view sequences are mainly used to evaluate approaches for arbitrary-view action recognition.</p><p>Subjects We totally invite 118 persons to attend the dataset collection. Each person averagely acts 10 action categories, and each action category is performed by 40 subjects in total. Because action categories are fitness actions, the age of subjects is from 18 to 30. We provide each subject a number ID in the collected dataset.</p><p>Categories There are total of 40 action categories in the dataset. We show all the categories in the <ref type="table" target="#tab_0">Table I</ref>. Among 40 categories, 15 categories are performed in two situations, standing to act and sitting on a chair to act. Other 25 action categories are all performed with the standing pose. These categories are given action IDs of a0 − a39. These categories of actions contain both of large motions of all body parts, e.g. spinal stretch, raising hands and jumping, and small movements of one part, e.g. head anticlockwise circle. They are much more complex than actions in existing datasets, e.g., hand waving, walking etc. Quantities When capturing actions in fixed viewpoints, each subject repeats each action 3 ∼ 5 times. For each sideview action capturing, i.e. view1 ∼ view7, there lays a sensor in the front view to capture synchronous action sequences. We use three Kinect sensors to synchronously capture two sideview and one front-view actions. Similarly, when we capture varying-view action sequences, we synchronously capture the front-view sequences. Thus each side-view action sequence has a synchronous sequence in the front viewpoint. Totally, 11 sequences are captured in 8 fixed viewpoints and 2 varyingview sequences are recorded per action category per subject. These synchronous action sequences can be used for view transformation between side views and the front viewpoints. In our dataset, one RGB video in fixed viewpoints generally sustains about 6 ∼ 7 seconds, and contains 200+ frames. RGB videos of varying-view sequences have about 55 ∼ 65 seconds, containing about 1730 ∼ 2000 frames. The length of RGB videos in our dataset are more than 83 hours in total. Depth and skeleton sequences have synchronization with RGB videos, thus depth sequences has similar frame numbers with the RGB videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with other datasets</head><p>We compare our action dataset with other multi-view action datasets in <ref type="table" target="#tab_0">Table II</ref>. It shows that our dataset has more subjects and viewpoints. Various subjects may be used to evaluate the generalization of recognition approaches with different persons. In terms of the viewpoint, besides of 8 fixed viewpoints, our dataset captures varying-view sequences which cover the 360 • full-circle views, which is superior to other datasets. Varying-view action sequences provide experiment samples for arbitrary-view action recognition.</p><p>Our dataset collects fitness actions because they involve both large movements and small motions, and it may be applied to fitness auxiliary with robots. Referring to the sample quantity, our dataset contains a large-scale action samples. The sample number of RGB videos is 25,600, and total 72,709 samples of RGB videos, depth and skeleton sequences. More importantly, each varying-view sequence is ten times the length of sequences in other datasets. We estimate the total length of RGB videos in hours for existing multi-view datasets and list them in <ref type="table" target="#tab_0">the Table II</ref>. The comparison indicates that our dataset has the longest video playing hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluations</head><p>We evaluate approaches in our dataset using four types of evaluations. The standard evaluation of the cross-subject recognition in <ref type="bibr" target="#b21">[21]</ref> is kept in our experiment. To evaluate the recognition performance between any two viewpoints and neighbor viewpoints, we propose two types of crossview recognition. Furthermore, we evaluate the arbitrary-view action recognition in our dataset.</p><p>Cross-subject recognition In our dataset, each subject acts 10 actions, and each action is surely acted by 40 subjects. For cross-subject recognition, we randomly select 51 subjects, and separate action samples acted by these subjects into the training group.    Cross-view recognition II The evaluation is in order to show the recognition performance between neighbor viewpoints. Action samples in four viewpoints which connect a square crossing shape in the Arbitrary-view recognition We evaluate arbitrary-view recognition in two ways. In a first way (Arbitrary-view I), we use action samples captured in 8 fixed viewpoints to train classifier models, and evaluate trained models on varyingview sequences. In the other way (Arbitrary-view II), we use varying-view sequences for training and also test the trained model on varying-view sequences. Action sequences captured in continuously varying views generally contain 2000+ frames per sequence. The sequence length is 10 times of action sequences captured in fixed viewpoints. In order to do experiments as the same situation as other evaluations, we clip each varying-view action sequence to 10 short clips with an equal length so that each short section has the similar length with sequences captured in fixed views. These short sections are  used independently in the evaluation of recognition models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a27Varying-View</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. VIEW-GUIDED SKELETON-CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The VS-CNN network</head><p>The architecture of the VS-CNN is shown in <ref type="figure" target="#fig_7">Figure 4</ref>. Since skeleton visualization <ref type="bibr" target="#b32">[32]</ref> is able to somewhat weaken difference of features in various views, we employ it to generate an initial skeleton representation of actions and use the representation as input features of VS-CNN. Moreover, as action samples in our dataset cover the 360 • full-circle view, it is difficult to use one traditional feature learning model to learn correct feature representations for all views. Thus, we separate the full-circle view into four view groups and design four feature learning channels which correspond to four view groups. A view-group predictor is designed to determine view groups for action samples and guides the training of corresponding feature learning channels and classifiers by inputting samples to corresponding channels according to prediction result of the view-group predictor. Then we fuse output score features of four channels and train a classifier to finally determine action categories.</p><p>1) View-group predictor: We separate the full-circle view into four view groups, and design a view-group predictor to realize automatically separation of action samples. View group separation is shown in <ref type="figure">Figure 5</ref>. The front viewpoint, viewpoints 1, 2 are separated into the first view group, while viewpoints 2, 3, 4 are defined as the second view group. Similarly, the third view group includes viewpoints 4, 5, and 6, and the fourth group includes viewpoints 6, 7, and the front viewpoint. These groups overlap each other so that samples belonging to overlap views drive the training of two feature learning channels. Since any two feature learning channels share part of common samples during training, each channel learns a common representation for samples in neighbor view <ref type="bibr">Group 4</ref> Group 1 二卡 <ref type="figure">Fig. 5</ref>. The 360 • full-circle view is separated into four groups, and these groups overlap each other. These overlaps will guide the VS-CNN to learn a common action representation in different view groups.</p><p>groups. In this way, we obtain a common representation of action samples in a full-circle view range. Therefore, our approach is able to overcome view invariance. The view-group predictor consists of 3 CNN layers, and a SoftMax is employed as classifier to determine the probability of one sample belonging to four view groups. The structure of the CNN network is: layer 1 ( 16 kernels, kernel size 3 × 3, 1 stride), layer 2 ( 32 kernels, kernel size 3 × 3, 1 stride ), layer 3 ( 32 kernels, kernel size 3 × 3, 2 strides ). Suppose that x represents an action sample, we use function f g (θ z , x) to represent the group prediction network, and θ z refers to network parameters. The view-group predictor outputs a vector,ŷ g = {z i , i = 1, · · · , 4}, which indicates prediction scores of four view groups. Here, i refers to the ith view group. The prediction score is further used to calculate weights by Equation <ref type="formula">(1)</ref>, a = {α i , i = 1, · · · , 4}, where α i = 1. Weights a are used for the weighted fusion of score features output from four classifiers. Equation <ref type="formula" target="#formula_1">(2)</ref> is designed as the prediction loss to train the view-group prediction network. Here, y g refers to the ground truth of view groups. ω refers to parameters of the view-group predictor.</p><formula xml:id="formula_0">α i = e −zi 4 i=1 e −zi .</formula><p>(1)</p><formula xml:id="formula_1">y g = exp(ωf g (θ z , x) i exp(ωf g (θ z , x)) . L g (x, y g ) = −y g logŷ g .<label>(2)</label></formula><p>2) View-guided feature learning channels: Corresponding to four view groups, we design four feature learning channels using the SK-CNN <ref type="bibr" target="#b32">[32]</ref> as base network. Each feature learning channel is composed of an SK-CNN backbone and one action classifier. According to max(z i ), action samples are separated into four view groups, and they are input to corresponding feature learning channels to learn action features and the following classifier gives a prediction score vector of action categories. Suppose that x is an action sample, we use f i (θ i , x) to represent feature learning of the ith channel. Here, θ i is network parameter. We employ a Softmax classifier to predict action categories of action samples. The cross entropy is adopted as a loss function for the training of feature learning networks and Softmax classifiers. The Equation <ref type="formula" target="#formula_2">(3)</ref> shows a loss function of the ith channel. ω i refers to classifier parameter. Here, y refers to the ground truth of action categories, andŷ i represents predicted results,ŷ i = {ŷ i j , j = 1, · · · , 40}. j refers to action category.</p><formula xml:id="formula_2">y i = exp(ω i f i (θ i , x)) j exp(ω i f i (θ i , x)) . L i (x,ŷ i ) = −y logŷ i .<label>(3)</label></formula><p>3) Channel fusion recognition: With action prediction scores output from four channels, a weighted fusion is performed through a fully connected layer with 40 neurons. Following that, a SoftMax classifier is adopted for the final action category determination. We also use the cross-entropy as loss function for classifier training, as shown in Equation <ref type="bibr" target="#b3">(4)</ref>. Here, y refers to the ground truth of action categories, andŷ represents predicted results of action categories, y = {ŷ j , j = 1, · · · , 40}. ω j is classifier parameter. Here, α i , i = 1, · · · , 4 is used to weight prediction scores of four channels.ŷ</p><formula xml:id="formula_3">= exp(ω j 4 i=1 α iŷ i ) j exp(ω j 4 i=1 α iŷ i ) . L(x, y) = −y logŷ.<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training and Testing</head><p>Training phase We employ the stochastic gradient descent algorithm to minimize loss functions, and train optimal parameters for VS-CNN. The training is performed in three steps. In the first step, we assign action samples with labels of view groups, and train the view-group predictor. In the second step, we input training samples to the trained view-group predictor and automatically separate samples into different view groups. These separated samples are given to corresponding feature learning channels to train feature learning networks and classifiers with the loss function of Equation (3). In the final step, we fuse prediction scores of four classifiers with weights α i and perform an end-to-end training again on the full VS-CNN network. The operation adjusts parameters in the view-group predictor and four feature learning channels, and seeks for optimal parameters for the final classifier. In the experiment of cross-view recognition I, since only one viewpoint is used for training, not all four channels are necessary. Thus we set α 1 = 1, α 2 ∼ α 4 = 0, to train one channel, ignoring other channels for network training.</p><p>Testing phase For testing, one sample is input to the viewgroup prediction module, and the predictor generates viewgroup scores { z ∈ 1×4 }. It is used to calculate weight { α ∈ 1×4 }. Following that, the testing sample is input to four feature learning channels for feature learning and obtaining prediction scores of action categories via channel classifier. We fuse prediction results by assigning weights { α} to them, and input fused score feature to the final classifier to determine the final recognition category for the test sample.</p><p>In addition, we modify the skeleton visualization approach <ref type="bibr" target="#b32">[32]</ref> by calculating the inter-frame difference of skeletons and adding the motion information to visualized skeleton images. The representation modification further weakens differences in different views so that it deals with the problem of view variety in action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of weight parameters α i</head><p>Based on the stochastic gradient descent algorithm, the parameter θ z and θ i in VS-CNN network is updated following the Equation <ref type="formula" target="#formula_4">(5)</ref> in training processing.</p><formula xml:id="formula_4">θ := θ − η ∂L(x, y) ∂θ .<label>(5)</label></formula><p>Observing the equation <ref type="formula" target="#formula_3">(4)</ref>, α i controls the parameter update of the ith network during training. When the α i have a large value, parameter θ i will be updated. Otherwise, the classifier network parameter θ i will be updated slowly or not be updated. In other words, the α i drives one of four classifiers to be trained during the training. It means that the view-group prediction module guides four classifier training in the VS-CNN. Therefore, the VS-CNN is able to classify action samples correctly in each view group and can deal with the full-circle-view action recognition by fusing four classifiers together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS AND DISCUSSIONS</head><p>We evaluate existing approaches and our proposed approach on the newly collected dataset. Four types of evaluations are performed, i.e. the cross-subject recognition, the cross-view recognition I and II, and the arbitrary-view recognition.</p><p>Using RGB videos, we report evaluation results of the joint heterogeneous features learning (JOULE) model <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b45">[45]</ref>, the ResNeXt network <ref type="bibr" target="#b40">[40]</ref>, C3D (Convolutional 3D Network) <ref type="bibr" target="#b46">[46]</ref> and LRCN (Long-term Recurrent Convolutional Network) <ref type="bibr" target="#b41">[41]</ref>. For the LRCN network, we use two feature learning networks, resnet34 and resnet50. We also give the evaluation report of depth sequences using the C3D approach <ref type="bibr" target="#b46">[46]</ref>. For skeleton data, we evaluate the Temporal Convolutional Neural Networks (TCN) <ref type="bibr" target="#b47">[47]</ref> and its modified version Res-TCN <ref type="bibr" target="#b43">[43]</ref>, LSTM with 3D rotated skeletons <ref type="bibr" target="#b21">[21]</ref>, P-LSTM <ref type="bibr" target="#b21">[21]</ref>, SK-CNN <ref type="bibr" target="#b32">[32]</ref>, and the ST-GCN <ref type="bibr" target="#b44">[44]</ref> for four types of evaluations. The two-layer LSTM and P-LSTM in <ref type="bibr" target="#b21">[21]</ref> are adopted for the evaluation.</p><p>For RGB videos and depth sequences, we evenly select 20 frames in each action sequence, and evenly select 40 frames in each skeleton sequence for experiments. The proposed VS-CNN model is evaluated on our dataset, and the results of four types of evaluations are compared with related approaches. In experiments, the average recognition accuracy is recorded for the comparison of performance. In the following experiment results, F V refers to the front viewpoint, and V 1 ∼ V 7 refer to the viewpoint 1 ∼ 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cross-subject recognition</head><p>The evaluation uses action samples captured in 8 fixed viewpoints. From 118 subjects, we select 51 subjects and separate  <ref type="table" target="#tab_0">Table IV</ref>. The average result of all viewpoints for each approach is shown in <ref type="table" target="#tab_0">Table III</ref>. <ref type="table" target="#tab_0">Table IV</ref> shows that accuracies of the viewpoint 1 ∼ 7 have a symmetrical distribution around the viewpoint 4 for all approaches. Accuracies of viewpoints 3 and 5 are generally lower than other viewpoints because actions in the two viewpoints suffer heavy occlusions. The front viewpoint always gets the highest accuracy. Comparing results of the RGB video, depth sequence, and skeleton data, results obtained using skeleton data are better than the depth and RGB video, that declares the superiority of skeleton data in action recognition. Comparing the results of the RGB video and the depth sequence, the two action feature modalities have balance performance. According to the <ref type="table" target="#tab_0">Table III</ref>, compared with other approaches, the VS-CNN outperforms other methods. Except for the VS-CNN, the JOULE, the ST-GCN, and the Res-TCN also have good performance due to their robust ability of action feature representation.</p><p>In addition, we show the confusion matrix of recognition results of cross-subject evaluation using the VS-CNN approach in <ref type="figure">Fig. 6</ref>. Here, only skeleton data is used for the evaluation. The write color represents recognition results with a value of 0, and the red color represents recognition results of 1. As shown in the figure, most action categories have weak confusion with other categories. Thus the dataset provides suitable categories for algorithm evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross-view recognition</head><p>In the experiment of Cross-view I, we train the VS-CNN network using action samples in one of 8 fixed viewpoints, and the test is performed in other 7 fixed viewpoints. To illustrate experiment performance, we calculate the average recognition accuracy per viewpoint and build a confusion matrix including recognition results of 8 viewpoints. <ref type="figure" target="#fig_9">Figure 7</ref> shows confusion matrices of cross-view recognition which are obtained by 9 recognition approaches. In each confusion matrix, the vertical <ref type="figure">Fig. 6</ref>. The confusion matrix of recognition results of cross-subject evaluation using the VS-CNN approach. It declares that actions in our dataset are much different from each other, so that is suitable for algorithm evaluation. and the horizontal axis refer to training viewpoints and test viewpoints, respectively. In the figure, deep colors represent high recognition accuracies, and light colors describe lower accuracy values. We also calculate the average result of all viewpoints for each recognition approach, and show average results of 9 recognition approaches in the <ref type="table" target="#tab_0">Table III.</ref> As shown in the <ref type="figure" target="#fig_9">Figure 7</ref>, results of all approaches at corners are better than other positions. These results correspond to the front viewpoint, the viewpoint 1 and 7. There is a 45 • angle between the viewpoint 1 and 7 and the front viewpoint. It indicates that view change within 45 • has less effect on recognition results. Comparing 9 confusion matrices, we find that rows and columns corresponding to the viewpoint 2 and 6 have lower accuracies. The reason is that only side silhouettes are captured in the two viewpoints, thus skeletons have heavy distortions. So that recognition relying on these skeletons has worse performance. Compared with other approaches in <ref type="table" target="#tab_0">Table III</ref>, the VS-CNN obtains a lower accuracy because samples of a single viewpoint are used in both the training and the test processing. Our model can not exploit its advantage   in this situation. It is obvious that all approaches perform worse in the cross-view recognition I than the cross-subject recognition and other evaluations. That is reasonable because action samples in different viewpoints have a large variance. The evaluation of Cross-view II is performed in two procedures. We separate viewpoints into two groups, where one group includes the front viewpoint, viewpoints 4, 2 and 6, and the second group contains viewpoints 1, 3, 5, and 7. Action samples of the two groups are used as training samples and test samples in turn. Results of both of the two experiments are concluded in <ref type="table" target="#tab_5">Table V</ref>. In the table, we list average recognition accuracies for each viewpoint in two rounds of evaluation. The final average results for all approaches are recorded in <ref type="table" target="#tab_0">the Table III</ref>.</p><p>The <ref type="table" target="#tab_5">Table V</ref> illustrates that results obtained in viewpoints of the front view, viewpoints 2, 4 and 6 are a little worse than the other 4 viewpoints because of noised skeletons in viewpoints of 1, 3, 5 and 7 caused by occlusions. Moreover, the results of the front view, viewpoints of 1 and 7 are better than other viewpoints as always. And nearly all approaches get worse performance in the viewpoint of 2, 3 and 6, corresponding to the view angles 90 • , 135 • , 225 • .</p><p>According to the <ref type="table" target="#tab_0">Table III</ref>, the VS-CNN performs much better than other approaches. The result of Cross-view II is only 5% lower than the cross-subject recognition, which illustrates that our evaluation rule using samples of 8 fixed viewpoints to train classifiers for the arbitrary-view recognition is reasonable. Compared with the SK-CNN, though the VS-CNN has lower performance in viewpoints of 2, 3, 5 in the <ref type="table" target="#tab_5">Table V</ref>, the proposed VS-CNN has a better performance than the SK-CNN considering the average result of all viewpoints, as shown in <ref type="table" target="#tab_0">Table III</ref>. In addition, the recognition using RGB videos obtains better performance than using depth sequences in two kinds of cross-view evaluations because the depth data has only one channel so that view variance causes heavy occlusion. Referring to three action modalities, i.e. RGB videos, depth sequences, and skeleton sequences, the depth modality has the worst performance in the cross-view evaluation. It is reasonable that depth images have lower resolution than RGB frames, and it is easy to suffer heavy occlusion when the capture view changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Arbitrary-view recognition</head><p>In the evaluation of Arbitrary-view I, we train recognition models using action samples of 8 fixed viewpoints, and perform the test on short sections of varying-view sequences.  For the Arbitrary-view II evaluation, we divide short sections in half according to subjects. The half part is a training set while the other one is a testing set. They are used for model training and testing.</p><p>In the experiment of Arbitrary-view I, we evaluate the performance of all approaches in each temporal segmented section and show the recognition accuracy per section in <ref type="figure" target="#fig_10">Figure 8</ref>. Here, varying-view sequences are segmented to 10 clips. Since each varying-view action sequence covers the entire 360 • view angle, one separated short section covers a view angle of 36 • . The figure shows that recognition accuracies vary in a shape of "W" in varying-view sequences. At the beginning and the end of varying-view sequences, recognition accuracies are high for all approaches because the view angle of the moving sensor is near the front viewpoint so that captured data quality of actions is better, no distortion and no occlusion. In varying-view sequences, the recognition accuracy gradually decreases and reaches the first valley point, then it increases until a peak point, and decreases again to arrive at the second valley point. Following that, the recognition accuracy increases once again until the end of the sequence. According to the figure, the two valley points lie at the 4th and 7th, 8th sections. If the position of the front view is defined as 0 • , the 4th segmented section occupies the view range of 108 • ∼ 144 • . And the 7th, 8th sections cover an angle range of 216 • ∼ 288 • . These two positions correspond to neighbor areas of the viewpoint V3 and V5 in the <ref type="figure" target="#fig_1">Figure 2</ref>. In these view ranges, occlusions lead to heavy noises in skeleton sequences, and information loss in RGB videos. Therefore, recognition results are worse.</p><p>For the experiment of Arbitrary-view II, we use action sections in the full-circle view sequences to train recognition models, and evaluate the performance of trained models on segmented varying-view sections. <ref type="figure">Figure 9</ref> shows average recognition accuracies of 10 sections in varying-view sequences which are obtained using various recognition approaches. Since the JOULE and the ResNeXt performs worse in most above experiments, we do not list results of them here. As shown in the <ref type="figure">Figure 9</ref>, we obtain better recognition performance using varying-view sequences to train recognition models compared with results of the arbitrary-view recognition I. Furthermore, we can see that curves of recognition accuracies have flat shapes for most of the recognition approaches in the <ref type="figure">Figure 9</ref>, that is much different from the <ref type="figure" target="#fig_10">Figure 8</ref>. It is because both of the training set and the test set cover the full-circle views of actions which improve recognition performance.</p><p>In addition, we further evaluate the performance of section segmentation in the experiment of Arbitrary-view recognition II. We change the number of segmented sections to 15 and evaluate the performance of all recognition approaches again in the experiment of Arbitrary-view II. We average recognition accuracies of all sections in varying-view sequences for all recognition approaches and compare these results with results obtained by segmenting one sequence into 10 sections in the <ref type="figure" target="#fig_0">Figure 10</ref>. The result comparison shows us that it is a better choice to separate varying-view sequences into 10 sections. In this situation, segmented clips have a similar length with action sequences captured in fixed viewpoints. It is suitable for our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Summary</head><p>Above evaluations certified that our proposed VS-CNN network outperforms existing approaches in experiments of crosssubject recognition, cross-view recognition, and arbitraryview recognition. Comparing different types of evaluations, the cross-subject recognition obtains the highest recognition accuracy, and the cross-view recognition I and the arbitraryview recognition I perform a little worse. In the cross-subject recognition, action samples have the same viewpoints in the training and the test steps, but it is a totally different situation in other three types of evaluations, especially in the crossview I and the arbitrary-view recognition. It is mainly due to unequal data distribution in the training and the test set in these experiments. The experiment comparison between the arbitrary-view recognition I and the arbitrary-view recognition II also indicates this problem clearly. However, it is impossible to collect actions in arbitrary views in our real-world HRI applications. It is required to propose approaches to recognize arbitrary-view actions based on training samples captured in limited views. It is still a challenging problem of action recognition with unknown viewpoints, and we will continue with the topic in our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>In this paper, we newly collected a large-scale RGB-D action dataset for arbitrary-view action analysis. The dataset contains samples captured in 8 fixed viewpoints and varyingview sequences that cover the entire 360 • view angels. Samples captured in fixed viewpoints provide training data for the arbitrary-view recognition, and also may be used for the multiview recognition. The dataset contained more viewpoints, more subjects, and especially varying-view sequences covering a full-circle 360 • view angles. The dataset provided sufficient data for multi-view and arbitrary-view action analysis. We further proposed a VS-CNN network to recognize arbitraryview actions, and we evaluate the proposed network for the cross-subject recognition, the cross-view recognition, and the arbitrary-view recognition on our dataset. All experiments are compared with related 8 action recognition approaches. These experiments certified the superior performance of the proposed VS-CNN network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Arbitrary-view actions in the HRI application. Robots move around the human and are expected to understand human behaviors in arbitrary views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Three types of capture setting for our dataset construction. Sizes of setting B and C are shown with black dashes and green dashes in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 (</head><label>3</label><figDesc>a) shows frame samples of 13 action categories captured in 8 fixed viewpoints and in varying-view sequences, while Figure 3(b) displays temporal frames of the action a27 (Standing opposite elbow-to-knee crunch) in a varying-view sequence. As shown in the figure, our dataset consists of complex motions and rapidly changing poses. Captured skeletons have distortions and loss of joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b) Temporal frame samples in the varying-view sequence of action a27.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Frame samples in the varying-view RGB-D action dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 2are used as training samples, and samples in the other four viewpoints are regarded as test samples. For example, samples of the front viewpoint, viewpoints 4, 2 and 6 are separated into a training group, and samples of viewpoints 1, 3, 5, and 7 are separated into the test group. Conversely, training is operated using samples captured in viewpoints 1, 3, 5 and 7, and samples of the front viewpoint, viewpoints 4, 2 and 6 are used for the test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>The architecture of the VS-CNN. The view-group predictor separates samples into four view groups and guides the training of corresponding feature learning channels and classifiers. Finally, a weighted fusion is performed on output score features of four classifiers and to train a classifier for final recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FV V 1 V 2 V 3 V 4 V 5 V 6 V 7 FV</head><label>1234567</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Confusion matrices of the cross-view I recognition. Results of all approaches at corners are better than other positions because they correspond to the front, 1 and 7 viewpoints. In confusion matrices, rows and columns corresponding to the viewpoint 2 and 6 have lower accuracies because of skeleton distortions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>0.4125 0.3475 0.30125 0.336875 0.38375 0.33625 0.3225 ResNeXt 0.474375 0.507808 0.504685 0.422861 0.386009 0.412867 0.349157 0.299188 TCN 0.460225 0.428814 0.424626 0.342861 0.387252 0.434751 0.37661 0.418998 Res-TCN 0.535575 0.531198 0.510678 0.467578 0.446957 0.437691 0.421927 0.431957 LSTM 0.683348 0.541416 0.471305 0.400727 0.462697 0.485823 0.405695 0.373289 P-LSTM 0.706944 0.562878 0.483111 0.365045 0.448982 0.48635 0.409014 0.363049 Skeleton-CNN 0.482529 0.465682 0.453104 0.371912 0.413819 0.436915 0.379395 0Evaluation results of the arbitrary-view recognition I. Recognition accuracies vary in a shape of "W" in varying-view sequences. Two valley points lie at the 4th and 7th, 8th sections, corresponding to the view range of 108 • ∼ 144 • and 216 • ∼ 288 • in the full-circle view. It is because occlusions lead to heavy skeleton noises.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Results of the arbitrary-view recognition II. Curves of recognition accuracies have flat shapes for most of the recognition approaches. It is because both of the training set and the test set cover the full-circle views of actions. Experiment comparison between separating varying-view sequences into 10 and 15 sections in the arbitrary-view recognition II. Results show that it is a better choice to clip varying-view sequences to 10 short sections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ACTION</head><label>I</label><figDesc>CATEGORIES IN THE VARYING-VIEW RGB-D ACTION DATASET.</figDesc><table><row><cell>IDs</cell><cell>Category</cell><cell></cell><cell>IDs</cell><cell>Category</cell><cell></cell><cell>IDs</cell><cell>Category</cell><cell></cell><cell>IDs</cell><cell>Category</cell><cell></cell><cell>IDs</cell><cell>Category</cell></row><row><cell>a0</cell><cell>Punching</cell><cell>and</cell><cell>a1</cell><cell>Marking</cell><cell>time</cell><cell>a2</cell><cell cols="2">Jumping jack</cell><cell>a3</cell><cell>Squatting</cell><cell></cell><cell>a4</cell><cell>Forward lunging</cell></row><row><cell></cell><cell>knee lifting</cell><cell></cell><cell></cell><cell cols="2">and knee lifting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>a5</cell><cell>Left lunging</cell><cell></cell><cell>a6</cell><cell cols="2">Left stretching</cell><cell>a7</cell><cell cols="2">Raising hand and</cell><cell>a8</cell><cell>Left kicking</cell><cell></cell><cell>a9</cell><cell>Rotation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>jumping</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>clapping</cell></row><row><cell>a10</cell><cell cols="2">Front raising in</cell><cell>a11</cell><cell cols="2">Pulling a chest</cell><cell>a12</cell><cell cols="2">Punching</cell><cell>a13</cell><cell cols="2">Wrist circling</cell><cell>a14</cell><cell>Single dumbbell</cell></row><row><cell></cell><cell>turn</cell><cell></cell><cell></cell><cell>expander</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>raising</cell></row><row><cell>a15</cell><cell cols="2">Shoulder raise</cell><cell>a16</cell><cell cols="2">Elbow circling</cell><cell>a17</cell><cell cols="2">Dumbbell one-</cell><cell>a18</cell><cell cols="2">Arm circling</cell><cell>a19</cell><cell>Dumbbell shrug-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>arm</cell><cell>shoulder</cell><cell></cell><cell></cell><cell></cell><cell>ging</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pressing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>a20</cell><cell cols="2">Pinching back</cell><cell>a21</cell><cell>Head</cell><cell></cell><cell>a22</cell><cell cols="2">Shoulder abduc-</cell><cell>a23</cell><cell cols="2">Deltoid muscle</cell><cell>a24</cell><cell>Straight forward</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">anticlockwise</cell><cell></cell><cell>tion</cell><cell></cell><cell></cell><cell>stretching</cell><cell></cell><cell>flexion</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>circling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>a25</cell><cell cols="2">Spinal stretching</cell><cell>a26</cell><cell>Dumbbell</cell><cell>side</cell><cell>a27</cell><cell>Standing</cell><cell></cell><cell>a28</cell><cell>Standing</cell><cell>body</cell><cell>a29</cell><cell>Overhead stretch-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>bend</cell><cell></cell><cell></cell><cell cols="2">opposite elbow-</cell><cell></cell><cell>rotation</cell><cell></cell><cell>ing</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">to-knee crunch</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a30</cell><cell>Upper</cell><cell>back</cell><cell>a31</cell><cell cols="2">Knee to chest</cell><cell>a32</cell><cell cols="2">Knee circling</cell><cell>a33</cell><cell>Alternate</cell><cell>knee</cell><cell>a34</cell><cell>Bent over twist</cell></row><row><cell></cell><cell>stretching</cell><cell></cell><cell></cell><cell>stretch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>lifting</cell><cell></cell></row><row><cell>a35</cell><cell cols="2">Rope skipping</cell><cell>a36</cell><cell>Standing</cell><cell>toe</cell><cell>a37</cell><cell>Standing</cell><cell></cell><cell>a38</cell><cell cols="2">Single-leg lateral</cell><cell>a39</cell><cell>High knees run-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>touches</cell><cell></cell><cell></cell><cell cols="2">Gastrocnemius</cell><cell></cell><cell>hopping</cell><cell></cell><cell>ning</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Calf Stretch</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The group consists of all action categories. The subject IDs selected for training are 1, 2, 6, 12, 13, 16, 21, 24, 28-31, 33, 35, 39, 41, 42, 45, 47, 50, 52, 54, 55, 57, 59, 61, 63, 64, 67, 69-71, 73, 77, 81, 84, 86-88, 90, 91, 93, 96, 99, 102-104, 107, 108, 112, 113. Action samples of the rest subjects are put into testing groups. The separation rule is used in all cross-subject recognition experiments in this paper. Cross-view recognition I To evaluate the performance of action recognition in cross viewpoints, action samples in one of 8 fixed viewpoints are used for training, and the test is Frame samples of 13 action categories in 8 fixed viewpoints and varying-view sequences.</figDesc><table><row><cell>a05View2</cell><cell>a06View3</cell><cell>a12View4</cell><cell>a21ViewFront</cell><cell>a23View7</cell></row><row><cell>a24View5</cell><cell>a25View5</cell><cell>a27View6</cell><cell>a28View7</cell><cell>a37View4</cell></row><row><cell>a35View5</cell><cell>a16Varying-View</cell><cell>a23Varying-View</cell><cell>a25Varying-View</cell><cell>a39View6</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>WITH OTHER MULTI-VIEW ACTION DATASETS.</figDesc><table><row><cell>Databases</cell><cell>Subjects</cell><cell>Categories</cell><cell>Viewpoints</cell><cell>Sensors</cell><cell>Data</cell><cell>Quantity (samples, RGB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>video length in hours)</cell></row><row><cell>IXMAS [14]</cell><cell>10</cell><cell>11</cell><cell>5</cell><cell>Camera</cell><cell>RGB</cell><cell>550, &lt; 2</cell></row><row><cell>Act4 2 [17]</cell><cell>24</cell><cell>14</cell><cell>4</cell><cell>Kinect v1</cell><cell>RGB,Depth</cell><cell>6844, 34</cell></row><row><cell>Multiview 3D Event [18]</cell><cell>8</cell><cell>8</cell><cell>3</cell><cell>Kinect v1</cell><cell>RGB,Depth,Skeleton</cell><cell>3815, ≈ 3.5</cell></row><row><cell>Northwestern-UCLA [19]</cell><cell>10</cell><cell>10</cell><cell>3</cell><cell>Kinect v1</cell><cell>RGB,Depth,Skeleton</cell><cell>1475, &lt; 1</cell></row><row><cell>UWA3D Multiview [20]</cell><cell>10</cell><cell>30</cell><cell>5</cell><cell>Kinect v1</cell><cell>RGB,Depth,Skeleton</cell><cell>1075, ≈ 1.5</cell></row><row><cell>NTURGB+D [21]</cell><cell>40</cell><cell>60</cell><cell>5, 16 settings</cell><cell>Kinect v2</cell><cell>RGB,Depth,Skeleton,IR</cell><cell>56,880, ≈ 79</cell></row><row><cell>Ours</cell><cell>118</cell><cell>40</cell><cell>8 fixed + varying</cell><cell>Kinect v2</cell><cell>RGB,Depth,Skeleton</cell><cell>25,600, 83</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(360 • )</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">operated on samples in another fixed viewpoint. Results of the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">cross-view recognition are reported using a confusion matrix</cell><cell></cell><cell></cell><cell></cell></row><row><cell>of all viewpoints.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESULTS</head><label>III</label><figDesc>OF FOUR TYPES OF EVALUATIONS. ALL APPROACHES PERFORM BETTER IN THE CROSS-SUBJECT RECOGNITION THAN OTHER EVALUATIONS BECAUSE TRAINING AND TEST SAMPLES HAVE THE SAME VIEWPOINTS. THE RESULT OF CROSS-VIEW II IS A LITTLE WORSE THAN THE CROSS-SUBJECT RECOGNITION.</figDesc><table><row><cell>Source</cell><cell>Approach</cell><cell>Cross-subject</cell><cell>Cross-View I</cell><cell>Cross-View II</cell><cell>Arbitrary-view I</cell><cell>Arbitrary-view II</cell></row><row><cell></cell><cell>JOULE [39]</cell><cell>0.65</cell><cell>0.31</cell><cell>0.60</cell><cell>0.35</cell><cell>0.60</cell></row><row><cell></cell><cell>ResNeXt [40]</cell><cell>0.58</cell><cell>0.32</cell><cell>0.48</cell><cell>0.43</cell><cell>0.52</cell></row><row><cell>RGB</cell><cell>C3D [46]</cell><cell>0.37</cell><cell>0.24</cell><cell>0.38</cell><cell>0.43</cell><cell>0.48</cell></row><row><cell></cell><cell cols="2">LRCN(Resnet34) [41] 0.45</cell><cell>0.11</cell><cell>0.17</cell><cell>0.29</cell><cell>0.25</cell></row><row><cell></cell><cell cols="2">LRCN(Resnet50) [41] 0.39</cell><cell>0.11</cell><cell>0.15</cell><cell>0.27</cell><cell>0.25</cell></row><row><cell>Depth</cell><cell>C3D [46]</cell><cell>0.53</cell><cell>0.10</cell><cell>0.22</cell><cell>0.34</cell><cell>0.33</cell></row><row><cell></cell><cell>TCN [47]</cell><cell>0.56</cell><cell>0.16</cell><cell>0.43</cell><cell>0.41</cell><cell>0.64</cell></row><row><cell></cell><cell>Res-TCN [43]</cell><cell>0.63</cell><cell>0.14</cell><cell>0.48</cell><cell>0.47</cell><cell>0.68</cell></row><row><cell></cell><cell>LSTM [21]</cell><cell>0.56</cell><cell>0.16</cell><cell>0.31</cell><cell>0.48</cell><cell>0.68</cell></row><row><cell>Skeleton</cell><cell>P-LSTM [21]</cell><cell>0.60</cell><cell>0.13</cell><cell>0.33</cell><cell>0.47</cell><cell>0.50</cell></row><row><cell></cell><cell>SK-CNN [32]</cell><cell>0.59</cell><cell>0.26</cell><cell>0.68</cell><cell>0.43</cell><cell>0.77</cell></row><row><cell></cell><cell>ST-GCN [44]</cell><cell>0.71</cell><cell>0.25</cell><cell>0.56</cell><cell>0.53</cell><cell>0.43</cell></row><row><cell></cell><cell>VS-CNN(Ours)</cell><cell>0.76</cell><cell>0.29</cell><cell>0.71</cell><cell>0.57</cell><cell>0.75</cell></row><row><cell cols="4">action samples acted by these subjects into the training group.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">We test action samples of rest subjects and record the statistic</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">of the average recognition accuracy for each fixed viewpoint.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Statistical results per viewpoint of all evaluated approaches</cell><cell></cell><cell></cell><cell></cell></row><row><cell>are listed in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV EVALUATION</head><label>IV</label><figDesc>OF CROSS-SUBJECT RECOGNITION. ACCURACIES OF THE VIEWPOINT 1 ∼ 7 HAVE A SYMMETRICAL DISTRIBUTION AROUND THE VIEWPOINT 4 FOR ALL APPROACHES. VIEWPOINTS 3 AND 5 HAVE LOWER ACCURACIES BECAUSE OF HEAVY OCCLUSIONS.</figDesc><table><row><cell>Source</cell><cell>Approach</cell><cell>FV</cell><cell>V1</cell><cell>V2</cell><cell>V3</cell><cell>V4</cell><cell>V5</cell><cell>V6</cell><cell>V7</cell></row><row><cell></cell><cell>JOULE [39]</cell><cell>0.84</cell><cell>0.74</cell><cell>0.53</cell><cell>0.59</cell><cell>0.60</cell><cell>0.57</cell><cell>0.55</cell><cell>0.78</cell></row><row><cell>RGB</cell><cell>ResNeXt [40] C3D [46]</cell><cell>0.65 0.47</cell><cell>0.59 0.32</cell><cell>0.57 0.32</cell><cell>0.54 0.33</cell><cell>0.60 0.44</cell><cell>0.54 0.40</cell><cell>0.60 0.33</cell><cell>0.59 0.34</cell></row><row><cell></cell><cell cols="2">LRCN(Resnet34) [41] 0.49</cell><cell>0.44</cell><cell>0.41</cell><cell>0.42</cell><cell>0.49</cell><cell>0.46</cell><cell>0.42</cell><cell>0.47</cell></row><row><cell></cell><cell cols="2">LRCN(Resnet50) [41] 0.47</cell><cell>0.36</cell><cell>0.35</cell><cell>0.41</cell><cell>0.44</cell><cell>0.40</cell><cell>0.37</cell><cell>0.36</cell></row><row><cell>Depth</cell><cell>C3D [46]</cell><cell>0.54</cell><cell>0.53</cell><cell>0.53</cell><cell>0.53</cell><cell>0.52</cell><cell>0.53</cell><cell>0.51</cell><cell>0.56</cell></row><row><cell></cell><cell>TCN [47]</cell><cell>0.78</cell><cell>0.67</cell><cell>0.49</cell><cell>0.40</cell><cell>0.49</cell><cell>0.45</cell><cell>0.50</cell><cell>0.67</cell></row><row><cell></cell><cell>Res-TCN [43]</cell><cell>0.81</cell><cell>0.67</cell><cell>0.60</cell><cell>0.53</cell><cell>0.55</cell><cell>0.53</cell><cell>0.61</cell><cell>0.73</cell></row><row><cell></cell><cell>LSTM [21]</cell><cell>0.79</cell><cell>0.67</cell><cell>0.43</cell><cell>0.47</cell><cell>0.52</cell><cell>0.48</cell><cell>0.43</cell><cell>0.66</cell></row><row><cell>Skeleton</cell><cell>P-LSTM [21]</cell><cell>0.78</cell><cell>0.70</cell><cell>0.52</cell><cell>0.49</cell><cell>0.58</cell><cell>0.50</cell><cell>0.53</cell><cell>0.74</cell></row><row><cell></cell><cell>SK-CNN [32]</cell><cell>0.75</cell><cell>0.67</cell><cell>0.54</cell><cell>0.48</cell><cell>0.57</cell><cell>0.47</cell><cell>0.57</cell><cell>0.63</cell></row><row><cell></cell><cell>ST-GCN [44]</cell><cell>0.80</cell><cell>0.72</cell><cell>0.68</cell><cell>0.65</cell><cell>0.67</cell><cell>0.69</cell><cell>0.72</cell><cell>0.75</cell></row><row><cell></cell><cell>VS-CNN(Ours)</cell><cell>0.88</cell><cell>0.85</cell><cell>0.73</cell><cell>0.67</cell><cell>0.74</cell><cell>0.69</cell><cell>0.70</cell><cell>0.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V RESULTS</head><label>V</label><figDesc>OF CROSS-VIEW RECOGNITION II. RESULTS OBTAINED IN VIEWPOINTS OF THE FRONT VIEW, VIEWPOINTS 2, 4 AND 6 ARE A LITTLE WORSE BECAUSE SKELETONS IN THE TRAINING SET ARE NOISED.</figDesc><table><row><cell></cell><cell>Training</cell><cell></cell><cell cols="2">V1, V3, V5, V7</cell><cell></cell><cell></cell><cell cols="2">FV, V4, V2, V6</cell><cell></cell></row><row><cell>Source</cell><cell>Test</cell><cell>FV</cell><cell>V2</cell><cell>V4</cell><cell>V6</cell><cell>V1</cell><cell>V3</cell><cell>V5</cell><cell>V7</cell></row><row><cell></cell><cell>JOULE [39]</cell><cell>0.74</cell><cell>0.49</cell><cell>0.57</cell><cell>0.55</cell><cell>0.74</cell><cell>0.48</cell><cell>0.47</cell><cell>0.80</cell></row><row><cell>RGB</cell><cell>ResNeXt [40] C3D [46]</cell><cell>0.51 0.51</cell><cell>0.40 0.38</cell><cell>0.54 0.15</cell><cell>0.39 0.11</cell><cell>0.52 0.59</cell><cell>0.44 0.32</cell><cell>0.48 0.51</cell><cell>0.52 0.46</cell></row><row><cell></cell><cell cols="2">LRCN(Resnet34) [41] 0.11</cell><cell>0.11</cell><cell>0.24</cell><cell>0.10</cell><cell>0.19</cell><cell>0.23</cell><cell>0.19</cell><cell>0.17</cell></row><row><cell></cell><cell cols="2">LRCN(Resnet50) [41] 0.11</cell><cell>0.18</cell><cell>0.26</cell><cell>0.14</cell><cell>0.12</cell><cell>0.10</cell><cell>0.14</cell><cell>0.09</cell></row><row><cell>Depth</cell><cell>C3D [46]</cell><cell>0.27</cell><cell>0.18</cell><cell>0.28</cell><cell>0.10</cell><cell>0.25</cell><cell>0.19</cell><cell>0.22</cell><cell>0.23</cell></row><row><cell></cell><cell>TCN [47]</cell><cell>0.59</cell><cell>0.20</cell><cell>0.31</cell><cell>0.20</cell><cell>0.72</cell><cell>0.33</cell><cell>0.40</cell><cell>0.68</cell></row><row><cell></cell><cell>Res-TCN [43]</cell><cell>0.58</cell><cell>0.27</cell><cell>0.39</cell><cell>0.24</cell><cell>0.75</cell><cell>0.40</cell><cell>0.45</cell><cell>0.73</cell></row><row><cell></cell><cell>LSTM [21]</cell><cell>0.42</cell><cell>0.17</cell><cell>0.32</cell><cell>0.16</cell><cell>0.41</cell><cell>0.28</cell><cell>0.29</cell><cell>0.41</cell></row><row><cell>Skeleton</cell><cell>P-LSTM [21]</cell><cell>0.43</cell><cell>0.13</cell><cell>0.31</cell><cell>0.14</cell><cell>0.50</cell><cell>0.28</cell><cell>0.32</cell><cell>0.53</cell></row><row><cell></cell><cell>SK-CNN [32]</cell><cell>0.75</cell><cell>0.62</cell><cell>0.64</cell><cell>0.57</cell><cell>0.79</cell><cell>0.62</cell><cell>0.63</cell><cell>0.82</cell></row><row><cell></cell><cell>ST-GCN [44]</cell><cell>0.72</cell><cell>0.32</cell><cell>0.60</cell><cell>0.29</cell><cell>0.74</cell><cell>0.52</cell><cell>0.56</cell><cell>0.74</cell></row><row><cell></cell><cell>VS-CNN(Ours)</cell><cell>0.87</cell><cell>0.54</cell><cell>0.71</cell><cell>0.60</cell><cell>0.87</cell><cell>0.58</cell><cell>0.60</cell><cell>0.87</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://mocap.cs.cmu.edu arXiv:1904.10681v1 [cs.CV] 24 Apr 2019 VOL. , NO. , APRIL 2019 2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The dataset has been released on https://github.com/HRI-UESTC/CFM-HRI-RGB-D-action-database.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical latent concept discovery for video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2149" to="2162" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative adversarial learning towards fast weakly supervised detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Describing video with attention based bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/pdf_IBM/10.1109/TCYB.2018.2831447</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view action recognition using local similarity random forests and sensor fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Collaborative sparse representation leaning model for rgbd action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="442" to="452" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Web video event recognition by semantic analysis from ubiquitous documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5689" to="5701" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatial pooling of heterogeneous features for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognition and detection of two-person interactive actions using automatically selected skeleton features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/pdf_IBM/10.1109/THMS.2017.2776211</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time rgb-d activity prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cooking gesture recognition using local feature and depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACMMM in workshop CEA</title>
		<meeting>of ACMMM in workshop CEA</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition using lstm and cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1707.02356</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">One-shot learning based pattern transition map for action early recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="364" to="370" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-view discrete hashing for fast image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2604" to="2617" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>No</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Free viewpoint action recognition using motion history volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="249" to="257" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Action recognition from arbitrary views using 3d exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via view knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human daily action analysis with multi-view and color-depth data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling 4d human-object interactions for event and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Histogram of oriented principal components for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2430" to="2443" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d pose from motion for cross-view action recognition via non-linear circulant temporal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d action recognition from novel viewpoints</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The kit robo-kitchen data set for the evaluation of view-based activity recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rybok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Friedberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">D</forename><surname>Hanebeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE-RAS International Conference on Humanoid Robots</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminative virtual views for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via a continuous virtual path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Benchmarking a multimodal and multiview and interactive dataset for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1781" to="1794" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning contrastive feature distribution model for interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="340" to="349" />
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Histogram of oriented principal components for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="914" to="927" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploiting feature and class relationships in video categorization with regularized deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="352" to="364" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human action recognition in unconstrained videos by explicit motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3781" to="3795" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust web image annotation via exploring multi-facet and structural knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4871" to="4884" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning 4d action feature models for arbitrary view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-view super vector for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detecting densely distributed graph patterns for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="553" to="565" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2186" to="2200" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoh</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exemplar-based recognition of human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="647" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
