<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<email>xusun@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have achieved promising performance on a wide range of graph-based tasks. Despite their success, one severe limitation of GNNs is the over-smoothing issue (indistinguishable representations of nodes in different classes). In this work, we present a systematic and quantitative study on the over-smoothing issue of GNNs. First, we introduce two quantitative metrics, MAD and MADGap, to measure the smoothness and oversmoothness of the graph nodes representations, respectively. Then, we verify that smoothing is the nature of GNNs and the critical factor leading to over-smoothness is the low information-to-noise ratio of the message received by the nodes, which is partially determined by the graph topology. Finally, we propose two methods to alleviate the oversmoothing issue from the topological view: (1) MADReg which adds a MADGap-based regularizer to the training objective; (2) AdaEdge which optimizes the graph topology based on the model predictions. Extensive experiments on 7 widely-used graph datasets with 10 typical GNN models show that the two proposed methods are effective for relieving the over-smoothing issue, thus improving the performance of various GNN models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Introduction 1 Graph Neural Networks form an effective framework for learning graph representation, which have proven powerful in various graph-based tasks <ref type="bibr" target="#b24">(Yang, Cohen, and Salakhutdinov 2016;</ref><ref type="bibr" target="#b19">Veličković et al. 2018;</ref><ref type="bibr" target="#b26">Zitnik and Leskovec 2017)</ref>. Despite their success in graph modeling, oversmoothing is a common issue faced by GNNs <ref type="bibr" target="#b10">(Li, Han, and Wu 2018;</ref><ref type="bibr" target="#b25">Zhou et al. 2018)</ref>, which means that the representations of the graph nodes of different classes would become indistinguishable when stacking multiple layers, which seriously hurts the model performance (e.g., classification accuracy). However, there is limited study on explaining why and how over-smoothing happens. In this work, we conduct a systematic and quantitative study of the over-smoothing issue of GNNs on 7 widely-used graph datasets with 10 typical GNN models, aiming to reveal what is the crucial factor Figure 1: The prediction accuracy (Acc) and MADGap of GCNs <ref type="bibr" target="#b6">(Kipf and Welling 2017)</ref> on the CORA dataset. We can observe a significantly high correlation between the accuracy and MADGap in two different situations: (a) different models: results of GCNs with different number of layers; (b) different training periods: results after each epoch in the 2layer GCN. The Pearson correlation coefficient is shown in the title and ** means statistically significant with p &lt; 0.01.</p><p>bringing in the over-smoothing problem of GNNs and find out a reasonable direction to alleviate it.</p><p>We first propose a quantitative metric Mean Average Distance (MAD), which calculates the mean average distance among node representations in the graph to measure the smoothness of the graph (smoothness means similarity of graph nodes representation in this paper). We observe that the MAD values of various GNNs become smaller as the number of GNN layers increases, which supports the argument that smoothing is the essential nature of GNNs. Hence, the node interaction through the GNN message propagation would make their representations closer, and the whole graph representation would inevitably become smoothing when stacking multiple layers.</p><p>Furthermore, we argue that one key factor leading to the over-smoothing issue is the over-mixing of information and noise. The interaction message from other nodes may be either helpful information or harmful noise. For example, in the node classification task, intra-class interaction can bring useful information, while inter-class interaction may lead to indistinguishable representations across classes. To measure the quality of the received message by the nodes, we define the information-to-noise ratio as the proportion of intra-class node pairs in all node pairs that have interactions through GNN model. Based on our hypothesis, we extend MAD to MADGap to measure the over-smoothness of graph (over-smoothness means similarity of representations among different classes' nodes in this paper). We notice that two nodes with close topological distance (can reach with a few hops) are more likely to belong to the same class, and vice versa. Therefore, we differentiate the role between remote and neighboring nodes and calculate the gap of MAD values (MADGap) between remote and neighboring nodes to estimate the over-smoothness of graph representation. Experimental results prove that MADGap does have a significantly high correlation with the model performance in general situations, and an example is shown in <ref type="figure">Figure 1</ref>. Further experiments show that both the model performance and the MADGap value rise as the information-to-noise ratio increases, which verifies our assumption that the informationto-noise ratio affects the smoothness of graph representation to a great extent.</p><p>After more in-depth analysis, we propose that low information-to-noise ratio is caused by the discrepancy between the graph topology and the objective of the downstream task. In the node classification task, if there are too many inter-class edges, the nodes will receive too much message from nodes of other classes after several propagation steps, which would result in over-smoothing. To prove our assumption, we optimize the graph topology by removing inter-class edges and adding intra-class edges based on the gold labels, which proves very effective in relieving oversmoothing and improving model performance. Hence, the graph topology has a great influence on the smoothness of graph representation and model performance. That is to say, there is a deviation from the natural graph to the downstream task. However, in the previous graph-related studies <ref type="bibr" target="#b19">(Veličković et al. 2018;</ref><ref type="bibr" target="#b7">Li et al. 2016;</ref><ref type="bibr" target="#b1">Bianchi et al. 2019)</ref>, researchers mainly focus on designing novel GNN architectures but pay less attention to improve the established graph topology.</p><p>Based on our observations, we propose two methods to relieve the over-smoothing issue from the topological view: (a) MADReg: we add a MADGap-based regularizer to the training objective to directly increase received information and reduce noise; (b) Adaptive Edge Optimization (AdaEdge 2 ): we iteratively train GNN models and conduct edge remove/add operations based on the prediction to adjust the graph adaptively for the learning target. Experimental results show that our two proposed methods can significantly relieve the over-smoothing issue and improve model performance in general cases, which further verifies our conclusions and provides a compelling perspective towards better GNNs performance.</p><p>The contributions of this work are threefold:</p><p>• We conduct a systematic and quantitative study of the over-smoothing issue on a wide range of graph datasets and models. We propose and verify that a key factor be-2 The algorithm name is changed from the previous AdaGraph to AdaEdge since the conflicting using of AdaGraph with other work. HyperGraph <ref type="bibr" target="#b0">(Bai, Zhang, and Torr 2019)</ref> Convolution &amp;Attention FeaSt <ref type="bibr" target="#b20">(Verma, Boyer, and Verbeek 2018)</ref> Convolution GraphSAGE <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec 2017)</ref> Convolution GAT <ref type="bibr" target="#b19">(Veličković et al. 2018)</ref> Attention ARMA <ref type="bibr" target="#b1">(Bianchi et al. 2019)</ref> Convolution GraphSAGE <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec 2017)</ref> Convolution HighOrder <ref type="bibr" target="#b13">(Morris et al. 2019)</ref> Attention GGNN <ref type="bibr" target="#b7">(Li et al. 2016</ref>) Gated <ref type="table">Table 1</ref>: Introduction of baseline GNN models. The information propagation method is also displayed.</p><p>hind the over-smoothing issue is the information-to-noise ratio which is influenced by the graph topology. • We design two quantitative metrics: MAD for smoothness and MADGap for over-smoothness of graph representation. Statistical analysis shows that MADGap has a significantly high correlation with model performance. • We propose two methods: MADReg and AdaEdge to relieve the over-smoothing issue of GNNs. Experimental results show that our proposed methods can significantly reduce over-smoothness and improve the performance of multiple GNNs on various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Models</head><p>node classification task, one of the most basic graphbased tasks, is usually conducted to verify the effectiveness of GNN architectures <ref type="bibr" target="#b19">(Veličković et al. 2018;</ref><ref type="bibr" target="#b5">Hamilton, Ying, and Leskovec 2017)</ref> or analyze the characteristics of GNNs <ref type="bibr" target="#b10">(Li, Han, and Wu 2018;</ref><ref type="bibr" target="#b11">Maehara 2019)</ref>. Therefore, we select the node classification task for our experiments. We conduct experiments on 7 public datasets in three types, namely, (1) citation network: CORA, Cite-Seer, PubMed <ref type="bibr" target="#b16">(Sen et al. 2008</ref>); (2) coauthor network: CS, Physics; 3 (3) Amazon product network: Computers, Photo <ref type="bibr" target="#b12">(McAuley et al. 2015)</ref>. We conduct our detailed analysis on the three citation networks, which are usually taken as the benchmarks for graph-related studies <ref type="bibr" target="#b10">(Li, Han, and Wu 2018;</ref><ref type="bibr" target="#b11">Maehara 2019)</ref> and verify the effectiveness of the proposed method on all these datasets.</p><p>To guarantee the generalizability of our conclusion, we conduct experiments with 10 typical GNN models in this work. The GNN models and their propagation methods are listed in <ref type="table">Table 1</ref>, in which the propagation taxonomy follows <ref type="bibr" target="#b25">Zhou et al. (2018)</ref>. The implementation of the baselines is partly based on <ref type="bibr" target="#b4">Fey and Lenssen (2019)</ref> and <ref type="bibr" target="#b11">Maehara (2019)</ref>. More details about the datasets and experimental settings are given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measuring Over-smoothing Problem from the Topological View</head><p>In this section, we aim to investigate what is the key factor leading to the over-smoothing problem. To this end, we propose two quantitative metrics MAD and MADGap to mea-sure the smoothness and over-smoothness of graph representation, which are further used to analyze why and how the over-smoothing issue happens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAD: Metric for Smoothness</head><p>To measure the smoothness of the graph representation, we first propose a quantitative metric: Mean Average Distance (MAD). MAD reflects the smoothness of graph representation by calculating the mean of the average distance from nodes to other nodes. Formally, given the graph representation matrix H ∈ R n×h (we use the hidden representation of the final layer of GNN. Term h is the hidden size), we first obtain the distance matrix D ∈ R n×n for H by computing the cosine distance between each node pair:</p><formula xml:id="formula_0">D ij = 1 − H i,: · H j,: |H i,: | · |H j,: | i, j ∈ [1, 2, · · · , n],<label>(1)</label></formula><p>where H k,: is the k-th row of H. The reason to use cosine distance is that cosine distance is not affected by the absolute value of the node vector, thus better reflecting the smoothness of graph representation. Then we filter the target node pairs by element-wise multiplication D with a mask matrix</p><formula xml:id="formula_1">M tgt D tgt = D • M tgt , (2) where • denotes element-wise multiplication; M tgt ∈ {0, 1} n×n ; M tgt ij = 1 only if node pair (i, j)</formula><p>is the target one. Next we access the average distanceD tgt for non-zero values along each row in D tgt :</p><formula xml:id="formula_2">D tgt i = n j=0 D tgt ij n j=0 1 D tgt ij ,<label>(3)</label></formula><p>where 1(x) = 1 if x &gt; 0 otherwise 0. Finally, the MAD value given the target node pairs is calculated by averaging the non-zero values inD tgt :</p><formula xml:id="formula_3">MAD tgt = n i=0D tgt i n i=0 1 D tgt i .<label>(4)</label></formula><p>Li, Han, and Wu (2018) perform a theoretical analysis on the graph convolution network (GCN), and conclude that performing smoothing operation on node representations is the key mechanism why GCN works. We extend the conclusion empirically to the 10 typical GNNs listed in <ref type="table">Table 1</ref> with the help of the proposed MAD metric. To this end, for each GNN model with different number of layers, we compute the MAD value MAD global by taking all node pairs into account, i.e., all values in M tgt are 1, to measure the global smoothness of the learned graph representation.</p><p>The results on the CORA dataset are shown in <ref type="figure" target="#fig_0">Figure 2</ref>. We can observe that as the number of GNN layers increases, the MAD values become smaller. Apart from this, the MAD value of high-layer GNNs gets close to 0, which means that all the node representations become indistinguishable. GNN models update the node representation based on the features from neighboring nodes. We observe that the interaction between nodes makes their representations similar to each other. Similar phenomenons that the smoothness rises as the layer increases are also observed in other datasets as presented in Appendix B. Therefore, we conclude that smoothing is an essential nature for GNNs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information-to-noise Ratio Largely Affects Over-smoothness</head><p>With the help of MAD, we can quantitatively measure the smoothness of graph representation. Here come two new questions: since smoothing is the nature of GNNs, what is over-smoothing, and what results in over-smoothing?</p><p>We assume that the over-smoothing problem is caused by the over-mixing of information and noise, which is influenced by the quality of the nodes received message. The interaction message from other nodes by GNN operation may be either helpful information or interference noise. For example, in the node classification task, interaction between nodes of the same class brings useful information, which makes their representations more similar to each other and the probability of being classified into the same class is increased. On the contrary, the contact of nodes from other classes brings the noise. Hence, the reason why GNNs work is that the received useful information is more than noise. On the other hand, when the noise is more than the information, the learned graph representation will become oversmoothing.</p><p>To quantitatively measure the quality of the received message of the nodes, we define the information-to-noise ratio as the proportion of intra-class node pairs in all contactable node pairs that have interactions through the GNN model. For example, at the second-order, the information-to-noise ratio for each node is the proportion of nodes of the same class in all the first-order and second-order neighbors; the information-to-noise ratio for the whole graph is the proportion of the intra-class pairs in all the node pairs that can be contacted in 2 steps. In <ref type="figure">Figure 3</ref>, we display the informationto-noise ratio of the whole graph for the CORA, CiteSeer and Pubmed datasets. We can find that there are more intraclass node pairs at low order and vice versa. When the model layer number gets large where the information-to-noise ra-2UGHU ,QIRUPDWLRQWRQRLVH5DWLR &amp;25$ &amp;LWH6HHU 3XEPHG <ref type="figure">Figure 3</ref>: The information-to-noise ratio at different neighbor orders (accumulated) for the CORA/CiteSeer/PubMed datasets. We can find that the information-to-noise ratio declines as the orders increases in all these three datasets.</p><p>tio is small, the interaction between high-order neighbors brings too much noise and dilutes the useful information, which is the reason for the over-smoothing issue. Based on this observation, we extend MAD to MADGap to measure the over-smoothness in the graph representation. From Figure 3 we notice that two nodes with small topological distance (low-order neighbours) are more likely to belong to the same category. Hence, we propose to utilize the graph topology to approximate the node category, and calculate the gap of MAD values differentiating remote and neighbour nodes to estimate the over-smoothness of the graph representation,</p><formula xml:id="formula_4">MADGap = MAD rmt − MAD neb ,<label>(5)</label></formula><p>where MAD rmt is the MAD value of the remote nodes in the graph topology and MAD neb is the MAD value of the neighbouring nodes. According to our assumption, large MADGap value indicates that the useful information received by the node is more than noise. At this time, GNNs perform reasonable extent of smoothing, and the model would perform well. On the contrary, small or negative MADGap means oversmoothing and inferior performance. To verify the effectiveness of MADGap, we calculate the MADGap value 4 and compute the Pearson coefficient between the MADGap and the prediction accuracy for various GNN models. We report the Pearson coefficient for GNNs with different layers on CORA, CiteSeer and PubMed datasets in <ref type="table" target="#tab_2">Table 2</ref>. According to the table, we can find that there exists a significantly high correlation between MADGap and the model performance, which validates that MADGap is a reliable metric to measure graph representation over-smoothness. Besides, MADGap can also be used as an observation indicator to estimate the model performance based on the graph topology without seeing the gold label. It is worth noting that 1-layer GNN usually has small MADGap and prediction accuracy <ref type="figure">(Figure 1)</ref>, which is caused by the insufficient information  transfer, while the over-smoothing issue of high-layer GNN is caused by receiving too much noise.</p><p>In <ref type="figure">Figure 4</ref>, we show the MADGap and prediction accuracy for node sets with different information-to-noise ratios in the same model. We can find that even with the same model and propagation step, nodes with higher rate of information-to-noise ratio generally have higher prediction accuracy with smaller over-smoothing degree. We also observe similar phenomena on other datasets, which are shown in Appendix C. This way, we further verify that it is the information-to-noise ratio that affects the graph representation over-smoothness to a great extent, thus influencing the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topology Affects the Information-to-noise Ratio</head><p>From the previous analysis, we can find that the key factor influencing the smoothness of graph representation is the information-to-noise ratio. Then the following question is: what affects the information-to-noise ratio? We argue that it is the graph topology that affects the information-to-noise ratio. The reason for the node receiving too much noise is related to the discordance between the natural graph and the task objective. Take node classification as an example. If there are too many inter-class edges, the nodes will receive too much noise after multiple steps of message propagation, which results in over-smoothing and bad performance.</p><p>The graph topology is constructed based on the natural links. For example, the edges in the citation network represent the citing behavior between papers and edges in the product network represent the products co-purchasing relations. GNN models rely on these natural links to learn node representations. However, natural links between nodes of different classes are harmful to the node classification task. Therefore, we propose to alleviate the over-smoothing issue of GNNs and improve their performance by optimizing the graph topology to match the downstream task.</p><p>To verify our assumption, we optimize the graph topology by removing inter-class edges and adding intra-class edges based on the gold labels. The results on the CORA dataset are shown in <ref type="figure">Figure 5</ref>. We can find that the MADGap value rises consistently as more inter-class edges are removed and  <ref type="figure">Figure 4</ref>: Performance (accuracy) and over-smoothness (MADGap) of node sets with different information-to-noise ratio (e.g., 0.1 means ratio≤ 0.1) on the CORA dataset (We display 4 out of 10 models results due to the limited space. We observe similar results in other models). All models have 2 layers. Results prove that nodes with higher information-to-noise ratio would have less over-smoothness degree and better prediction result. <ref type="figure">Figure 5</ref>: The gold label based topology adjustment experiment on the CORA dataset. We show the results of both removing inter-class edges (first row, where the X-axis represents the removing rate) and adding intra-class edges (second row, where the X-axis represents the intra-class edge ratio compared to the raw graph) on GCN, GAT, GraphSAGE and ChebGCN. Results show that both of these methods are very helpful for relieving the over-smoothing issue and improving model performance.</p><p>more intra-class edges are added, resulting in better model performance. Therefore, optimizing graph topology is helpful in relieving the over-smoothing problem and improving model performance.</p><p>In summary, we find that the graph topology has a great influence on the smoothness of graph representation and model performance. However, there is still discordance between the natural links and the downstream tasks. Most existing works mainly focus on designing novel GNN architectures but pay less attention to the established graph topology. Hence, we further investigate to improve the performance of GNNs by optimizing the graph topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relieving Over-smoothing Problem from the Topological View</head><p>Inspired by the previous analysis, we propose two methods to relieve the over-smoothing issue from the topological view: (1) MADReg: we add a MADGap-based regularizer to the training objective;</p><p>(2) Adaptive Edge Optimization (AdaEdge): we adjust the graph topology adaptively by iteratively training GNN models and conducting edge remove/add operations based on the prediction result. Neither of these two methods is restricted to specific model architectures and can be used in the training process of general GNN models. Experiments demonstrate their effectiveness in a variety of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MADReg: MADGap as Regularizer</head><p>In the previous experiments, we find that MADGap shows a significantly high correlation with model performance. Hence, we add MADGap to the training objective to make the graph nodes receive more useful information and less interference noise:</p><formula xml:id="formula_5">L = −l log p(l |X, A, Θ) − λMADGap,<label>(6)</label></formula><p>where X is the input feature matrix, A is the adjacency matrix,l and l are the predicted and gold labels of the node respectively. Θ is the parameters of GNN and λ is the regularization coefficient to control the influence of MADReg. We calculate MADGap on the training set to be consistent with the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AdaEdge: Adaptive Edge Optimization</head><p>As discussed in the previous section, after optimizing the topology based on gold label (adding the intra-class edges <ref type="figure">Figure 6</ref>: MADReg and AdaEdge results on the CORA/CiteSeer/PubMed datasets. The number of GNN layers is 4, where the over-smoothing issue is severe. The box plot shows the mean value and the standard deviation of the prediction accuracy and the MADGap values of 50 turns results (5 dataset splitting methods and 10 random seeds for each splitting following <ref type="bibr" target="#b17">Shchur et al. (2018)</ref> and <ref type="bibr" target="#b18">Sun, Koniusz, and Wang (2019)</ref>. More details can be found in Appendix A). And we can find that the two proposed methods can effectively relieve the over-smoothing issue and improve model performance in most cases.</p><p>and removing the inter-class edges), the over-smoothing issue is notably alleviated, and the model performance is greatly improved. Inspired by this, we propose a selftraining algorithm called AdaEdge to optimize the graph topology based on the prediction result of the model to adaptively adjust the topology of the graph to make it more reasonable for the specific task objective. Specifically, we first train GNN on the original graph and adjust the graph topology based on the prediction result of the model by deleting inter-class edges and adding intra-class edges. Then we retrain the GNN model on the updated graph from scratch. We perform the above graph topology optimization operation multiple times. The details of the AdaEdge algorithm are introduced in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relieving Over-smoothing in High-order Layers</head><p>To verify the effectiveness of the two proposed methods, we conduct controlled experiments for all the 10 baseline GNN models on CORA/CiteSeer/PubMed datasets. We calculate the prediction accuracy and MADGap value for the GNN models with 4 layers, where the over-smoothing issue is serious. The results are shown in <ref type="figure">Figure 6</ref>. We present 6 out of 10 models results due to the space limit; the other models can be found in Appendix E. We can find that in the high-order layer situation where the over-smoothing issue is severe, the MADReg and AdaEdge methods can effectively relieve the over-smoothing issue and improve model performance for most models in all three datasets. The effectiveness of MADReg and AdaEdge further validates our assumption and provides a general and effective solution to relieve the over-smoothing problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improving Performance of GNNs</head><p>In <ref type="table" target="#tab_4">Table 3</ref>, we show the controlled experiments for GNN models trained on the original graph and the updated graph obtained by the AdaEdge method on all the 7 datasets. We select the best hyper-parameters when training GNN on the original graph and fix all these hyper-parameters when training on the updated graph. Experimental results show that the AdaEdge method can effectively improve the model performance in most cases, which proves that optimizing the graph topology is quite helpful for improving model performance. We analyze the cases of the AdaEdge method with little or no improvement and find that this is caused by the incorrect operations when adjusting the topology. Therefore, when the ratio of incorrect operations is too large, it will bring serious interference to the model training and bring in little or no improvement. Due to the space limit, the results of MADReg are shown in Appendix F. Typically, the baselines achieve their best performance with small number of GNN layers, where the over-smoothing issue is not severe. Under this condition, MADReg can hardly improve the performance by enlarging the MADGap value. However, when the over-smoothing issue becomes more severe while the GNN layer number grows larger, MADReg is still capable of improving the performance of the baselines significantly. Above all, both AdaEdge and MADReg are effective for improving GNNs performance, and AdaEdge generalizes better when the over-smoothing issue is not severe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Graph Neural Networks (GNNs)</p><p>GNNs have proven effective in various non-Euclidean graph structures, such as social network <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec 2017)</ref>, biology network (Zitnik and Leskovec   <ref type="bibr" target="#b16">(Sen et al. 2008)</ref>. Recently, many novel GNN architectures have been developed for graph-based tasks. <ref type="bibr" target="#b19">Veličković et al. (2018)</ref> propose the graph attention network to use self-attention to aggregate information from neighboring nodes. <ref type="bibr" target="#b5">Hamilton, Ying, and Leskovec (2017)</ref> propose a general inductive framework to generate node embedding by sampling and aggregating features from the neighboring nodes. There are also other GNNs proposed, such as ARMA <ref type="bibr" target="#b1">(Bianchi et al. 2019)</ref>, FeaSt <ref type="bibr" target="#b20">(Verma, Boyer, and Verbeek 2018)</ref>, HyperGraph <ref type="bibr" target="#b0">(Bai, Zhang, and Torr 2019)</ref> and so on. <ref type="bibr" target="#b21">Xu et al. (2018)</ref> propose jumping knowledge networks to help the GNN model to leverage the information from highorder neighbours for a better node representation. However, all these models focus on improving the information propagation and aggregation operation on the static graph while paying less attention to the graph topology. In this work, we propose to explicitly optimize the graph topology to make it more suitable for the downstream task. <ref type="bibr" target="#b14">Pareja et al. (2019)</ref> propose the EvolveGCN that uses the RNN to evolve the graph model itself over time.  allow for a selective and node-adaptive aggregation of the neighboring embeddings of potentially differing locality. <ref type="bibr" target="#b23">Yang et al. (2019b)</ref> propose a new variation of GCN by jointly refining the topology and training the fully connected network. These existing works about dynamic graph rely on the adaptive ability of the model itself and focus on special GNN architecture (e.g., GCN), while our AdaEdge method optimizes the graph topology with a clear target (adding intra-class edges and removing inter-class edges) and can be used in general GNN architectures. <ref type="bibr" target="#b15">Rong et al. (2019)</ref> propose DropEdge method to drop edges randomly at each training epoch for data augmentation while our AdaEdge method adjusts edges before training to optimize the graph topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Over-smoothing Problem in GNNs</head><p>Previous works <ref type="bibr" target="#b10">(Li, Han, and Wu 2018;</ref><ref type="bibr" target="#b25">Zhou et al. 2018)</ref> have proven that over-smoothing is a common phenomenon in GNNs. <ref type="bibr" target="#b10">Li, Han, and Wu (2018)</ref> prove that the graph convolution of the GCN model is actually a special form of Laplacian smoothing. <ref type="bibr" target="#b3">Deng, Dong, and Zhu (2019)</ref> propose that smoothness is helpful for node classification and design methods to encourage the smoothness of the output distribution, while <ref type="bibr" target="#b22">Yang et al. (2019a)</ref> propose that nodes may be mis-classified by topology based attribute smoothing and try to overcome this issue. In this work, we prove that smoothing is the essential feature of GNNs, and then classify the smoothing into two kinds by the informationto-noise ratio: reasonable smoothing that makes GNN work, and over-smoothing that causes the bad performance. From this view, the methods from <ref type="bibr" target="#b3">Deng, Dong, and Zhu (2019)</ref> and <ref type="bibr" target="#b22">Yang et al. (2019a)</ref> can be regarded as improving reasonable smoothing and relieve over-smoothing, respectively. Besides, <ref type="bibr" target="#b8">Li et al. (2019a)</ref> propose to use LSTM in GNN to solve over-smoothing issue in text classification. However, existing works usually mention the over-smoothing phenomenon, but there lacks systematic or quantitative research about it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this work, we conduct a systematic and quantitative study of the over-smoothing issue faced by GNNs. We first design two quantitative metrics: MAD for smoothness and MADGap for over-smoothness. From the quantitative measurement results on multiple GNNs and graph datasets, we find that smoothing is the essential nature of GNNs; oversmoothness is caused by the over-mixing of information and the noise. Furthermore, we find that there is a significantly high correlation between the MADGap and the model performance. Besides, we prove that the information-to-noise ratio is related to the graph topology, and we can relieve the over-smoothing issue by optimizing the graph topology to make it more suitable for downstream tasks. Followingly, we propose two methods to relieve the over-smoothing issue in GNNs: the MADReg and the AdaEdge methods. Extensive results prove that these two methods can effectively relieve the over-smoothing problem and improve model performance in general situations.</p><p>Although we have shown optimizing graph topology is an effective way of improving GNNs performance, our proposed AdaEdge method still suffers from the wrong graph adjustment operation problem. How to reduce these operations is a promising research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Settings</head><p>In this section, we will introduce the graph task datasets and the baseline GNN models considered in this work first. And then we will introduce the experiment details and notations used in this paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Dataset</head><p>We conduct experiment on 7 node classification datasets in 3 types:</p><p>• Citation Network The CORA, CiteSeer, PubMed datasets <ref type="bibr" target="#b16">(Sen et al. 2008)</ref> are citation networks which are usually taken as be benchmarks of graph related studies <ref type="bibr" target="#b10">(Li, Han, and Wu 2018;</ref><ref type="bibr" target="#b11">Maehara 2019)</ref>. The nodes are papers in computer science field with bag-of-word features of paper title. The edges represent the citation relation amaong papers and the label is paper category. • Coauthor Network Coauthor CS and Coauthor Physics are co-authorship graphs based on the Microsoft Academic Graph from the KDD Cup 2016 challenge 3. 5 Here, nodes are authors, that are connected by an edge if they co-authored a paper; node features represent paper keywords for each authors papers, and class labels indicate most active fields of study for each author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Amazon Network Amazon Computers and Amazon</head><p>Photo <ref type="bibr" target="#b12">(McAuley et al. 2015)</ref> are segments of the Amazon co-purchase graph, where nodes represent goods, edges indicate that two goods are frequently bought together, node features are bag-of-words encoded product reviews, and class labels are given by the product category.</p><p>Datasets related statistical information are shown in the Table 4. All these graphs are undirected graphs with no edge weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Details</head><p>Previous works <ref type="bibr" target="#b19">(Veličković et al. 2018;</ref><ref type="bibr" target="#b0">Bai, Zhang, and Torr 2019;</ref><ref type="bibr" target="#b9">Li et al. 2019b</ref>) on GNN study usually run experiment multi-turns to eliminate random effects. And <ref type="bibr" target="#b17">Shchur et al. (2018)</ref> pointed out that in the semi-supervised node classification task, the train/valid/test split of dataset has a significant influence on the final result. Following <ref type="bibr" target="#b17">Shchur et al.;</ref><ref type="bibr">Sun, Koniusz, and Wang (2018;</ref><ref type="bibr" target="#b11">2019)</ref> we apply the 20/30/rest splitting method, which means we randomly sample 20 nodes in each category for training set and 30 for validation set; all the rest nodes are as test data. In order to ensure the credibility of the results, we select 5 random train/valid/test split of dataset and run 10 turns with different random seeds in each split. Then we measure the average number of all the 50 turns' results. Besides, in order to avoid the random effects caused by dataset split or initial seeds and observe the influence of the proposed methods more clearly, we use the same random dataset split and initial seed list for the baseline method and the proposed method in each controlled experiment, and the dataset split and seed list was randomly generated before each controlled experiment. We also fix all the other hyperparameters in each controlled experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations Used in This Paper</head><p>Given a undirected graph G = (V , E), where the V is the node set with |V | = n and E is the edge set. The adjacency matrix is denoted by A = [a ij ] ∈ R n×n . The raw node features are denoted by X = [x 1 , x 2 , . . . , x n ] ∈ R n×c and each x i ∈ R c is the input feature vector for the i-th node. Each node has a corresponding label l indicating the node class. In the semi-supervised node classification task, the gold labels of nodes in the trainset are known and the target is to predict the labels for nodes in the testset. We use GNN k to represent a k-layer (propagate step) graph neural network and the label predicted by this GNN is represented byl. Besides, the hidden states of all node after j layer(s) is denoted by H j ∈ R n×hj (h j represents the hidden-size of GNN j-th layer) and h ji denotes the hidden state vector of the i-th node representation after j layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MAD Global Values on More Datasets</head><p>In <ref type="figure">Figure 9</ref>, we display the MAD values of various GNNs with different layers on CiteSeer and PubMed dataset. We can observe that in these two datasets, the MAD values of all baseline GNN models decreases with the increase of model layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Infomation-to-noise Ratio Experiment on</head><p>More Datasets</p><p>In <ref type="figure" target="#fig_2">Figure 7</ref>, we display the model performance and the MADGap value of node sets with different infomation-tonoise ratio at 2-order neighbours in CiteSeer and PubMed dataset. We can find that in these four GNN models, the model performance and the MADGap value rises with the increase of the intra-class node ratio, which will bring more useful information for the graph nodes. So it is the information-to-noise ratio that largely affects the node representation over-smoothness thus has a influence on the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D AdaEdge Algorithm</head><p>The details of AdaEdge Algorithm is displayed in Algorithm 1. Besides, we use several heuristic approaches to adjust graph topology more wisely: (1) Operation edge for nodes with high prediction confidence (the max value after softmax operation); (2) Operation edge for nodes belong to classes with high prediction precision; (3) Skip operation by a certain probability to control the sparsity of the added and removed edges; (4) Operation edge for nodes with certain degrees.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Supplementary Result of Relieving Over-smoothing</head><p>In <ref type="figure">Figure 8</ref>, we display more models results of relieving over-smoothing in high layer GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Error Analysis of MADReg</head><p>In <ref type="table" target="#tab_8">Table 5</ref>, we show the results for GCN with different number of layers on the CORA/CiteSeer/PubMed datasets and we observe that the MADReg method can relieve the over-smoothing problem and improve model performance effectively especially for GNNs with more layers where the over-smoothing issue is more severe. In the shallow GCN, our MADReg method can not effectively improve the model performance because the model has already been well trained and the over-smoothing issue is not serious.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The MAD values of various GNNs with different layers on the CORA dataset. Darker color means larger MAD value. We can find that the smoothness of graph representation rises as the model layer increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Infomation-to-noise Ratio Experiment on CiteSeer (top plot) and PubMed (bottom plot) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>More Results on the CORA/CiteSeer/PubMed datasets. The number of model layer is 4, where the over-smoothing issue is serious. The box graph shows the mean value and the standard deviation of the prediction accuracy and the MADGap values. And we can find that the two proposed methods can effectively relieve the over-smoothing issue and improve model performance in most cases. MAD results on CiteSeer (top plot) and PubMed (bottom plot) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The Pearson coefficient between accuracy and MADGap for various models on CORA/CiteSeer/PubMed datasets. Pearson coefficient is calculated based on the results of models with different layers (1-6). * means statistically significant with p &lt; 0.05 and ** means p &lt; 0.01.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Controlled experiments of AdaEdge (+AE) on all the 7 datasets. We show the mean value, the standard deviation and the t-test significance of 50 turns results. * means statistically significance with p &lt; 0.05 and ** means p &lt; 0.01. Darker color means larger improvement. The missing results are due to the huge consumption of GPU memory of large graphs.</figDesc><table><row><cell>2017), business graph (McAuley et al. 2015) and academic</cell></row><row><cell>graph</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Statistical information about datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results of MADReg method for GCN with different layers on CORA/CiteSeer/PubMed dataset. Darker color means larger improvement over the baseline.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Accepted by AAAI 2020. This complete version contains the appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://kddcup2016.azurewebsites.net</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In this work, we calculate MAD neb based on nodes with orders ≤ 3 and MAD rmt based on nodes with orders ≥ 8.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://kddcup2016.azurewebsites.net</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by a Tencent Research Grant and National Natural Science Foundation of China (No. 61673028). Xu Sun is the corresponding author of this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 AdaEdge</head><p>Require: The GNN model GNN k , the feature matrix X, the raw adjacency matrix A, the node size N ,the operation order order, the limit number num+ and num−, the limit confidence conf+ and conf− and max training times maxt. 1: function ADDEDGE(A, pred, conf ) 2:</p><p>add count ← 0 3:</p><p>A ← A 4:</p><p>for node1 ni ∈ [0, N ) do 5:</p><p>for node2 nj ∈ [ni, N ) do 6:</p><p>if <ref type="formula">Aij==0</ref>  if acci ≤ acci−1 then 50:</p><p>reutrn acci−1 51: end if 52:</p><p>A i ← ADJUSTGRAPH(pred i , conf i ) 53: end for 54: reutrn accmax t −1</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08150</idno>
		<title level="m">Hypergraph Convolution and Hypergraph Attention</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01343</idno>
		<title level="m">Graph Neural Networks with Convolutional Arma Filters</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Batch virtual adversarial training for graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09192</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04849</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds. Fey, M. 2019. Just Jump: Dynamic Neighborhood Aggregation in Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08166</idno>
		<title level="m">Recursive graphical neural networks for text classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coherent Comment Generation for Chinese Articles with a Graph-to-Sequence Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4843" to="4852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeper Insights into Graph Convolutional Networks for Semi-supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image-based Recommendations on Styles and Substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman go Neural: Higher-order Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pareja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kanezashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Leisersen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10191</idno>
		<title level="m">Evolvegcn: Evolving graph convolutional networks for dynamic graphs</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The truly deep graph convolutional networks for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10903</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of Graph Neural Network Evaluation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fisher-Bures Adversary Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">161</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feastnet: Feature-steered Graph Convolutions for 3d Shape Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2598" to="2606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual Self-Paced Graph Convolutional Network: Towards Reducing Attribute Distortions Induced by Topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="4062" to="4069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Topology Optimization based Graph Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="4054" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting Semi-supervised Learning with Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning,ICML 2016</title>
		<meeting>the 33nd International Conference on Machine Learning,ICML 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph Neural Networks: A Review of Methods and Applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting Multicellular Function through Multi-layer Tissue Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
