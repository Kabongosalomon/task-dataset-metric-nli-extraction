<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Learning of a Fisher Vector Encoding for Part Features in Fine-grained Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Korsch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">Friedrich-Schiller-University Jena</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bodesheim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">Friedrich-Schiller-University Jena</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">Friedrich-Schiller-University Jena</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Michael Stifel Center Jena</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end Learning of a Fisher Vector Encoding for Part Features in Fine-grained Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>End-to-end learning</term>
					<term>Fisher vector encoding</term>
					<term>part-based fine- grained recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Part-based approaches for fine-grained recognition do not show the expected performance gain over global methods, although being able to explicitly focus on small details that are relevant for distinguishing highly similar classes. We assume that part-based methods suffer from a missing representation of local features, which is invariant to the order of parts and can handle a varying number of visible parts appropriately. The order of parts is artificial and often only given by ground-truth annotations, whereas viewpoint variations and occlusions result in parts that are not observable. Therefore, we propose integrating a Fisher vector encoding of part features into convolutional neural networks. The parameters for this encoding are estimated jointly with those of the neural network in an end-to-end manner. Our approach improves state-of-the-art accuracies for bird species classification on CUB-200-2011 from 90.40% to 90.95%, on NA-Birds from 89.20% to 90.30%, and on Birdsnap from 84.30% to 86.97%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Part-or attention-based approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref> are common choices for fine-grained visual categorization (FGVC) because they can explicitly focus on small details that are relevant for distinguishing highly similar classes, e.g., different bird species. Quite surprisingly, methods that perform the categorization with global image features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41]</ref> also achieve excellent results and it is hard to tell from the empirical results reported in the literature which general approach (global or part-based) is superior, given that all of them show comparable results in terms of recognition performance. We hypothesize that part-based algorithms are currently not able to exploit their full potential due to the problems that arise from the initial detection of parts, especially regarding a unified representation of individual part features after the detection. Since learning individual part detectors requires part annotations and thus additional, time-consuming efforts by domain experts, methods for unsupervised part detection have been developed that already obtain remarkable classification results <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38]</ref>. However, unsupervised part detection faces various challenges,  <ref type="figure">Fig. 1</ref>. Comparison of traditional part feature aggregation with our proposed method based on Fisher vector encoding. In the traditional case, the problem of missing features may occur when some parts can not be detected but the resulting gaps need to be filled for the feature concatenation. Furthermore, the semantics of extracted features is not clear due to unsupervised part detection algorithms that may not preserve a consistent order of part features. With our approach, we are able to compute a unified representation of fixed length for an arbitrary number of unordered part features. This representation can then be used by any type of classifier, including simple linear classifiers and fully-connected layers in a deep neural network.</p><p>such as missing parts caused by different types of occlusions and parts with ambiguous semantic meaning. Hence, it remains unclear whether detected parts are reasonable and semantically consistent. This is also depicted in <ref type="figure">Fig. 1</ref>. Furthermore, part-based classifiers usually require both a fixed number of parts to be determined for each image and a pre-defined order of the extracted part features, which are then handed to the classifier. These are strong restrictions for the application, especially when considering varying poses and viewpoints that lead to hidden parts not visible in an image. In general, we believe that a common way for representing a varying number of unordered part features obtained from each single image is missing.</p><p>We therefore propose the integration of a well-known feature transformation method typically applied to local descriptors of keypoints, namely the Fisher vector encoding (FVE) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, into convolutional neural network (CNN) architectures. The FVE enables handling different numbers of parts for each image, e.g., only those that are visible given the actual pose of the bird and the viewpoint, by computing a unified representation of fixed length independent of the number of detected parts <ref type="figure">(Fig. 1</ref>). Note that the problem of missing parts can also be observed when considering the ground-truth annotations of widely-used fine-grained datasets for bird species categorization <ref type="bibr">[2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>, which are often used to evaluate the performance of new classification algorithms without taking the part detection into account. Common methods for filling the gaps of missing parts are simple, like the insertion of a constant value, e.g., filling with zeros <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39]</ref>. Other strategies insert the whole input image as a part (which is then rather a global feature than a local one), or a cropped part of the image around its center point. The latter can be motivated by a spatial prior, i.e., objects are more often located in the center of an image. Nevertheless, a gap filling strategy for handling missing parts is not required when using our method. Furthermore, our approach allows for neglecting an artificial order of parts, since there is usually no natural order of parts and each part should be treated equally.</p><p>In contrast to previous approaches that apply some kind of FVE together with CNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref>, our method determines the underlying Gaussian mixture model (GMM) with an iterative EM algorithm using mini-batch updates of parameters. With our approach, the parameters of the GMM are estimated jointly with the parameters of the CNN in an end-to-end manner. As the consequence, the GMM has a direct influence on the feature extraction within the CNN and also adapts to the changing feature representations during learning. Sect. 3 describes the details of our approach. The benefits are shown in defining new state-of-the-art results for CUB-200-2011 <ref type="bibr" target="#b34">[35]</ref> from 90.40% to 90.95%, NA-Birds <ref type="bibr" target="#b32">[33]</ref> from 89.20% to 90.30%, and Birdsnap <ref type="bibr">[2]</ref> from 84.30% to 86.97%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We divide this section into three parts. First, related work on FVE in the context of deep neural networks is discussed. These approaches either do not allow for learning the GMM parameters end-to-end but rather estimating parameters separately after neural network training or simply treat GMM parameters as conventional network parameters that are learned without any clustering objective by artificially enforcing reasonable values for the mixture parameters such as positive variances for Gaussian distributions. Second, we shortly review existing algorithms for iterative EM algorithms since we borrow ideas from these approaches for enabling end-to-end learning of GMM parameters together with CNN weights as part of our proposed FVE. Third, we list current state-of-theart techniques for fine-grained categorization, especially focussing on part-based methods. Note that we are not aware of any work in the field of part-based finegrained categorization that makes use of an FVE in conjunction with features from deep neural networks. Furthermore, we have not found any application of an iterative EM algorithm within a CNN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Variants of Deep Fisher Vector Encoding (Deep FVE)</head><p>As one of the first attempts, Simonyan et al. <ref type="bibr" target="#b26">[27]</ref> presented a Fisher vector layer for building deep networks. They encode an input image or pre-extracted SIFT features in multiple layers with an FVE and present a new pooling method that is applied after each encoding layer. A global pooling operation in the penultimate layer results in a single vector that is classified by an ensemble of linear SVMs. Due to some restrictions, the entire network is trained greedily layer by layer and not end-to-end. Another deep architecture presented by Sydorov et al. <ref type="bibr" target="#b28">[29]</ref> learns an FVE by updating GMM parameters based on the classification loss rather than running an EM algorithm. In each iteration, an SVM classifier is trained, then gradients w.r.t. the GMM parameters are computed that are used to update them accordingly. Cimpoi et al. <ref type="bibr" target="#b4">[5]</ref> proposed a CNN together with an FVE that encodes local CNN features by a single feature representation. This representation is finally used by a 1-vs-rest SVM to perform classification. Song et al. <ref type="bibr" target="#b27">[28]</ref> were able to further improve this approach by additional processing of the encoded features, but still without end-to-end learning.</p><p>In contrast to the aforementioned methods that compute an FVE of local features extracted separately from the image, Wieshollek et al. <ref type="bibr" target="#b35">[36]</ref> and Tang et al. <ref type="bibr" target="#b30">[31]</ref> deploy the FVE directly in a neural network. As a result, the features are learned jointly with the parameters for both the classification and the mixture model. However, although the GMM parameters are estimated jointly with the other network parameters using gradient descent, the training procedure has some drawbacks. On the one hand, artificial constraints have to be applied in order to obtain reasonable mixture parameters, e.g., positive variances. On the other hand, due to the formulas for the FVE, the resulting gradients cause numerically unstable computations. Both of these drawbacks can be circumvented by our proposed method, which estimates the parameters of the mixture model with an iterative EM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">EM Algorithms and GMM estimation</head><p>The standard technique for estimating GMM parameters is the EM algorithm, which is an iterative process of alternating between maximum likelihood estimation of mixture parameters and computing soft assignments of samples to the mixture components. In the default setting, all samples are used in both steps, but this leads to an increased runtime for large-scale datasets. To reduce the computational costs, one can only use a subset of samples for the parameter estimation or rely on existing online versions of the EM algorithm <ref type="bibr">[3,</ref><ref type="bibr" target="#b21">22]</ref>. As an example, Cappé and Moulines <ref type="bibr">[3]</ref> approximate the expectation over the entire dataset with an exponential moving average over batches of the data. Based on this work, Chen et al. <ref type="bibr" target="#b3">[4]</ref> propose a variance reduction of the estimates in each step, which results in faster and more stable convergence. A comparison of these algorithms is given by the work of Karimi et al. <ref type="bibr" target="#b14">[15]</ref>, who also introduce a new algorithm and establish non-asymptotic convergence bounds for global convergence. Further approaches <ref type="bibr">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref> propose similar solutions with different applications and motivations for the iterative parameter update.</p><p>In our work, we employ the ideas of iterative parameter update and estimator correction. Furthermore, we demonstrate how to integrate these ideas in a neural network and estimate the parameters jointly with the network weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fine-grained visual categorization</head><p>In the literature, two main directions can be observed for fine-grained recognition: global and part-based methods. A global methods use the input image as a whole and employ clever strategies for pre-training <ref type="bibr" target="#b5">[6]</ref>, augmentation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b17">18]</ref>, or pooling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41]</ref>. In contrast, part-or attention-based approaches apply sophisticated detection techniques in order to determine interesting areas in the image and to extract detailed local features from these patches. This results in part features as an additional source of information for boosting the classification performance.</p><p>He et al. <ref type="bibr" target="#b11">[12]</ref> propose a reinforcement learning method for estimating how many and which image regions are helpful to distinguish the categories. They use multi-scale image representations for localizing the object and afterwards estimate discriminative part regions. Ge et al. <ref type="bibr" target="#b7">[8]</ref> present the current stateof-the-art approach on the CUB-200-2011 dataset. Based on weakly-supervised instance detection and segmentations, part proposals are generated and constrained by a part model. The final classification is performed with a stacked LSTM classifier and context encoding. The method of Zhang et al. <ref type="bibr" target="#b37">[38]</ref> also yields good results on the CUB-200-2011 and the NA-Birds dataset. Expert models arranged in multiple stages predict both class assignments and attention maps that are used by the next expert to crop the image and refine the observed data. Finally, a gating network is used to weight the decisions of the individual experts.</p><p>Compared to the previous approaches, we use a different part detection method that is described at the beginning of the next section prior to presenting the details of our proposed FVE for part features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fisher Vector Encoding (FVE) of Part Features</head><p>In this section, we present our approach for an FVE of part features, which allows for joint end-to-end learning of all parameters, i.e., the parameters of the underlying GMM as well as the parameters of the CNN that computes the part features. It can be applied to any set of parts that are extracted from an image, hence it is possible to combine it with different part detection algorithms. In this paper, we use the code 1 for a part detection method provided by Korsch et al. <ref type="bibr" target="#b16">[17]</ref> The authors use an initial classification of the entire input image to identify features that are used for this classification. Then, the pixels in the receptive field of these features are clustered and divided into candidate regions. Bounding boxes are estimated around these regions and used as parts in the final part-based classification.</p><p>Given a set of parts specified by their corresponding image regions, we propose the computation of a set of local features for each part with a CNN followed by an FVE of these part features to obtain a unified part representation of fixed length independent of the number of detected parts. <ref type="figure" target="#fig_1">Fig. 2</ref> visualizes our approach. Note that any CNN architecture can be used as a backbone network for computing part features, even with pre-trained weights that are fine-tuned when estimating the GMM parameters for the FVE. In order to characterize our proposed FVE that is learned end-to-end together with the network parameters, we present the mathematical formulation of the underlying mixture model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mixture Models and Parameter Estimation via EM Algorithm</head><p>Given a set of feature vectors X = {x 1 , . . . , x N } denoted by D-dimensional vectors x n ∈ R D (∀ n ∈ [1, N ]) we assume that these representations are generated i.i.d. following a density function p(x n ). In our case, these are part features extracted with a CNN. The function p(x n ) is supposed to be a finite mixture model with K components p(x n |Θ) = K k=1 α k p k (x n |θ k ). Each mixture component k has a density function p k (x n |θ k ) with parameters θ k . The mixture weights α k denote the prior probability that a random sample x n has been generated by component k such that</p><formula xml:id="formula_0">K k=1 α k = 1. The entire set of parameters for the mixture model is Θ = {α 1 , . . . , α K , θ 1 , . . . , θ K }.</formula><p>From this definition, we can compute the soft assignment w n,k of each feature vector x n to component k based on parameters Θ as follows:</p><formula xml:id="formula_1">w n,k = α k p k (x n |θ k ) K =1 α p (x n |θ ) .<label>(1)</label></formula><p>Without further prior knowledge, we assume that each component k is a Gaussian distribution with parameters θ k = {µ k , Σ k } leading to the definition of a Gaussian mixture model (GMM). Assuming that the dimensions of each x n are independent, the covariance matrices Σ k become diagonal and can be represented by covariance vectors σ 2 k = diag(Σ k ). Hence, the density function of component k can be written as:</p><formula xml:id="formula_2">p k (x n |µ k , σ k ) = 1 D d=1 √ 2πσ k,d exp − D d=1 (x n,d − µ k,d ) 2 2(σ k,d ) 2 .<label>(2)</label></formula><p>To estimate the parameters Θ of the GMM from the feature vectors in X , an EM algorithm is typically applied. It alternates between an E-step and an M-step until convergence. In the E-step, soft assignments w new n,k as defined in Eq. <ref type="formula" target="#formula_1">(1)</ref> are computed for each feature vector x n and each mixture component k. These soft assignments can be represented by a matrix W of size N ×K, where the entries in each row sum up to 1. In the M-step, new mixture parameters are computed using the soft assignment matrix W and the feature vectors x n via:</p><formula xml:id="formula_3">α new k = N k N ,<label>(3)</label></formula><formula xml:id="formula_4">µ new k = 1 N k N n=1 w new n,k x n ,<label>(4)</label></formula><formula xml:id="formula_5">σ new k = 1 N k N n=1 w new n,k (x n − µ new k ) 2 .<label>(5)</label></formula><p>In these Equations, N k = N n=1 w n,k denotes the effective number of samples assigned to component k. However, as it can be seen from these update rules, all N samples are required for estimating the new parameter values. To overcome this drawback and to allow for the integration of an additional layer in a CNN that performs both GMM estimation and FVE using mini-batches, we propose an EM-algorithm with mini-batch updates in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EM-Algorithm with Mini-batch Updates</head><p>To estimate a GMM and compute an FVE of part features within a deep learning architecture, the EM algorithm needs to be adapted to the batch-wise learning of neural networks. We suggest performing a single E-step and a single M-Step for each mini-batch within the learning procedure of the neural network, as already proposed in <ref type="bibr">[3,</ref><ref type="bibr" target="#b21">22]</ref>. Hence, we do not sum over all N training samples in Equations (3) to (5), but over all N b samples in the current training batch b. The resulting new parameter values are then used to update the model parameters through an exponential moving average (EMA) with parameter λ ∈ (0, 1) and [t] indicating the time steps t in the iterative estimation process:</p><formula xml:id="formula_6">α k [t] = λα k [t − 1] + (1 − λ)α new k ,<label>(6)</label></formula><formula xml:id="formula_7">µ k [t] = λµ k [t − 1] + (1 − λ)µ new k ,<label>(7)</label></formula><formula xml:id="formula_8">σ k [t] = λσ k [t − 1] + (1 − λ)σ new k .<label>(8)</label></formula><p>Since plain EMA is a biased estimator, we utilize an additional bias correction, as it is also done in the Adam optimizer <ref type="bibr" target="#b15">[16]</ref>, via:</p><formula xml:id="formula_9">α k [t],μ k [t],σ k [t] = α k [t] 1 − λ t , µ k [t] 1 − λ t , σ k [t] 1 − λ t<label>(9)</label></formula><p>In our experiments, we have chosen λ = 0.9, since this is also the default value for the weight decay rate in batch-normalization layers <ref type="bibr" target="#b13">[14]</ref> and the Adam optimizer <ref type="bibr" target="#b15">[16]</ref>, which both also perform EMA estimates. Furthermore, we empirically tested other values but found that λ = 0.9 leads to the best and most stable results. Using the iterative update rules for the GMM parameters, we can define a Fisher vector encoding layer for any neural network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">New Fisher Vector Encoding Layer (FVE-Layer)</head><p>Since we have shown how to estimate the parameters of a GMM within the standard training procedure of CNNs by only processing mini-batches, we can implement an FVE-Layer that uses the estimated GMM parameters for computing the FVE of a set of part features. Following Perronnin et al. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, the FVE is derived from the gradient space of the GMM by considering the gradients of the log-likelihood with respect to the GMM parameters Θ and assuming independence of the N I part features X I = {x 1 , . . . , x N I } from a single image I:</p><formula xml:id="formula_10">G Θ (X I ) = ∇ Θ log p(X I |Θ) = N I n=1 ∇ Θ log p(x n |Θ) .<label>(10)</label></formula><p>These gradients, also called Fisher scores, describe how parameters contribute to the process of generating a particular sample. In a similar way, the Fisher scores for component k of the GMM with parameters θ k = (µ k , σ k ) are:</p><formula xml:id="formula_11">G µ k,d (X I ) = ∇ µ k,d log p(X I |Θ) = N I n=1 w n,k x n,d − µ k,d (σ k,d ) 2 ,<label>(11)</label></formula><formula xml:id="formula_12">G σ k,d (X I ) = ∇ σ k,d log p(X I |Θ) = N I n=1 w n,k (x n,d − µ k,d ) 2 (σ k,d ) 3 − 1 σ k,d ,<label>(12)</label></formula><p>where d ∈ {1, . . . , D} denotes the corresponding dimension of the feature vectors x n , mean vectors µ k , and standard deviation vectors σ k . We use the approximated normalized Fisher scores introduced by <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, which leads to the following formulas for the individual components of the FVE:</p><formula xml:id="formula_13">F µ k,d (X I ) = 1 √ N I α k N I n=1 w n,k x n,d − µ k,d σ k,d ,<label>(13)</label></formula><formula xml:id="formula_14">F σ k,d (X I ) = 1 √ 2N I α k N I n=1 w n,k (x n,d − µ k,d ) 2 (σ k,d ) 2 − 1 .<label>(14)</label></formula><p>These scores can directly be computed for all parameters µ k,d and σ k,d (with k = 1, . . . , K and d = 1, . . . , D) based on the estimated GMM parameters. Their concatenation results in a unified representation of dimension 2KD that we use as an FVE of part features. In this way, we have described the calculations of the new FVE-Layer that can be integrated into a deep learning architecture.</p><p>It is interesting to note that this layer differs from a typical deep learning layer (like convolutional or fully-connected) in the sense that its parameters are not trained through back-propagation, but through the iterative updates described in Sect. 3.2. A similar idea is employed in the batch-normalization layer <ref type="bibr" target="#b13">[14]</ref>, where running means and running variances are estimated iteratively based on every batch in the dataset. Subsequently, the estimated values are used to transform the outputs of the previous layer such that they have zero mean and unit variance afterward. The estimation of the running mean and running variance is identical to the estimation of a GMM with only one component. We have confirmed this observation in our experiments by comparing the estimated parameters of a batch-normalization layer and FVE-Layer with one mixture. However, the FVE-Layer uses a different feature transformation compared to batch-normalization, namely the FVE.</p><p>As it is also the case for the batch-normalization layer, the new FVE-Layer can be integrated into any network architecture and allows for end-to-end learning despite the fact that the parameters of the layer are not estimated by backpropagation and gradient descent. The integration is possible because we are able to back-propagate the gradients through the FVE-Layer to the previous layers by specifying the gradients of this layer with respect to its inputs. Therefore, it is sufficient to provide the gradients for the individual components of the FVE, namely F µ k,d (X I ) and F σ k,d (X I ), w.r.t. an arbitrary feature x n,d * , i.e., with respect to the dimension d * ∈ {1, . . . , D} of the n-th sample:</p><formula xml:id="formula_15">∂F µ k,d (X I ) ∂x n,d * = 1 √ N I α k ∂w n,k ∂x n,d * x n,d − µ k,d σ k,d + δ d,d * w n,k σ k,d * ,<label>(15)</label></formula><formula xml:id="formula_16">∂F σ k,d (X I ) ∂x n,d * = 1 √ 2N I α k ∂w n,k ∂x n,d * (x n,d − µ k,d ) 2 (σ k,d ) 2 − 1 +δ d,d * 2w n,k (x n,d * − µ k,d * ) (σ k,d * ) 2 .<label>(16)</label></formula><p>In both equations, we use δ d,d * to denote the Kronecker delta being 1 if d = d * and 0 else, as well as the derivative of w n,k w.r.t. x n,d * that is given by:</p><formula xml:id="formula_17">∂w n,k ∂x n,d * = w n,k − (x n,d * − µ k,d * ) (σ k,d * ) 2 + K =1 w n, (x n,d * − µ ,d * ) (σ ,d * ) 2 .<label>(17)</label></formula><p>Further details for the derivation of these gradients can be found in the supplementary material. Note that similar formulas are also provided by <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Part Features from ConvMaps</head><p>So far, we have not specified which part features are used to estimate the GMM parameters and to compute the FVE. Based on the outputs of the last convolutional layer of the CNN called ConvMaps, it would be possible to perform global average pooling for these ConvMaps to obtain a single feature vector for each part. The GMM parameters would then be estimated based on a set of N train ·P vectors with N train being the number of training images and P the average or fixed number of parts per image.</p><p>In contrast, we propose using a set of local feature vectors extracted from the ConvMaps of each part. If the typical output of a convolutional layer with C channels computed for a single part has the shape C×H×W with H and W denoting the spatial extent of the ConvMaps, then we use the H·W vectors of dimension C to describe that part. Hence, the number of local feature vectors for learning the GMM parameters increases to N train ·P ·H·W . For a single parameter update using a batch size of B images, B·P ·H·W local features are used. Furthermore, the FVE for part features of a single image is then calculated using P ·H·W vectors of dimension C rather than only P vectors. Since the number of parts P is usually rather low (less than 20), we expect more stable results for an FVE that makes use of a larger set of vectors. In our experiments, the spatial resolution of ConvMaps is 8×8.</p><p>Additionally, we have observed that some of these C-dimensional local feature vectors (H·W vectors for each of the P parts) have low L 2 -norm, especially if the corresponding receptive field mainly contains background pixels. However, since we are only interested in using local features that exceed a certain level of activation, i.e., that carry important information, we include an additional filtering step for the local feature vectors prior to both the estimation of the GMM parameters and the computation of the FVE. In fact, we only use local features with an L 2 -norm greater than the mean L 2 -norm of all local features obtained from the same image. During our experiments, we found that this filtering leads to more stable and balanced estimates for the GMM parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We have evaluated our method on widely used datasets for fine-grained bird species categorization since it is the most challenging domain in our opinion. This can also be observed from current state-of-the-art results. In the case of other fine-grained domains like aircraft, cars, or flowers, the methods already achieve accuracies above 95 %. For bird species classification, results are slightly below 90 % except for one approach that exceeds this value on one specific dataset. We believe that distinguishing different bird species is most challenging because the algorithms have to focus on really small details. In the following, we give a short description of the datasets used in our experiments. <ref type="bibr" target="#b34">[35]</ref> is the most popular fine-grained dataset for benchmarking. It consists of 5,994 training and 5,794 test images. Besides the class labels for 200 bird species, the dataset provides additional annotations. These are bounding boxes, part annotations, and attribute labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB-200-2011</head><p>NA-Birds <ref type="bibr" target="#b32">[33]</ref> is similar to the previous dataset, but much more challenging due to its 555 classes. Although it contains more images in total, namely 23,929 for training and 24,633 for testing, this dataset is more imbalanced than CUB-200-2011. NA-Birds also provides bounding boxes and part annotations, but instead of attribute annotations, a class hierarchy is available.</p><p>Birdsnap <ref type="bibr">[2]</ref> is the least common dataset we use in our experiments. Similar to NA-Birds, it contains 500 bird species and also provides bounding boxes and part annotations. A special property of Birdsnap is that only download links are provided and unfortunately, not all images are available under these links. From 49,829 links in total, we were able to obtain 40,871 images. Hence, we have worked with 38,959 training (out of 47,386) and 1,912 test images (out of 2,443).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>As a backbone of the presented method, we take the InceptionV3 CNN architecture <ref type="bibr" target="#b29">[30]</ref>. We use the pre-trained weights proposed by Cui et al. <ref type="bibr" target="#b5">[6]</ref>. They have pre-trained the network on the iNaturalist 2017 dataset <ref type="bibr" target="#b33">[34]</ref> and could show that this is more beneficial for animal datasets than pre-training on ImageNet.</p><p>For a fair comparison, we use fixed hyperparameters for every experiment. We train each model for 60 epochs with an RMSProp optimizer. In the beginning, the learning rate equals 10 −4 and it is decreased by a factor of 0.1 after 20 and 40 epochs. Due to limitations of the GPU memory, we apply the mini-batch training technique with a mini-batch size of 64. If the effective batch size for computing the gradients is smaller (e.g., due to the number of extracted parts per image), the weights are updated as soon as the mini-batch size is exceeded. Furthermore, we repeat each experiment at least 5 times in order to observe the significance and robustness of the presented approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Order and Visibility of Parts</head><p>First, we evaluate how well our approach is able to cope with the issues caused by unsupervised part detectors. These issues are (i) parts that are not visible and (ii) missing semantic meaning of the detected parts. In order to have a fully controllable environment and to be independent of any particular part detection algorithm, we use the ground-truth part annotations provided with the CUB-200-2011 dataset in this experiment. We compare the typical feature aggregation strategy consisting of global average pooling (GAP) followed by a concatenation of part features with our proposed FVE and also include a baseline CNN trained on the global image only for comparison. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>In the first two settings that use part features, we have shuffled the given order of ground-truth parts. Here, the missing parts are complemented with zero filling strategy like in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39]</ref>. We observe from the results that our proposed FVE  achieves better results compared to GAP. In a further setup, we have evaluated our FVE using only the visible parts and achieve comparable results to the previous setting. The last two lines in <ref type="table" target="#tab_0">Table 1</ref> show results for the two strategies when the order of the parts is kept to have the semantic meaning assigned to each part (applying zero filling strategy for missing parts). As expected, GAP benefits from an order of parts but our proposed FVE obtains similar results in all settings highlighting its robustness against unordered and missing parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Joint Learning vs. Conventional Pipeline</head><p>We now compare our joint learning approach for the FVE to a conventional pipeline that is for example used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref> and estimates the GMM parameters separately from the CNN fine-tuning. This conventional pipeline consists of the following steps: (1) fine-tune a CNN on the target dataset, (2) extract part features with the fine-tuned CNN, (3) train a GMM on the extracted part features, (4) use the GMM parameters for the FVE of the part features, and finally (5) train a linear SVM on the resulting representation. In steps (3) and (4), we perform the pre-and post-processing steps proposed by Perronnin et al. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, namely PCA reduction of the features and normalization of the Fisher vector.</p><p>Again, we use ground-truth part annotations of the CUB-200-2011 dataset and also evaluate the effect of the number of mixture components in both settings. Since this leads to an increasing number of experiments with increasing computation times, we have reduced the number of ground-truth parts from 15 to 4 via a grouping of parts as proposed by Korsch et al. <ref type="bibr" target="#b16">[17]</ref>, who have shown that this grouping can even improve the performance.</p><p>The results for a varying number of components between 1 and 50 are visualized in <ref type="figure" target="#fig_2">Fig. 3</ref>. We observe that our joint approach is more stable for a varying number of components used in the mixture model and clearly achieves higher accuracies in all setups. One reason for this is the adaptation of the CNN features to the FVE and vice versa enabled through the joint learning. In the conventional pipeline, only the GMM and the resulting FVE is adapted to the CNN features but not the CNN features to the encoding. Hence, our proposed end-to-end learning of the FVE leads to improved part representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with State-of-the-art</head><p>In our last experiment, we test our proposed FVE-Layer together with an unsupervised part detector that provides classification-specific parts (CS-Parts) <ref type="bibr" target="#b16">[17]</ref>. However, in contrast to Korsch et al. <ref type="bibr" target="#b16">[17]</ref>, we use a separate CNN for the calculation of part features. This part-CNN is fine-tuned on the detected parts and also adapts the extracted features according to the FVE. Besides the part-CNN, we also extract features from the global image with another CNN and both networks output a prediction. For the part-CNN, the prediction is performed based on the FVE of the part features, whereas the prediction on the global image is made based on standard CNN features. Both predictions are then weighted equally and summed up to the final prediction that is reported in the last row of <ref type="table">Table 2</ref>. Additionally, we compare this setting with results of predictions based on concatenated part features and GAP shown in the penultimate row, and with a baseline using only the predictions obtained from the global image.</p><p>Compared to all reported results, our method performs best on all three finegrained datasets defining new state-of-the-art results. The largest improvement can be seen on the Birdsnap dataset (∼ 2.7 % points). In addition, also the results on NA-Birds look promising, since we present the first approach reaching an accuracy greater than 90 % on this challenging dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Runtime Evaluation</head><p>Finally, we want to show how much computational overhead our proposed FVE-Layer introduces. Therefore, we have trained the model with the FVE-Layer on CUB-200-2011 for 5 epochs on a single GPU workstation (Intel i7-4770 and NVIDIA GTX1080 Ti). Furthermore, we utilized the runtime profiler of python and observed the cumulative runtime of (1) overall model execution, (2) GMM parameter update, and (3) FVE with the GMM parameters. The results in <ref type="table">Table 3</ref> show that the new layer introduces only a marginal overhead to the overall runtime for the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have proposed a new FVE-Layer for aggregating part features of a CNN in the context of fine-grained categorization. With this layer, we can compute a unified representation of fixed length for a varying number of local part features. Thus, a deep learning architecture equipped with this layer can cope with missing parts as well as with an arbitrary order of part features, e.g., given by an unsupervised part detector. Furthermore, our proposed layer can be trained end-to-end together with other layers requiring only minimal computational overhead. In our experiments, we have shown that the proposed method outperforms the conventional pipelines due to the joint learning of both part features and their encoding. Furthermore, the FVE-Layer is more robust compared to typical CNN architectures if the correct semantic meaning of detected parts is not present, as it is the case for unsupervised part detection. Abstract. This document contains supplementary material for the paper End-to-end Learning of a Fisher Vector Encoding for Part Features in Fine-grained Recognition (Paper ID 6720) submitted to ECCV 2020. In Sect. S1, we provide detailed derivations for the partial derivatives of our new FVE-Layer because we have only included the resulting formulas in the paper. Some information for using the source code that we will publish on GitHub can be found in Sect. S2. Finally, a short description of a video for visualizing the estimation of a GMM with our proposed iterative EM-algorithm is given in Sect. S3. The video is provided together with this document as supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1 Partial Derivatives for the New FVE-Layer</head><p>Equations <ref type="bibr" target="#b14">(15)</ref> to <ref type="bibr" target="#b16">(17)</ref> in the paper show the gradients of our proposed Fisher vector encoding layer (FVE-Layer) w.r.t. the inputs x, more precisely w.r.t. an arbitrary feature dimension d * of an arbitrary input sample x n . In the following, we derive these equations by applying standard rules of differentiation for calculating the partial derivatives of the FVE. We consider each component of the FVE, namely each dimension d of the encoding by distinguishing components of the FVE that are related to the means µ k of the k-th mixture component and related to the corresponding standard deviations σ k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.1 Partial Derivatives for Fisher Vector Elements belonging to the Means µ k</head><p>First, we derive the partial derivatives for every dimension d of the mean vector µ k from an arbitrary mixture component k, i.e., we want to compute ∂Fµ k,d (X I ) ∂x n,d * . From Eq. (13) of the paper, we have:</p><formula xml:id="formula_18">F µ k,d (X I ) = 1 √ N I α k N I n=1 w n,k x n,d − µ k,d σ k,d ,<label>(S1)</label></formula><p>with X I = {x 1 , . . . , x N I } being the N I part features of image I. For the partial derivative w.r.t. a single feature value x n,d * , we only need to consider one summand in the sum of Eq. (S1), i.e., the one that corresponds to the feature index n. The remaining parts of the sum are independent of x n and therefore vanish.</p><p>In addition, note that</p><formula xml:id="formula_19">1 √ N I α k</formula><p>is a constant factor. Thus, we have:</p><formula xml:id="formula_20">∂F µ k,d (X I ) ∂x n,d * = 1 √ N I α k ∂ w n,k x n,d −µ k,d σ k,d ∂x n,d * .<label>(S2)</label></formula><p>Now, we distinguish two cases: (i) d = d * and (ii) d = d * . In the first case, the ratio</p><formula xml:id="formula_21">x n,d −µ k,d σ k,d</formula><p>is a constant factor because it is independent of x n,d * (due to different dimensions d = d * ) and we only need to compute ∂w n,k ∂x n,d * , which we derive later in Sect. S1.3. Thus, the derivative in the first case becomes:</p><formula xml:id="formula_22">∀ d = d * : ∂F µ k,d (X I ) ∂x n,d * = 1 √ N I α k ∂w n,k ∂x n,d * x n,d − µ k,d σ k,d .<label>(S3)</label></formula><p>In the second case (d = d * ), we apply the product rule:</p><formula xml:id="formula_23">∂f (x, y)g(x, y) ∂x = ∂f (x, y) ∂x g(x, y) + f (x, y) ∂g(x, y) ∂x .<label>(S4)</label></formula><p>to Eq. (S2) and arrive at:</p><formula xml:id="formula_24">∂F µ k,d * (X I ) ∂x n,d * = 1 √ N I α k   ∂w n,k ∂x n,d * x n,d * − µ k,d * σ k,d * + w n,k ∂ x n,d * −µ k,d * σ k,d * ∂x n,d *   = 1 √ N I α k ∂w n,k ∂x n,d * x n,d * − µ k,d * σ k,d * + w n,k σ k,d * .<label>(S5)</label></formula><p>We can now merge Equations (S3) and (S5) to provide a general formula for the gradients in Eq. (S2) by using the Kronecker delta:</p><formula xml:id="formula_25">δ d,d * = 1 if d = d * 0 else (S6)</formula><p>and obtain Eq. (15) of the paper:</p><formula xml:id="formula_26">∂F µ k,d (X I ) ∂x n,d * = 1 √ N I α k ∂w n,k ∂x n,d * x n,d − µ k,d σ k,d + δ d,d * w n,k σ k,d * .<label>(S7)</label></formula><p>Note again that ∂w n,k ∂x n,d * is derived in Sect. S1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.2 Partial Derivatives for Fisher Vector Elements belonging to the Standard Deviations σ k</head><p>Similar calculations can now be performed for deriving the partial derivatives for all standard deviations σ k,d , i.e., all dimensions d of the vector σ k that corresponds to an arbitrary mixture component k. These partial derivatives are denoted by ∂Fσ k,d (X I ) ∂x n,d * . From Eq. (13) of the paper, we have:</p><formula xml:id="formula_27">F σ k,d (X I ) = 1 √ 2N I α k N I n=1 w n,k (x n,d − µ k,d ) 2 (σ k,d ) 2 − 1 .<label>(S8)</label></formula><p>As shown in the previous section by Equations (S1) and (S2), we only need to consider one summand in the sum of Eq. S8 because the remaining parts of the sum are independent of x n and therefore vanish. We again notice that the first term</p><formula xml:id="formula_28">1 √ 2N I α k</formula><p>is a constant factor and obtain:</p><formula xml:id="formula_29">∂F σ k,d (X I ) ∂x n,d * = 1 √ 2N I α k ∂ w n,k (x n,d −µ k,d ) 2 (σ k,d ) 2 − 1 ∂x n,d * .<label>(S9)</label></formula><p>It makes sense to distinguish the same two cases as in the previous section: (i) d = d * and (ii) d = d * . In the first case, the term</p><formula xml:id="formula_30">(x n,d −µ k,d ) 2 (σ k,d ) 2 − 1 is a constant</formula><p>factor because it is independent of x n,d * (due to different dimensions d = d * ) and we only need to compute ∂w n,k ∂x n,d * , which we derive later in Sect. S1.3. Thus, the derivative in the first case becomes:</p><formula xml:id="formula_31">∀ d = d * : ∂F σ k,d (X I ) ∂x n,d * = 1 √ 2N I α k ∂w n,k ∂x n,d * (x n,d − µ k,d ) 2 (σ k,d ) 2 − 1 . (S10)</formula><p>In the second case (d = d * ), we apply the product rule from Eq. (S4) again and arrive at:</p><formula xml:id="formula_32">∂F σ k,d * (X I ) ∂x n,d * = 1 √ 2N I α k   ∂w n,k ∂x n,d * (x n,d * − µ k,d * ) 2 (σ k,d * ) 2 − 1 +w n,k ∂ (x n,d * −µ k,d * ) 2 (σ k,d * ) 2 − 1 ∂x n,d *   = 1 √ 2N I α k ∂w n,k ∂x n,d * (x n,d * − µ k,d * ) 2 (σ k,d * ) 2 − 1 + 2w n,k (x n,d * − µ k,d * ) (σ k,d * ) 2 .<label>(S11)</label></formula><p>By merging Equations (S10) and (S11) as well as using the Kronecker delta from Eq. (S6), we obtain Eq. (16) of the paper:</p><formula xml:id="formula_33">∂F σ k,d (X I ) ∂x n,d * = 1 √ 2N I α k ∂w n,k ∂x n,d * (x n,d − µ k,d ) 2 (σ k,d ) 2 − 1 +δ d,d * 2w n,k (x n,d * − µ k,d * ) (σ k,d * ) 2 .</formula><p>(S12)</p><p>In the following section, we consider the missing partial derivative ∂w n,k ∂x n,d * , which we have used in the previous formulas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.3 Partial Derivatives for the Soft Assignments w n,k</head><p>We first recap the formula of w n,k for the soft assignment of the n-th sample to the k-th mixture component:</p><formula xml:id="formula_34">w n,k = α k p k (x n |θ k ) K =1 α p (x n |θ )</formula><p>.</p><p>(S13)</p><p>To compute the partial derivative ∂w n,k ∂x n,d * , we apply the quotient rule:</p><formula xml:id="formula_35">∂ f (x,y) g(x,y) ∂x = ∂f (x,y) ∂x g(x, y) − f (x, y) ∂g(x,y) ∂x g(x, y) 2 (S14)</formula><p>to Eq. (S13), which yields:</p><formula xml:id="formula_36">∂w n,k ∂x n,d * = ∂α k p k (xn|θ k ) ∂x n,d * K =1 α p (x n |θ ) − α k p k (x n |θ k ) ∂ K =1 α p (xn|θ ) ∂x n,d * K =1 α p (x n |θ ) 2 (S15)</formula><p>We split this equation into two parts, L and R, as follows:</p><formula xml:id="formula_37">∂w n,k ∂x n,d * = L − R ,<label>(S16)</label></formula><formula xml:id="formula_38">L = ∂α k p k (xn|θ k ) ∂x n,d * K =1 α p (x n |θ ) K =1 α p (x n |θ ) 2 = ∂α k p k (xn|θ k ) ∂x n,d * K =1 α p (x n |θ ) ,<label>(S17)</label></formula><formula xml:id="formula_39">R = α k p k (x n |θ k ) ∂ K =1 α p (xn|θ ) ∂x n,d * K =1 α p (x n |θ ) 2 = α k p k (x n |θ k ) ∂ K =1 α p (xn|θ ) ∂x n,d * K =1 α p (x n |θ ) K =1 α p (x n |θ ) = w n,k   ∂ K =1 α p (xn|θ ) ∂x n,d * K =1 α p (x n |θ )   .</formula><p>(S18)</p><p>Hence, we need to determine two partial derivatives, ∂α k p k (xn|θ k ) ∂x n,d * in L as well as</p><formula xml:id="formula_40">∂ K =1 α p (xn|θ ) ∂x n,d * in R.</formula><p>Assuming Gaussian distributions that are parameterized by θ k = (µ k , σ k ) with diagonal covariance matrices for all mixture components, the density function p k (x n |θ k ) of the k-th component is given by:</p><formula xml:id="formula_41">p k (x n |θ k ) = p k (x n |µ k , σ k ) = 1 D d=1 √ 2πσ k,d exp − D d=1 (x n,d − µ k,d ) 2 2(σ k,d ) 2 .</formula><p>(S19)</p><p>For the partial derivative in the numerator of L (Eq. (S17)), we apply the chain rule: ∂f (g(x, y)) ∂x = ∂f (g(x, y)) ∂g(x, y)</p><p>∂g(x, y) ∂x (S20) and obtain: ∂α k p k (x n |θ k ) ∂x n,d * = α k p k (x n |θ k ) − (x n,d * − µ k,d * ) (σ k,d * ) 2 .</p><p>Thus, L becomes:</p><formula xml:id="formula_43">L = α k p k (x n |θ k ) − (x n,d * −µ k,d * ) (σ k,d * ) 2 K =1 α p (x n |θ ) = w n,k − (x n,d * − µ k,d * ) (σ k,d * ) 2 .<label>(S22)</label></formula><p>For the partial derivative in the numerator of R (Eq. (S18)), we also apply the chain rule from Eq. (S20) and obtain: </p><p>This result is provided in Eq. (17) of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Source Code Details</head><p>We published the code on GitHub 1 . The training procedure is implemented in python using the chainer <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3]</ref> deep learning framework. To reproduce the results, please follow the instructions in the README.md. First, you will need to download the datasets and the weights of the pre-trained model. Next, create an environment using anaconda and install the required packages. Finally, you should be able to run the train.sh script with the corresponding parameters for each experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3 Visualizations of the Iterative EM Algorithm</head><p>To visualize the iterative estimation procedure, we provide a video showing the training of a GMM-Layer. This is a special case of our FVE-Layer and uses the same learning algorithm (our iterative EM-algorithm with mini-batch updates). The only difference is the encoding, which is the identity function for the GMM-Layer in contrast to the FVE for our new FVE-Layer. We generated random 2D data for 10 classes such that each class consists of 400 random samples. These samples are drawn from 10 Gaussian distributions, one for every class. We have set the standard deviations for both dimensions to 0.1 for all distributions. The means are selected in a way such that the class centers µ k with k ∈ {0, . . . , 9} are arranged on the unit circle:</p><formula xml:id="formula_45">µ k = cos kπ 5 , sin kπ 5 T .<label>(S26)</label></formula><p>To increase the difficulty for the iterative EM algorithm, we also doubled the standard deviations such that the class distributions clearly overlap. In each iteration, we have selected a random subset of the data, i.e., a batch of 128 samples for the simple case and 512 samples for the difficult case, and performed a parameter update as presented in the paper. The video shows the estimated clusters after each iteration of our iterative EM-algorithm. For comparison, we also visualize a GMM that has been estimated on the entire dataset. It can be observed that the clusters determined by our iterative EM algorithm match quite well with those of the reference GMM. Interestingly, the estimates converge in less than 50 iterations. Other configurations (number of classes, number of samples, etc.) can be tested with the visualize.sh script provided together with our implementation. <ref type="bibr">1</ref> The link will be added after review to preserve the double-blind process</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of our proposed method. During training, we estimate the parameters of the GMM that leads to the FVE using an EM-algorithm with mini-batch updates described in Sect. 3.2. The resulting FVE-Layer, which is explained in Sect. 3.3, can be integrated in any deep network architecture. We use this new layer for computing a unified part representation that aggregates local features extracted from ConvMaps of a CNN as described in Sect. 3.4. Our approach enables joint end-to-end learning of both CNN parameters and GMM parameters for the FVE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Comparison of the conventional pipeline with our proposed joint learning approach described in Sect. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>∂KK 2 K 2 K 2 =</head><label>222</label><figDesc>=1 α p (x n |θ ) ∂x n,d * = K =1 α p (x n |θ ) − (x n,d * − µ ,d * ) (σ ,d * ) 2 . =1 α p (x n |θ ) − (x n,d * −µ ,d * ) (σ ,d * ) =1 α p (x n |θ ) x n |θ ) − (x n,d * −µ ,d * ) (σ ,d * ) =1 α p (x n |θ ) = w n,k K =1 w n, − (x n,d * − µ ,d * ) (σ ,d * ) 2 .(S24)Inserting L from Eq. (S22) and R from Eq. (S24) into Eq. (S16), we obtain:∂w n,k ∂x n,d * = w n,k − (x n,d * − µ k,d * ) (σ k,d * ) 2 − w n,k K =1 w n, − (x n,d * − µ ,d * ) (σ ,d * ) w n,k − (x n,d * − µ k,d * ) (σ k,d * ) 2 + K =1w n, (x n,d * − µ ,d * ) (σ ,d * ) 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of our proposed FVE with global average pooling (GAP) regarding order and visibility of parts.</figDesc><table><row><cell cols="2">Method</cell><cell>Setup</cell><cell></cell><cell></cell><cell cols="2">Accuracy (std)</cell></row><row><cell cols="2">GAP</cell><cell cols="2">baseline with no parts</cell><cell></cell><cell cols="2">89.54 (±0.15)</cell></row><row><cell cols="2">GAP</cell><cell cols="2">shuffled GT parts</cell><cell></cell><cell cols="2">90.01 (±0.18)</cell></row><row><cell cols="2">FVE (ours)</cell><cell cols="2">shuffled GT parts</cell><cell></cell><cell cols="2">90.56 (±0.15)</cell></row><row><cell cols="2">FVE (ours)</cell><cell cols="2">only visible GT parts</cell><cell></cell><cell cols="2">90.51 (±0.21)</cell></row><row><cell cols="2">GAP</cell><cell cols="2">ordered GT parts</cell><cell></cell><cell cols="2">90.53 (±0.30)</cell></row><row><cell cols="2">FVE (ours)</cell><cell cols="2">ordered GT parts</cell><cell></cell><cell cols="2">90.49 (±0.18)</cell></row><row><cell></cell><cell>94</cell><cell cols="4">joint learning of GMM parameters and CNN weights</cell></row><row><cell>Accuracy [%]</cell><cell>90 92</cell><cell cols="5">separate estimation of GMM parameters after CNN training</cell></row><row><cell></cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell cols="4">Number of mixture components (log axis)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison of our proposed FVE for part features with various state-of-theart methods (bold = best per dataset). Computation times for our proposed FVE-Layer.</figDesc><table><row><cell>Method</cell><cell>CUB-200-2011</cell><cell>NA-Birds</cell><cell>Birdsnap</cell></row><row><cell>Cui et al. [6]</cell><cell>89.6</cell><cell>87.9</cell><cell>-</cell></row><row><cell>Stacked LSTM [8]</cell><cell>90.4</cell><cell>-</cell><cell>-</cell></row><row><cell>FixSENet-154 [32]</cell><cell>88.7</cell><cell>89.2</cell><cell>84.3</cell></row><row><cell>CS-Parts [17]</cell><cell>89.5</cell><cell>88.5</cell><cell>-</cell></row><row><cell>MGE-CNN [38]</cell><cell>89.4</cell><cell>88.6</cell><cell>-</cell></row><row><cell>WS-DAN [13]</cell><cell>89.3</cell><cell>-</cell><cell>-</cell></row><row><cell>PAIRS [10]</cell><cell>89.2</cell><cell>87.9</cell><cell>-</cell></row><row><cell>No Parts (baseline)</cell><cell>89.54 (±0.15)</cell><cell>87.57 (±0.07)</cell><cell>83.67 (±0.11)</cell></row><row><cell>GAP (with CS-Parts)</cell><cell>90.54 (±0.15)</cell><cell>89.92 (±0.05)</cell><cell>86.14 (±0.39)</cell></row><row><cell>FVE (ours, with CS-Parts)</cell><cell>90.95 (±0.05)</cell><cell>90.30 (±0.08)</cell><cell>87.02 (±0.38)</cell></row><row><cell>Algorithm</cell><cell cols="2">Runtime per iteration</cell><cell>Relative time</cell></row><row><cell>Overall model execution</cell><cell cols="2">270.0 ms</cell><cell>100.00 %</cell></row><row><cell cols="2">Updating the GMM parameters</cell><cell>3.0 ms</cell><cell>1.11 %</cell></row><row><cell>Computing the FVE</cell><cell></cell><cell>9.5 ms</cell><cell>3.52 %</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/DiKorsch/l1_parts</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequential em for unsupervised adaptive gaussian mixture model based classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awwad</forename><surname>Shiekh Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Data Mining in Pattern Recognition</title>
		<editor>Perner, P.</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="96" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On-line expectation-maximization algorithm for latent data models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Cappe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="593" to="613" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic expectation maximization with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7967" to="7977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for finegrained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonparametric part transfer for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Göring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Freytag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2489" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aligned to the object, not to the image: A unified posealigned representation for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1876" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised adaptive gmm for bci</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE/EMBS Conference on Neural Engineering</title>
		<imprint>
			<date type="published" when="2009-04" />
			<biblScope unit="page" from="295" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Which and how many regions to gaze: Focus discriminative regions for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09891</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the global convergence of (fast) incremental expectation maximization methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Moulines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lavielle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2833" to="2843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classification-specific parts for improving fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bodesheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the German Conference on Pattern Recognition</title>
		<meeting>the German Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="62" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Online em for unsupervised models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of human language technologies: The 2009 annual conference of the North American chapter of the association for computational linguistics</title>
		<meeting>human language technologies: The 2009 annual conference of the North American chapter of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bird part localization using exemplar-based models with enforced pose and subcategory consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2520" to="2527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A view of the em algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6314</biblScope>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalized orderless pooling performs implicit salient matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4970" to="4979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The whole is more than its parts? from explicit to implicit pose normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep fisher networks for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Locallytransferred fisher vectors for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>O&amp;apos;donnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4912" to="4920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep fisher kernels-end to end learning of the fisher kernel gmm parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sydorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1402" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep fishernet for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2244" to="2250" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8250" to="8260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lensch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02549</idno>
		<title level="m">Backpropagation training for fisher vectors within neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised part mining for finegrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09941</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a mixture of granularity-specific experts for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8331" to="8340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning deep bilinear transformation for fine-grained image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4279" to="4288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Chainermn: Scalable distributed deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems in the Conference on Neural Information Processing Systems</title>
		<meeting>Workshop on Machine Learning Systems in the Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Chainer: A deep learning framework for accelerating the research cycle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Okuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Uenishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems in the Conference on Neural Information Processing Systems</title>
		<meeting>Workshop on Machine Learning Systems in the Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
