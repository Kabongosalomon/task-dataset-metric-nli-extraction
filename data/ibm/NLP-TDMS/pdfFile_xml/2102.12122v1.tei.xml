<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions Conv 1 Conv 2 TASK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<email>wangwenhai362@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">IIAI 5 SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
							<email>lutong@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">IIAI 5 SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions Conv 1 Conv 2 TASK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>network without convolutions. We investigate an alternative model beyond CNN for the tasks of dense predictions such as object detection, semantic and instance segmentation, other than image classification.</p><p>Inspired by the success of Transformer <ref type="bibr" target="#b54">[51]</ref> in natural language processing (NLP), many researchers are trying to explore applications of Transformer in computer vision. For example, some works <ref type="bibr" target="#b7">[4,</ref><ref type="bibr" target="#b68">64,</ref><ref type="bibr" target="#b58">55,</ref><ref type="bibr" target="#b46">43,</ref><ref type="bibr" target="#b20">17]</ref> model the vision task as a dictionary lookup problem with learnable queries, and use the Transformer decoder as a task-specific head on the top of the CNN backbone, such as VGG <ref type="bibr" target="#b44">[41]</ref> and ResNet <ref type="bibr" target="#b18">[15]</ref>. While some prior arts have incorporated the attention modules [53, <ref type="bibr" target="#b38">35,</ref><ref type="bibr" target="#b64">61]</ref> into CNNs, as far as we know, exploring a clean and convolution-free Transformer backbone to address dense prediction tasks in computer vision is rarely studied.</p><p>Recently, Dosovitskiy et al. <ref type="bibr" target="#b13">[10]</ref> employs Transformer for image classification. This is an interesting and meaningful attempt to replace the CNN backbone by a convolutionfree model. As shown in <ref type="figure">Figure 1</ref> (b), ViT has a columnar structure with coarse image patches (i.e., dividing image with a large patch size) as input <ref type="bibr" target="#b4">1</ref> . Although ViT is applicable to image classification, it is challenging to be directly adapted to pixel-level dense predictions, e.g., object detection and segmentation, because (1) its output feature map has only a single scale with low resolution and (2) its computations and memory cost are relatively high even for common input image size (e.g., shorter edge of 800 pixels in COCO detection benchmark).</p><p>To compensate the above limitations, this work proposes a convolution-free backbone network using Transformer model, termed Pyramid Vision Transformer (PVT), which can serve as a versatile backbone in many downstream tasks, including image-level prediction as well as pixel-level dense predictions. Specifically, as illustrated in <ref type="figure">Figure 1</ref> (c), different from ViT, PVT overcomes the difficulties of conventional Transformer by (1) taking finegrained image patches (i.e., 4 × 4 per patch) as input to learn high-resolution representation, which is essential for dense prediction tasks, (2) introducing a progressive shrinking pyramid to reduce the sequence length of Transformer when the depth of network is increased, significantly reducing the computational consumption, and (3) adopting a spatial-reduction attention (SRA) layer to further reduce the resource cost to learn high-resolution feature maps.</p><p>Overall, the proposed PVT possesses the following merits. Firstly, compared to the conventional CNN backbones (see <ref type="figure">Figure 1</ref> (a)) where the receptive field increases when the depth increases, PVT always produces a global recep- <ref type="bibr" target="#b4">1</ref> Due to resource constraints, ViT cannot use fine-grained image patches (e.g., 4 × 4 per patch) as input, and can only receive coarse patches (e.g., 32 × 32 per patch) as input, which leads to its low output resolution (e.g., 32-stride).   <ref type="bibr" target="#b18">[15]</ref> 56.7 38.5 X101-32x4d <ref type="bibr" target="#b59">[56]</ref> 56.4 39.9 ViT-S/32 <ref type="bibr" target="#b13">[10]</ref> 60.8 31.7 PVT-M (ours) 53.9 41.9 X101-64x4d <ref type="bibr" target="#b59">[56]</ref> 95.5 41.0 PVT-L (ours) 71.1 42.6 <ref type="figure">Figure 2</ref>: Performance comparison on COCO val2017 of different backbones using RetinaNet for object detection. We see that when the number of parameters among different models are comparable, the PVT variants significantly outperform their corresponding counterparts in COCO such as ResNets (R) <ref type="bibr" target="#b18">[15]</ref>, ViT <ref type="bibr" target="#b13">[10]</ref> and ResNeXts (X) <ref type="bibr" target="#b59">[56]</ref>. For instance, with comparable numbers of parameters, PVT-S outperforms R50 by 4.1 AP (40.4 vs. 36.3), where "T", "S", "M" and "L" denote our PVT models with tiny, small, medium and large size. We also see that the original ViT models could be limited in object detection.</p><p>tive field (by attentions among all small patches), which is more suitable for detection and segmentation than CNNs' local receptive field. Secondly, compared to ViT (see <ref type="figure">Figure 1 (b)</ref>), due to the advance of the pyramid structure, our method is easier to be plugged into many representative dense prediction pipelines, e.g., RetinaNet <ref type="bibr" target="#b30">[27]</ref> and Mask R-CNN. Thirdly, with PVT, we can build a convolutionfree pipeline by combining PVT with other Transformer decoders designed for different tasks, such as PVT+DETR <ref type="bibr" target="#b7">[4]</ref> for object detection. For example, to the best of our knowledge, our experiments present the first end-to-end object detection pipeline, PVT+DETR, which is entirely convolution-free. It achieves 34.7 on the COCO val2017, outperforming the original DETR based on ResNet50.</p><p>The main contributions of this work are listed below.</p><p>• We propose Pyramid Vision Transformer (PVT), which is the first backbone designed for various pixellevel dense prediction tasks without convolutions. Combining PVT and DETR, we can build an end-toend object detection system without convolutions and hand-crafted components such as dense anchors and non-maximum suppression (NMS). • We overcome many difficulties when porting Transformer to dense pixel-level predictions, by designing progressive shrinking pyramid and spatial-reduction attention (SRA), which are able to reduce the resource consumption of using Transformer, making PVT flexible to learn multi-scale and high-resolution feature maps. • We verify PVT by applying it to many different tasks, e.g., image classification, object detection, and semantic segmentation, and compare it with the welldesigned ResNets <ref type="bibr" target="#b18">[15]</ref> and ResNeXts <ref type="bibr" target="#b59">[56]</ref>. As shown in <ref type="figure">Figure 2</ref>, we see that PVT with different numbers of parameters can consistently improve performance compared to prior arts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolutional Backbones in Computer Vision</head><p>Convolutional block is the work-horse of deep neural networks in visual recognition. The standard and fundamental convolutional block was first introduced in [23] to distinguish hand-writing numbers. The block contains convolutional kernels with a certain receptive field that captures favorable visual context. To introduce the translation equivariance, the weights of convolutional kernels are shared over the entire image space. With the rapid development of the computational resources (e.g., GPUs), the successful training for a stack of a few convolutional blocks <ref type="bibr" target="#b25">[22,</ref><ref type="bibr" target="#b44">41]</ref> on large-scale image classification dataset (e.g., ImageNet <ref type="bibr" target="#b41">[38]</ref>) becomes possible. GoogLeNet <ref type="bibr" target="#b49">[46]</ref> demonstrated that a convolutional operator containing multiple kernel paths can achieve very competitive performance. The effectiveness of multi-path convolutional block was further validated in Inception series <ref type="bibr" target="#b50">[47,</ref><ref type="bibr" target="#b48">45]</ref>, ResNeXt <ref type="bibr" target="#b59">[56]</ref>, DPN <ref type="bibr" target="#b11">[8]</ref>, MixNet <ref type="bibr" target="#b55">[52]</ref> and SKNet <ref type="bibr" target="#b27">[24]</ref>. Further, ResNet <ref type="bibr" target="#b18">[15]</ref> proposed skip connections in convolutional blocks, which makes very deep networks possible and has impressive effects in the field of computer vision. DenseNet <ref type="bibr" target="#b21">[18]</ref> introduced densely connected topology, which connects each convolutional block to its previous blocks. More recent advances can be found in recent survey/review papers <ref type="bibr" target="#b23">[20,</ref><ref type="bibr" target="#b43">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dense Prediction Tasks</head><p>Preliminary. The dense prediction task aims to perform pixel-level classification or regression on the feature map. Object detection and semantic segmentation are two representative dense prediction tasks.</p><p>Object Detection. In the deep learning era, CNN <ref type="bibr" target="#b26">[23]</ref> has become the dominant framework for object detection, which includes single-stage detectors (e.g., SSD <ref type="bibr" target="#b33">[30]</ref>, Reti-naNet <ref type="bibr" target="#b30">[27]</ref>, FCOS <ref type="bibr" target="#b52">[49]</ref>, GFL <ref type="bibr" target="#b28">[25]</ref>, PolarMask <ref type="bibr" target="#b57">[54]</ref> and OneNet <ref type="bibr" target="#b45">[42]</ref>) and multiple-stage detectors (Faster R-CNN <ref type="bibr" target="#b39">[36]</ref>, Mask R-CNN <ref type="bibr" target="#b17">[14]</ref>, Cascade R-CNN <ref type="bibr" target="#b6">[3]</ref> and Sparse R-CNN <ref type="bibr" target="#b47">[44]</ref>). Most of these popular object detectors are built on high-resolution or multi-scale feature maps, to obtain a good detection performance. Recently, DETR <ref type="bibr" target="#b7">[4]</ref> and deformable DETR <ref type="bibr" target="#b68">[64]</ref> combine the CNN backbone and the Transformer decoder to build an end-to-end object detector. Like the previous CNN-based detectors, they also require high-resolution or multi-scale feature maps for accurate object detection.</p><p>Semantic Segmentation. CNN also plays an important role in semantic segmentation. In the early stage, FCN <ref type="bibr" target="#b34">[31]</ref> introduced fully convolutional architectures to generate a spatial segmentation map for a given image of any size. After that, deconvolution operation was introduced by Noh et al. <ref type="bibr" target="#b37">[34]</ref> and achieved impressive performance on PASCAL VOC 2012 dataset <ref type="bibr" target="#b42">[39]</ref>. Inspired by FCN, U-Net <ref type="bibr" target="#b40">[37]</ref> is proposed for especially the medical image segmentation domain, which bridges the information flow between corresponding low-level and high-level feature maps with the same spatial sizes. To explore richer global context representation, Zhao et al. <ref type="bibr" target="#b66">[62]</ref> designs a pyramid pooling module over various pooling scales, and Kirillov et al. <ref type="bibr" target="#b24">[21]</ref> develop a lightweight segmentation head termed Semantic FPN, based on FPN <ref type="bibr" target="#b29">[26]</ref>. DeepLab family <ref type="bibr" target="#b9">[6,</ref><ref type="bibr" target="#b32">29]</ref> applies the dilated convolution to enlarge the receptive field while keeping the feature map resolution. Similar to object detection methods, semantic segmentation methods also rely on high-resolution or multi-scale feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Self-Attention and Transformer in Vision</head><p>As convolutional filter weights are usually fixed after training, it is inflexible for them to adapt dynamically to input variation. Therefore, many methods have been proposed to alleviate this problem via using self-attention operations. The non-local block [53] attempted to model long-range dependencies in both space and time, which is shown beneficial for accurate video classification. Despite its success, the non-local operator suffers from the high memory and computation cost. Criss-cross <ref type="bibr" target="#b22">[19]</ref> further reduced the complexity by generating sparse attention maps only through the criss-cross path. Ramachandran et al. <ref type="bibr" target="#b38">[35]</ref> proposed stand-alone self-attention was propose to replace convolutional layers with local self-attention units. AANet <ref type="bibr" target="#b5">[2]</ref> achieves competitive results when combining the self-attention and convolutional operations. DETR <ref type="bibr" target="#b7">[4]</ref> utilize the Transformer decoder to model object detection as an end-to-end dictionary lookup problem with learn- </p><formula xml:id="formula_0">Reshape Stage i !"# !"# ! $ × ! !"# ! × !"# ! ×( ! $ !"# ) !"# ! × !"# ! × !</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position Embedding</head><p>Element-wise Add</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Map</head><p>Norm Norm Feed Forward Multi-Head Attention Spacial Reduction SRA <ref type="figure">Figure 3</ref>: Overall architecture of the proposed Pyramid Vision Transformer (PVT). The entire model is divided into four stages, and each stage is comprised of a patch embedding layer, and a L i -layer Transformer encoder. Following the pyramid structure, the output resolution of the four stages progressively shrinks from 4-stride to 32-stride. able queries, successfully removing the hand-crafted process such as Non-Maximal Suppression (NMS). Based on DETR, deformable DETR <ref type="bibr" target="#b68">[64]</ref> further introduces a deformable attention layer to focus on a sparse set of contextual elements which obtains fast convergence and better performance. Recently, Vision Transformer (ViT) <ref type="bibr" target="#b13">[10]</ref> employs a pure Transformer <ref type="bibr" target="#b54">[51]</ref> model to make image classification by treating an image as a sequence of patches. DeiT <ref type="bibr" target="#b53">[50]</ref> further extends ViT by using a novel distillation approach. Different from previous methods, this work try to introduce the pyramid structure into Transformer and design a pure Transformer backbone for dense prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pyramid Vision Transformer (PVT)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>Our goal is to introduce the pyramid structure into Transformer, so that it can generate multi-scale feature maps for dense prediction tasks (e.g., object detection and semantic segmentation). An overview of PVT is depicted in <ref type="figure">Figure 3</ref>. Similar to CNN backbones <ref type="bibr" target="#b18">[15]</ref>, our method has four stages that generate feature maps of different scales. All stages share a similar architecture, which consists of a patch embedding layer and L i Transformer encoder layers.</p><p>In the first stage, given an input image with the size of H ×W ×3, we first divide it into HW 4 2 patches 2 , and the size 2 Same as ResNet, we keep the highest resolution of our output feature map at 4-stride. of each patch is 4×4×3. Then, we feed the flattened patches to a linear projection and get embedded patches with size of HW 4 2 × C 1 . After that, the embedded patches along with position embedding pass through a Transformer encoder with L 1 layer, and the output is reshaped to a feature map F 1 , and its size is H 4 × W 4 × C 1 . In the same way, using the feature map from the prior stage as input, we obtain the following feature maps F 2 , F 3 , and F 4 , whose strides are 8, 16, and 32 pixels with respect to the input image. With the feature pyramid {F 1 , F 2 , F 3 , F 4 }, our method can be easily applied to most downstream tasks, including image classification, object detection, and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Pyramid for Transformer</head><p>Unlike CNN backbone networks <ref type="bibr" target="#b18">[15]</ref> that use convolution stride to obtain multi-scale feature maps, our PVT use progressive shrinking strategy to control the scale of feature maps by patch embedding layers.</p><p>Here, we denote the patch size of the i-th stage as P i . At the beginning of the stage i, we first evenly divide the input feature map F i−1 ∈ R Hi−1×Wi−1×Ci−1 into Hi−1Wi−1 P 2 i patches, and then each patch is flatten and projected to a C i -dim embedding. After the linear projection, the shape of the embedded patches can be viewed as Hi−1 Pi × Wi−1 Pi × C i , where the height and width are P i times smaller than the input.</p><p>In this way, we can flexibly adjust the scale of the feature map in each stage, making it possible to construct a feature pyramid for Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head Attention</head><p>Multi-Head Attention <ref type="figure">Figure 4</ref>: Multi-head attention (MHA) vs. spatialreduction attention (SRA). With spatial-reduction operation, the computational/memory cost of our SRA could be much lower than that of MHA, so our SRA is more friendly to high-resolution feature maps.</p><formula xml:id="formula_1">Q K V Spatial-Reduction Attention (ours) Multi-Head Attention ! ! ! " × ! Q K V ( ! ! )× !</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Reduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transformer Encoder</head><p>For the Transformer encoder in the stage i, it has L i encoder layers, and each encoder layer is composed of an attention layer and a feed-forward layer <ref type="bibr" target="#b54">[51]</ref>. Since our method needs to process high-resolution (e.g., 4-stride) feature maps, we propose a spatial-reduction attention (SRA) layer to replace the traditional multi-head attention (MHA) layer <ref type="bibr" target="#b54">[51]</ref> in the encoder.</p><p>Similar to MHA, our SRA also receives a query Q, a key K, and a value V as input, and outputs a refined feature. The difference is that our SRA will reduce the spatial scale of K and V before the attention operation as shown in <ref type="figure">Figure 4</ref>, which largely reduces the computation/memory overhead. Details of SRA in the stage i can be formulated as follows:</p><formula xml:id="formula_2">SRA(Q, K, V ) = Concat(head 0 , ..., head Ni )W O , (1) head j = Attention(QW Q j , SR(K)W K j , SR(V )W V j ), (2) where W Q j ∈ R Ci×d k , W K j ∈ R Ci×d head , W V j ∈ R Ci×d head , and W O ∈ R hi×d head are parameters of linear projections.</formula><p>N i is the head number of the Transformer encoder in the stage i. Therefore, the dimension of each head d head is equal to Ci Ni . SR(·) is the spatial-reduction operation, which is defined as:</p><formula xml:id="formula_3">SR(x) = Norm(Reshape(x, R i )W S ).<label>(3)</label></formula><p>Here, R i denotes the reduction ratio of the attention layers in the stage i. Reshape(x, R i ) is the operation of reshaping the input x ∈ R (HiWi)×Ci to the sequence with the size of</p><formula xml:id="formula_4">HiWi R 2 i ×(R 2 i C i ). W S ∈ R (R 2 i Ci)</formula><p>×Ci is a linear projection that reduces the dimension of input sequence to C i . Norm(·) refers to layer normalization <ref type="bibr" target="#b4">[1]</ref>. Same as Transformer <ref type="bibr" target="#b54">[51]</ref>, Attention(·) is the attention operation that is calculated as:</p><formula xml:id="formula_5">Attention(q, k, v) = Softmax( qk T √ d head )v.<label>(4)</label></formula><p>Through these formulas, we can figure out that the computational/memory costs of our Attention(·) operation is R 2 i times lower than that of MHA, and thus it can handle larger input feature maps/sequences with limited resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Details</head><p>In summary, the hyper parameters of our method are listed as follows:</p><p>• P i : the patch size of the stage i; • C i : the channel number of the output of the stage i; • L i : the number of encoder layers in the stage i; • R i : the reduction ratio of the SRA in the stage i; • N i : the head number of the SRA in the stage i; • E i : the expansion ratio of the feed-forward layer <ref type="bibr" target="#b54">[51]</ref> in the stage i;</p><p>Following the design rules of ResNet <ref type="bibr" target="#b18">[15]</ref>, we (1) use small output channel numbers in shallow stages; and (2) concentrate the major computation resource in intermediate stages.</p><p>To provide instances for discussion, we describe a series of PVT models with different scales, namely PVT-Tiny, -Small, -Medium, and -Large, in <ref type="table" target="#tab_4">Table 1</ref>. More details of employing these models in specific downstream tasks will be introduced in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussion</head><p>The most related work to our method is ViT <ref type="bibr" target="#b13">[10]</ref>. Here we discuss in detail the relationship and differences between them.</p><p>Both PVT and ViT are pure Transformer models without convolution operation. The main difference between them is the pyramid structure. Similar to the traditional Transformer <ref type="bibr" target="#b54">[51]</ref>, the length of ViT's output sequence is the same as the input, which means that the output of ViT is singlescale (see <ref type="figure">Figure 1 (b)</ref>). Moreover, due to the limited resource., the output of ViT is coarse-grained (e.g., the patch size is 16 or 32 pixels), and thus its output resolution is relatively low (e.g., <ref type="bibr" target="#b19">16</ref>-stride or 32-stride). As a result, it is difficult to directly apply ViT in dense prediction tasks that require high-resolution or multi-scale feature maps.</p><p>Our PVT breaks the routine of Transformer by introducing a progressive shrinking pyramid. It can generate multi-scale feature maps like a traditional CNN backbone. In addition, we also designed a simple but effective attention layer-SRA, to process high-resolution feature maps and reduce computation/memory costs. Benefiting from the above designs, our method has the following advantages over ViT: 1) more flexible-can generate feature maps of different scales, channels in differ- Transformer Encoder</p><formula xml:id="formula_6">  R 1 = 8 N 1 = 1 E 1 = 8   × 2   R 1 = 8 N 1 = 1 E 1 = 8   × 3   R 1 = 8 N 1 = 1 E 1 = 8   × 3   R 1 = 8 N 1 = 1 E 1 = 8   × 3 Stage 2 H 8 × W 8 Patch Embedding P 2 = 2; C 2 = 128</formula><p>Transformer Encoder</p><formula xml:id="formula_7">  R 2 = 4 N 2 = 2 E 2 = 8   × 2   R 2 = 4 N 2 = 2 E 2 = 8   × 3   R 2 = 4 N 2 = 2 E 2 = 8   × 3   R 2 = 4 N 2 = 2 E 2 = 8   × 8 Stage 3 H 16 × W 16</formula><p>Patch Embedding P 3 = 2; C 3 = 320</p><p>Transformer Encoder</p><formula xml:id="formula_8">  R 3 = 2 N 3 = 5 E 3 = 4   × 2   R 3 = 2 N 3 = 5 E 3 = 4   × 6   R 3 = 2 N 3 = 5 E 3 = 4   × 18   R 3 = 2 N 3 = 5 E 3 = 4   × 27 Stage 4 H 32 × W 32</formula><p>Patch Embedding P 4 = 2; C 4 = 512</p><p>Transformer Encoder  <ref type="formula">1)</ref> with the growth of network depth, the hidden dimension gradually increases, and the output resolution progressively shrinks; (2) the major computation resource is concentrated in Stage 3. ent stages; 2) more versatile-can be easily plugged and played in most downstream task models; 3) more friendly to computation/memory-can process the feature map with higher resolution.</p><formula xml:id="formula_9">  R 4 = 1 N 4 = 8 E 4 = 4   × 2   R 4 = 1 N 4 = 8 E 4 = 4   × 3   R 4 = 1 N 4 = 8 E 4 = 4   × 3   R 4 = 1 N 4 = 8 E 4 = 4   × 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applied to Downstream Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image-Level Prediction</head><p>Image classification is the most representative task of image-level prediction. Following ViT <ref type="bibr" target="#b13">[10]</ref> and DeiT <ref type="bibr" target="#b53">[50]</ref>, we append a learnable classification token to the input of the last stage, and then use a fully connected layer to make classification on the top of the classification token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pixel-Level Dense Prediction</head><p>In addition to image-level prediction, dense prediction that requires performing pixel-level classification or regression on the feature map is also often seen in downstream tasks. Here, we discuss two typical tasks, namely object detection, and semantic segmentation.</p><p>Object Detection. We apply our PVT models to two representative object detection methods, namely Reti-naNet <ref type="bibr" target="#b30">[27]</ref> and Mask R-CNN <ref type="bibr" target="#b17">[14]</ref>. RetinaNet is a widelyused single-stage detector, and Mask R-CNN is one of the mainstream two-stage instance segmentation frameworks. The implementation details are listed as follows: (1) Same as ResNet, we directly use the output feature pyramid {F 1 , F 2 , F 3 , F 4 } as the input of FPN <ref type="bibr" target="#b29">[26]</ref>, and then the refined feature maps are fed to the follow-up detection or in-stance segmentation head. <ref type="bibr" target="#b5">(2)</ref> In object detection, the input can be an arbitrary shape, so the position embeddings pretrained on ImageNet may no longer be meaningful. Therefore, we perform bilinear interpolation on the pre-trained position embeddings according to the input image. (3) During the training of the detection model, all layers in PVT will not be frozen.</p><p>Semantic Segmentation. We choose Semantic FPN <ref type="bibr" target="#b24">[21]</ref> as the baseline, which is a simple segmentation method without special operations (e.g., dilated convolution). Therefore, using it as the baseline can well examine the original effectiveness of backbones. Similar to the implementation in target detection, we feed the feature pyramid directly to the semantic FPN, and use bilinear interpolation to resize the pre-trained position embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We compare our PVT with the two most representative CNN backbones, namely ResNet <ref type="bibr" target="#b18">[15]</ref>, and ResNeXt <ref type="bibr" target="#b59">[56]</ref>, which are widely used in benchmarks of many downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image Classification</head><p>Experiment Settings. We perform image classification experiments on the ImageNet dataset. The ImageNet 2012 dataset <ref type="bibr" target="#b41">[38]</ref> comprises 1.28 million training images and 50K validation images from 1,000 categories. We train models on the training set, and report the top-1 error on the validation set. For a fair comparison, we follow DeiT <ref type="bibr">[</ref>  <ref type="table" target="#tab_6">Table 2</ref>: Image classification performance on the Ima-geNet validation set. "Top-1" denotes the top-1 error rate. "#Param" refers to the number of parameters. "GFLOPs" is calculated under the input scale of 224 × 224. "*" indicates the performance of the method trained with the strategy in its original paper.</p><p>to perform the random-size cropping to 224 ×224, random horizontal flipping <ref type="bibr" target="#b49">[46]</ref>, and mixup <ref type="bibr" target="#b62">[59]</ref> for data augmentation. Label-smoothing regularization <ref type="bibr" target="#b50">[47]</ref> is used during training. We use AdamW <ref type="bibr" target="#b36">[33]</ref> with the momentum of 0.9, a mini-batch size of 128, and a weight decay of 5 × 10 −2 by default. The initial learning rate is set to 1 × 10 −3 and decreases following the cosine schedule <ref type="bibr" target="#b35">[32]</ref>. All models are trained for 300 epochs from scratch on 8 V100 GPUs. To benchmark, we apply a center crop on the validation set, where 224×224 pixels are cropped for evaluating the recognition accuracy.</p><p>Results. In  <ref type="bibr" target="#b53">[50]</ref>. This result is within expectations, because the pyramid structure may be beneficial to dense prediction tasks, but the gains it brings to image classification are limited. Note that, ViT and DeiT may have limitations as they are particularly designed for classification tasks, which are not suitable for dense prediction tasks that usually require effective feature pyramids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Object Detection</head><p>Experiment Settings. We conduct object detection experiments on the challenging COCO benchmark <ref type="bibr" target="#b31">[28]</ref>. All models are trained on the COCO train2017 (∼118k images) and evaluated on the val2017 (5k images). We evaluate our PVT backbones on two standard detectors: Reti-naNet <ref type="bibr" target="#b30">[27]</ref> and Mask R-CNN <ref type="bibr" target="#b17">[14]</ref>. During training, we first use the pre-trained weights on ImageNet to initialize the backbone and Xavier <ref type="bibr" target="#b16">[13]</ref> to initialize the newly added layers. Our models are trained with the batch size of 16 on 8 V100 GPUs and optimized by AdamW <ref type="bibr" target="#b36">[33]</ref> with the initial learning rate of 1×10 −4 . Following the common setting <ref type="bibr" target="#b30">[27,</ref><ref type="bibr" target="#b17">14,</ref><ref type="bibr" target="#b8">5]</ref>, we adopt 1× or 3× training schedule (i.e., 12 or 36 epochs) to train all detection models. The training image is resized to the shorter side of 800 pixels, while the longer side does not exceed 1333 pixels. When using 3× training schedule, we also randomly resize the shorter side of the input image within the range of [640, 800]. In the testing phase, the shorter side of the input image is fixed to 800 pixels. <ref type="table">Table 3</ref>, using RetinaNet for object detection, we find that when the parameter number is comparable, the PVT variants significantly surpasses their counterparts, showing that our PVT can be a good alternative to the CNN backbone for object detection. For example, with 1× training schedule, RetinaNet+PVT-Tiny is 4.9 AP better than RetinaNet+ResNet18 <ref type="bibr" target="#b18">[15]</ref>  Similar results are found in the instance segmentation experiment using Mask R-CNN, as shown in <ref type="table">Table 4</ref>. With 1× training schedule, Mask R-CNN+PVT-Tiny achieves 35.1 mask AP (AP m ), which is 3.9 AP m better than Mask R-CNN+ResNet18 and even 0.7 AP m higher than Mask R-CNN+ResNet50. The best AP m obtained by Mask R-CNN+PVT-Large 40.7, which is 1.0 AP m higher than the model based on ResNeXt101-64x4d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Semantic Segmentation</head><p>Experiment Settings. We choose ADE20K <ref type="bibr" target="#b67">[63]</ref>, a challenging scene parsing benchmark for semantic segmentation. ADE20K contains 150 fine-grained semantic cate-  <ref type="table">Table 3</ref>: Object detection performance on the COCO val2017. "#Param" refers to parameter number. "MS" means using multi-scale training <ref type="bibr" target="#b30">[27,</ref><ref type="bibr" target="#b17">14]</ref> Backbone <ref type="table">Table 4</ref>: Object detection and instance segmentation performance on the COCO val2017. "#Param" refers to parameter number. AP b and AP m denote bounding box AP and mask AP, respectively. "MS" means using multi-scale training <ref type="bibr" target="#b30">[27,</ref><ref type="bibr" target="#b17">14]</ref>.   <ref type="figure">FPN [21]</ref>, a simple segmentation method without dilated convolutions <ref type="bibr" target="#b60">[57]</ref>. In the training phase, the backbone is initialized with the pre-trained weights on ImageNet <ref type="bibr" target="#b12">[9]</ref>, and other newly added layers are initialized with Xavier <ref type="bibr" target="#b16">[13]</ref>. We optimize our models by AdamW <ref type="bibr" target="#b36">[33]</ref> with the initial learning rate of 1e-4. Following the common settings <ref type="bibr" target="#b24">[21,</ref><ref type="bibr" target="#b9">6]</ref>, we train our models for 80k iterations with the batch size of 16 on 4 V100 GPUs. The learning rate is decayed according to the polynomial decay schedule with the power of 0.9. We randomly resize and crop the training image to 512 × 512 and scale the image to the shorter side of 512 during testing.</p><formula xml:id="formula_10">#Param (M) Mask R-CNN 1x Mask R-CNN 3x + MS AP b AP b 50 AP b 75 AP m AP m 50 AP m 75 AP b AP b 50 AP b 75 AP m AP m 50 AP m 75</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Results. As shown in <ref type="table" target="#tab_10">Table 5</ref>, our PVT consistently outperforms ResNet <ref type="bibr" target="#b18">[15]</ref> and ResNeXt <ref type="bibr" target="#b59">[56]</ref>   <ref type="table" target="#tab_13">Table 6</ref>: Performance of the pure Transformer object detection pipeline. We build a pure Transformer detector by combining PVT and DETR <ref type="bibr" target="#b7">[4]</ref>, whose AP is 2.4 higher than the original DETR based on ResNet50 <ref type="bibr" target="#b18">[15]</ref>.  <ref type="table">Table 7</ref>: Performance of the pure Transformer semantic segmentation pipeline. We build a pure Transformer detector by combining PVT and Trans2Seg <ref type="bibr" target="#b58">[55]</ref>. It is 2.9% higher than ResNet50-d16+Trans2Seg and 1.1% higher than ResNet50-d8+DeeplabV3+ with lower GFlops. "d8" and "d16" means dilation 8 and 16, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Pure Transformer Dense Prediction</head><p>We replace ResNet50 with PVT on DETR <ref type="bibr" target="#b7">[4]</ref> for detection and Trans2Seg <ref type="bibr" target="#b58">[55]</ref> for segmentation, building pure Transformer dense prediction pipelines. The results prove that fully Transformer without convolution also works well on both object detection and semantic segmentation.</p><p>PVT+DETR. We build a pure Transformer model for object detection by combining our PVT with DETR <ref type="bibr" target="#b7">[4]</ref>, which has a Transformer-based detection head. We train models on the COCO train2017 for 50 epochs with the initial learning rate of 1 × 10 −4 . The learning rate is divided by 10 at the 33rd epoch. We use random flip and random scale as the data augmentation. All other experiment settings is the same as that of Sec. 5.2. As reported in  <ref type="table">Table 7</ref>.</p><p>We find that PVT-Small+Trans2Seg achieves 42.6 mIoU, outperforming ResNet50-d8+DeeplabV3+ <ref type="bibr">(41.5)</ref>. Note that, ResNet50-d8+DeeplabV3+ has 120.5 GFLOPs due  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Study</head><p>Experiment Settings. We conduct ablation studies on ImageNet <ref type="bibr" target="#b12">[9]</ref> and COCO <ref type="bibr" target="#b31">[28]</ref> datasets. The experimental settings on ImageNet are the same as the settings in Sec. 5.1. On the COCO dataset, all models are trained using 1× training schedule (i.e., 12 epochs), and other experimental settings follow the settings in Sec. 5.2.</p><p>Pyramid Structure. Pyramid structure is crucial when applying Transformer to dense prediction tasks. Previous ViT is a columnar framework, whose output is single-scale. It raises a problem that when using coarse image patches (e.g., 32 × 32 per patch) as input, the resolution of the output feature map will be low, resulting in poor detection performance (31.7 AP on the COCO val2017), as shown in <ref type="table" target="#tab_15">Table 8</ref>. When using fine-grained image patches (e.g., 4 × 4 per patch) as input, ViT will exhaust the GPU memory (e.g., 32G). Differently, our method avoids this problem through a progressive shrinking pyramid. Our method can process high-resolution feature maps in shallow stages and lowresolution feature maps in deep stages. Thus, our method obtains a promising AP of 40.4 on the COCO val2017, 8.7 AP higher than ViT-Small/32 (40.4 vs. 31.7).</p><p>Deeper vs. Wider. The problem of whether the CNN backbone should go deeper or wider has been extensively discussed in previous work <ref type="bibr" target="#b18">[15,</ref><ref type="bibr" target="#b61">58]</ref>. Here, we explore this problem in our PVT. For fair comparisons, we multiply the hidden dimensions {C 1 , C 2 , C 3 , C 4 } of PVT-Small by a scale factor 1.4 to make it have an equivalent parameter number to the deep model (i.e., PVT-Medium). As shown in <ref type="table" target="#tab_17">Table 9</ref>, the deep model (i.e., PVT-Medium) consistently works better than the wide model (i.e., PVT-Small-Wide) on both ImageNet and COCO. Therefore, going deeper is more   effective than going wider in the design of PVT. Based on this observation, in <ref type="table" target="#tab_4">Table 1</ref>, we develop PVT models with different scales by increasing the model depth.</p><p>Pre-trained Weights. Most dense prediction models (e.g., RetinaNet <ref type="bibr" target="#b30">[27]</ref>) rely on the backbone whose weights are pre-trained on ImageNet. We also discuss this problem in our PVT. In the top of <ref type="figure">Figure 5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GFLOPs Input Scale</head><p>ViT-Small/16</p><p>ViT-Small/32</p><p>PVT-Small ResNet50 <ref type="figure">Figure 6</ref>: GFLOPs under different input scales.</p><p>The growth rate of GFLOPs: ViT-Small/16 <ref type="bibr" target="#b13">[10]</ref>&gt;ViT-Small/32 <ref type="bibr" target="#b13">[10]</ref>&gt;PVT-Small&gt;ResNet50 <ref type="bibr" target="#b18">[15]</ref>. When the input scale is less than 640 × 640, the GFLOPs of PVT-Small and ResNet50 <ref type="bibr" target="#b18">[15]</ref> are close.  the 3× training schedule and multi-scale training. Therefore, like CNN-based models, pre-training weights can also help PVT-based models converge faster and better. Moreover, in the bottom of <ref type="figure">Figure 5</ref>, we also see that the convergence speed of PVT-based models (red curves) is faster than that of ResNet-based models (green curves).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Computation Cost. With the increment of input scale, the growth rate of GFLOPs of our method is greater than ResNet <ref type="bibr" target="#b18">[15]</ref>, but less than ViT <ref type="bibr" target="#b13">[10]</ref>, as shown in <ref type="figure">Figure 6</ref>, which means that our PVT is more suitable for tasks with medium-resolution input (e.g., shorter size does not exceed 800 pixels). On COCO, the shorter side of the input image is 800 pixels. Under this condition, the inference speed of RetinaNet based on PVT-Small is slower than the one based on ResNet50, as reported in <ref type="table" target="#tab_4">Table 10</ref>. A direct solution for this problem is to reduce the input scale. When reducing the shorter side of the input image to 640 pixels, the model based on PVT-Small runs faster than the ResNet50-based model, but our AP is 2.4 higher (38.7 vs. 36.3). Another possible solution is to develop a novel self-attention layer with lower computational complexity for visual tasks. This is a direction worth exploring, and we will pay consistent efforts in it in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>In this paper, we introduce PVT, a pure Transformer backbone for dense prediction tasks such as object detection and semantic segmentation. We develop a progressive shrinking pyramid and a spatial-reduction attention layer to obtain multi-scale feature maps under limited computation/memory resources. Extensive experiments on object detection and semantic segmentation benchmarks verify that our PVT is stronger than well-designed CNN backbones under comparable numbers of parameters.</p><p>Although PVT can serve as an alternative to the CNN backbone (e.g., ResNet, ResNeXt), there are still some specific modules and operations designed for CNNs but not considered in this work, such as SE <ref type="bibr" target="#b19">[16]</ref>, SK <ref type="bibr" target="#b27">[24]</ref>, dilated convolution <ref type="bibr" target="#b60">[57]</ref>, and NAS <ref type="bibr" target="#b51">[48]</ref>. Moreover, with years of rapid developments, there are many well-engineered CNN backbones such as Res2Net <ref type="bibr" target="#b15">[12]</ref>, EfficientNet <ref type="bibr" target="#b51">[48]</ref>, and ResNeSt <ref type="bibr" target="#b63">[60]</ref>. On the contrary, the Transformer-based model in computer vision is still in its early stage of development. Therefore, we believe there are many potential technologies to be explored in the future. We hope that our method could serve as a good starting point.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>w pre-train 1x PVT-Small w/ pre-train 3x PVT-Small w/o pre-train 1x PVT-Small w/o pre-train 3x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, we plot the validation AP curves of RetinaNet-PVT-Small w/ (red curves) and w/o (blue curves) pre-trained weights. We find that the model w/ pre-trained weights converges better than the one w/o pre-trained weights, and the gap between their final AP reaches 13.8 under the 1× training schedule and8.4 under    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Detailed settings of PVT series. The design follows the two rules of ResNet [15]: (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>, we find that our PVT models are su-</cell></row><row><cell>perior to conventional CNN backbones under similar pa-</cell></row><row><cell>rameter numbers and computation budgets. For example,</cell></row><row><cell>when the GFLOPs are roughly similar, the top-1 error of</cell></row><row><cell>PVT-Small reaches 20.2, which is 1.3 higher than that of</cell></row><row><cell>ResNet50 [15] (21.5). Meanwhile, under similar or rela-</cell></row></table><note>tively lower complexity, our PVT family archives perfor- mance comparable (i.e., 18.2 vs. 18.3) to the recently pro- posed Transformer-based models, such as ViT [10] and DeiT</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>(36.7 vs. 31.8). Moreover, with 3× training schedule and multi-scale training, RetinaNet+PVT-Large archive 43.4 AP, surpassing RetinaNet+ResNeXt101-64x4d [56] 41.8 AP, while our parameter number is 30% fewer (71.1M vs. 95.5M).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>AP 75 AP S AP M AP L AP AP 50 AP 75 AP S AP M AP L ResNet18 [15] 21.3 31.8 49.6 33.6 16.3 34.3 43.2 35.4 53.9 37.6 19.5 38.2 46.8 PVT-Tiny (ours) 23.0 36.7(+4.9) 56.9 38.9 22.6 38.8 50.0 39.4(+4.0) 59.8 42.0 25.5 42.0 52.1 ResNet50 [15] 37.7 36.3 55.3 38.6 19.3 40.0 48.8 39.0 58.4 41.8 22.4 42.8 51.</figDesc><table><row><cell>Backbone</cell><cell>#Param (M)</cell><cell>AP</cell><cell cols="2">RetinaNet 1x AP 50 6 RetinaNet 3x + MS</cell></row><row><cell>PVT-Small (ours)</cell><cell cols="4">34.2 40.4(+4.1) 61.3 43.0 25.0 42.9 55.7 42.2(+3.2) 62.7 45.0 26.2 45.2 57.2</cell></row><row><cell>ResNet101 [15]</cell><cell cols="2">56.7 38.5</cell><cell>57.8 41.2 21.4 42.6 51.1 40.9</cell><cell>60.1 44.0 23.7 45.0 53.8</cell></row><row><cell cols="5">ResNeXt101-32x4d [56] 56.4 39.9(+1.4) 59.6 42.7 22.3 44.2 52.5 41.4(+0.5) 61.0 44.3 23.9 45.5 53.7</cell></row><row><cell>PVT-Medium (ours)</cell><cell cols="4">53.9 41.9(+3.4) 63.1 44.3 25.0 44.9 57.6 43.2(+2.3) 63.8 46.1 27.3 46.3 58.9</cell></row><row><cell cols="3">ResNeXt101-64x4d [56] 95.5 41.0</cell><cell>60.9 44.0 23.9 45.2 54.0 41.8</cell><cell>61.5 44.4 25.2 45.4 54.6</cell></row><row><cell>PVT-Large (ours)</cell><cell cols="4">71.1 42.6(+1.6) 63.7 45.4 25.8 46.0 58.4 43.4(+1.6) 63.6 46.1 26.1 46.0 59.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Semantic segmentation performance of different backbones on the ADE20K validation set.</figDesc><table><row><cell>"#Param"</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>AP 75 AP S AP M AP L ResNet50 32.3 53.9 32.3 10.7 33.8 53.0 PVT-Small (ours) 34.7(+2.4) 55.7 35.4 12.0 36.4 56.7</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell>AP 50</cell><cell>DETR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>under different pa-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>rameter scales, using Semantic FPN for semantic segmenta-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tion. For example, with almost the same parameter number,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>our PVT-Tiny/Small/Medium is at least 2.8 mIoU higher</cell></row><row><cell></cell><cell></cell><cell></cell><cell>than ResNet-18/50/101. In addition, although the parame-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ter number of our Semantic FPN+PVT-Large is 20% lower</cell></row><row><cell></cell><cell></cell><cell></cell><cell>than that of Semantic FPN+ResNeXt101-64x4d, the mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell>is still 1.9 higher (42.1 vs. 40.2), showing that for semantic</cell></row><row><cell></cell><cell></cell><cell></cell><cell>segmentation, our PVT can extract better features than the</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CNN backbone benefitting from the global attention mech-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>anism.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 ,</head><label>6</label><figDesc>PVT+DETR archieve 34.7 AP on the COCO val2017, outperforming ResNet50+DETR by 2.4 AP (34.7 vs. 32.3).</figDesc><table><row><cell>PVT+Trans2Seg. We build a pure Transformer model</cell></row><row><cell>for semantic segmentation by combining our PVT with</cell></row><row><cell>Trans2Seg [55], a segmentation head based on Transformer</cell></row><row><cell>decoder. According to the experiment settings in Sec. 5.3,</cell></row><row><cell>we perform experiments on ADE20K [63] dataset with 40k</cell></row><row><cell>iterations training, single scale testing, and compare it with</cell></row><row><cell>ResNet50+Trans2Seg, DeeplabV3+ with dilation 8 (d8) and</cell></row><row><cell>16 (d16), as shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Performance comparison between ViT and our PVT using RetinaNet for object detection. ViT-Small/4 is out of GPU memory due to small patch size (i.e., 4 × 4 per patch). ViT-Small/32 obtain the 31.7 AP on the COCO val2017, which is 8.7 lower than our PVT.</figDesc><table><row><cell>to the high computation cost of dilated convolution, and</cell></row><row><cell>our PVT-Small+Trans2Seg has only 31.6 GFLOPs. PVT-</cell></row><row><cell>Small+Trans2Seg also performs better than ResNet50-</cell></row><row><cell>d16+Trans2Seg (31.6G Flops vs. 79.3 GFLOPs, 42.6 vs.</cell></row><row><cell>39.7, only 40% GFLOPs and 2.9 mIoU higher).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Figure 5: AP curves of RetinaNet on the COCO val2017 under different backbone settings. Top: using weights pre-trained on ImageNet vs. random initialization. Bottom: PVT-S vs. R50<ref type="bibr" target="#b18">[15]</ref>.</figDesc><table><row><cell></cell><cell>40.4</cell><cell></cell><cell>42.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>33.8</cell></row><row><cell></cell><cell>26.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>42.2</cell></row><row><cell></cell><cell>40.4</cell><cell></cell><cell>39.0</cell></row><row><cell></cell><cell>36.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>PVT-Small 1x</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PVT-Small 3x</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50 1x</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50 3x</cell></row><row><cell>Method</cell><cell>#Param (M)</cell><cell>Top-1</cell><cell>RetinaNet 1x AP AP 50 AP 75</cell></row><row><cell cols="2">PVT-Small-Wide 46.8</cell><cell cols="2">19.3 40.8 61.8 43.3</cell></row><row><cell>PVT-Medium</cell><cell>44.2</cell><cell cols="2">18.8 41.9 63.1 44.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc>Deeper vs. Wider. "Top-1" denotes the top-1 error on the ImageNet validation set. "AP" denotes the bounding box AP on the COCO val2017.</figDesc><table><row><cell>The deep model (i.e., PVT-</cell></row><row><cell>Medium) obtains better performance than the wide model</cell></row><row><cell>(i.e., PVT-Small-Wide ) under comparable parameter num-</cell></row><row><cell>ber.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>Latency and AP under different input scales. "Scale" and "Time" denote the input scale and time cost per image. When the shorter side of the input image is 640 pixels, the PVT-S+RetinaNet has lower GFLOPs and time cost than R50 [15]+RetinaNet, while obtaining 2.4 better AP (38.7 vs. 36.3). Note that, the time cost is tested on one V100 GPU with the batch size of 1.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pvt-Tiny</surname></persName>
		</author>
		<idno>ours) 32.9 36.7(+2.7) 59.2 39.3 35.1(+3.9) 56.7 37.3 39.8(+2.9) 62.2 43.0 37.4(+3.8) 59.3 39.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pvt-Small</surname></persName>
		</author>
		<idno>ours) 44.1 40.4(+2.4) 62.9 43.8 37.8(+3.4) 60.1 40.3 43.0(+2.0) 65.3 46.9 39.9(+2.8) 62.5 42.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pvt-Medium</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ours) 63.9 42.0(+1.6) 64.4 45.6 39.0(+2.6) 61.6 42.1 44.2(+1.4) 66.0 48.2 40.5(+2.0) 63.1 43.5</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pvt-Large</surname></persName>
		</author>
		<idno>ours) 81.0 42.9(+0.1) 65.0 46.6 39.5(+1.1) 61.9 42.5 44.5(+0.1) 66.0 48.3 40.7(+1.0) 63.4 43.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt</title>
		<meeting>IEEE Conf. Comp. Vis. Patt</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Transformer is all you need: Multimodal multitask learning with a unified transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10772</idno>
		<imprint>
			<biblScope unit="volume">2211</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Umme Zahoora, and Aqsa Saeed Qureshi. A survey of the recent architectures of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asifullah</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anabia</forename><surname>Sohail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5455" to="5516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf</title>
		<meeting>Advances in Neural Inf</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf</title>
		<meeting>Advances in Neural Inf</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Standalone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Application of convolutional neural network for image classification on pascal voc challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyash</forename><surname>Shetty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03785</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">dataset. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Onenet: Towards end-to-end one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05780,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
		<title level="m">Transtrack: Multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12450,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mixed link networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artificial Intell</title>
		<meeting>Int. Joint Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12193" to="12202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08461</idno>
		<title level="m">Segmenting transparent object in the wild with transformer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Wide residual networks for mitosis detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwan</forename><surname>Zerhouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dávid</forename><surname>Lányi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matheus</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Gabrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 14th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="924" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf</title>
		<meeting>IEEE Conf</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Comp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="10076" to="10085" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
