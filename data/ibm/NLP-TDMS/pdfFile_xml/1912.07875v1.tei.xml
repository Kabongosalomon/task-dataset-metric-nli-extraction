<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIBRI-LIGHT: A BENCHMARK FOR ASR WITH LIMITED OR NO SUPERVISION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-17">17 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivière</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Mazaré</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename><forename type="middle">V</forename><surname>Liptchinsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">EHESS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL-University</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<address>
									<region>INRIA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LIBRI-LIGHT: A BENCHMARK FOR ASR WITH LIMITED OR NO SUPERVISION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-17">17 Dec 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new collection of spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived from open-source audio books from the LibriVox project. It contains over 60K hours of audio, which is, to our knowledge, the largest freely-available corpus of speech. The audio has been segmented using voice activity detection and is tagged with SNR, speaker ID and genre descriptions. Additionally, we provide baseline systems and evaluation metrics working under three settings: (1) the zero resource/unsupervised setting (ABX), (2) the semisupervised setting (PER, CER) and (3) the distant supervision setting (WER). Settings <ref type="formula">(2)</ref> and (3) use limited textual resources (10 minutes to 10 hours) aligned with the speech. Setting (3) uses large amounts of unaligned text. They are evaluated on the standard LibriSpeech dev and test sets for comparison with the supervised state-of-the-art.</p><p>Index Terms-unsupervised and semi-supervised learning, distant supervision, dataset, zero-and low resource ASR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic Speech Recognition (ASR) has made striking progress in the recent years with the deployment of increasingly large deep neural networks trained on increasingly large sets of annotated speech (from thousands to tens of thousands of hours). This approach is hit by diminishing returns as the costs of annotating even larger datasets become prohibitive. It is also difficult to scale beyond a handful of high-resource languages and address the needs of a long tail of low-resource languages, dialectal and idiolectal variants, accents, and registers. As such, there has been a recent surge of interest in weakly supervised solutions that use datasets with fewer human annotations. In the semi-supervised setting, only a fraction of the dataset is labelled and the rest is unlabelled <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, while in a distant supervision setting, the dataset is mostly or entirely unlabelled, but large quantities of unaligned text provide a language model corpus <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Other approaches have addressed pretraining with labels from other * Contributed equally, in random order. languages <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> or pretraining using unsupervised objectives <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. At the extreme of this continuum, zero resource ASR discovers its own units from raw speech <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Despite many interesting results, the field lacks a common benchmark (datasets, evaluations, or baselines) for comparing ideas and results across these settings. Here, we introduce Libri-light, a large open-source corpus (60K hours) of unlabelled speech and a common set of metrics to evaluate three settings: (1) the zero-resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER and CER), and (3) the distant supervision setting (WER). The last two settings use a limited-resource training set (10 min, 1h, 10h), and the last one large in-domain and out-of-domain text to train language models. The test sets are identical to LibriSpeech <ref type="bibr" target="#b11">[12]</ref> so as to facilitate comparison of weakly supervised results with the state-of-the art in supervised learning. We also provide a baseline system on these three settings. All datasets, metrics and baseline systems are open source 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The release of open source software and datasets has facilitated rapid progress in machine learning and in particular large vocabulary ASR. LibriSpeech is one of the first largescale open-source datasets and contains over 1000 hours of audio books, together with textual annotations aligned at the sentence level. Mozilla's CommonVoice project has facilitated data collection across several languages and currently contains 2900 hours of read speech in 37 languages 2 . A. Black at CMU has compiled the Wilderness dataset which consists of the text of the Bible read in 750 languages <ref type="bibr" target="#b12">[13]</ref>. Other open-source resources are available from OpenSLR 3 .</p><p>The Zero Resource Challenge has released a series of datasets and metrics for the unsupervised setting <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>  <ref type="bibr" target="#b3">4</ref> , but the datasets are generally small (between 2.5 and 50 h). In this work, we substantially expand dataset size and use the same evaluation metrics (ABX <ref type="bibr" target="#b13">[14]</ref>) for comparability. The IARPA Babel program <ref type="bibr" target="#b14">[15]</ref> has also initiated a push towards limited supervision for less studied languages. In its most difficult track, the dataset contains only 10 hours of transcribed speech in conjunction with with larger amounts of untranscribed audio. Here, we retain 10 hours as a upper bound, and add lower-resource sets containing 1 hours and 10 minutes of labeled audio. While distant supervision has been the focus of two JHU-JSALT workshops (2016 <ref type="bibr" target="#b15">[16]</ref>, 2019 <ref type="bibr" target="#b16">[17]</ref>) but no benchmark has yet emerged.  The dataset is composed four parts: a train set with unlabelled speech, a train set with limited labels, dev/test sets, and a train set containing unaligned text; see <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DATASET AND METRICS</head><p>Unlabelled Speech Training Set. This dataset was obtained by extracting audio files for English speech from the LibriVox repository 5 containing open source audio books. Files were downloaded and converted to 16kHz FLAC. We then removed corrupted files, files with unknown or multiple speakers, and speakers appearing in LibriSpeech dev and test sets. The potentially duplicated versions of books based on titles were set aside (and distributed as a duplicate subset, totalling 4500 hours). We then ran a Voice Activity Detection (VAD) model using the wav2letter++ framework <ref type="bibr" target="#b17">[18]</ref> on the recordings to tag onsets and offsets of speech segments.  <ref type="table">Table 1</ref>: Datasets stats in Libri-light. *Six different versions of the 10 min datasets have been constructed, the union of these small datasets make up the 1h dataset.</p><p>The VAD segments were used to derive an average SNR for each file. For each file, we constructed JSON metadata including title, unique speaker ID, SNR, genre, and list of valid VAD block (block of more than 500ms of speech indicated by onsets and offsets). We created three dataset splits based on different sizes: (unlab-60k), (unlab-6k) and (unlab-600), matched in genre distribution (the smaller cuts are included in the larger ones, see the stats in <ref type="table">Table 1</ref>). The distributions by speaker and genres are in <ref type="figure" target="#fig_0">Figure 1</ref>. The total amount of speech in the dataset is 62.2K hours, including the duplicate files.</p><p>Limited-resource Training Set. For training with limited supervision, we selected three subsets of the LibriSpeech training set: a 10 hour set, a 1 hour set, and six 10-minute sets (the six 10-minute sets together make up the 1h set, and the 1h set is included in the 10h set). In each set, half of the utterances are from the clean and other training sets, respectively. We additionally provide orthographic transcriptions from LibriSpeech and phonetic transcriptions generated from phonemizer <ref type="bibr" target="#b5">6</ref> .</p><p>Dev  <ref type="table">Table 2</ref>: ABX errors on unsupervised CPC trained features. Within-and across-speaker phoneme discriminability scores (lower is better) on the LibriSpeech dev and test sets as a function of varying quantities of unlabelled speech.</p><p>truth phonetic sequences for the dev and test sets were generated as above; in addition, for ABX evaluation, forced alignment was obtained using a model trained on LibriSpeech.</p><p>Unaligned Text Training Set. For training a language model in the distant supervision setting, we consider the LM corpus provided in LibriSpeech 7 which contains 800M tokens and a vocabulary size of 200k from 14.5k public books from Project Gutenberg. This corpus only partially overlaps with the content of our unlabelled training set and can thus be considered in-domain. Several options exist for publicly available out-of-domain corpora (wikitext103, 1Billion word, etc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Metrics</head><p>We provide 3 sets of metrics for the unsupervised, distantlysupervised, and semi-supervised settings.</p><p>For the unsupervised setting, the aim is to extract speech representations (discrete or continuous) which encode the phonetic content of the language while ignoring irrelevant information (channel, speaker, etc). The representation is evaluated using ABX error, a distance-based metric used in previous zero resource challenges <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. For a given pair of sounds (for instance, "bit" versus "bet"), we compute the probability that the distance between a random token of "bit" (X) is closer to another token of "bit" (A) than to a token of "bet" (B). The ABX error rate is obtained by averaging across all such minimal pairs of phone trigrams in the corpus. For the "within-speaker" score, A, B and X are from the same speakers; in the "across-speaker" score, A and B are from the same speaker, but X is from a different speaker (see <ref type="bibr" target="#b18">[19]</ref>).</p><p>For the semi-supervised setting, we evaluate the quality of learned acoustic representations with little annotated data. Models can be trained either with character or phonetic targets using limited data and measured by either Character Error Rate (CER) or Phoneme Error Rate (PER).</p><p>For distant supervision, we evaluate how the learned representations can be used to decode speech at the word level using a pre-trained language model. We use Word Error Rate (WER) for the evaluation. Because the dev and test sets are from LibriSpeech, this allows to compare distant supervision directly with SOTA supervised models. More details on dataset and metrics in Supplementary Section S1.  <ref type="table">Table 3</ref>: PER/CER in the semi-supervised setting. A pretrained CPC system plus a linear classifier trained on limited amounts of labels compared to the same system trained from scratch (PER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BASELINE SYSTEMS</head><p>In the unsupervised setting, we use a PyTorch implementation of the Contrastive Predictive Coding (CPC) system <ref type="bibr" target="#b6">[7]</ref> trained to predict the hidden states of N future speech frames and containing an encoder, a sequence model, and a predictor. The encoder maps waveforms to hidden states (one 512 dimensional embedding every 10 ms frames) using a stack of 5 convolutional layers. The sequence model encodes the hidden states into a 512-dimensional phonetic embedding with one layer of Gated Recurrent Units (GRUs). The predictor maps the last phonetic embedding onto a future hidden state using a linear projection (one linear projection per time step, varying from 1 to 12). To avoid collapsing to a trivial solution, the model is trained discriminatively; the loss function aims at decreasing the dot product between predicted and actual future frames while increasing it for frames belonging to negative sequences (distant time windows). We used a reimplementation of the original paper, which we modified to increase stability and performance, as we could not reproduce the original paper results with the provided descriptions. These changes are as follows: we replaced batch-norm with channel-wise normalization, we reduced the hidden and phonetic embeddings to 256 dimensions, used a LSTM instead of a GRU, and used a 1-layer transformer network instead of a linear projection. The original paper obtained 65.5% accuracy on phoneme classification with a linear classifier trained on top of the frozen system's phonetic embedding. Our modified system obtained 68.9% accuracy, while using 4 times fewer parameters in the encoder+sequence model part of the system. We trained it on the three cuts (unlab-600, unlab-6k and unlab-60k).  Middle: A CPC system trained with unlabelled speech, finetuned with limited data and integrated with a 4-gram word language model (Librispeech-LM). Bottom: A small MFSC TDS system trained on limited labeled data (graphemes or phonemes). The pseudo-labels for the 60k corpus segmented into 36-second chunks are generated and are used to retrain a larger TDS system.</p><p>In the semi-supervised setting, we use our baseline pretrained CPC system supplemented with a linear classifier trained with CTC loss on the limited-resource set's phone labels (only the linear layer is fine-tuned). We also provide a from-scratch control with the same architecture trained end-to-end.</p><p>For the distant supervision setting, we run two experiments: (1) we use our pretrained CPC system with an improved CTC layer (LSTM) which we fine-tune with orthographic labels in the limited-resource set. We decode with a python wrapped version of the wav2letter++ decoder <ref type="bibr" target="#b17">[18]</ref>, using a 4-gram KenLM <ref type="bibr" target="#b21">[22]</ref> language model trained on the unaligned text set. <ref type="bibr" target="#b1">(2)</ref> We use CTC to train small Melfilterbanks-based TDS systems <ref type="bibr" target="#b22">[23]</ref>, (7 TDS blocks, 20M parameters, total stride 2) on phonemes/graphemes respec-tively. We create pseudo-labels by beam-search decoding the 60k-hours unlabelled data with a 4-gram KenLM decoder trained on LibriSpeech-LM. These labels are used to train larger TDS systems (11 TDS blocks, 37M parameters) from scratch which generate WERs when decoding with the same LM. More details on baselines in Supplementary Section S2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>The results for the unsupervised setting are shown in <ref type="table">Table 2</ref>. CPC constructs embeddings with good ABX scores compared to an MFCC baseline, and are in the same range as the SOTA in the Zero Resource Speech Challenge 2017 for English (6.2% within and 8.7% across <ref type="bibr" target="#b23">[24]</ref>). The results in the semi-supervised setting ( <ref type="table">Table 3)</ref> show gains in PER in using unsupervised pretraining for several different amounts of fine tuning. The results on the distant supervision <ref type="table" target="#tab_5">(Table 4</ref>), while far from supervised state-of-the-art, show that increasing the amount of unsupervised pretraining helps. Pseudo-labels are beneficial but only if the generating and fine-tuned models are initially good ( <ref type="table">Table 3</ref> and 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We have introduced a new large dataset for benchmarking ASR systems trained with limited or no supervision. We found that unsupervised training with increasingly larger dataset yield better features and can significantly boost the performance of systems trained with limited amounts of labels (from 10 min to 10 hours) both for a phoneme recognition task in a semi-supervised setting and for a word recognition in a distant-supervision setting. The baselines were not particularly optimized for the tasks and are provided only as a proof-of-concept; there is a significant margin with fully-supervised systems. Obvious improvements include using larger models, speaker-adversarial losses, fine tuning the entire system (not just the top layers), and pseudo-labels retraining in all settings. Active learning <ref type="bibr" target="#b24">[25]</ref> could further select useful parts of the dataset (we have provided SNR data to facilitate this effort). Yet another approach might apply language modeling techniques directly on unlabelled audio to improve the representations before fine-tuning them <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1. SUPPLEMENTARY DATASETS AND METRICS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.1. Datasets and meta-data</head><p>The dataset is constructed according to the following pipeline: data download, exclusion of bad files, conversion to flac, extraction of VAD, SNR, and Perplexity, the construction of JSON files and the split in three cuts of varying sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.1.1. VAD</head><p>Voice Activity Detection is accomplished using a TDS acoustic model <ref type="bibr" target="#b22">[23]</ref> trained using CTC loss <ref type="bibr" target="#b27">[28]</ref> on the LibriSpeech dataset using the orthographic transcription. The trained model was used to perform inference (greedy frame-byframe decoding) with the wav2letter++ <ref type="bibr" target="#b17">[18]</ref> audio analysis pipeline on the unlabeled audio by mapping all of the letters to SPEECH and the silence symbol to NONSPEECH. The posterior SPEECH probability is added as meta data on the JSON of each file.</p><p>The TDS model used for VAD has 100 million parameters and consists of clusters of 2, 3, 4, and 5 TDS blocks separated by 2D convolutions. The duration of audio contained in each label is dependent on the stride of the underlying acoustic model; the model used has a stride of 8. The model is trained with word-pieces using the recipe outlined in <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.1.2. SNR</head><p>The Signal-to-noise (SNR) ratio is calculated using the VAD labels predicted above. For each 80ms frame the VAD will return a posterior of whether the frame is speech or not. We decided to take &lt; 0.8 as the speech threshold, and &gt; 0.995 as noise threshold. If a speech frame is detected, we also automatically include 2 subsequent frames, to compensate for the spiky predictions from the VAD model. Any other frames that does not belong to either bucket are ignored since we are not confident whether they are speech or noise. Finally we compute the SNR ratio by using its definition SNR dB = 10 log 10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.1.4. JSON files and splits</head><p>To construct the JSON, we duplicated the metadata of the original book JSON file from LibriVox into each of the files associated for a given book (including unique book ID and speaker ID, and we added tags for SNR, perplexity and our own macro-genre tags, by folding the existing ones into 7 categories: "Literature", "Science, Craft &amp; Essay", "Ancient", "Religion", "Poetry", "Theater", and "Undefined". We also added VAD information as a list of onsets and offsets of voice activity.</p><p>The files were splitted into cuts of different sizes by trying to maintain the same distribution of macro-genres in the three cuts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.2. The ABX metric</head><p>Given a frame-wise distance metric, we want to check that features coding for the same phonemes have a more similar representation than features coding for different phonemes. To quantify this property, we use a minimal pair ABX task as defined in <ref type="bibr" target="#b13">[14]</ref>; given a set of sounds S(x) from a category x and a set of sounds S(y) from a category y we compute: Here, θ(x, y) is the probability of a sample from category x to be closer to another element from x than to one fromy. To compute ABX we average the error 1 − θ over all categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. SUPPLEMENTARY BASELINE MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.1. CPC model and training</head><p>To train a feature model in an unsupervised fashion, we used the method implemented by Riviere and al. in <ref type="bibr">[?]</ref>, inspired by the Contrastive Predictive Coding algorithm (CPC) presented in <ref type="bibr" target="#b6">[7]</ref>. We will briefly introduce the algorithm in this section, though we refer the reader to the original papers for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.1.1. Contrastive Predictive Coding</head><p>CPC relies on forward modeling: given an input sequence of features, we try to predict the k future representations of the sequence. The network must discriminate each future ground truth feature from negative examples randomly sampled in the batch.</p><p>More precisely, the model goes like this:</p><p>1. The raw waveform w goes through a convolutional network g c , resulting in a feature sequence (x t ) t∈1...T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Then we form the current phoneme representation z t by applying a recurrent network g ar to x t .</p><p>3. Finally, we predict (x t0+1 , ..., x t0+k ) from (z t ) t≤t0 using a prediction network g p .</p><p>When using theses feature for another task we always consider z t , the output of the recurrent layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.1.2. Architecture Details</head><p>For g c , we use five convolutional layers with strides [5, 4, 2, 2, 2], filter-sizes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4]</ref> and 256 hidden units with ReLU activations. Besides, the features are normalized channelwise between each convolution. In the end, this network has a downsampling factor of 160, meaning that on a 16kH audio input each feature will encode 10ms of data. Furthermore, g ar is a one-layer LSTM with also a 256 dimensional hidden state. Finally, the predictor g p is a one-layer transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.1.3. Training Details</head><p>We considered input sequences of 1280ms gathered in batches of 32 sequences per GPU with a total of 128 GPUs. Our training took approximatly two days on NVIDIA Tesla V100-SXM2-16GB. Besides, in a given batch, all sequences were sampled within the same speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.2. TDS model and training</head><p>Given the limited amount of supervised training data, we select to use a smaller TDS model <ref type="bibr" target="#b22">[23]</ref> with 20 million parameters. The model has a stride 2 in the first convolution, and three groups of TDS blocks with channels (10, 14, 18) and (2, 2, 3) blocks in each group. While on the whole 60k hours training data, we use the original architecture introduced in <ref type="bibr" target="#b22">[23]</ref> with 37 million parameters. The only difference is that we reduce the overall stride from 8 to 2. We use dropout to prevent over-fitting, and its value is set to 0.4 and 0.1 in the 20M and 37M models.</p><p>In terms of model optimization, we use plain SGD with momentum. The initial learning rate and momentum are set to 0.1 and 0.5 respectively. In the supervised setting, the models are trained for 1500 epochs on 8 GPUs in total with learning rate halved after each 200 epochs and total batch size 64 and 16 for character-and phone-based system. In the semisupervised scenario, the models are trained for 150 epochs on 32 GPUs in total with learning rate halved after every 30 epochs and total batch size 256.</p><p>The beam-search decoding parameters are tuned on only dev-other for the 20M TDS models to generate pseudo-labels, while they are tuned independently for the final 37M model. The same official LibriSpeech 4-gram LM is used in both decoding procedures. The decoding beam size is 1000 in all the experiments.  <ref type="table">Table S1</ref>: PER/CER of acoustic models trained in with pseudo-labels. Top: small phone-based TDS <ref type="bibr" target="#b22">[23]</ref> models with limited labels using wav2letter++ <ref type="bibr" target="#b17">[18]</ref>, generating pseudolabels on the 60K dataset with an in-domain LM, retraining a larger TDS acoustic model (PER). Bottom: the same trained on characters (CER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3. SUPPLEMENTARY RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3.1. Pseudo-labels experiment</head><p>Table S1 provides PER/CER in the distant supervision setting with models trained on pseudo-labels. The TDS models above and generated pseudo-labels are trained and generated with the exactly the same procedure introduced in Section 4. Note that the PER/CER results above are not comparable to the semi-supervised ones in <ref type="table">Table 3</ref> as pseudo-labels here are generated with the official LibriSpeech LM, whose training set is a super-set of the transcriptions in the supervised training set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Corpus statistics. (a) Durations in hours per speakers (b) Durations for the 25 most frequent genres.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig</head><label></label><figDesc>. S1: Librivox SNR histogram S1.<ref type="bibr" target="#b0">1</ref>.3. PerplexityPerplexity was obtained by performing beam search decoding of the trained TSD model defined above supplemented by a 4gram word Language Model trained on LibriSpeech LM. It was computed as the mean of the log probability of the posterior on each file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 ½</head><label>2</label><figDesc>θ(x, y) = 1 nm(m − 1) a∈S(x) b∈S(y) c∈S(x)\{a}θ (a, b, c) With:θ (a, b, c) = ½ d(a,c)&lt;d(a,b) + 1 d(a,c)=d(a,b) m, n = |S(x)|, |S(y)|</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and Test Set. The dev and test sets are the same as that of LibriSpeech (5.4 hours for dev-clean, 5.3 hours for dev-other, 5.4 hours for test-clean, and 5.1 hours for testother) and are intended for testing and tuning. All dev or test set audio has been removed from training sets. The ground-</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ABX within speaker</cell><cell></cell><cell></cell><cell cols="2">ABX across speaker</cell><cell></cell></row><row><cell>System</cell><cell cols="4">dev-clean dev-other test-clean test-other</cell><cell cols="4">dev-clean dev-other test-clean test-other</cell></row><row><cell>MFCC Baseline</cell><cell>10.95</cell><cell>13.55</cell><cell>10.58</cell><cell>13.60</cell><cell>20.94</cell><cell>29.41</cell><cell>20.45</cell><cell>28.5</cell></row><row><cell>CPC unlab-600</cell><cell>7.36</cell><cell>9.39</cell><cell>6.90</cell><cell>9.59</cell><cell>9.58</cell><cell>14.67</cell><cell>9.00</cell><cell>15.1</cell></row><row><cell>CPC unlab-6k</cell><cell>6.51</cell><cell>8.42</cell><cell>6.22</cell><cell>8.55</cell><cell>8.48</cell><cell>13.39</cell><cell>8.05</cell><cell>13.81</cell></row><row><cell>CPC unlab-60k</cell><cell>6.11</cell><cell>8.17</cell><cell>5.83</cell><cell>8.14</cell><cell>8.05</cell><cell>12.83</cell><cell>7.56</cell><cell>13.42</cell></row></table><note>6 https://gitlab.coml.lscp.ens.fr/mbernard/phonemizer</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>WER in the distant supervision setting. Top: State-of-the-art supervised systems using our 4-gram-LMs.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/libri-light 2 https://voice.mozilla.org 3 http://openslr.org/ 4 https://zerospeech.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining active and semi-supervised learning for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="186" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Self-training for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09116</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Almost-unsupervised speech recognition with close-to-zero resource based on phonetic structures learned from very small unpaired speech and text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>arXiv preprint:1810.12566</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised cross-modal alignment of speech and text embedding spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7354" to="7364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Crosslanguage knowledge transfer using multilingual deep neural network with shared hidden layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7304" to="7308" />
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The language-independent bottleneck features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grézl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Janda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Egorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="336" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The zero resource speech challenge 2015: Proposed approaches and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Versteegh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLTU-Procedia Computer Science</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The zero resource speech challenge 2017</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjumea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ASRU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The zero resource speech challenge 2019: TTS without T</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Algayres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjumea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miskic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dugrain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakriani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CMU wilderness multilingual speech dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5971" to="5975" />
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Evaluating speech features with the minimal-pair abx task: Analysis of the classical mfc/plp pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>INTERSPEECH-2013</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Iarpa babel program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An empirical evaluation of zero resource acoustic unit discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kesiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5305" to="5309" />
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distant supervision for representation learning in speech and handwriting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wav2letter++: A fast open-source speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6460" to="6464" />
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">ABX-discriminability measures and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Letter based speech recognition with gated convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rwth asr systems for librispeech: Hybrid vs attention-w/o data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03072</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth workshop on statistical machine translation. Association for Computational Linguistics</title>
		<meeting>the sixth workshop on statistical machine translation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence speech recognition with time-depth separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature optimized DPGMM clustering for unsupervised subword modeling: A contribution to zerospeech 2017</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="740" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Active learning for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorin</surname></persName>
		</author>
		<editor>ICASSP. IEEE</editor>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">3904</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Speech2vec: A sequenceto-sequence framework for learning word embeddings from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08976</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">vq-wav2vec: Self-supervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>under review</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
