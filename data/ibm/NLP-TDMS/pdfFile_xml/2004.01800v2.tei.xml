<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporally Distributed Networks for Fast Video Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporally Distributed Networks for Fast Video Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present TDNet, a temporally distributed network designed for fast and accurate video semantic segmentation. We observe that features extracted from a certain high-level layer of a deep CNN can be approximated by composing features extracted from several shallower subnetworks. Leveraging the inherent temporal continuity in videos, we distribute these sub-networks over sequential frames. Therefore, at each time step, we only need to perform a lightweight computation to extract a sub-features group from a single sub-network. The full features used for segmentation are then recomposed by the application of a novel attention propagation module that compensates for geometry deformation between frames. A grouped knowledge distillation loss is also introduced to further improve the representation power at both full and sub-feature levels. Experiments on Cityscapes, CamVid, and NYUD-v2 demonstrate that our method achieves state-of-the-art accuracy with significantly faster speed and lower latency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video semantic segmentation aims to assign pixel-wise semantic labels to video frames. As an important task for visual understanding, it has attracted more and more attention from the research community <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>. The recent successes in dense labeling tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b60">61]</ref> have revealed that strong feature representations are critical for accurate segmentation results. However, computing strong features typically require deep networks with high computation cost, thus making it challenging for realworld applications like self-driving cars, robot sensing, and augmented-reality, which require both high accuracy and low latency.</p><p>The most straightforward strategy for video semantic segmentation is to apply a deep image segmentation model to each frame independently, but this strategy does not leverage temporal information provided in the video dynamic scenes. One solution, is to apply the same model to all frames and add additional layers on top to model temporal context to extract better features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref>. How- (ms/f) <ref type="figure">Figure 1</ref>. Performance on Cityscapes. Our proposed TDNet variants (denoted as and ) linked to their corresponding deep image segmentation backbones (denoted as ) with similar number of parameters. Compared with video semantic segmentation methods NetWarp <ref type="bibr" target="#b9">[10]</ref>, PEARL <ref type="bibr" target="#b18">[19]</ref>, ACCEL <ref type="bibr" target="#b17">[18]</ref>, LVS-LLS <ref type="bibr" target="#b26">[27]</ref>, GRFP <ref type="bibr" target="#b34">[35]</ref>, ClockNet <ref type="bibr" target="#b40">[41]</ref>, DFF <ref type="bibr" target="#b59">[60]</ref>, and real-time segmentation models LadderNet <ref type="bibr" target="#b20">[21]</ref>, GUNet <ref type="bibr" target="#b31">[32]</ref>, and ICNet <ref type="bibr" target="#b56">[57]</ref>, our TDNet achieves a better balance of accuracy and speed. ever, such methods do not help improve efficiency as all features must be recomputed at each frame. To reduce redundant computation, a reasonable approach is to apply a strong image segmentation model only at keyframes, and reuse the high-level feature for other frames <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b59">60]</ref>. However, the spatial misalignment of other frames with respect to the keyframes is challenging to compensate for and often leads to decreased accuracy comparing to the baseline image segmentation models as reported in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b59">60]</ref>. Additionally, these methods have different computational loads between keyframes and non-keyframes, which results in high maximum latency and unbalanced occupation of computation resources that may decrease system efficiency.</p><p>To address these challenges, we propose a novel deep learning model for high-accuracy and low-latency semantic video segmentation named Temporally Distributed Network (TDNet). Our model is inspired by Group Convolution <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>, which shows that extracting features with separated filter groups not only allows for model parallelization, but also helps learn better representations. Given a deep image segmentation network like PSPNet <ref type="bibr" target="#b57">[58]</ref>, we divide the features extracted by the deep model into N (e.g. N =2 or 4) groups, and use N distinct shallow sub-networks to approximate each group of feature channels. By forcing each sub-network to cover a separate feature subspace, a strong feature representation can be produced by reassembling the output of these sub-networks. For balanced and efficient computation over time, we let the N sub-networks share the same shallow architecture, which is set to be 1 N of the original deep model's size to preserve a similar total model capacity <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>When segmenting video streams, the N sub-networks are sequentially and circularly assigned to frames over time, such that complementary sub-feature groups are alternatively extracted over time and only one new sub-feature group needs to be computed at each time step. To compensate for spatial misalignment caused by motion across frames, we propose an attention propagation module for reassembling features from different time steps. To further enhance the network's representational power, we also present a grouped distillation loss to transfer knowledge from a full deep model to our distributed feature network at both full and sub-feature group levels. With this new model, we only need to run a light-weight forward propagation at each frame, and can aggregate full features by reusing sub-features extracted in previous frames. As shown in <ref type="figure">Fig 1,</ref> our method outperforms state-of-the-art methods while maintaining lower latency. We validate our approach through extensive experiments over multiple benchmarks.</p><p>In summary, our contributions include: i) a temporally distributed network architecture and grouped knowledge distillation loss that accelerates state-of-the-art semantic segmentation models for videos with more than 2× lower latency at comparable accuracy; ii) an attention propagation module to efficiently aggregate distributed feature groups over time with robustness to geometry variation across frames; iii) better accuracy and latency than previous state-of-the-art video semantic segmentation methods on three challenging datasets including Cityscapes, Camvid, and NYUD-v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image semantic segmentation is an active area of research that has witnessed significant improvements in performance with the success of deep learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43]</ref>. As a pioneer work, the Fully Convolutional Network (FCN) <ref type="bibr" target="#b29">[30]</ref> replaced the last fully connected layer for classification with convolutional layers, thus allowing for dense label prediction. Based on this formulation, follow-up methods have been proposed for efficient segmentation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b56">57]</ref> or high-quality segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>Semantic segmentation has also been widely applied to videos <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b47">48]</ref>, with different approaches employed to balance the trade-off between quality and speed. A number of methods leverage temporal context in a video by repeatedly applying the same deep model to each frame and temporally aggregating features with additional network layers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref>. Although these methods improve accuracy over single frame approaches, they incur additional computation over a per-frame model. Another group of methods target efficient video segmentation by utilizing temporal continuity to propagate and reuse the high-level features extracted at key frames <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b59">60]</ref>. The challenge of these methods is how to robustly propagate pixel-level information over time, which might be misaligned due to motion between frames. To address this, Shelhamer et al. <ref type="bibr" target="#b40">[41]</ref> and Carreira et al. <ref type="bibr" target="#b1">[2]</ref> directly reuse high-level features extracted from deep layers at a low resolution, which they show are relatively stable over time. Another approach, employed by Zhu et al. <ref type="bibr" target="#b59">[60]</ref> is to adopt optical flow to warp high-level features at keyframes to non keyframes. Jain et al. <ref type="bibr" target="#b17">[18]</ref> further updates the flow warped feature maps with shallow features extracted at the current frame. However, using optical flow incurs significant computation cost and can fail with large motion, disocclusions, and non-textured regions. To avoid using optical flow, Li et al. <ref type="bibr" target="#b26">[27]</ref> instead proposes to use spatially variant convolution to adaptively aggregate features within a local window, which however is still limited by motion beyond that of the predefined window. As indicated in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b59">60]</ref>, though the overall computation is reduced compared to their image segmentation baselines, the accuracy is also decreased. In addition, due to the extraction of high-level features at keyframes, these methods exhibit inconsistency speeds, with the maximum latency equivalent to that of the singleframe deep model. In contrast to these, our approach does not use keyframe features, and substitutes optical-flow with an attention propagation module, which we show improves both efficiency and robustness to motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporally Distributed Network</head><p>In this section, we describe the architecture of a Temporally Distributed Network (TDNet), with an overview in <ref type="figure" target="#fig_1">Fig 2.</ref> In Sec. 3.1 we introduce the main idea of distributing sub-networks to extract feature groups from different temporal frames. In Sec 3.2, we present our attention propagation module designed for effective aggregation of spatially misaligned feature groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Distributed Networks</head><p>Inspired by the recent success of Group Convolution <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref> which show that adopting separate convolutional paths can increase a model's effectiveness by enhancing the sparsity of filter relationships, we propose to divide features from a deep neural network into a group of sub-features and approximate them using a set of shallow sub-networks each of which only covers a subspace of the original model's feature representation.</p><p>In addition, we observe that the full feature map is large, and dimension reduction <ref type="figure" target="#fig_1">(Fig 2(a)</ref>) is costly. In PSP-Net50 <ref type="bibr" target="#b57">[58]</ref>, the feature map has 4096 channels and dimension reduction takes about a third of the total computation. To further improve efficiency, based on block matrix multiplication <ref type="bibr" target="#b8">[9]</ref>, we convert the convolutional layer for dimension reduction to the summation of series of convolution operations at the subspace level, which enables us to distribute these subspace-level convolution operations to their respective subnetworks. As a result, the output of the dimension reduction layers is recomposed simply by addition, before being used in the prediction head of the network. Keeping a similar total model size to the original deep model, we show that aggregating multiple shallow network paths can have a similarly strong representational power as the original deep model <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>In the context of single image segmentation, the advantage of such an approach is that it allows for faster computation by extracting feature paths in parallel on multiple devices. However, in the context of segmenting video sequences, we can take advantage of their inherent temporal continuity and distribute the computation along the temporal dimension. We apply this distributed feature extraction method to video by applying the sub-networks to sequential frames, and refer to the new architecture as Temporally Distributed Network (TDNet). As shown in <ref type="figure" target="#fig_1">Fig 2(b)</ref>, TDNet avoids redundant sub-features computation by reusing the sub-feature groups computed at previous time steps. The full feature representation at each frame is then produced by aggregating previously computed feature groups with the current one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Aggregation</head><p>A big challenge of aggregating feature groups extracted at different time steps is the spatial misalignment caused by motion between frames. Optical flow-based warping is a popular tool to correct for such changes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b59">60]</ref>, but it is expensive to compute, prone to errors, and restricted to a single match per pixel. To tackle such challenges, we propose an Attention Propagation Module (APM), which is based on the non-local attention mechanism <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59]</ref>, but extended to deal with spatio-temporal variations for the video semantic segmentation task. We now define how we integrate the APM into TDNet.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, TDNet is composed of two phases, the Encoding Phase and Segmentation Phase. The encoding phase extracts alternating sub-feature maps over time.</p><p>Rather than just generating the Value feature maps which contain the path-specific sub-feature groups, we also let the sub-networks produce Query and Key maps for building correlations between pixels across frames. Formally, the feature path-i produces a sub-feature map X i ∈ R C×H×W . Then, as in prior work <ref type="bibr" target="#b48">[49]</ref>, the corresponding encoding module "Encoding-i" converts X i into a value map V i ∈ R C×H×W , as well as lower dimensional query and key</p><formula xml:id="formula_0">maps Q i ∈ R C 8 ×H×W , K i ∈ R C 8 ×H×W with three 1×1 convolutional layers.</formula><p>In the segmentation phase, the goal is to produce segmentation results based on the full features recomposed from the outputs of sub-networks from previous frames. Assuming we have m (m=4 in <ref type="figure" target="#fig_2">Fig. 3</ref>) independent feature paths derived from video frames, and would like to build a full feature representation for frame t by combining the outputs of the previous m-1 frames with the current frame. We achieve this with spatio-temporal attention <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b50">51]</ref>, where we independently compute the Affinity between pixels of the current frame t and the previous m-1 frames.</p><formula xml:id="formula_1">Aff p = Sof tmax( Q t K p √ d k )<label>(1)</label></formula><p>where p indicates a previous frame and d k is the dimension of the Query and Key. Then, the sub-feature maps at the current frame and previous m-1 frames are merged as,</p><formula xml:id="formula_2">V t = V t + t−1 p=t−m+1 φ(Aff p V p )<label>(2)</label></formula><p>With this attention mechanism, we effectively capture the non-local correlation between pixels across frames, with time complexity of O((m − 1)d k H 2 W 2 ) for the affinity in Eq. 1. However, features for semantic segmentation are high resolution and Eq 2 incurs a high computation cost. To Since we circularly distribute sub-networks over sequential frames, any four-frame temporal window will cover a full set of the sub-networks. In order to segment frame t, we apply the attention propagation module to propagate and merge sub-feature maps previously extracted from (t-3, t-2, t-1) with the sub-feature map from t. For the next frame t+1, a full feature representation is aggregated by similarly reusing the sub-features extract at frames (t-2, t-1, t).</p><p>improve efficiency, we downsample the attention maps and propagate them over time.</p><p>Attention Downsampling. We adopt a simple yet effective strategy, which is to downsample the reference data as indicated by the "Downsampling" module in <ref type="figure" target="#fig_2">Fig. 3</ref>. Formally, when segmenting a frame T , we apply a spatial pooling operation γ n (·) with stride n to the previous m-1 frames' Query, Key, and Value maps,</p><formula xml:id="formula_3">q i = γ n (Q i ), k i = γ n (K i ), v i = γ n (V i )<label>(3)</label></formula><p>With these downsampled maps, the complexity for Eq. 2</p><formula xml:id="formula_4">decreases to O( (m−1)d k H 2 W 2 )<label>n 2</label></formula><p>). We conduct experiments and find that n=4 works well to preserve necessary spatial information while greatly decreasing the computational cost (see Sec 5.3).</p><p>Attention Propagation. Next, we propose a propagation approach, where instead of computing the attention between the current frame and all previous ones, we restrict computation to neighboring frames, and propagate it through the window. This allows us not only to reduce the number of attention maps we have to compute, but also to also restrict attention computation to subsequent frames, where motion is smaller. Given a time window composed of frames from t − m + 1 to t together their respective downsampled Query, Key, and Value maps, then for an intermediate frame p ∈ (t − m + 1, t), the attention is propagated as,</p><formula xml:id="formula_5">v p = φ Sof tmax( q p k p−1 √ d k )v p−1 + v p<label>(4)</label></formula><p>where v t−m+1 = γ n (V t−m+1 ), q, k, and v are the downsampled maps as in Eq. 3, d k is the number of dimensions for Query and Key, and φ p is a 1×1 convolutional layer. The final feature representation at frame t is then computed as,</p><formula xml:id="formula_6">V t = φ Sof tmax( Q t k t−1 √ d k )v t−1 + V t<label>(5)</label></formula><p>and segmentation maps are generated by:</p><formula xml:id="formula_7">S m = π m (V m ),</formula><p>where π m is the final prediction layer associated with subnetwork m.</p><p>With this proposed framework, the time complexity is re-</p><formula xml:id="formula_8">duced to O( (m−2)·d k H 2 W 2 ) n 4 + d k H 2 W 2 ) n 2 ) ≈ O( d k H 2 W 2 ) n 2 ).</formula><p>Since the attention is extracted from neighboring frames only, the resulting feature are also more robust to scene motion. We notice that recent work <ref type="bibr" target="#b61">[62]</ref> also adopt pooling operation to achieve efficient attention models, but this is in the context of image semantic segmentation, while our model extends this strategy to deal with video data.  <ref type="figure">Figure 4</ref>. The knowledge distillation. In the "Overall KD", we align the full outputs between the teacher model (e.g. PSPNet101) and the student model (e.g. out TDNet). In the "Grouped KD", we match the outputs based on only one sub-network to the teacher model's output conditioned on the respective feature subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Grouped Knowledge Distillation</head><p>During training, we further enhance the complementarity of sub-feature maps in the full feature space by introducing a knowledge distillation <ref type="bibr" target="#b14">[15]</ref> strategy, using a strong deep model designed for single images as the teacher network. In addition to transferring knowledge in the fullfeature space <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref>, we propose a grouped knowledge distillation loss to further transfer knowledge at the subspace level in order to make the information extracted from different paths more complementary to one another.</p><p>The idea of a grouped distillation loss is illustrated in <ref type="figure">Fig. 4</ref>. We take a deep baseline model like PSPNet101 as the teacher, and take our TDNet with m sub-networks as the student network. The goal is to not only align the output distributions at the whole-model level, but also at a subfeature group level. Based on block matrix multiplication <ref type="bibr" target="#b8">[9]</ref>, we evenly separate the teacher model's feature reduction layer into m independent sub-convolution groups, which output a set of sub-feature groups {f i |i = 1, ..., m}. Thus, the original segmentation result is π T ( f ), and the contribution of the i-th feature group is π T (f i ), given π T (·) being the teacher model's segmentation layer. In TDNet, the target frame's Value map V m is combined with propagated previous information to be V m , thus the full model output is π S (V m ) and the m-th feature path's contribution is π S (V m ), given π S (·) is the final segmentation layers. Based on these, our final loss function is,</p><formula xml:id="formula_9">Loss =CE(π S (V i , gt)) + α · KL(π S (V i )||π T ( f )) + β · KL(π S (V i )||π T (f i )) (6)</formula><p>where CE is the cross entropy loss, and KL means the KLdivergence. The first term is the supervised training with ground truth. The second term distills knowledge at the whole-model level. The third term transfers knowledge at feature group level. We set α and β to be 0.5 in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our method on Cityscapes <ref type="bibr" target="#b4">[5]</ref> and Camvid <ref type="bibr" target="#b0">[1]</ref> for street views, and NYUDv2 <ref type="bibr" target="#b32">[33]</ref> for indoor scenes. On all of these datasets, our method achieves stateof-the-art accuracy with a much faster speed and lower and evenly distributed latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setup and Implementation</head><p>Datasets &amp; Evaluation Metrics. Cityscapes [5] contains 2,975/500/1,525 snippets for training/validation/testing. The 20 th frame of each snippet is annotated with 19 classes for semantic segmentation. Camvid <ref type="bibr" target="#b0">[1]</ref> consists of 4 videos with 11-class pixelwise annotations at 1Hz. The annotated frames are grouped into 467/100/233 for training/validation/testing. NYUDv2 <ref type="bibr" target="#b32">[33]</ref> contains 518 indoor videos with 795 training frames and 654 testing frames being rectified and annotated with 40-class semantic labels. Based on these labeled frames, we create rectified video snippets from the raw Kinetic videos, which we will release for testing. Following the practice in previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>, we evaluate mean Intersection-over-Union (mIoU) on Cityscapes, and mean accuracy and mIoU on Camvid and NYUDv2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models &amp; Baselines.</head><p>We demonstrate the effectiveness of TDNet on different backbones. We select two stateof-the-art image segmentation models for our experiments: PSPNet <ref type="bibr" target="#b57">[58]</ref>, and BiseNet * <ref type="bibr" target="#b53">[54]</ref>. The latter is a modified/improved version of <ref type="bibr" target="#b53">[54]</ref> with the Spatial Path being replaced with the output of ResBlock-2, which we found to have higher efficiency and better training convergence. We extend these image models with temporally distributed framework to boost the performance, yielding the models: TD 2 -PSP50, TD 4 -PSP18: the former consists of two PSPNet-50 <ref type="bibr" target="#b57">[58]</ref> backbones with halved output channels as sub-networks, whereas TD 4 -PSP18 is made of four PSPNet-18 sub-networks. The model capacity of the temporally distributed models is comparable to the image segmentation network they are based on (PSPNet-101). TD 2 -Bise34, TD 4 -Bise18. Similarly, we build TD 2 -Bise34 with two BiseNet * -34 as sub-networks, and TD 4 -Bise18 with four BiseNet * -18 as sub-networks for the real-time applications. Like in PSPNet case, the model capacity of the temporally distributed networks is comparable to the BiseNet * -101.</p><p>Speed Measurement &amp; Comparison. All testing experiments are conducted with a batch-size of one on a single Titan Xp in the Pytorch framework. We found that previous methods are implemented with different deep-learning frameworks and evaluated on different types of devices, so for consistent comparisons, we report the speed/latency for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mIoU(%)</head><p>Speed Max Latency val test (ms/f) (ms) CLK <ref type="bibr" target="#b40">[41]</ref> 64. <ref type="bibr">4 -158 198</ref> DFF <ref type="bibr" target="#b59">[60]</ref> 69.2 -156 575 GRFP(5) <ref type="bibr" target="#b34">[35]</ref> 73.6 72.9 255 255 LVS-LLS <ref type="bibr" target="#b26">[27]</ref> 75.9 -119 119 PEARL <ref type="bibr" target="#b18">[19]</ref> 76.5 75.2 800 800 LVS <ref type="bibr" target="#b26">[27]</ref> 76.8 -171 380</p><p>PSPNet18 <ref type="bibr" target="#b57">[58]</ref> 75.5 -91 91</p><p>PSPNet50 <ref type="bibr" target="#b57">[58]</ref> 78. these previous methods based on benchmark-based conversions 1 and our reimplementations.</p><p>Training &amp; Testing Details. Both our models and baselines are initialized with Imagenet <ref type="bibr" target="#b5">[6]</ref> pretrained parameters and then trained to convergence to achieve the best performance. To train TDNet with m subnetworks, each training sample is composed of m consecutive frames and the supervision is the ground truth from the last one. We perform random cropping, random scaling and flipping for data augmentation. Networks are trained by stochastic gradient descent with momentum 0.9 and weight decay 5e-4 for 80k iterations. The learning rate is initialized as 0.01 and decayed by (1 − iter max−iter ) 0.9 . During testing, we resize the output to the input's original resolution for evaluation. On datasets like Cityscapes and NYUDv2 which have temporally sparse annotations, we compute the accuracy for all possible orders of sub-networks and average them as final results. We found that different orders of sub-networks achieve very similar mIoU values, which indicates that TDNet is stable with respect to sub-feature paths (see supplementary materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>Cityscapes Dataset. We compare our method with the recent state-of-the-art models for semantic video segmentation in <ref type="table">Table 1</ref>. Compared with LVS <ref type="bibr" target="#b26">[27]</ref>, TD 4 -PSP18, achieves similar performance with only a half the average time cost, and TD 2 -PSP50 further improves accuracy by 3 percent in terms of mIoU. Unlike keyframe-based methods like LVS <ref type="bibr" target="#b26">[27]</ref>, ClockNet <ref type="bibr" target="#b40">[41]</ref>, DFF <ref type="bibr" target="#b59">[60]</ref> that have fluctuating latency between keyframes and non-key frames (e.g. 575ms v.s. 156ms for DFF <ref type="bibr" target="#b59">[60]</ref>), our method runs with a balanced computation load over time. With a similar total number of parameters as PSPNet101 <ref type="bibr" target="#b57">[58]</ref>, TD 2 -PSP50 reduces the per-frame time cost by half from 360ms to <ref type="bibr" target="#b0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="63.2">-33</head><p>ICNet <ref type="bibr" target="#b56">[57]</ref> 67.7 69.5 20</p><p>LadderNet <ref type="bibr" target="#b20">[21]</ref> 72.8 -33</p><p>SwiftNet <ref type="bibr" target="#b36">[37]</ref> 75.4 -23</p><p>BiseNet * 18 <ref type="bibr" target="#b53">[54]</ref> 73.8 73. <ref type="bibr">5 20</ref> BiseNet * 34 <ref type="bibr" target="#b53">[54]</ref> 76.0 -27</p><p>BiseNet * 101 <ref type="bibr" target="#b53">[54]</ref> 76.5 -72</p><formula xml:id="formula_10">TD 4 -Bise18</formula><p>75.0 74.9 21</p><formula xml:id="formula_11">TD 2 -Bise34</formula><p>76.4 -26 <ref type="table">Table 2</ref>. Evaluation of high-efficiency approaches on the Cityscapes dataset.</p><p>178ms while improving accuracy. The sub-networks in TD 2 -PSP50 are adapted from PSPNet50, so we also compare their performance, and can see that TD 2 -PSP50 outperforms PSPNet50 by 1.8% mIoU with a faster average latency. As shown in the last row, TD 4 -PSP18 can further reduce the latency to a quarter, but due to the shallow subnetworks (based on a PSPNet18 model), the performance drops comparing to PSPNet101. However, it still achieves state-of-the-art accuracy and outperforms previous methods by a large gap in terms of latency. Some qualitative results are shown in <ref type="figure">Fig. 5(a)</ref> To validate our method's effectiveness for more realistic tasks, we evaluate our real-time models TD 2 -Bise34 and TD 4 -Bise18 ( <ref type="table">Table 2</ref>). As we can see, TD 2 -Bise34 outperforms all the previous real-time methods like ICNet <ref type="bibr" target="#b56">[57]</ref>, LadderNet <ref type="bibr" target="#b20">[21]</ref>, and SwiftNet <ref type="bibr" target="#b36">[37]</ref> by a large gap, at a comparable, real-time speed. With a similar total model size to BiseNet * 101, TD 2 -Bise34 achieves better performance while being roughly three times faster. TD 4 -Bise18 drops the accuracy but further improves the speed to nearly 50 FPS. Both TD 2 -Bise34 and TD 4 -Bise18 improve over their single path baselines at a similar time cost, which validates the effectiveness of our TDNet for real-time tasks.</p><p>Camvid Dataset. We also report the evaluation of Camvid dataset in <ref type="table">Table 3</ref>. We can see that TD 2 -PSP50 outperforms the previous state-of-the-art method Netwarp <ref type="bibr" target="#b9">[10]</ref> by about 9% mIoU while being roughly four times faster. Comparing to the PSPNet101 baselines with a similar model capacity, TD 2 -PSP50 reduces about half of the computation cost with comparable accuracy. The four-path version further reduces the latency by half but also decreases the accuracy. This again shows that a proper depth is necessary for feature path, although even so, TD 4 -PSP18 still outperforms previous methods with a large gap both in terms of mIoU and speed.</p><p>NYUDv2 Dataset. To show that our method is not limited to street-view like scenes, we also reorganize the indoor NYUDepth-v2 dataset to make it suitable for seman-Method mIoU(%) Mean Acc.(%) Speed(ms/f) LVS <ref type="bibr" target="#b26">[27]</ref> -82.9 84 PEARL <ref type="bibr" target="#b18">[19]</ref> -83.2 300 GRFP(5) <ref type="bibr" target="#b34">[35]</ref> 66.1 -230 ACCEL <ref type="bibr" target="#b17">[18]</ref> 66.7 -132</p><p>Netwarp <ref type="bibr" target="#b9">[10]</ref> 67.1 -363</p><p>PSPNet18 <ref type="bibr" target="#b57">[58]</ref> 71.0 78. <ref type="bibr">7 40</ref> PSPNet50 <ref type="bibr" target="#b57">[58]</ref> 74.7 81.5 100</p><p>PSPNet101 <ref type="bibr" target="#b57">[58]</ref> 76  <ref type="table">Table 5</ref>. The mIoU (%) for different components in our knowledge distillation loss (Eq. 6) for TD 4 -PSP18.</p><p>tic video segmentation task. As most previous methods for video semantic segmentation do not evaluate on this dataset, we only find one related work to compare against; STD2P <ref type="bibr" target="#b13">[14]</ref>. As shown in <ref type="table">Table 4</ref>, TD 2 -PSP50 outperforms STD2P in terms of both accuracy and speed. TD 4 -PSP18 achieves a worse accuracy but is more than 5× faster. TD 2 -PSP50 again successfully halves the latency but keeps the accuracy of the baseline PSPNet101, and also achieves about 1.6% improvement in mIoU comparing to PSPNet18 without increasing the latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Method Analysis</head><p>Grouped Knowledge Distillation. The knowledge distillation based training loss (Eq. 6) consistently helps to improve performance on the three datasets. In order to investigate the effect of different components in the loss, we train TD 4 -PSP18 with different settings and show the results in <ref type="table">Table 5</ref>. The overall knowledge distillation <ref type="bibr" target="#b14">[15]</ref> works by providing extra information about intra-class similarity and inter-class diversity. Thereby, it is less effective to improve a fully trained base model on Cityscapes due to the highlystructured contents and relatively fewer categories. However, when combined with our grouped knowledge distillation, the performance can be still boosted with nearly a half percent in terms of mIoU. This shows the effectiveness of our grouped knowledge distillation to provide extra regularization. On the NYUD-v2 dataset which contains more diverse scenes and more categories, our method achieves significant improvements with an 1.2% absolute improvement in mIoU.</p><p>Attention Propagation Module. Here, we compare our attention propagation module (APM) with other aggregation methods such as: no motion compensation, e.g., just adding feature groups (Add), optical-flow based warping (OFW) and the vanilla Spatio-Temporal Attention (STA) mechanism <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b50">51]</ref>. As shown in <ref type="figure" target="#fig_4">Fig. 6(a)</ref>, without considering the spatial misalignment (Add) leads to the worst accuracy. Our APM outperforms OFW and STA in both accuracy and latency. In <ref type="figure" target="#fig_4">Fig. 6(b)</ref>, we evaluate our method's robustness to motion between frames by varying the temporal step in input frames sampling. As shown in the figure, APM shows the best robustness, even with a sampling gap of 6 frames where flow based methods fail, our APM drops very slightly in contrast to other methods.</p><p>Attention Downsampling. In the downsampling operation used to improve the efficiency of computing attention, we apply spatial max pooling with a stride n. We show the influence of n in <ref type="table" target="#tab_3">Table 6</ref>. By increasing n from 1 to 4, the computation is decreased drastically, while the accuracy is fairly stable. This indicates that the downsampling strategy is effective in extracting spatial information in a sparse way. However, while further increasing n to 32, the accuracy decreases due to the information being too sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Subnetworks v.s. Independent Subnetworks.</head><p>When processing a video, the effectiveness of TDNet may come from two aspects: the enlarged representation capacity by distributed subnetworks and the temporal context information provided by neighboring frames. In <ref type="table">Table 7</ref>, we analyze the contributions of each by using a single subnetwork used for each path, or a group of independent subnetworks. As we can see, aggregating features ex-</p><formula xml:id="formula_12">frame t frame t-1 frame t-2 frame t-3 Target Frame TD 2 -PSP50 TD 4 -PSP18</formula><p>Ground Truth (a) Qualitative Results (b) Attention Visualization <ref type="figure">Figure 5</ref>. Qualitative results of our method on Cityscapes and NYUD-v2 (a), and a visualization of the attention map in our attentive propagation network (b). Given a pixel in frame t (denoted as a green cross), we back-propagate the correlation scores with the affinity matrices, and then visualize the normalized soft weights as heat map over the other frames in the window.  tracted via a shared single subnetwork can improve the performance of image segmentation baseline, and independent sub-networks can further improve mIoU by 1% without increasing computation cost. This shows that TDNet does not only benefit from the temporal context information but is also effectively enlarging the representation capacity by the temporally distributing distinct subnetworks.</p><p>Effect of Sub-networks. As shown in the last part, TD-Net benefits from enforcing different sub-networks extract complementary feature groups. Here, we provide detailed ablation studies about the contributions of these subnetworks. <ref type="table" target="#tab_4">Table 8</ref> shows the analysis for TD 4 -PSP18, where P4 represents the sub-network at the target frame, and P1∼P3 are the sub-networks applied on the previous frames. As we can see, by removing feature paths from the first frame, the accuracy consistently decreases for both datasets, which proves the effectiveness of feature distribution. To show how these paths are aggregated, in <ref type="figure">Fig 5(b)</ref> we visualize the attention maps of the attention propagation module in TD 4 -PSP18. As shown in the figure, given a pixel (denoted as green crosses) in the target frame t, pixels of the corresponding semantic category in the previous frame t-1 are matched. However, in the previous frames t-2 and t-3, background pixels are collected. It should be noted that in the attention propagation module, there are layers φ (in Eq. 4 and Eq. 5) which process the aggregated features. Thus frames t-2 and t-3 provide contextual information, and frames t-1 and t provide local object information, which are combined together to form strong and robust features for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a novel temporally distributed network for fast semantic video segmentation. By computing the feature maps across different frames and merging them with a novel attention propagation module, our method retains high accuracy while significantly improving the latency of processing video frames. We show that using a grouped knowledge distillation loss, further boost the performance. TDNet consistently outperforms previous methods in both accuracy and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>As opposed to applying a single deep model to segment each frame independently (a), in TDNet (b) we distribute feature extraction evenly across sequential frames to reduce redundant computation, and then aggregate them using the Attention Propagation Module (APM), to achieve strong features for accurate segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of TDNet with four sub-networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>TD 4 -PSP18 with different temporal aggregation methods on Cityscapes dataset. "APM" denotes our attention propagation module. "STA" represents spatio-temporal attention<ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b50">51]</ref>. "OFW" is the optical-flow [8] based fusion. "Add" means simply adding feature maps. P1 P2 P3 P4 Cityscapes NYUDepth-V2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Effect of different downsampling stride n on Cityscapes.</figDesc><table><row><cell cols="2">Model</cell><cell>n=1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>TD 2 -PSP50</cell><cell>mIoU (%) latency (ms)</cell><cell cols="6">80.0 80.0 79.9 79.8 79.6 79.1 251 205 178 175 170 169</cell></row><row><cell>TD 4 -PSP18</cell><cell>mIoU (%) latency (ms)</cell><cell cols="6">76.9 76.8 76.8 76.5 76.1 75.7 268 103 85 81 75 75</cell></row><row><cell>TD 4 -Bise18</cell><cell>mIoU (%) latency (ms)</cell><cell cols="6">75.0 75.0 75.0 74.8 74.7 74.4 140 31 21 19 18 18</cell></row><row><cell cols="8">Framework Single Path Baseline Shared Independent</cell></row><row><cell>TD 2 -PSP50 TD 4 -PSP18</cell><cell></cell><cell>78.2 75.5</cell><cell></cell><cell>78.5 75.7</cell><cell></cell><cell>79.9 76.8</cell></row><row><cell cols="8">Table 7. Comparisons on Cityscapes for using a shared sub-</cell></row><row><cell cols="8">network or independent sub-networks. The last column shows the</cell></row><row><cell cols="7">baseline model corresponding to TDNet's sub-network.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 .</head><label>8</label><figDesc>Ablation study on TD 4 -PSP18 showing how performance decreases with progressively fewer sub-features accumulated.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Kate Saenko for the useful discussions and suggestions. This work was supported in part by DARPA and NSF, and a gift funding from Adobe Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Massively parallel video networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorica</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Mazare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multiscale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Elementary matrix theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Whitley Eves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic video cnns through representation warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic multiscale filters for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge adaptation for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Std2p: Rgbd semantic segmentation using spatio-temporal data-driven pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep roots: Improving cnn efficiency with hierarchical filter groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yani</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accel: A corrective fusion network for efficient semantic segmentation on video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samvit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video scene parsing with predictive feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Haijie Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li Yongjun Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing Lu Jun</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ladder-style densenets for semantic segmentation of large natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Segvic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Krapac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature space optimization for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention-guided unified network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Low-latency video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yule</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structured knowledge distillation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Budget-aware deep semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Guided upsampling network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Mazzini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet Kohli Nathan</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Architecture search of dynamic cells for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic video segmentation by gated recurrent flow propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient video semantic segmentation with labels propagation and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Clockwork convnets for video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dagrecurrent neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<title level="m">Varun Jampani, and Sanja Fidler. Iccv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Normalized cut loss for weakly-supervised CNN segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, and Yuri Boykov</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>On regularized losses for weakly-supervised CNN segmentation</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Semantic video segmentation: Exploring inference efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngbae</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truong</forename><surname>Nguyen</surname></persName>
		</author>
		<editor>ISOCC. IEEE</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic video segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Syuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsuan-Kung</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">An empirical study of spatial attention mechanisms in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
