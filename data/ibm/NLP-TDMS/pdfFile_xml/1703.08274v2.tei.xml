<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
							<email>zpengfei@stu.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Shannxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
							<email>jlxing@nlpr.ia.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<email>wezeng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Shannxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
							<email>nnzheng@mail.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Shannxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skeleton-based human action recognition has recently attracted increasing attention due to the popularity of 3D skeleton data. One main challenge lies in the large view variations in captured human actions. We propose a novel view adaptation scheme to automatically regulate observation viewpoints during the occurrence of an action. Rather than re-positioning the skeletons based on a human defined prior criterion, we design a view adaptive recurrent neural network (RNN) with LSTM architecture, which enables the network itself to adapt to the most suitable observation viewpoints from end to end. Extensive experiment analyses show that the proposed view adaptive RNN model strives to (1) transform the skeletons of various views to much more consistent viewpoints and (2) maintain the continuity of the action rather than transforming every frame to the same position with the same body orientation. Our model achieves significant improvement over the state-of-the-art approaches on three benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognizing human actions has remained one of the most important and challenging problems in computer vision. Demands on human action recognition techniques are growing very fast and have expanded in many domains, such as visual surveillance, human-computer interaction, video indexing/retrieval, video summary, and video understanding <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Considering the differences in inputs, human action recognition can be categorized into color video-based and 3D skeleton-based approaches. While color video based human action recognition has been extensively studied over the past few decades, 3D skeleton based human representa- tion for action recognition has recently attracted a lot of research attention because of its high level representation and robustness to variations of viewpoints, appearances, and surrounding distractions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47]</ref>. Biological observations from the early seminal work of Johansson suggest that humans can recognize actions from just the motion of a few joints of the human body, even without appearance information <ref type="bibr" target="#b18">[19]</ref>. Besides, the prevalence of cost-effective depth cameras such as Microsoft Kinect <ref type="bibr" target="#b47">[48]</ref>, Intel RealSense <ref type="bibr" target="#b0">[1]</ref>, dual camera devices, and the advance of a powerful technique of human pose estimation from depth <ref type="bibr" target="#b33">[34]</ref> make 3D skeleton data easily obtainable. Like the many previous works listed in the survey paper <ref type="bibr" target="#b9">[10]</ref>, we focus on skeletonbased action recognition.</p><p>One of the main challenges in skeleton-based human action recognition is the complex viewpoint variations when capturing human action data. First, in a practical scenario, the capturing viewpoints of the camera differ among different sequences, e.g., the facing angle, position of the camera, resulting in large differences among skeleton representations. Second, the actor could conduct an action towards different orientations. Moreover, he/she may dynamically change his/her orientations as time goes on. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the skeleton representations of the same posture are rather different when captured from different viewpoints. In practice, the variation of the observation viewpoints makes action recognition a very challenging problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. Attempts have been made in previous works to overcome the view variations for robust action recognition <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8]</ref>. Most of these works, however, are designed for color videobased human recognition. The investigation of view invariance for skeleton-based human recognition, however, still remains under explored.</p><p>There are only a few attempts in previous works to consider the effect from view variations. A general treatment employs a pre-processing step to transform the 3D joint coordinates from the camera coordinate system to a personcentric coordinate system by placing the body center at the origin, followed by rotating the skeleton such that the body plane is parallel to the (x, y)-plane, to make the skeleton data invariant to absolute location, and the body orientation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. Such a pre-processing gains partial view-invariant. However, it also has many drawbacks. On one hand, it loses partial motion information, e.g., the moving trajectory and speed of the body center, and the changing dynamics of the body orientation. For example, the action of walking becomes walking in the same place and the action of dancing with body rotating becomes dancing with body facing a fixed orientation. On the other hand, the processing (i.e., translation, rotation) is not explicitly designed with the target of optimizing action recognition in mind but is based on human defined criteria, which reduces the space for exploiting optimal viewpoints. How to design a system which provides superior viewpoint for action recognition is still an under-explored problem, and warrants more investigation.</p><p>In this work, we address the view variation problem for high performance skeleton-based action recognition. Instead of processing the 3D skeletons based on human defined criteria for solving view variations, we propose a view adaptation scheme which automatically regulates the observation viewpoint at each frame to obtain the skeleton representation under the new view. Note that the regulation of the viewpoint of the camera is equivalent to the transformation of the skeleton to a new coordinate system. To this end, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we design a view adaptive RNN with LSTM architecture to learn and determine the appropriate viewpoints based on the input skeleton. The skeleton newly represented in the determined observation viewpoint is used for easier action recognition by a main LSTM network. With the objective of maximizing recognition performance, the entire network is end-to-end trained to encourage the view adaptation subnetwork to learn and determine suitable viewpoints.</p><p>To summarize, we make the following contributions.</p><p>• We propose a self-regulated view adaption scheme which re-positions the observation viewpoints dynamically to facilitate better recognition of the action from skeleton data.</p><p>• We integrate the proposed view adaption scheme into an end-to-end LSTM network which automatically determines the "best" observation viewpoints during recognition.</p><p>• We have made many observations and analyses of the results from the view adaptation model. We find that the proposed model automatically regulates the skeletons to more consistent observation viewpoints while maintaining the continuity of an action. Based on the above contributions, we present an end-to-end, high performance action recognition system. Extensive experiment analyses and evaluations demonstrate its strong ability to overcome the view variation problem, and its state-of-the-art performance on three benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">View Invariant Action Recognition</head><p>Human actions may be observed from arbitrary camera viewpoints in realistic scenes. This factor is a barrier for the development of efficient action recognition techniques. Researchers have paid much attention to this issue and designed view-invariant approaches for action recognition from color videos <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8]</ref>. One category of approaches requires multiple view videos for training <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25]</ref>. For example, the 3D histogram of Oriented Gradients based Bag of Words model <ref type="bibr" target="#b40">[41]</ref> is learned from all viewpoints of data to provide robustness to view changes. Another category of approaches designs view-invariant feature representations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3]</ref> like self-similarity descriptors <ref type="bibr" target="#b19">[20]</ref> or descriptions based on trajectory curvature <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3]</ref>. There is also a category of approaches that employ knowledge transfer-based models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b28">29]</ref>. They find a view independent latent space in which features from different views are directly comparable. Considering the different domains of the color videos and skeleton sequences, the approaches designed for color videos cannot be directly extended to skeleton-based action recognition.</p><p>As a comparison, the study of viewpoint influences on skeleton-based action recognition is under-explored. The commonly used strategies are monotonous where a preprocessing of skeleton is performed <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. Unfortunately, they result in the loss of partial relative motion information. Sequence-based pre-processing, which performs the same transformation on all frames with the parameters determined from the first frame so that the motion is invariant to the initial body position and initial orientation, can preserve motion information. However, since the human body is not rigid, the definition of the body plane by the joints of "hip", "shoulder", "neck" is not always suitable for the purpose of orientation alignment <ref type="bibr" target="#b39">[40]</ref>. After the alignment of such a defined body plane, a person who is bending over will have his/her legs obliquely upward. Wang et al. <ref type="bibr" target="#b39">[40]</ref> use only the up-right pose frames in a sequence to determine the body plane by averaging the rotation transformation. However, a sequence may not contain an up-right pose.</p><p>In contrast to the above works, we leverage a contentdependent view adaptation model to automatically learn and determine the suitable viewpoints for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">RNN for Skeleton-based Action Recognition</head><p>Earlier works used hand-crafted features for action recognition from the skeleton <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b44">45]</ref>. Many recent works leverage the Recurrent Neuron Networks to recognize human actions from raw skeleton input, with feature learning and temporal dynamic modeling achieved by the neuron networks. Du et al. <ref type="bibr" target="#b4">[5]</ref> proposes an end-to-end hierarchical RNN for action recognition which takes each body part as input to each RNN subnetwork and fuses the output of subnetworks hierarchically. Zhu et al. <ref type="bibr" target="#b50">[51]</ref> propose the automatic exploration of the co-occurrence of discriminative skeleton joints in an LSTM network using group sparse regularization. In the part aware LSTM model <ref type="bibr" target="#b30">[31]</ref>, the memory unit of the LSTM model is separated to part-based sub-cells to push the network towards learning long-term context representations for each individual part. To learn both the spatial and temporal relationships among joints, the spatial-temporal LSTM network extends the deep LSTM architecture to two concurrent domains, i.e., the temporal domain and the spatial domain <ref type="bibr" target="#b23">[24]</ref>. To further exploit joint discriminations, the spatial-temporal attention model <ref type="bibr" target="#b34">[35]</ref> further introduces the attention mechanism into the network to enable it to selectively focus on discriminative joints of the skeleton within one frame, and pay different levels of attention to the outputs from multiple frames.</p><p>Most of the above works take the center and orientation aligned skeletons as input to the RNNs, by using the human defined alignment criteria. In contrast, our model automatically determines the observation viewpoints and thus the skeleton representations for efficient action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RNN and LSTM Overview</head><p>Output To make the paper self-contained, in this section we briefly review the Recurrent Neural Network (RNN), and the RNN with Long Short-Term Memory (LSTM) <ref type="bibr" target="#b11">[12]</ref>, based on which our framework is built.</p><formula xml:id="formula_0">Input 1 t h t x t h θ (a) tanh tanh Input gate Output gate Cell Forget gate Output Input V V V 1 t h t x t c t o t i t h t f (b)</formula><p>RNN is a powerful model for sequential data modeling and feature extraction, which allows the previous information to persist <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>. <ref type="figure" target="#fig_2">Fig. 3 (a)</ref> shows an RNN neuron, where the output response h t at time step t is determined by the input x t and the hidden outputs from RNN themselves at the last time step h t−1 . However, such a standard RNN faces the vanishing gradient effect in practice <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9]</ref>, which is not very capable of handling long-term dependencies. The advanced RNN architecture of LSTM <ref type="bibr" target="#b11">[12]</ref> mitigates this problem. <ref type="figure" target="#fig_2">Fig. 3 (b)</ref> shows an LSTM neuron. The key to LSTM is the cell state c t , which is kind of like a conveyor belt <ref type="bibr" target="#b25">[26]</ref>. The removal of the previous information or addition of the current information to the cell state are regulated with linear interactions by the forget gate f t and the input gate i t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">View Adaptation Model using LSTM</head><p>We propose an end-to-end LSTM network with a view adaptation module for skeleton-based human action recog-nition. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the overall architecture of the proposed network, which consists of a View Adaptation Subnetwork and a Main LSTM Network. In the following subsections, we first formulate the problem of observation viewpoint regulation. Then we describe our proposed view adaptation network in detail, which is capable of adaptively determining the most suitable observation viewpoints frame by frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem Formulation</head><p>The raw 3D skeletons are recorded corresponding to the camera coordinate system (global coordinate system), with the origin located at the position of the camera sensor. To be insensitive to the initial position of an action and to facilitate our study, for each sequence, we translate the global coordinate system to the body center of the first frame as our new global coordinate system O. Note that the input skeleton V t to our system as in <ref type="figure" target="#fig_1">Fig. 2</ref> is the skeleton representation under this global coordinate system.</p><p>One can choose to observe an action from suitable views. Thanks to the availability of the 3D skeletons captured from a fixed view, it is possible to set up a movable virtual camera and observe the action from new observation viewpoints as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. With the skeleton at frame t reobserved from the movable virtual camera viewpoint (observation viewpoint), the skeleton can be transformed to a representation under the movable virtual camera coordinate system, which is also referred to as the observation coordi-</p><formula xml:id="formula_1">nate system O t .</formula><p>Given a skeleton sequence S with T frames, under the global coordinate system O, the j th skeleton joint on the t th frame is denoted as v t,j = [x t,j , y t,j , z t,j ] T , where t ∈ (1, · · · , T ), j ∈ (1, · · · , J), J denotes the total number of skeleton joints in a frame. We denote the set of joints in the t th frame as V t = {v t,1 , · · · , v t,J }.</p><p>For the t th frame, assume the movable virtual camera is placed at a suitable viewpoint, with the corresponding observation coordinate system obtained from a translation by d t ∈ R 3 , and a rotation of α t , β t , γ t radians anticlockwise around the X-axis, Y -axis, and Z-axis, respectively, of the global coordinate system. Therefore, the representation of the j th skeleton joint v t,j = [x t,j , y t,j , z t,j ] T of the t th frame under this</p><formula xml:id="formula_2">observation coordinate system O t is v t,j = [x t,j , y t,j , z t,j ] T = R t × (v t,j − d t ).</formula><p>(1)</p><p>R t can be represented as</p><formula xml:id="formula_3">R t = R x t,α × R y t,β × R z t,γ ,<label>(2)</label></formula><p>where R y t,γ denotes the coordinate transform for rotating the original coordinate system around the Y -axis by β t ra- dians anticlockwise, which is defined as</p><formula xml:id="formula_4">R y t,β =   cos(β t ) sin(β t ) 0 − sin(β t ) cos(β t ) 0 0 0 1   .<label>(3)</label></formula><p>Similarly, R x t,α and R z t,γ denote the coordinate transforms for rotating the original coordinate system around the Xaxis by α t radians, and around the Z-axis by γ t radians anticlockwise, respectively.</p><p>Note that all the skeleton joints in the t th frame share the same transform parameters, i.e., α t , β t , γ t , d t , considering that the changing of viewpoints is a rigid motion. Given these transform parameters, the skeleton representation V t = {v t,1 , · · · , v t,J } under the new observation coordinate can be obtained from <ref type="bibr" target="#b0">(1)</ref>. Besides, the viewpoints can vary for different frames. The key problem becomes how to determine the viewpoints of the movable virtual camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">View Adaptive Recurrent Neural Network</head><p>We use a View Adaptation Subnetwork to automatically determine the observation viewpoints, i.e., α, β, γ, d t (as discussed in section 4.1), and use a Main LSTM Network to learn the temporal dynamics and perform the feature abstractions from the view-regulated skeleton data for the action recognition, from end to end, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>View Adaptation Subnetwork. A regulation of observation viewpoint corresponds to the re-positioning of the movable virtual camera, which can be described by the translation and rotation of this virtual camera (observation coordination system). At a time slot corresponding to the t th frame, with the skeleton V t as input, two branches of LSTM subnetworks are utilized to learn the rotation parameters α t , β t , γ t to obtain the rotation matrix R t , and the translation vector d t , corresponding to the global coordinate system.</p><p>The branch of rotation subnetwork for learning rotation parameters consists of an LSTM layer, and a full connection (FC) layer. The rotation parameters are obtained as</p><formula xml:id="formula_5">[α t , β t , γ t ] T = W r h r t + b r ,<label>(4)</label></formula><p>where h r t ∈ R N ×1 is the hidden output vector of the LSTM layer with N denoting the number of LSTM neurons, W r ∈ R 3×N and b r ∈ R 3×1 denote the weight matrix and offset vector of the FC layer, respectively. With the rotation parameters, the rotation matrix R t is obtained by <ref type="bibr" target="#b1">(2)</ref>.</p><p>The branch of translation subnetwork for learning translation parameters consists of an LSTM layer, and a FC layer. The translation vector d t is calculated as</p><formula xml:id="formula_6">d t = W d h d t + b d ,<label>(5)</label></formula><p>where h d t ∈ R N ×1 is the hidden output vector of its LSTM layer, W d ∈ R 3×N and b d ∈ R 3×1 denotes the weight matrix and offset vector of the FC layer. Under the observation viewpoint of the t th frame, the representation of the skeleton V t is then obtained through <ref type="bibr" target="#b0">(1)</ref>.</p><p>Note that to obtain an efficient view adaptation subnetwork, we have experimented with many alternative designations and found the current design very efficient. First, we use separated LSTM layers for the rotation and translation model learning rather than using shared LSTM layers because the rotation and translation are different operations which are difficult to learn from the shared LSTM neurons. Second, we use the same skeleton input for both the rotation branch subnetwork and the translation branch subnetwork rather than taking the output of one branch (e.g., translation / rotation) as the input of another (e.g., rotation / translation). This is because the learning of the model under the consistent global coordinate system is easier.</p><p>Main LSTM Network. The LSTM network is capable of modeling long-term temporal dynamics and automatically learning feature representations. Similar to the designs in <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b34">35]</ref>, we build a main LSTM network by stacking three LSTM layers, followed by one FC layer with a SoftMax classifier. The number of neurons of the FC layer is equal to the number of action classes.</p><p>End-to-End Training. The entire network is end-to-end trainable. We use cross-entropy loss as the training loss <ref type="bibr" target="#b34">[35]</ref>. The gradients of loss flow back not only within each subnetwork, but also from the Main LSTM Network to the View Adaptation Subnetwork. Let us denote the loss backpropagated to the output of the View Adaptation Subnetwork by v t,j , where j ∈ (1, · · · , J) and J is the number of skeleton joints. Then, the loss back-propagated to the output of the branch for determining the translation vector of</p><formula xml:id="formula_7">d t is dt = j=J j=1 ∂v t,j ∂d t v t,j ,<label>(6)</label></formula><p>where denotes element-wise product. Similarly, the loss back-propagated to the output of the branch for determining the rotation parameters can be obtained. For example, the loss back-propagated to the output of β t is</p><formula xml:id="formula_8">βt = j=J j=1 ∂v t,j ∂R t ∂R t ∂β t v t,j .<label>(7)</label></formula><p>With the end-to-end training feasible, the view adaptation model is guided to select the suitable observation viewpoints for enhancing recognition accuracy. Our scheme has the following characteristics. Firstly, it automatically chooses the suitable observation viewpoints based on the contents, rather than using human predefined criteria. Secondly, the view adaptation model is optimized for the purpose of high accuracy recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment Results</head><p>We evaluate the effectiveness of our proposed view adaptation scheme on three benchmark datasets. In-depth analyses are made on the NTU dataset. To better understand the model, visualizations of the skeleton representations under the observation viewpoints are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Settings</head><p>NTU RGB+D Dataset (NTU) <ref type="bibr" target="#b30">[31]</ref>. This Kinect captured dataset is currently the largest dataset with RGB+D videos and skeleton data for human action recognition, with 56880 video samples. It contains 60 different action classes including daily actions, mutual, and health-related actions. Samples are captured from 17 setups of cameras, where in different setups, the height and distances of the cameras to the subjects are different. For each setup, the three cameras were located at the same height but from different horizontal angles: −45 o (camera 2), 0 o (camera 1), +45 o (camera 3). Each subject performed each action twice, once facing towards the left camera and once towards the right camera. Each subject has 25 joints. The standard evaluations include Cross-Subject (CS) evaluation, where the 40 subjects are split into training and testing groups, and Cross-View (CV) evaluation, where the samples of cameras 2 and 3 are used for training while those of camera 1 for testing. <ref type="bibr" target="#b45">[46]</ref>. This Kinect captured dataset is an interaction dataset with two subjects, containing 282 sequences of 8 classes with subject independent 5-fold cross validation. Each subject has 15 joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SBU Kinect Interaction Dataset (SBU)</head><p>SYSU 3D Human-Object Interaction Set (SYSU) <ref type="bibr" target="#b12">[13]</ref>. This Kinect captured dataset contains 12 actions performed by 40 subjects. It has 480 sequences. Each subject has 20 joints. We evaluate performance on two standard protocols <ref type="bibr" target="#b12">[13]</ref>. For setting-1, half of samples are used for training and the rest for testing for each activity. For setting-2, half of subjects are used for training and the rest for testing. 30-fold cross validation is utilized. Downsampling the sequences in temporal is performed on this dataset in considering that the maximum length of the sequences is high.</p><p>Implementation Details. We build our frameworks based on the platform of Keras <ref type="bibr" target="#b3">[4]</ref> toolbox with theano <ref type="bibr" target="#b37">[38]</ref>. Dropout <ref type="bibr" target="#b35">[36]</ref> with a probability of 0.5 is used to alleviate overfitting. Gradient clipping similar to <ref type="bibr" target="#b36">[37]</ref> is used by enforcing a hard constraint on the norm of the gradient (to not exceed 1) to avoid the exploding gradient problem. Adam <ref type="bibr" target="#b20">[21]</ref> is adapted to train all the networks, and the initial learning rate is set as 0.005.</p><p>In our network design, we use 100 LSTM neurons in each LSTM layer for the NTU and the SYSU datasets. To avoid overfitting, we use 50 LSTM neurons in each LSTM layer for the SBU dataset, which has much smaller numbers of training samples than that of the NTU and the SYSU datasets. We set the batch sizes for the NTU, SYSU, and SBU dataset to 256, 64, and 8, respectively. For the View Adaptation Subnetwork, we initialize the full connection layer parameters to zeros for efficient training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons to Other State-of-the-Art</head><p>We show the performance comparisons of our proposed view adaptation scheme (VA-LSTM) with other state-of-theart approaches in <ref type="table" target="#tab_0">Table 1</ref>, <ref type="table" target="#tab_1">Table 2, and Table 3</ref> for the NTU, SBU and SYSU datasets, respectively. We can see that our scheme significantly outperforms the state-of-theart approaches by about 6%, 4%, 1% in accuracy for the NTU, SBU, SYSU dataset respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Efficiency of the View Adaptation Model</head><p>To validate the effectiveness of the proposed view adaptation model, we make two sets of comparisons as summarized in <ref type="table" target="#tab_3">Table 4</ref>. One set of comparisons evaluates the efficiency among the different pre-processing based methods and our proposed scheme. Another set of results evaluates the efficiency of the view adaptation models.</p><p>VA-LSTM is our proposed final view adaptation scheme which automatically regulates the observation viewpoints in the network. This is the scheme where both the translation and rotation branches are connected, i.e., the switch s rota   <ref type="bibr" target="#b45">[46]</ref> 49.7 Joint feature <ref type="bibr" target="#b45">[46]</ref> 80.3 Raw skeleton <ref type="bibr" target="#b16">[17]</ref> 79.4 Joint feature <ref type="bibr" target="#b16">[17]</ref> 86.9 HBRNN-L <ref type="bibr" target="#b4">[5]</ref> 80.4 Co-occurrence RNN <ref type="bibr" target="#b50">[51]</ref> 90.4 STA-LSTM <ref type="bibr" target="#b34">[35]</ref> 91.5 ST-LSTM (Tree Traversal) + Trust Gate <ref type="bibr" target="#b23">[24]</ref> 93.3 VA-LSTM 97.2 and s trans are on as in <ref type="figure" target="#fig_1">Fig. 2</ref>. VA-trans-LSTM is our scheme which only allows the translation of the viewpoint, i.e., the switch s rota is off while s trans is on. In comparison, S-trans+LSTM is our baseline scheme without enabling the view adaptation model, i.e., the switch s rota and s trans are both off, where V t = V t . Note that the input V t is the same as that of our view adaptation schemes, where the global coordinate system is moved to the body center of the first frame for the entire sequence to be insensitive to the initial position (see section 4.1). We refer to this pre-processing as sequence level translation, i.e., S-trans. VA-rota-LSTM is our scheme which only allows the rotation of the viewpoints, i.e., the switch s rota is on while s trans is off. From <ref type="table" target="#tab_3">Table 4</ref>, we observe that the proposed final view adaptation scheme outperforms the baseline scheme S-trans+LSTM by 3.4% and 5.3% in accuracy for CS and CV settings, respectively, thanks to the introduction of the proposed view adaptation module.</p><p>One may wonder how the performance is when using the pre-processed skeletons, basing on the widely used human defined processing criteria, before inputing to the Main LSTM Network. Such pre-processings can be considered as the human defined rules for determining the viewpoints. We name the pre-processing based schemes in the manner of C+LSTM, where C indicates the pre-processing strategy, e.g., F-trans+LSTM. The 3 rd to 7 th rows show the perfor- mance of schemes using different pre-processing strategies. F-trans means performing frame level translation to have the body center at the coordinate system origin for each frame. S-rota means the sequence level rotation with the rotation parameters calculated from the first frame, which is to fix the X-axis to be parallel to the vector from "left shoulder" to "right shoulder", Y -axis to be parallel to the vector from "spline base" to "spine", and Z-axis as the new X ×Y . Similarly, F-rota means the frame level rotation. F-trans&amp;F-rota means both F-trans and F-rota are performed, which is similar to the pre-processing in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. The scheme Raw+LSTM in the 2 nd row denotes a scheme which uses the original skeleton without any pre-processing as the input to the Main LSTM Network. Note that for 3D skeletons, the distance of a subject to the camera does not influence the scale of the skeletons. Therefore, the scaling operation is not considered in our framework. From the comparisons in <ref type="table" target="#tab_3">Table 4</ref>, we have the following observations and conclusions. (1) Our final scheme significantly outperforms the commonly used pre-processing strategies. In comparison with F-trans&amp;F-rota+LSTM <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>, our scheme achieves improvement by 5.3% and 3.7% in accuracy for CS and CV settings, respectively. In comparison with S-trans&amp;S-rota+LSTM, our scheme achieves improvement by 3.0% and 2.2% in accuracy. <ref type="bibr" target="#b1">(2)</ref> When only the rotation (or the translation) is allowed for adjusting the viewpoints, our scheme still consistently outperforms the schemes with human defined rotation (or translation) pre-processing. (3) Frame level pre-processing is inferior to the sequence level pre-processing, because the former loses more information, e.g., the motion across frames. (4) Being insensitive to the initial position of an action, S-trans+LSTM significantly outperforms the scheme with raw skeletons as input Raw+LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Visualization of the Learned Views</head><p>At each frame, the view adaptation subnetwork determines the observation viewpoint (by re-localizing the virtual movable camera) and then transforms the input skeleton V t to the representation V t in the new viewpoint for optimizing recognition performance. We visualize the representations V t and V t for better understanding of our model. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the skeletons from different sequences captured from different viewpoints of the same posture. Interestingly, the transformed skeletons (green) of various viewpoints have much more consistent viewpoints, i.e., frontal viewpoint here. Another example is shown in <ref type="figure" target="#fig_4">Fig. 6</ref> with the skeleton frames of the same action performed by differ- <ref type="figure">Figure 5</ref>: Frames of the same posture captured from different viewpoints for the same subject. 2 nd row: original skeletons. 3 rd row: skeleton representations from the observation viewpoints of our model. Note the third skeleton is very noisy due to occlusion during Kinect shooting.  <ref type="figure">Figure 7</ref>: Frames from sequences of actions: (a) "bow"; (b) "staggering". 2 nd row: original skeleton. 3 rd row: skeleton after the pre-processing with F-trans&amp;F-rota. 4 th row: skeleton representation from the observation viewpoints of our model. ent subjects. We can see that they are transformed to similar viewpoints. A similar phenomenon is observed in different actions and sequences.</p><p>To visualize the skeleton representations in the sequence along time, we show some frames of an action under the original and new observation viewpoints in <ref type="figure">Fig. 7</ref>. We can see that after our view adaptation model is applied, the subjects even for different actions are oriented toward a more consistent view. Different from frame level pre-processing (as in the 3 rd row), the transformed skeletons among frames are continuous and looks much natural. In <ref type="figure">Fig. 7</ref> (a) of action "bow", the orientation of the body after the processing of our model is parallel to X-axis while the legs after frame level pre-processing becomes obliquely upward. In <ref type="figure">Fig. 7</ref> (b) of action "staggering", the position changes of the subject after the processing of our model remain while such motion is lost for the pre-processing results.</p><p>From observations, we find that the learned view adaptation model tends to (1) regulate the observation viewpoints to present the subjects as if observed in a consistent viewpoint cross sequences and actions; (2) maintain the continuity of an action without losing much of the relative motions.</p><p>Optimized with the target of maximizing the recognition performance, the proposed view adaptation model is much effective in choosing the suitable viewpoints. The consistency of viewpoints for various actions/subjects overcomes the challenge caused by the diversity of viewpoints in video capturing, enabling the network to focus on the learning of action-specific features. Besides, unlike some pre-processing strategy, the valuable motion information is preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present an end-to-end view adaptation model for human action recognition from skeleton data. Instead of following the human predefined criterion to re-position the skeletons for action recognition, our network is capable of regulating the observation viewpoints to the suitable ones by itself, with the optimization target of maximizing recognition performance. It overcomes the limitations of the human defined pre-processing approaches by exploiting the optimal viewpoints through the content dependent recurrent neuron network model. Experiment results demonstrate that the proposed model can significantly improve the recognition performance on three benchmark datasets and achieve state-of-the-art results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Skeleton representations of the same posture captured from different viewpoints (different camera position, angle, and the subject orientation) are very different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of our end-to-end view adaptive RNN, which consists of a View Adaptation Subnetwork, and a Main LSTM Network. The View Adaptation Subnetwork determines the suitable observation viewpoint at each time slot. With the skeleton representations under the new observation viewpoints, the main LSTM network determines the action class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Structures of the neurons. (a) RNN; (b) LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the regulation of the observation viewpoint (movable virtual camera). A skeleton sequence is a record of the skeletons from the first frame f = 1 to the last frame f = T under the global coordinate system O. The action can be re-observed by a movable virtual camera under the observation coordinate systems. For the t th frame, the observation coordinate system is at a new position d t with a rotation of α t , β t , γ t radians anticlockwise around the X-axis, Y -axis, and Z-axis, respectively, corresponding to the global coordinate system. The skeleton can then be represented under this observation coordinate system O t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Frames of the same action "drinking" captured from different viewpoints for different subjects. 2 nd row: original skeletons. 3 rd row: skeleton representations from the observation viewpoints of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons on the NTU dataset with Cross-Subject and Cross-View settings in accuracy (%).</figDesc><table><row><cell>Methods</cell><cell>CS</cell><cell>CV</cell></row><row><cell>Skeleton Quads [6]</cell><cell cols="2">38.6 41.4</cell></row><row><cell>Lie Group [39]</cell><cell cols="2">50.1 52.8</cell></row><row><cell>Dynamic Skeletons [13]</cell><cell cols="2">60.2 65.2</cell></row><row><cell>HBRNN-L [5]</cell><cell cols="2">59.1 64.0</cell></row><row><cell>Part-aware LSTM [31]</cell><cell cols="2">62.9 70.3</cell></row><row><cell cols="3">ST-LSTM (Tree Traversal) + Trust Gate [24] 69.2 77.7</cell></row><row><cell>STA-LSTM [35]</cell><cell cols="2">73.4 81.2</cell></row><row><cell>VA-LSTM</cell><cell cols="2">79.4 87.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons on the SBU dataset in accuracy (%).</figDesc><table><row><cell>Methods</cell><cell>Acc. (%)</cell></row><row><cell>Raw skeleton</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparisons on the SYSU dataset in accuracy (%).</figDesc><table><row><cell>Methods</cell><cell>setting-1</cell><cell>setting-2</cell></row><row><cell>LAFF [14]</cell><cell>-</cell><cell>54.2</cell></row><row><cell>Dynamic Skeletons [13]</cell><cell>75.5</cell><cell>76.9</cell></row><row><cell>VA-LSTM</cell><cell>76.9</cell><cell>77.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of pre-processing methods and our view adaptation model on the NTU dataset in accuracy (%).</figDesc><table><row><cell></cell><cell>Methods</cell><cell>CS</cell><cell>CV</cell></row><row><cell>wo/ pre-proc.</cell><cell>Raw + LSTM</cell><cell cols="2">66.3 73.4</cell></row><row><cell></cell><cell>S-trans + LSTM</cell><cell cols="2">76.0 82.3</cell></row><row><cell></cell><cell>F-trans + LSTM</cell><cell cols="2">75.1 80.5</cell></row><row><cell>Pre-proc.</cell><cell>S-trans&amp;S-rota + LSTM S-trans&amp;F-rota + LSTM</cell><cell cols="2">76.4 85.4 75.0 85.1</cell></row><row><cell></cell><cell>F-trans&amp;F-rota + LSTM</cell><cell cols="2">74.1 83.9</cell></row><row><cell></cell><cell>VA-trans-LSTM</cell><cell cols="2">77.7 84.9</cell></row><row><cell>View adap.</cell><cell>VA-rota-LSTM VA-LSTM</cell><cell cols="2">79.4 87.1 79.4 87.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Junliang Xing is partly supported by the Natural Science Foundation of China (Grant No. 61672519), Jianru Xue is partly supported by National Key Research and Development Plan 2016YFB1001004.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel Realsense</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/realsense" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human activity recognition from 3d data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="70" to="80" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Viewinvariant motion trajectory-based activity classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">I</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Khokhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schonfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="54" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4513" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to recognize activities from the wrong view point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tabrizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="154" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">View-invariant human action recognition via robust locally adaptive multi-view learning. Frontiers of Information Technology &amp; Electronic Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="917" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01006</idno>
		<title level="m">Space-time representation of people based on 3D skeletal data: A review</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A field guide to dynamical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for RGB-D activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time RGB-D activity prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">View-invariant action recognition based on artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iosifidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="412" to="424" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Advances in view-invariant human motion analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive body part contrast mining for human interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Informative joints based human action recognition using skeleton contexts. Signal Processing: Image Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="29" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for it is analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception and Psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-view action recognition from temporal self-similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="293" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative virtual views for crossview action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2855" to="2862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via view knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3209" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07043</idno>
		<title level="m">Spatio-temporal LSTM with trust gates for 3D human action recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Latent multitask learning for view-invariant action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3128" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lstm</surname></persName>
		</author>
		<ptr target="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d skeleton-based human action classification: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Presti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">La</forename><surname>Cascia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="130" to="147" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2458" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">View-invariance in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="316" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">View-invariant action recognition using fundamental ratios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">View-invariant action recognition from point triplets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1898" to="1905" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An end-toend spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3D skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Action Recognition with Depth Cameras</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="11" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Making action recognition robust to occlusions and viewpoint changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Özuysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="635" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey of visionbased methods for action representation, segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyerc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">View-invariant action recognition using latent kernelized structural svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="411" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cross-view action recognition over heterogeneous feature spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3D joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-person interaction detection using bodypose features and multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Honorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rgb-d-based action recognition datasets: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="86" to="105" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Mul-tiMedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via a continuous virtual path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2690" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning view-invariant sparse representations for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3176" to="3183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3697" to="3703" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
