<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Attentive Pairwise Interaction for Fine-Grained Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqin</forename><surname>Zhuang</surname></persName>
							<email>pq.zhuang@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT Branch</orgName>
								<orgName type="department" key="dep2">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
							<email>yl.wang@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT Branch</orgName>
								<orgName type="department" key="dep2">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT Branch</orgName>
								<orgName type="department" key="dep2">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Attentive Pairwise Interaction for Fine-Grained Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained classification is a challenging problem, due to subtle differences among highly-confused categories. Most approaches address this difficulty by learning discriminative representation of individual input image. On the other hand, humans can effectively identify contrastive clues by comparing image pairs. Inspired by this fact, this paper proposes a simple but effective Attentive Pairwise Interaction Network (API-Net), which can progressively recognize a pair of fine-grained images by interaction. Specifically, API-Net first learns a mutual feature vector to capture semantic differences in the input pair. It then compares this mutual vector with individual vectors to generate gates for each input image. These distinct gate vectors inherit mutual context on semantic differences, which allow API-Net to attentively capture contrastive clues by pairwise interaction between two images. Additionally, we train API-Net in an end-to-end manner with a score ranking regularization, which can further generalize API-Net by taking feature priorities into account. We conduct extensive experiments on five popular benchmarks in fine-grained classification. API-Net outperforms the recent SOTA methods, i.e., CUB-200-2011 (90.0%), Aircraft (93.9%), Stanford Cars (95.3%), Stanford Dogs (90.3%), and NABirds (88.1%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past years, CNNs have achieved remarkable successes for visual recognition <ref type="bibr" target="#b6">(He et al. 2016;</ref><ref type="bibr" target="#b6">Huang et al. 2017</ref>). However, these classical models are often limited to distinguish fine-grained categories, due to highly-confused appearances. Subsequently, a number of fine-grained frameworks have been proposed by finding key part regions , learning patch relations <ref type="bibr" target="#b16">(Lin, RoyChowdhury, and Maji 2015;</ref><ref type="bibr" target="#b0">Cai, Zuo, and Zhang 2017;</ref><ref type="bibr" target="#b26">Yu et al. 2018)</ref>, etc. But most of them take individual image as input, which may limit their ability to identify contrastive clues from different images for fine-grained classification. . Caspian Tern and Elegant Tern are two highly-confused bird species. Humans often distinguish them by pairwise comparison, instead of checking each individual image alone. First, humans would exploit contrastive clues (e.g. body and mouth) from the image pair, and then check each image with these mutual contexts to further discover distinct attentions on each image. Finally, humans recognize both images via comparing subtle differences jointly. To mimic this capacity, we propose a novel API-Net. More details can be found in Section 1.</p><p>On the contrary, humans often recognize fine-grained objects by comparing image pairs (Bruner 2017), instead of checking single images alone. For example, Caspian Tern and Elegant Tern are two highly-confused bird species, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. If we only check individual image alone, it is difficult to recognize which categories it belongs to, especially when the bird is self-occluded with a noisy back-ground. Alternatively, we often take a pair of images together, and summarize contrastive visual appearances as context, e.g., body, mouth and etc. Then, we check individual images with these mutual contexts, so that we can further understand distinct aspects of each image, e.g., the body of the bird is an important part for the left image, while the mouth of the bird is a key characteristic for the right image. With such discriminative guidance, we pay different attentions to the body and the mouth of two birds. Note that, for each bird in the pair, we not only check its prominent part but also have a glance at the distinct part found from the other bird. This comparative interaction can effectively tell us that, Caspian Tern has a fatter body, while Elegant Tern has a more curved mouth. Consequently, we recognize both images jointly.</p><p>To mimic this capacity of human beings, we introduce a novel Attentive Pairwise Interaction Network (API-Net) for fine-grained classification. It can adaptively discover contrastive clues from a pair of fine-grained images, and attentively distinguish them via pair interaction.</p><p>More specifically, API can effectively recognize two finegrained images, by a progressive comparison procedure like human beings. To achieve this goal, API-Net consists of three submodules, i.e., mutual vector learning, gate vector generation, and pairwise interaction. By taking a pair of finegrained images as input, API-Net first learns a mutual vector to summarize contrastive clues of input pair as context. Then, it compares mutual vector with individual vectors. This allows API-Net to generate distinct gates, which can effectively highlight semantic differences respectively from the view of each individual image.</p><p>Consequently, API-Net uses these gates as discriminative attentions to perform pairwise interaction. In this case, each image can generate two enhanced feature vectors, which are activated respectively from its own gate vector and the gate vector of the other image in the pair. Via an end-to-end training manner with a score-ranking regularization, API-Net can promote the discriminative ability of all these features jointly with different priorities. Additionally, it is worth mentioning that, one can easily embed API into any CNN backbones for fine-grained classification, and flexibly unload it for single-input test images without loss of generalization capacity. Such plug-and-play property makes API as a preferable choice in practice. Finally, we evaluate API-Net on five popular benchmarks in fine-grained recognition, namely CUB-200-2011, Aircraft, Stanford Cars, Stanford Dogs and NABirds. The extensive results show that, API-Net achieves the state-of-the-art performance on all these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>A number of research works have been recently proposed for fine-grained classification. In the following, we mainly summarize and discuss those related works.</p><p>Object Parts Localization. These approaches mainly utilize the pre-defined bounding boxes or part annotations to capture visual details in the local regions . However, collecting such annotations is often labor-intensive or infeasible in practice. Hence, several weakly-supervised localization approaches have been recently proposed by designing complex spatial attention mechanisms (e.g., RA-CNN , MA-CNN ), learning a bank of discriminative filters <ref type="bibr" target="#b23">(Wang, Morariu, and Davis 2018)</ref>, guiding region detection with multi-agent cooperation , etc. But these approaches mainly focus on mining local characteristics of fine-grained images. Consequently, they may lack the capacity of discriminative feature learning.</p><p>Discriminative Feature Learning. To learn the representative features, many approaches have been exploited by patch interactions <ref type="bibr" target="#b16">(Lin, RoyChowdhury, and Maji 2015;</ref><ref type="bibr" target="#b0">Cai, Zuo, and Zhang 2017;</ref><ref type="bibr" target="#b26">Yu et al. 2018)</ref>. A well-known approach is B-CNN <ref type="bibr" target="#b16">(Lin, RoyChowdhury, and Maji 2015)</ref>, which performs bilinear pooling on the representations of two local patches in an image. Following this direction, several high-order approaches have been proposed via polynomial kernel formulation (Cai, Zuo, and Zhang 2017), crosslayer bilinear representation <ref type="bibr" target="#b26">(Yu et al. 2018)</ref>, etc. However, these approaches take a single image alone as input, while neglecting comparisons between different images, i.e., an important clue to distinguish highly-confused objects.</p><p>Metric Learning. Metric learning refers to the method that uses similarity measurements to model relations between image pairs <ref type="bibr">(Kulis and others 2013)</ref>. It has been widely used in Face Verification <ref type="bibr" target="#b17">(Schroff, Kalenichenko, and Philbin 2015)</ref>, Person ReID (Hermans, Beyer, and Leibe 2017) and so on. Recently, it has been introduced for finegrained classification, e.g., triplet loss design <ref type="bibr" target="#b28">(Zhang et al. 2016a)</ref>, pairwise confusion regularization <ref type="bibr" target="#b3">(Dubey et al. 2018a</ref>), multi-attention multi-class constraint (Sun et al. 2018), etc. However, these approaches mainly leverage metric learning to improve sample distributions in the feature space. Hence, they often lack the adaptation capacity, w.r.t., how to discover visual differences between a pair of images.</p><p>Different from previous approaches, our API-Net can adaptively summarize contrastive clues, by learning a mutual vector from an image pair. As a result, we can leverage it as guidance, and attentively distinguish two fine-grained images via pairwise interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Attentive Pairwise Interaction</head><p>In this section, we describe Attentive Pairwise Interaction Network (API-Net) for fine-grained classification. Our design is partially inspired by the observation that, instead of learning visual concepts alone with individual image, humans often compare a pair of images jointly to distinguish subtle differences between similar objects (Bruner 2017).</p><p>To imitate this capacity, we simultaneously take a pair of images as input to API-Net, and progressively distinguish them by three elaborative submodules, i.e., mutual vector learning, gate vector generation, and pairwise interaction. The whole framework is demonstrated in <ref type="figure">Fig. 2</ref>.</p><p>Mutual Vector Learning. First, we feed two fine-grained images into a CNN backbone, and extract their D-dimension <ref type="figure">Figure 2</ref>: The framework of API-Net (Best view in color). API can progressively recognize a pair of fine-grained images, based on a novel human-like learning procedure. It consists of three submodules. 1. Mutual Vector Learning. API learns a mutual vector x m from individual x 1 and x 2 (Eq. 1). In this case, it can summarize contrastive cues in the pair. 2. Gate Vector Generation. API further compares x m with x 1 and x 2 , and generates two distinct gate vectors g i (Eq. 2). These gates allow API to discover distinct clues respectively from the view of each individual image. 3. Pairwise Interaction. API performs pairwise interaction with guidance of gate vectors (Eq. 3-6). Via training API-Net with a score-ranking regularization, we can distinguish all these features jointly with the consideration of feature priorities (Eq. 8-10). Additionally, API is a practical plug-and-play module, i.e., one can combine API with CNNs during training, and flexibly unload it for single-input test images. feature vectors respectively, i.e., x 1 and x 2 ∈ R D . Then, we learn a mutual vector x m ∈ R D from individual x 1 and x 2 ,</p><formula xml:id="formula_0">x m = f m ([x 1 , x 2 ]),<label>(1)</label></formula><p>where f m (·) is a mapping function of [x 1 , x 2 ], e.g., a simple MLP works well in our experiments. Since x m is adaptively summarized from both x 1 and x 2 , it often contains feature channels which indicate high-level contrastive clues (e.g. body and mouth of two birds in <ref type="figure" target="#fig_0">Fig.1</ref>) in the pair. Gate Vector Generation. After learning the mutual vector x m , we propose to compare it with x 1 and x 2 . The main reason is that, we should further generate distinct clues respectively from the view of each individual image, in order to distinguish this pair afterwards.</p><p>In particular, we perform channel-wise product between x m and x i , so that we can leverage x m as guidance to find which channels of individual x i may contain contrastive clues. Then, we add a sigmoid function to generate the gate vector g i ∈ R D ,</p><formula xml:id="formula_1">g i = sigmoid(x m x i ), i ∈ {1, 2}.</formula><p>(2)</p><p>As a result, g i becomes a discriminative attention which highlights semantic differences with a distinct view of each individual x i , e.g., the body of the bird is important in one image of <ref type="figure" target="#fig_0">Fig.1</ref>, while the mouth of the bird is a key in the other image. In our experiments, we evaluate different gating strategies and show effectiveness of our design. Pairwise Interaction. Next, we perform pairwise interaction by gate vectors. Our design is partially motivated by the fact that, to capture subtle differences in a pair of finegrained images, human check each image not only with its prominent parts but also with distinct parts from the other image. For this reason, we introduce an interaction mechanism via residual attention,</p><formula xml:id="formula_2">x self 1 = x 1 + x 1 g 1 ,<label>(3)</label></formula><p>x self</p><formula xml:id="formula_3">2 = x 2 + x 2 g 2 ,<label>(4)</label></formula><p>x other</p><formula xml:id="formula_4">1 = x 1 + x 1 g 2 ,<label>(5)</label></formula><p>x other 2 = x 2 + x 2 g 1 .</p><p>As we can see, each individual feature x i in the pair produces two attentive feature vectors, i.e., x self i ∈ R D is highlighted by its own gate vector, and x other i ∈ R D is activated by the gate vector of the other image in the pair. In this case, we enhance x i with discriminative clues that come from both images. Via distinguishing all these features jointly, we can reduce confusion in this fine-grained pair.</p><p>Training. After obtaining four attentive features x j i in the pair (where i ∈ {1, 2}, j ∈ {self, other}), we feed them into a sof tmax classifier,</p><formula xml:id="formula_6">p j i = sof tmax(Wx j i + b),<label>(7)</label></formula><p>where p j i ∈ R C is the prediction score vector, C is the number of object categories, and {W, b} is the parameter set of classifier. To train the whole API-Net effectively, we design the following loss L for a pair,</p><formula xml:id="formula_7">L = L ce + λL rk ,<label>(8)</label></formula><p>where L ce is a cross entropy loss, and L rk is a score ranking regularization with coefficient λ. 1) Cross Entropy Loss. The main loss is the cross entropy loss L ce ,</p><formula xml:id="formula_8">L ce = − i∈{1,2} j∈{self,other} y i log(p j i ),<label>(9)</label></formula><p>where y i is the one-hot label vector for image i in the pair, denotes the vector transposition. By using this loss, API-Net can gradually recognize all the attentive features x j i , with supervision of label y i .</p><p>2) Score Ranking Regularization. Additionally, we introduce a hinge loss as the score ranking regularization L rk , <ref type="formula" target="#formula_0">(10)</ref> where p j i (c i ) ∈ R is the score obtained from the prediction vector p j i , and c i is the corresponding index associated with the ground truth label of image i. Our motivation of this design is that, x self i is activated by its own gate vector. Hence, it should be more discriminative to recognize the corresponding image, compared with x other i .</p><formula xml:id="formula_9">L rk = i∈{1,2} max(0, p other i (c i )−p self i (c i )+ ),</formula><p>To take this knowledge into account, we use L rk to promote the priority of x self i , i.e., the score difference</p><formula xml:id="formula_10">p self i (c i )−p other i (c i ) should be larger than a margin .</formula><p>With such a regularization, API-Net learns to recognize each image in the pair by adaptively taking feature priorities into account.</p><p>3) Pair Construction. Eq. (8) defines the loss for a training pair. Next, we explain how to construct multiple pairs in a batch for end-to-end training. Specifically, we randomly sample N cl classes in a batch. For each class, we randomly sample N im training images. Then, we feed all these images into CNN backbone to generate their feature vectors. For each image, we compare its feature with others in the batch, according to Euclidean distance. As a result, we can construct two pairs for each image, i.e., the intra / inter pair consists of its feature and its most similar feature from intra / inter classes in the batch. This design allows our API-Net for learning to distinguish which are highly-confused or truly-similar pairs. We also investigate different construction strategies in our experiments. Consequently, there are 2 × N cl × N im pairs in each batch. We pass them into our API module, and summarize the loss L over all these pairs for end-to-end training.</p><p>Testing. We would like to emphasize that, API is a practical plug-and-play module for fine-grained classification. In the training phase, this module can summarize contrastive clues from a pair, which can gradually generalize the discriminative power of CNN representations for each individual image. Hence, in the testing phase, one can simply unload API for single-input test images, without much loss of generalization. Specifically, we feed a test image into the CNN backbone to extract its feature x ∈ R D . Then, we directly put x into softmax classifier (Eq. 7). The resulting score vector p ∈ R C is used for label prediction. By doing so, our testing manner is just the same as that of a plain CNN, which largely boosts the value of API-Net for fine-grained classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Data Sets. We evaluate API-Net on five popular fine-grained benchmarks, i.e., <ref type="bibr">CUB-200-2011</ref><ref type="bibr" target="#b19">(Wah et al. 2011</ref><ref type="bibr">), Aircraft(Maji et al. 2013</ref>, Stanford Cars <ref type="bibr" target="#b10">(Krause et al. 2013)</ref>, Stanford Dogs <ref type="bibr" target="#b8">(Khosla et al. 2011)</ref> and NABirds <ref type="bibr" target="#b18">(Van Horn et al. 2015</ref> Implementation Details. Unless stated otherwise, we implement API-Net as follows. First, we resize each image as 512 × 512, and crop a 448 × 448 patch as input to API-Net (train: random cropping, test: center cropping). Furthermore, we use ResNet-101 (pretrained on ImageNet) as CNN backbone, and extract the feature vector x i ∈ R 2048 after global pooling average operation. Second, for all the datasets, we randomly sample 30 categories in each batch. For each category, we randomly sample 4 images. For each image, we find its most similar image from its own class and the rest classes, according to Euclidean distance between features. As a result, we obtain an intra pair and an inter pair for each image in the batch. For each pair, we concatenate x 1 and x 2 as input to a two-layer MLP, i.e., FC(4096→512)-FC(512→2048). Consequently, this operation generates the mutual vector x m ∈ R 2048 . Finally, we implement our network by PyTorch. For all the datasets, the coefficient λ in Eq. (8) is 1.0, and the margin in the score-ranking regularization is 0.05. We use the standard SGD with momentum of 0.9, weight decay of 0.0005. Furthermore, the initial learning rate is 0.01 (0.001 for Stanford Dogs), and adopt cosine annealing strategy to adjust it. The total number of training epochs is 100 (50 for Stanford Dogs). Besides, during training phase, we freeze the conv layers and only train the newly-added fully-connected layers in the first 8 epochs(12 epochs for Standord Dogs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Studies</head><p>To investigate the properties of our API-Net, we evaluate its key designs on CUB-200-2011. For fairness, when we explore different strategies of one design, others are set as the basic strategy stated in the implementation details.</p><p>Basic Methods. First of all, we compare our API-Net with Baseline, i.e., the standard ResNet-101 without our API design. As expected, API-Net largely outperforms it in <ref type="table" target="#tab_1">Table  1</ref>, showing the effectiveness of API module.</p><p>Mutual Vector. To demonstrate the essentiality of x m in Eq. (1), we investigate different operations to generate it. (I) Individual Operation. The key of x m is to learn mutual information from both images in the pair. For comparison, we introduce a baseline without it. Specifically, we replace mutual learning in Eq. (1) by individual learningx i = f m (x i ), and usex i to generate the gate vector</p><formula xml:id="formula_11">g i = sigmoid(x i ) where i ∈ {1, 2}. (II) Compact Bilin- ear Pooling Operation.</formula><p>We generate x m by compact bilinear pooling <ref type="bibr" target="#b5">(Gao et al. 2016</ref>) between x 1 and x 2 . (III) Elementwise Operations. We perform a number of widely-used elementwise operations to generate x m , including Subtract Square x m = (x 1 − x 2 ) 2 , Sum x m = x 1 + x 2 , and Product x m = x 1 x 2 . (IV) Weight Attention Operation. We use a two-layer MLP to generate the weight of x 1 and x 2 , i.e., [w 1 , w 2 ] = sof tmax(f w ([x 1 , x 2 ])), where the output dimension of the 1st FC layer is 512. Then, we use the normalized weight vector as attention to generate the mutual Method Baseline API-Net Accuracy 85.4 88.6   </p><formula xml:id="formula_12">= w i x i . (V) MLP Operation.</formula><p>It is the mapping function described in the implementation details. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the Individual operation (i.e., the setting without x m ) performs worst. Hence, it is necessary to learn mutual context by x m , which often plays an important role in finding distinct clues for individual image. Moreover, the performance of x m operations is competitive. We choose the simple but effective MLP to generate the mutual vector in our experiments.</p><p>Gate Vector. We discuss different approaches to generate the gate vector. (I) Single. Since x m inherits mutual characteristics from both x 1 and x 2 , a straightforward choice is to use g m = sigmoid(x m ) as a single gate vector. Subsequently, one can train API-Net with attentive features</p><formula xml:id="formula_13">x self i = x i + x i g m , where i ∈ {1, 2}. (II)</formula><p>Pair. This is the proposed setting in Eq. (2). As shown in <ref type="table" target="#tab_3">Table 3</ref>, the Pair setting outperforms the Single setting. It illustrates that, we have to discover discriminative clues from the view of each image, by comparing x i with x m .</p><p>Interaction. We explore different interaction strategies. (I) L ce . We train API-Net only with cross entropy loss L ce . In this case, score-ranking regularization L rk is not used for training. (II) L ce +L rk . We train API-Net with the proposed loss L. In <ref type="table" target="#tab_3">Table 3</ref>, our proposed loss performs better. It illustrates that, L rk can further generalize pairwise interaction with different feature priorities.</p><p>Pair Construction. We investigate different strategies to construct input image pairs. (I) Random. We randomly sample 240 image pairs in a batch. (II) Class-Image. We sample 240 image pairs, according to class (i.e., Intra / Inter) and Euclidean distance between features (i.e., Similar(S) / Dissimilar(D)). This can generate 8 Class-Image settings in <ref type="table" target="#tab_5">Table 4</ref>. For example, we randomly sample 30 classes in a   batch. For each class, we randomly sample 4 images. For each image, we construct 2 pairs, i.e., the intra / inter pair consists of this image and its most similar image from intra / inter classes. This is denoted as Intra(S) &amp; Inter(S). The results of different settings are shown in <ref type="table" target="#tab_5">Table 4</ref>. First, most Class-Image settings outperform the Random setting. It illustrates that, one should take the prior knowledge of class and similarity into account, when constructing image pairs. Second, Inter(S) outperforms Inter(D), no matter which the intra setting is. The reason is that, each pair in Inter(S) / Inter(D) consists of two most similar / dissimilar images from different classes, i.e., the pairs in Inter(S) increases the difficulty of both being recognized correctly at the same time. By checking such pairs in Inter(S), API-Net can be trained to distinguish subtle semantic differences. Third, the performance is competitive between Intra(S) and Intra(D), no matter which the inter setting is. This is mainly because intra pairs are with the same label. API-Net does not need to put much effort to recognize why these pairs are similar. Finally, the settings with both intra and inter pairs outperform those with only intra or inter pairs. It is credited to the fact that, API-Net can gradually distinguish which are highlyconfused or truly-similar pairs, by leveraging both intra and  <ref type="bibr" target="#b24">(Wei et al. 2018)</ref> No 85.8 KP <ref type="bibr" target="#b2">(Cui et al. 2017)</ref> No 86.2 HBP <ref type="bibr" target="#b26">(Yu et al. 2018)</ref> No 87.1 G 2 DeNet <ref type="bibr" target="#b22">(Wang, Li, and Zhang 2017)</ref> No 87.1 MG-CNN  VGGNet-19</p><p>No 81.7 B-CNN <ref type="bibr" target="#b16">(Lin, RoyChowdhury, and Maji 2015)</ref> No 84.1 RACNN  No 85.3 MACNN  No 86.5 Deep <ref type="bibr">KSPD (Engin et al. 2018)</ref> No 86.5 ST-CNN <ref type="bibr" target="#b7">(Jaderberg et al. 2015)</ref> GoogleNet No 84.1 FCAN  ResNet   <ref type="bibr" target="#b22">(Wang, Li, and Zhang 2017)</ref> No 89.0 Grassmann Pool <ref type="bibr" target="#b24">(Wei et al. 2018)</ref> No 89.8 HBP <ref type="bibr" target="#b26">(Yu et al. 2018)</ref> No 90.3 DFL-CNN <ref type="bibr" target="#b23">(Wang, Morariu, and Davis 2018)</ref> No 92.0 B-CNN <ref type="bibr" target="#b16">(Lin, RoyChowdhury, and Maji 2015)</ref> VGGNet-19</p><p>No 84.1 RACNN  No 88.4 MACNN   inter pairs. We choose the setting with the best performance, i.e., Intra(S) &amp; Inter(S) in our experiment. Class Size &amp; Image Size. After choosing Intra(S) &amp; Inter(S) in the Class-Image setting, we further explore the number of sampled classes and images in each batch. When varying the class/image size, we fix the image/class size as 4/30. The results are shown in <ref type="table" target="#tab_6">Table 5</ref>. One can see that, API-Net is more sensitive to class size than image size. The main reason is that, more classes often produce richer diversity of images pairs. We choose the best setting in our experiment, i.e., class size=30 and image size=4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with The-State-of-The-Art</head><p>We compare API-Net with a number of recent works on five widely-used benchmarks in <ref type="table" target="#tab_9">Table 6</ref>-10. First, API-Net outperforms the object-part-localization approaches such as RACNN  and MACNN , showing the importance of API-Net for dis-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Extra S. Cars BoT  VGGNet-16 Yes 92.5 PA-CNN <ref type="bibr" target="#b11">(Krause et al. 2015)</ref> VGGNet-19 Yes 92.8 FCAN  ResNet     <ref type="bibr" target="#b26">(Yu et al. 2018)</ref>. It demonstrates that, we need to put more efforts to distinguish two different images jointly, instead of learning with individual image alone. Third, API-Net outperforms the metriclearning approaches such as PC <ref type="bibr" target="#b3">(Dubey et al. 2018a</ref>). Particularly, we also re-implement TripletNet (Hoffer and Ailon 2015) in <ref type="table" target="#tab_9">Table 6</ref>, a classical approach optimized by extra triplet loss, and compare it with API-Net. They all illustrate that, we should further exploit how to discover differential visual clues in the image pair, instead of straightforwardly regularizing the sample distribution in the feature space. Fourth, API-Net even outperforms the approaches with extra supervision such as localization annotations <ref type="bibr" target="#b27">(Zhang et al. 2014</ref>), super-category labels <ref type="bibr" target="#b1">(Chen et al. 2018)</ref>, or text <ref type="bibr" target="#b13">(Li et al. 2018a</ref>). It shows the effectiveness of API mod- <ref type="figure">Figure 3</ref>: Visualization. As shown in red-dashed boxes, many feature maps of Baseline are confused or noisy, e.g., the object regions are blurring, or certain background regions are activated. On the contrary, our API-Net can effectively discover and then distinguish discriminative clues via attentive pairwise interaction. More explanations can be found in Section 4.3.</p><p>ule. Finally, without complex architectures and multi-stage learning in the previous works, our API module can be easily integrated into the standard CNN training procedure and serve as a plug-and-play unit for discovering subtle visual differences. Hence, API-Net is a concise and practical deep learning approach for fine-grained classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visualization</head><p>We further visualize API-Net in <ref type="figure">Fig. 3</ref>. First, we choose two pairs from the highly-confused categories in CUB-200-2011, i.e., Caspian Tern (CT) vs. Elegant Tern (ET), and Le Conte Sparrow (LCS) vs. Nelson Sharp Tailed Sparrow (NSTS). Second, we feed each pair into API-Net, and find top-5 activated channels of gate vectors g 1 and g 2 . Subsequently, we show the corresponding feature maps (14×14) before global pooling. Additionally, we show the corresponding feature maps in Baseline, i.e., ResNet-101 without attentive pairwise interaction. As shown in red-dashed boxes of <ref type="figure">Fig. 3</ref>, many feature maps of Baseline are confused or noisy, e.g., the object regions are blurring, or certain background regions are activated. On the contrary, our API-Net can effectively discover distinct features, e.g., g 1 mainly focuses on the body of Caspian Tern, while g 2 mainly focuses on the mouth of Elegant Tern. These contrastive clues allow API to correctly distinguish such two birds.</p><p>Additionally, it is interesting to mention that, API-Net can automatically pay attention to discriminative object parts in the feature maps, even though it mainly works on high-level feature vectors. This observation indicates that, CNN can be well generalized via attentive pairwise interaction. As expected, our API-Net successfully recognizes all these pairs, while Baseline makes wrong predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel Attentive Pairwise Interaction Network (API-Net) for fine-grained classification. It can adaptively discover contrastive cues from a pair of images, and attentively distinguish them via pairwise interaction. The results on five popular fine-grained benchmarks show that, API-Net achieves the state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Motivation (Best view in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Specifically, CUB-200-2011 / Aircraft / Stanford Cars / Stanford Dogs / NABirds consists of 11,788 / 10,000 / 16,185 / 20,580 / 48,562 images, from 200 bird / 100 airplane / 196 car / 120 dog / 555 bird classes. We use the official train &amp; test splits for evaluation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with basic methods. Baseline is the standard ResNet-101 without our API design.</figDesc><table><row><cell>Mutual Vector</cell><cell>Accuracy</cell></row><row><cell>Individual</cell><cell>87.9</cell></row><row><cell>Compact Bilinear Pooling</cell><cell>88.2</cell></row><row><cell>Subtract Square</cell><cell>88.3</cell></row><row><cell>Sum</cell><cell>88.5</cell></row><row><cell>Product</cell><cell>88.4</cell></row><row><cell>Weight Attention</cell><cell>88.4</cell></row><row><cell>MLP</cell><cell>88.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Different operations of mutual vector (CUB-200-</cell></row><row><cell cols="4">2011). More details can be found in Section 4.1.</cell></row><row><cell cols="4">Gate Vector Accuracy Interaction Accuracy</cell></row><row><cell>Single</cell><cell>87.7</cell><cell>L ce</cell><cell>88.1</cell></row><row><cell>Pair</cell><cell>88.6</cell><cell>L ce + L rk</cell><cell>88.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Different strategies of gate vector and interaction(CUB-200-2011). More details can be found in Section 4.1.</figDesc><table /><note>vector, i.e., x m</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Image Size (N im ) N im =2 N im =3 N</figDesc><table><row><cell cols="4">: Different strategies of pair construction (CUB-200-</cell></row><row><cell cols="4">2011). Random: We randomly sample 240 image pairs in a</cell></row><row><cell cols="4">batch. Class-Image: We sample 240 image pairs in a batch,</cell></row><row><cell cols="4">according to class (i.e., Intra / Inter) and Euclidean distance</cell></row><row><cell cols="4">(i.e., Similar(S) / Dissimilar(D)). This can generate 8 Class-</cell></row><row><cell cols="4">Image settings. For example, we randomly sample 30 classes</cell></row><row><cell cols="4">in a batch. For each class, we randomly sample 4 images.</cell></row><row><cell cols="4">For each image, we construct 2 pairs, i.e., the intra / inter</cell></row><row><cell cols="4">pair consists of this image and its most similar image from</cell></row><row><cell cols="4">intra / inter classes. This is denoted as Intra(S) &amp; Inter(S).</cell></row><row><cell cols="4">More explanations can be found in Section 4.1.</cell></row><row><cell>Class Size (N cl )</cell><cell cols="3">N cl =10 N cl =20 N cl =30</cell></row><row><cell>Accuracy</cell><cell>83.5</cell><cell>87.0</cell><cell>88.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>im =4</cell></row><row><cell>Accuracy</cell><cell>88.1</cell><cell>88.2</cell><cell>88.6</cell></row><row><cell>(N cl , N im )</cell><cell>(24, 5)</cell><cell>(30, 4)</cell><cell>(40, 3)</cell></row><row><cell>Accuracy</cell><cell>87.7</cell><cell>88.6</cell><cell>88.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Class Size &amp; Image Size (CUB-200-2011). After</cell></row><row><cell>choosing Intra(S) &amp; Inter(S) in the Class-Image rule, we</cell></row><row><cell>further explore the number of sampled classes and images</cell></row><row><cell>in each batch. When varying the class/image size, we fix the</cell></row><row><cell>image/class size as 4/30.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="4">: Comparison with The-State-of-The-Art (CUB-200-</cell></row><row><cell cols="2">2011). Extra S.: Extra Supervision.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Extra S. Aircraft</cell></row><row><cell>BoT (Wang et al. 2016)</cell><cell>VGGNet-16</cell><cell>Yes</cell><cell>88.4</cell></row><row><cell>MG-CNN (Wang et al. 2015)</cell><cell>VGGNet-19</cell><cell>Yes</cell><cell>86.6</cell></row><row><cell>KP (Cui et al. 2017)</cell><cell></cell><cell>No</cell><cell>86.9</cell></row><row><cell>LRBP (Kong and Fowlkes 2017)</cell><cell></cell><cell>No</cell><cell>87.3</cell></row><row><cell>G 2 DeNet</cell><cell>VGGNet-16</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Comparison with The-State-of-The-Art (Aircraft).</cell></row><row><cell>Extra S.: Extra Supervision.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="4">: Comparison with The-State-of-The-Art (Stanford</cell></row><row><cell>Cars). Extra S.: Extra Supervision.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Extra S. Dogs</cell></row><row><cell>TA-FGVC (Li et al. 2018a)</cell><cell>ResNet-50</cell><cell>Yes</cell><cell>88.9</cell></row><row><cell>PDFR (Zhang et al. 2016b)</cell><cell>AlexNet</cell><cell>No</cell><cell>72.0</cell></row><row><cell>DVAN (Zhao et al. 2017)</cell><cell>VGGNet-16</cell><cell>No</cell><cell>81.5</cell></row><row><cell>B-CNN (Lin, RoyChowdhury, and Maji 2015) RACNN (Fu, Zheng, and Mei 2017)</cell><cell>VGGNet-19</cell><cell>No No</cell><cell>82.1 87.3</cell></row><row><cell>FCAN (Liu et al. 2016)</cell><cell>ResNet-50</cell><cell>No</cell><cell>84.2</cell></row><row><cell>MAMC (Sun et al. 2018)</cell><cell>ResNet-101</cell><cell>No</cell><cell>85.2</cell></row><row><cell>MaxEnt (Dubey et al. 2018b) PC (Dubey et al. 2018a)</cell><cell>DenseNet-161</cell><cell>No No</cell><cell>83.6 83.8</cell></row><row><cell>Our API-Net</cell><cell>ResNet-50</cell><cell>No</cell><cell>88.3</cell></row><row><cell>Our API-Net</cell><cell>ResNet-101</cell><cell>No</cell><cell>90.3</cell></row><row><cell>Our API-Net</cell><cell>DenseNet-161</cell><cell>No</cell><cell>89.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Comparison with The-State-of-The-Art (Stanford Dogs). Extra S.: Extra Supervision.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Extra S. NABirds</cell></row><row><cell>Van et al. (Van Horn et al. 2015)</cell><cell>AlexNet</cell><cell>Yes</cell><cell>75.0</cell></row><row><cell>Branson et al. (Branson et al. 2014)</cell><cell>AlexNet</cell><cell>No</cell><cell>35.7</cell></row><row><cell>PC (Dubey et al. 2018a) MaxEnt (Dubey et al. 2018b)</cell><cell>DenseNet-161</cell><cell>No No</cell><cell>82.8 83.0</cell></row><row><cell>Our API-Net</cell><cell>ResNet-50</cell><cell>No</cell><cell>86.2</cell></row><row><cell>Our API-Net</cell><cell>ResNet-101</cell><cell>No</cell><cell>86.6</cell></row><row><cell>Our API-Net</cell><cell>DenseNet-161</cell><cell>No</cell><cell>88.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>: Comparison with The-State-of-The-Art</cell></row><row><cell>(NABirds). Extra S.: Extra Supervision.</cell></row><row><cell>criminative feature learning. Second, API-Net outperforms</cell></row><row><cell>the patch-interaction approaches such as B-CNN (Lin, Roy-</cell></row><row><cell>Chowdhury, and Maji 2015), HBP</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>This work is partially supported by the National Key </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Branson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Bird species categorization using pose normalized deep convolutional nets</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fine-grained representation learning and recognition by exploiting hierarchical semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04505</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepkspd: learning kernel-matrix-based spd representation for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="612" to="627" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei ;</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<idno type="arXiv">arXiv:1703.07737</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jaderberg</surname></persName>
		</author>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lowrank bilinear pooling for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5546" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Metric learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="287" to="364" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">I read, i saw, i tell: texts assisted fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="663" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="947" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep lac: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roychowdhury</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<idno>arXiv:1306.5151</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Philbin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="805" to="821" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiple granularity descriptors for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2399" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mining discriminative triplets of patches for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1163" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">G2denet: Global gaussian distribution embedding network and its application to visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2730" to="2739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morariu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis ;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grassmann pooling as compact homogeneous bilinear pooling for fine-grained visual classification</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="355" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="574" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Embedding label structures for fine-grained feature representation</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1114" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
