<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ionut</forename><forename type="middle">Cosmin</forename><surname>Duta</surname></persName>
							<email>icduta@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: https://github.com/iduta/pyconv</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work introduces pyramidal convolution (PyConv), which is capable of processing the input at multiple filter scales. PyConv contains a pyramid of kernels, where each level involves different types of filters with varying size and depth, which are able to capture different levels of details in the scene. On top of these improved recognition capabilities, PyConv is also efficient and, with our formulation, it does not increase the computational cost and parameters compared to standard convolution. Moreover, it is very flexible and extensible, providing a large space of potential network architectures for different applications. PyConv has the potential to impact nearly every computer vision task and, in this work, we present different architectures based on PyConv for four main tasks on visual recognition: image classification, video action classification/recognition, object detection and semantic image segmentation/parsing. Our approach shows significant improvements over all these core tasks in comparison with the baselines. For instance, on image recognition, our 50-layers network outperforms in terms of recognition performance on ImageNet dataset its counterpart baseline ResNet with 152 layers, while having 2.39 times less parameters, 2.52 times lower computational complexity and more than 3 times less layers. On image segmentation, our novel framework sets a new state-of-the-art on the challenging ADE20K benchmark for scene parsing. Code is available at: https://github.com/iduta/pyconv</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> represent the workhorses of the most current computer vision applications. Nearly every recent state-of-the-art architecture for different tasks on visual recognition is based on CNNs <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b17">[18]</ref>. At the core of a CNN there is the convolution, which learns spatial kernels/filters for visual recognition. Most CNNs use a relatively small kernel size, usually 3×3, forced by the fact that increasing the size comes with significant costs in terms of number of parameters and computational complexity. To cope with the fact that a small kernel size cannot cover a large region of the input, CNNs use a chain of convolutions with small kernel size and downsampling layers, to gradually reduce the size of the input and to increase the receptive field of the network. However, there are two issues that can appear. First, even though for many of current CNNs the theoretical receptive field can cover a big part of the input or even the whole input, in <ref type="bibr" target="#b18">[19]</ref> it is shown that the empirical receptive field is much smaller than the theoretical one, even more than 2.7 times smaller in the higher layers of the network. Second, downsampling the input without previously having access to enough context information (especially in complex scenes as in <ref type="figure" target="#fig_1">Fig. 1</ref>) may affect the learning process and the recognition performance of the network, as useful details are lost since the receptive filed is not large enough to capture different dependencies in the scene before performing the downsampling.  Natural images can contain extremely complicated scenes. Two examples (an outdoor and an indoor scenes in the wild) are presented in <ref type="figure" target="#fig_1">Fig. 1</ref> on the left, while on the right we have the semantic label of each pixel in the scene (taken from the ADE20K dataset <ref type="bibr" target="#b19">[20]</ref>). Parsing these images and providing a semantic category of each pixel is very challenging, however, it is one of the holy grail goals of the computer vision field. We can notice in these examples a big number of class categories in the same scene, some partially occluded, and different object scales.</p><p>We can see in <ref type="figure" target="#fig_1">Fig. 1</ref> that some class categories can have a very large spatial representations (e.g. buildings, trees or sofas) while other categories can have significantly smaller representations in the image (e.g. persons, books or the bottle). Furthermore, the same object category can appear at different scales in the same image. For instance, the scales of cars in <ref type="figure" target="#fig_1">Fig. 1</ref> vary significantly, from being one of the biggest objects in the image to cover just a very tiny portion of the scene. To be able to capture such a diversity of categories and such a variability in their scales, the use of a single type of kernel (as in standard convolution) with a single spatial size, may not be an optimal solution for such complexity. One of the long standing goals of computer vision is the ability to process the input at multiple scales for capturing detailed information about the content of the scene. One of the most notorious example in the hand-crafted features era is SIFT <ref type="bibr" target="#b20">[21]</ref>, which extracts the descriptors at different scales. However, in the current deep learning era with learned features, the standard convolution is not implicitly equipped with the ability to process the input at multiple scales, and contains a single type of kernel with a single spatial size and depth.</p><p>To address the aforementioned challenges, this work provides the following main contributions: <ref type="bibr" target="#b0">(1)</ref> We introduce pyramidal convolution (PyConv), which contains different levels of kernels with varying size and depth. Besides enlarging the receptive field, PyConv can process the input using increasing kernel sizes in parallel, to capture different levels of details. On top of these advantages, PyConv is very efficient and, with our formulation, it can maintain a similar number of parameters and computational costs as the standard convolution. PyConv is very flexible and extendable, opening the door for a large variety of network architectures for numerous tasks of computer vision (see Section 3). (2) We propose two network architectures for image classification task that outperform the baselines by a significant margin. Moreover, they are efficient in terms of number of parameters and computational costs and can outperform other more complex architectures (see <ref type="bibr">Section 4)</ref>. (3) We propose a new framework for semantic segmentation. Our novel head for parsing the output provided by a backbone can capture different levels of context information from local to global. It provides state-of-the-art results on scene parsing (see <ref type="bibr">Section 5)</ref>. (4) We present network architectures based on PyConv for object detection and video classification tasks, where we report significant improvements in recognition performance over the baseline (see Appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Among the various methods employed for image recognition, the residual networks (ResNets) family <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b15">[16]</ref> represents one of the most influential and widely used. By using a shortcut connection, it facilitates the learning process of the network. These networks are used as backbones for various complex tasks, such as object detection and instance segmentation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b17">[18]</ref>. We use ResNets as baselines and make use of such architectures when building our different networks.</p><p>The seminal work <ref type="bibr" target="#b2">[3]</ref> uses a form of grouped convolution to distribute the computation of the convolution over two GPUs for overcoming the limitations of computational resources (especially memory). Furthermore, also <ref type="bibr" target="#b15">[16]</ref> uses grouped convolution but with the aim of improving the recognition performance in the ResNeXt architectures. We also make use of grouped convolution but in a different architecture. The works <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b21">[22]</ref> propose squeeze-and-excitation and non-local blocks to capture context information. However, these are additional blocks that need to be inserted into the CNN; therefore, these approaches still need to use a spatial convolution in their overall CNN architecture (thus, they can be complementary to our approach). Furthermore, these blocks significantly increase the model and computational complexity.</p><p>On the challenging task of semantic segmentation, a very powerful network architecture is PSP-Net <ref type="bibr" target="#b22">[23]</ref>, which uses a pyramid pooling module (PPM) head on top of a backbone in order to parse the scene for extracting different levels of details. Another powerful architecture is presented in <ref type="bibr" target="#b23">[24]</ref>, which uses atrous spatial pyramid pooling (ASPP) head on top of a backbone. In contrast to these competitive works, we propose a novel head for parsing the feature maps provided by a backbone, using a local multi-scale context aggregation module and a global multi-scale context aggregation block for efficient parsing of the input. Our novel framework for image segmentation is not only very competitive in terms of recognition performance but is also significantly more efficient in terms of model and computational complexity than these strong architectures.  The standard convolution, illustrated in <ref type="figure" target="#fig_2">Fig. 2(a)</ref>, contains a single type of kernel: with a single spatial size K 1 2 (in the case of square kernels, e.g., height×width: 3×3 = 3 2 , K 1 = 3) and the depth equal to the number of input feature maps F M i . The result of applying a number of F M o kernels (all having the same spatial resolution and the same depth) over F M i input feature maps is a number of F M o output feature maps (with spatial height H and width W ). Thus, the number of parameters and FLOPs (floating point operations) required for the standard convolution are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pyramidal Convolution</head><formula xml:id="formula_0">parameters = K 1 2 · F M i · F M o ; F LOP s = K 1 2 · F M i · F M o · (W · H).</formula><p>The proposed pyramidal convolution (PyConv), illustrated in <ref type="figure" target="#fig_2">Fig. 2(b)</ref>, contains a pyramid with n levels of different types of kernels. The goal of the proposed PyConv is to process the input at different kernel scales without increasing the computational cost or the model complexity (in terms of parameters). At each level of the PyConv, the kernel contains a different spatial size, increasing kernel size from the bottom of the pyramid (level 1 of PyConv) to the top (level n of PyConv). Simultaneously with increasing the spatial size, the depth of the kernel is decreased from level 1 to level n. Therefore, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>(b), this results in two interconnected pyramids, facing opposite directions. One pyramid has the base at the bottom (evolving to the top by decreasing the kernel depth) and the other pyramid has the base on top, where the convolution kernel has the largest spatial size (evolving to the bottom by decreasing the spatial size of the kernel).  To be able to use different depths of the kernels at each level of PyConv, the input feature maps are split into different groups, and apply the kernels independently for each input feature maps group. This is called grouped convolution, an illustration is presented in <ref type="figure" target="#fig_4">Fig. 3</ref> where we show three examples (the color encodes the group assignment). In these examples, there are eight input and output feature maps. <ref type="figure" target="#fig_4">Fig. 3</ref>(a) shows the case comprising a single group of input feature maps, this is the standard convolution, where the depth of the kernels is equal to the number of input feature maps. In this case, each output feature map is connected to all input feature maps. <ref type="figure" target="#fig_4">Fig. 3</ref>(b) shows the case when the input feature maps are split into two groups, where the kernels are applied independently over each group, therefore, the depth of the kernels is reduced by two. As shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, when the number of groups is increased, the connectivity (and thus the depth of the kernels) decreases. As a result, the number of parameters and the computational cost of a convolution is reduced by a factor equal to the number of groups.</p><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 2(b)</ref>, for the input feature maps F M i , each level of the PyConv {1, 2, 3, ..., n} applies different kernels with a different spatial size for each level {K 1 2 , K 2 2 , K 3 2 , ..., K n 2 } and with different kernel depths {F M i , F Mi , F Mi , ..., F Mi</p><formula xml:id="formula_1">( Kn 2 K 1 2 )</formula><p>}, which outputs a different number of output feature maps {F M o1 , F M o2 , F M o3 , ..., F M on } (with height H and width W ). Therefore, the number of parameters and the computational cost (in terms of FLOPs) for PyConv are:</p><formula xml:id="formula_2">parameters = F LOP s = K n 2 · F Mi ( Kn 2 K 1 2 ) · F M on + K n 2 · F Mi ( Kn 2 K 1 2 ) · F M on · (W · H)+ . . . . . . K 3 2 · F Mi ( K 3 2 K 1 2 ) · F M o3 + K 3 2 · F Mi ( K 3 2 K 1 2 ) · F M o3 · (W · H)+ K 2 2 · F Mi ( K 2 2 K 1 2 ) · F M o2 + K 2 2 · F Mi ( K 2 2 K 1 2 ) · F M o2 · (W · H)+ K 1 2 · F M i · F M o1 ; K 1 2 · F M i · F M o1 · (W · H),<label>(1)</label></formula><formula xml:id="formula_3">where F M on + · · · + F M o3 + F M o2 + F M o1 = F M o .</formula><p>Each row in these equations represents the number of parameters and computational cost for a level in PyConv. If each level of PyConv outputs an equal number of feature maps, then the number of parameters and the computational cost of PyConv are distributed evenly along each pyramid level.</p><p>With this formulation, regardless of the number of levels of PyConv and the continuously increasing kernel spatial sizes from K 1 2 to K n 2 , the computational cost and the number of parameters is similar to the standard convolution with a single kernel size K 1 2 . To link the illustration in <ref type="figure" target="#fig_4">Fig. 3</ref> with Equations 1, the denominator of F M i in Equations 1 refers to the number of groups (G) that the input feature maps F M i are split in <ref type="figure" target="#fig_4">Fig. 3</ref>.</p><p>In practice, when building a PyConv there are several additional rules. The denominator of F M i at each level of the pyramid in Equations 1, should be a divisor of F M i . In other words, at each pyramid level, the number of feature maps from each created group should be equal. Therefore, as an approximation, when choosing the number of groups for each level of the pyramid (and thus the depth of the kernel), we can take the closest number to the denominator of F M i from the list of possible divisors of F M i . Furthermore, the number of groups for each level should be also a divisor for the number of output feature maps of each level of PyConv. To be able to easily create different network architectures with PyConv, it is recommended that the number of input feature maps, the groups for each level of pyramid, and the number of output feature maps for each level of PyConv, to be numbers of power of 2. Next sections show practical examples.</p><p>The main advantages of the proposed PyConv are: (1) Multi-Scale Processing. Besides the fact that, compared to the standard convolution, PyConv can enlarge the receptive field of the kernel without additional costs, it also applies in parallel different types of kernels, having different spatial resolutions and depths. Therefore, PyConv parses the input at multiple scales capturing more detailed information. This double-oriented pyramid of kernels types, where on one side the kernel sizes are increasing and on the other side the kernel depths (connectivity) are decreasing (and vice-versa), allows PyConv to provide a very diverse pool of combinations of different kernel types that the network can explore during learning. The network can explore from large receptive fields of the kernels with lower connectivity to smaller receptive fields with higher connectivity. These different types of kernels of PyConv bring complementary information and help boosting the recognition performance of the network. The kernels with smaller receptive field can focus on details, capturing information about smaller objects and/or parts of the objects, while increasing the kernels size provides more reliable details about larger objects and/or context information.</p><p>(2) Efficiency. In comparison with the standard convolution, PyConv maintains, by default, a similar number of model parameters and requirements in computational resources, as shown in Equation 1. Furthermore, PyConv offers a high degree of parallelism due to the fact that the pyramid levels can be independently computed in parallel. Thus, PyConv can also offer the possibility of customizable heavy network architectures (in the case where the architecture cannot fit into the memory of a computing unit and/or it is too expensive in terms of FLOPs), where the levels of PyConv can be executed independently on different computing units and then the outputs can be merged.</p><p>(3) Flexibility. PyConv opens the door for a great variety of network architectures. The user has the flexibility to choose the number of layers of the pyramid, the kernel sizes and depths at each PyConv level, without paying the price of increasing the number of parameters or the computational costs. Furthermore, the number of output feature maps can be different at each level. For instance, for a particular final task it may be more useful to have less output feature maps from the kernels with small receptive fields and more output feature maps from the kernels with bigger receptive fields. Also, the PyConv settings can be different along the network, thus, at each layer of the network we can have different PyConv settings. For example, we can start with several layers for PyConv, and based on the resolution of the input feature maps at each layer of the network, we can decrease the levels of PyConv as the resolution decreases along the network. That being said, we can now build architectures using PyConv for different visual recognition tasks.  For our PyConv network architectures on image classification, we use a residual bottleneck building block similar to the one reported in <ref type="bibr" target="#b6">[7]</ref>. <ref type="figure">Fig. 4</ref> shows an example of a building block used on the first stage of our network. First, it applies a 1×1 conv to reduce the input feature maps to 64, then we use our proposed PyConv with four levels of different kernels sizes: 9×9, 7×7, 5×5, 3×3. Also the depth of the kernels varies along each level, from 16 groups to full depth/connectivity. Each level outputs 16 feature maps, which sums a 64 output feature maps for the PyConv. Then a 1×1 conv is applied to regain the initial number of feature maps. As common, batch normalization <ref type="bibr" target="#b5">[6]</ref> and ReLU activation function <ref type="bibr" target="#b24">[25]</ref> follow a conv block. Finally, there is a shortcut connection that can help with the identity mapping.</p><p>Our proposed network for image classification, PyConvResNet, is illustrated in <ref type="table" target="#tab_2">Table 1</ref>. For direct comparison we place aside also the baseline architecture ResNet <ref type="bibr" target="#b6">[7]</ref>. <ref type="table" target="#tab_2">Table 1</ref> presents the case for a 50-layers deep network, for the other depths, the number of layers are increased as in <ref type="bibr" target="#b6">[7]</ref>. Along the network we can identify four main stages, based on the spatial size of the feature maps. For PyConvResNet, we start with a PyConv with four levels. Since the spatial size of the feature maps decreases at each stage, we reduce also the PyConv levels. On the last main stage, the network ends up with only one level for PyConv, which is basically the standard convolution. This is appropriate because the spatial size of the feature maps is only 7×7, thus, three successive convolutions of size 3×3 cover well the feature maps. Regarding the efficiency, PyConvResNet provides also a slight decrease in FLOPs.  As we highlighted, flexibility is a strong point of PyConv, <ref type="table" target="#tab_2">Table 1</ref> presents another architecture based on PyConv, PyConvHGResNet, which uses a higher grouping for each level. For this architecture we set a minimum of 32 groups and a maximum of 64 in the PyConv. The number of feature maps for the spatial convolutions is doubled to provide better capabilities on learning spatial filters. Note that for stage one of the network, it is not possible to increase the number of groups more than 32 since this is the number of input and output feature maps for each level. Thus, PyConvHGResNet produces a slight increase in FLOPs.</p><formula xml:id="formula_4">×3        1×1, 64 PyConv4, 64:   9×9, 16, G=16 7×7, 16, G=8 5×5, 16, G=4 3×3, 16, G=1   1×1, 256        ×3        1×1, 128 PyConv4, 128:   9×9, 32, G=32 7×7, 32, G=32 5×5, 32, G=32 3×3, 32, G=32   1×1, 256        ×3 2 28×28 1×1,</formula><p>As our networks contain different levels of kernels, it can perform the downsampling of the feature maps using different kernel sizes. This is important as downsampling produces loss of spatial resolution and therefore loss of details, but having different kernel sizes to perform the downsampling can take into account different levels of spatial context dependencies to perform the dowsampling in parallel. As can be seen in <ref type="table" target="#tab_2">Table 1</ref>, the original ResNet <ref type="bibr" target="#b6">[7]</ref> uses a max pooling layer before the first stage of the network to downsample the feature maps and to get the translation invariance. Different from the original ResNet, we move the max pooling on the first projection shortcut, just before the 1×1 conv (usually, the first shortcut of a stage contains a projection 1×1 conv to adapt the number of feature maps and their spatial resolution for the summation with the output of the block). This is similar to projection shortcut in <ref type="bibr" target="#b25">[26]</ref>. Therefore, for the original ResNet, the downsampling is not performed by the first stage (as the max pooling performs this before), the next three main stages perform the downsampling on their first block. In our networks, all four main stages perform the downsampling in their first block.</p><p>This change does not increase the number of parameters of the network and does not affect significantly the computational costs (as can be seen in <ref type="table" target="#tab_2">Table 1</ref>, as the first block uses the spatial convolutions with the stride 2), providing advantages in recognition performance for our networks.</p><p>Moving the max pooling to the shortcut gives our approach the opportunity to have access to larger spatial resolution of the feature maps in the first block of the first stage, to downsample the input using multiple kernel scales and, at the same time, to benefit from the translation invariance provided by max pooling. The results show that our networks provide improved recognition capabilities.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PyConv Network on Semantic Segmentation</head><p>Our proposed framework for scene parsing (image segmentation) is illustrated in <ref type="figure">Fig. 5</ref>. To build an effective pipeline for scene parsing, it is necessary to create a head that can parse the feature maps provided by the backbone and obtain not only local but also global information. The head should be able to deal with fine details and, at the same time, take into account the context information. We propose a novel head for scene parsing (image segmentation) task, PyConv Parsing Head (PyConvPH). The proposed PyConvPH is able to deal with both local and global information at multiple scales.</p><p>PyConvPH contains three main components: (1) Local PyConv block (LocalPyConv), which is mostly responsible for smaller objects and capturing local fine details at multiple scales, as shown in <ref type="figure">Fig. 5</ref>. It also applies different type of kernels, with different spatial sizes and depths, which can also be seen as a local multi-scale context aggregation module. The detailed information about each component of LocalPyConv is represented in <ref type="figure" target="#fig_8">Fig. 6(a)</ref>. LocalPyConv takes the output feature maps from the backbone and applies a 1×1 conv to reduce the number of feature maps to 512. Then, it performs a PyConv with four layers to capture different local details at four scales of the kernel 9×9, 7×7, 5×5, and 3×3. Additionally, the kernels have different connectivity, represented by the number of groups (G). Finally, it applies a 1×1 conv to combine the information extracted at different kernel sizes and depths. As is standard, all convolution blocks are followed by a batch normalization layer <ref type="bibr" target="#b5">[6]</ref> and a ReLU activation function <ref type="bibr" target="#b24">[25]</ref>.</p><p>(2) Global PyConv block (GlobalPyConv) is responsible for capturing global details about the scene, and for dealing with very large objects. It is a multi-scale global aggregation module. The components of GlobalPyConv are represented in <ref type="figure" target="#fig_8">Fig. 6(b)</ref>. As the input image size can vary, to ensure that we can capture full global information we keep the largest spatial size dimension as 9. We apply an adaptive average pooling that reduces the spatial size of the feature maps to 9×9 (in the case of square images), which still maintains reasonable spatial resolution. Then we apply a 1×1 conv to reduce the number of feature maps to 512. We use a PyConv with four layers similarly as in the LocalPyConv. However, as now we have decreased the spatial size of the feature maps to 9×9, the PyConv kernels can cover very large parts of the input, ultimately, the layer with a 9×9 convolution covers the whole input and captures full global information, as illustrated also in <ref type="figure">Fig. 5</ref>. Then we apply a 1×1 conv to fuse the information from different scales. Finally, we upsample the feature maps to the initial size before the adaptive average pooling, using a bilinear interpolation.</p><p>(3) Merge Local-Global PyConv block performs first the concatenation of the output feature maps from the LocalPyConv and GlobalPyConv blocks. Over the resulting 1024 feature maps, it applies a PyConv with one level, which is basically a standard 3×3 conv that outputs 256 feature maps. We use here a single level for PyConv because the previous layers already captured all levels of context information and it is more important at this point to focus on merging this information (thus, to use full connectivity of the kernels) as it approaches the final classification stage. To provide the final output, the framework continues with an upsample layer (using also bilinear interpolation) to restore the feature maps to the initial input image size; finally, there is a classification layer which contains a 1×1 conv, to provide the output with a dimension equal to the number of classes. As illustrated in <ref type="figure">Fig. 5</ref>, our proposed framework is able to capture local and global information at multiple scales of kernels, parsing the image and providing a strong representation. Furthermore, our framework is also very efficient, and in the following we provide the exact numbers and the comparison with other state-of-the-art frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Experimental setup. For image classification task we perform our experiments on the commonly used ImageNet dataset <ref type="bibr" target="#b26">[27]</ref>. It consists of 1000 classes, 1.28 million training images and 50K validation images. We report both top-1 and top-5 error rates. We follow the settings in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b27">[28]</ref> and use the SGD optimizer with a standard momentum of 0.9, and weight decay of 0.0001. We train the model for 90 epochs, starting with a learning rate of 0.1 and reducing it by 1/10 at the 30-th, 60-th and 80-th epochs, similarly to <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The models are trained using 8 GPUs V100. We use the standard 256 training mini-batch size and data augmentation as in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b27">[28]</ref>, training/testing on 224×224 image crop. For image segmentation we use ADE20K benchmark <ref type="bibr" target="#b19">[20]</ref>, which is one of the most challenging datasets for image segmentation/parsing. It contains 150 classes and a high level of scenes diversity, containing both object and stuff classes. It is divided in 20K/2K/3K images for training, validation and testing. As standard, we report both pixel-wise accuracy (pAcc.) and mean of class-wise intersection over union (mIoU). We train for 100 epochs with a mini-batch size of 16 over 8 GPUs V100, using train/test image crop size of 473×473. We follow the training settings as in <ref type="bibr" target="#b22">[23]</ref>, including the auxiliary loss, with the weight 0.4. PyConv results on image recognition. We present in <ref type="table" target="#tab_5">Table 2</ref> the ablation experiments results of the proposed Py-Conv for image recognition task on the ImageNet dataset where, using the network with 50 layers, we vary the number of levels of PyConv. We first provide a direct comparison to the baseline ResNet <ref type="bibr" target="#b6">[7]</ref> without any additional changes, just replacing the standard 3x3 conv with our PyConv. The column "PyConv levels" points to the number of levels used at each of the four main stages of the network. The PyConv levels (1, 1, 1, 1) represent the case when we use a single level for PyConv on all four stages, which is basically the baseline ResNet.</p><p>Remarkably, just increasing the number of PyConv levels to two provides a significant improvement in recognition performance, improving the top-1 error rate from 23.88 to 23.12. At the same time it requires less number of parameters and FLOPS than the baseline. Note that by just using two levels for PyConv (5×5 and 3×3 kernels), it has already significantly increased the receptive field at each stage of the network. Gradually increasing the levels of PyConv at each level brings further improvement, for the PyConv levels (4, 3, 2, 1) it brings the top-1 error rate to 22.97 with even lower number of parameters and FLOPs. We also run the experiment using only the top level of the PyConv at each main stage network, basically the opposite case of the baseline which uses only the bottom level. Therefore, top(4, 3, 2, 1) refers to the case when using only the fourth level of the PyConv for the stage 1 (9×9 kernel), third level for stage 2 (7×7 kernel), second level for stage 3 (5×5 kernel) and first level of stage 4 (3×3 kernel). This configuration also provides significant improvements in recognition performance compared to the baseline while having a lower number of parameters and FLOPs, showing that our formulation of increasing the kernel sizes for building the network is beneficial in many possible configurations.</p><p>We also add one more layer to PyConv for each stage of the network, (5, 4, 3, 2) case, where the fifth level has a 11×11 kernel, but we do not notice further improvements in recognition performance.</p><p>In the rest of the paper we use (4, 3, 2, 1) levels of PyConv for image classification task. However, we find this configuration reasonably good for this task with the input image resolution (224×224), however, if, for instance, the input image resolution is increased, then other settings of PyConv may provide even further improvements. Moving the max pooling to the shortcut, which provides access for PyConv to perform the downsampling at multiple kernel sizes, improves further the top-1 error rate to 22.46. To further benefit from the translation invariance and to address the fact that a 1×1 conv lacks the spatial resolution for performing downsampling, we maintain a max pooling on the projection shortcut in the first block of each following stages. Our final network result is 22.12 top-1 error requiring only 24.85 million parameters and 3.88 GFLOPs. The conclusion is that regardless of the settings of PyConv, using our formulation, it consistently provides better results than the baseline. <ref type="figure" target="#fig_9">Fig. 7</ref> shows the training and validation curves for comparing our networks, PyConvResNet and PyConvHGResNet, with baseline ResNet over 50, 101 and 152 layers, where we can notice that our networks significantly improve the learning convergence. For instance, on 50 depth, on first interval (first 30 epochs, before the first reduction of the learning rate), our PyConvResNet needs    <ref type="bibr" target="#b7">[8]</ref> and ResNeXt <ref type="bibr" target="#b15">[16]</ref>, our networks outperform more complex architectures, like SE-ResNet <ref type="bibr" target="#b16">[17]</ref>, despite that it uses an additional squeeze-and-excitation block, which increases model complexity.</p><p>The above results mainly aim to show the advantages of our PyConv over the standard convolution by running all the networks with the same standard training settings for a fair comparison. Note that there are other works which report better results on ImageNet, such as <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>. However, the improvements are mainly due to the training settings. For instance, <ref type="bibr" target="#b29">[30]</ref> uses very complex training settings, such as, complex data augmentation (autoAugment <ref type="bibr" target="#b31">[32]</ref>) with different regularization techniques (dropout <ref type="bibr" target="#b32">[33]</ref>), stochastic depth <ref type="bibr" target="#b33">[34]</ref>, the training is performed on a powerful Google TPU computational architecture over 350 epochs with a large batch of 2048. The works <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, besides using a strong computational architecture with many GPUs, take advantage of a large dataset of 3.5B images collected from Instagram (this dataset is not publicly available). Therefore, these resources are not handy to everyone. However, the results show that PyConv is superior to standard convolution and combining it with <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref> can bring further improvements. While on ImageNet we do not have access to such scale of computational and data resources to directly compete with state-of-the-art, we do push further and show that our proposed framework obtains state-of-the-art results on challenging task of image segmentation.</p><p>To support our claim, that our networks can be easily improved using more complex training settings, we integrate an additional data augmentation, CutMix <ref type="bibr" target="#b34">[35]</ref>. As CutMix requires more epochs to converge, we increase the training epochs to 300 and use a cosine scheduler <ref type="bibr" target="#b35">[36]</ref> for learning rate decaying. To speed-up the training, we increase the batch size to 1024 and use mixed precision <ref type="bibr" target="#b36">[37]</ref>. <ref type="table" target="#tab_8">Table 4</ref> presents the comparison results of PyConvResNet for the baseline training settings and with the CutMix data augmentation. For both depths, 50-and 101-layers, just adding these simple additional training settings improve significantly the results. For the same trained models, in addition to the standard test crop size of 224×224 we also run the testing on 320×320 crop size. This results show that there is still room for improvement if more complex training settings are included (as the training settings from <ref type="bibr" target="#b29">[30]</ref>) and/or additional data used for training (as in <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>), however, this requires significantly more computational and data resources, which are not easily available.  PyConv results on semantic segmentation. We compare our proposed framework, PyConvSegNet, with two of the most powerful architectures for semantic segmentation <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b23">[24]</ref>. <ref type="table" target="#tab_9">Table 5</ref> presents head-to-head comparison of our method with state-of-the-art heads on image segmentation: PSPNet with Pyramid Pooling Module (PPM) head, and DeepLabv3 with Atrous Spatial Pyramid Pooling (ASPP). The baseline is constructed as in <ref type="bibr" target="#b22">[23]</ref>, which as head, it basically applies a 3×3 conv over the output feature maps provided by the backbone. For a fair and direct comparison, all methods use the same auxiliary loss (deep supervision) exactly as in <ref type="bibr" target="#b22">[23]</ref>. For a comprehensive view, the reports in terms of number of parameters and FLOPs include the auxiliary loss components. As <ref type="bibr" target="#b22">[23]</ref> uses an output stride for the backbone of 8 and [24] uses 16, we report the experiments for both cases. We run these experiments using the ResNet with 50 layers as backbone. <ref type="table" target="#tab_9">Table 5</ref> shows that our proposed head is not only more accurate than the other methods, but it is also more efficient, requiring significantly smaller number of parameters and FLOPs than <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b23">[24]</ref>. We can also see that without a strong head on top of the backbone, the baseline reports significantly worse results.  <ref type="table" target="#tab_10">Table 6</ref> shows the results of PyConvSegNet using different depths of the backbones ResNet and PyConvResNet. Besides the single-scale (SS) inference results, we show also the results using multi-scale inference (MS) (scales equal to {0.5, 0.75, 1, 1.25, 1.5, 1.75}). <ref type="table" target="#tab_11">Table 7</ref> presents the comparisons of our approach with the state-of-the-art on both validation and testing sets. Notably, our approach PyConvSegNet, with 152 layers for backbone, outperforms PSPNet <ref type="bibr" target="#b22">[23]</ref> with its 269-layers heavy backbone, which also requires significantly more parameters and FLOPs for their PPM head. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we proposed pyramidal convolution (PyConv), which contains several levels of kernels with varying scales. PyConv shows significant improvements for different visual recognition tasks and, at the same time, it is also efficient and flexible, providing a very large pool of potential network architectures. Our novel framework for image segmentation provides state-of-the-art results. In addition to a broad range of visual recognition tasks, PyConv can have a significant impact in many other directions, such as image restoration, completion/inpainting, noise/artifact removal, enhancement and image/video super-resolution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>In this Appendix we present additional experiments, architectures details and/or analysis. It contains three main sections: Section A.1 presents the details of our architecture for object detection; Section A.2 presents the details for the video classification pipeline; Finally, Section A.3 shows some visual examples on image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 PyConv on object detection</head><p>As we already presented in the the main paper the final result on object detection, that we outperform the baseline by a significant margin (see main contribution (4) in the main paper), this section provides the details of our architecture on object detection and the exact numbers of the results.</p><p>As our proposed PyConv uses different levels of kernel sizes in parallel, it can provide significant benefits for object detection task, where the objects can appear in the image at different scales. For object detection, we integrate our PyConv in a powerful approach, Single Shot Detector (SSD) <ref type="bibr" target="#b47">[48]</ref>. SSD is a very efficient single stage framework for object detection, which performs the detection at multiple feature maps resolutions. Our proposed framework for object detection, PyConvSSD, is illustrated in <ref type="figure" target="#fig_10">Fig. 8</ref>. The framework contains two main parts:</p><p>(1) PyConvResNet Backbone. In our framework we use the proposed PyConvResNet as backbone, which was previously pre-trained on ImageNet dataset <ref type="bibr" target="#b26">[27]</ref>. To maintain a high efficiency of the framework, and also to heave a similar number of output feature maps as in the backbone used in <ref type="bibr" target="#b47">[48]</ref>, we remove from our PyConvResNet backbone all layers after the third stage. We also set all strides in the stage 3 of the backbone network to 1. With this, PyConvResNet provides (as output of the stage 3) 1024 output feature maps (S3 F M ) with the spatial resolution 38×38 (for an input image size of 300×300).</p><p>(2) PyConvSSD Head. Our PyConvSSD head illustrated in <ref type="figure" target="#fig_10">Fig. 8</ref> uses the proposed PyConv to further extract different features using different kernel sizes in parallel. Over the resulted feature maps for the third stage of the backbone we apply a PyConv with four levels (kernel sizes: 9×9, 7×7, 5×5, 3×3). Also PyConv performs the downsampling (stride s=2) of the feature maps using these multiple kernel sizes in parallel. As the feature maps resolution decreases we also decrease the levels of the pyramid for PyConv. The last two PyConv contains only one level (which is basically the standard 3×3) as the spatial resolution of the feature maps is very small. Note that the last two PyConvs use a stride s=1 and the spatial resolution is decreased just by not using padding. Thus, the head decreases the spatial resolution of the feature maps from 38×38 to 1×1. All the output feature maps from the PyConvs in the head are used for detections.  default box position, relative to each feature maps location. Also, for each bounding box, the network should output the confidences for each class category (in total C class categories). For providing the detections the framework uses a classifier which is represented by a 3×3 convolution, that outputs for each bounding box the confidences for all class categories (C). For localization the framework uses also a 3×3 convolution to output the four localization values for each regressed default bounding box. In total, the framework outputs 8732 detections (for 300×300 input image size), which pass through a non-maximum suppression to provide the final detections.</p><p>Different from the original SSD framework <ref type="bibr" target="#b47">[48]</ref>, for a fair and direct comparison, in the baseline SSD, we replaced the VGG backbone <ref type="bibr" target="#b3">[4]</ref> with ResNet <ref type="bibr" target="#b6">[7]</ref>, as ResNet is far superior to VGG in terms of recognition performance and computational costs as shown in <ref type="bibr" target="#b6">[7]</ref>. Therefore, as main differences from our PyConvSSD, the baseline SSD in this work uses ResNet <ref type="bibr" target="#b6">[7]</ref> as backbone and the SSD head uses standard 3×3 conv (instead of PyConv) as in the original framework <ref type="bibr" target="#b47">[48]</ref>. For showing the exact numbers to compare our PyConvSSD with the baseline on object detection, we use COCO dataset <ref type="bibr" target="#b48">[49]</ref>, which contains 81 categories. We use for training COCO train2017 (118K images) and for testing COCO val2017 (5K images). We train for 130 epochs using 8 GPUs with 32 batch size each, resulting in 60K training iterations. We use for training SGD optimiser with momentum 0.9, weight decay 0.0005, with the learning rate 0.02 (reduced by 1/10 before 86-th and 108-th epoch).</p><p>We also use a linear warmup in the first epoch <ref type="bibr" target="#b27">[28]</ref>. For data augmentation, we perform random crop as in <ref type="bibr" target="#b47">[48]</ref>, color jitter and horizontal flip. We use an input image size of 300×300 and report the metrics as in <ref type="bibr" target="#b47">[48]</ref>. <ref type="table" target="#tab_14">Table 8</ref> shows the comparison results of PyConvSSD with the baseline, over 50-and 101-layers backbones. While being more efficient in terms of number of parameters and FLOPs, the proposed PyConvSSD reports significant improvements over the baseline over all metrics. Notably, PyConvSSD with 50 layers backbone is even competitive with the baseline using 101 layers as backbone. This results show a grasp of the benefits for PyConv on object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PyConv on video classification</head><p>In the main paper we introduced the main result for video classification, that we report significant results over the baseline (see main contribution <ref type="formula">(4)</ref>). This section presents the details of the architecture and the exact numbers. PyConv can show significant benefits on video related tasks as it can enlarge the receptive field and process the input using multiple kernels scales in parallel not only spatially but also in the temporal dimension. Extending the networks from image recognition to video involves extending the 2D spatial convolution to 3D spatio-temporal convolution. <ref type="table" target="#tab_15">Table 9</ref> presents the baseline network ResNet3D and our proposed network PyConvResNet3D, which are the initial 2D networks extended to work with video input. The input for the network is represented by 16-frame input clips, with spatial size is 224×224. As the temporal size is smaller than spatial dimensions, for our PyConv we do not need to use equally large size on the upper layers of the pyramid. In the first stage of the network, our PyConv with four layers contains kernel sizes of: 7×9×9, 5×7×7, 3×5×5 and 3×3×3 (the temporal dimension comes first).</p><p>For video classification, we perform the experiments on Kinetics-400 <ref type="bibr" target="#b49">[50]</ref>, which is a large-scale video recognition dataset that contains ∼246k training videos and 20k validation videos, with 400 action classes. Similar to image recognition, use the SGD optimizer with a standard momentum of 0.9 and weight decay of 0.0001, we train the model for 90 epochs, starting with a learning rate of 0.1 and reducing it by 1/10 at the 30-th, 60-th and 80-th epochs, similar to <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The models are trained from scratch, using the weights initialization of <ref type="bibr" target="#b50">[51]</ref> for all convolutional layers; for training we use a minibatch of 64 clips over 8 GPUs. Data augmentation is similar to <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b21">[22]</ref>. For training, we randomly select 16-frame input clips from the video. We also skip four frames to cover a longer video period within a clip. The spatial size is 224×224, randomly cropped from a scaled video, where  ResNet3D-50 train ResNet3D-50 val PyConvResNet3D-50 train PyConvResNet3D-50 val the shorter side is randomly selected from the interval [256, 320], similar to <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b21">[22]</ref>. As the networks on video data are prone to overfitting due to the increase in number of parameters, we use dropout <ref type="bibr" target="#b51">[52]</ref> after the global average pooling layer, with a 0.5 dropout ratio. For the final validation, following common practice, we uniformly select a maximum of 10 clips per video. Each clip is scaled to 256 pixels for the shorter spatial side. We take 3 spatial crops to cover the spatial dimensions. In total, this results in a maximum of 30 clips per video, for each of which we obtain a prediction. To get the final prediction for a video, we average the softmax scores. We report both, top-1 and top-5 error rates. <ref type="table" target="#tab_2">Table 10</ref> presents the result comparing our network, PyConvResNet3D, with the baseline over 50layers depth. PyConvResNet3D improves significantly the results over baseline, for top-1 error, from 37.01% to 34.56%. In the same time our network requires less number of parameters and FLOPs than the baseline. <ref type="figure" target="#fig_11">Fig. 9</ref> shows the training and validation curves where we can see that our network improves significantly the training convergence. This results show the potential of PyConv on video related tasks. <ref type="figure" target="#fig_1">Fig. 10</ref> shows some qualitative examples for visually comparing our proposed approach for image segmentation, PyConSegNet, with state-of-the-art approaches PSPNet <ref type="bibr" target="#b22">[23]</ref> and DeepLabv3 <ref type="bibr" target="#b23">[24]</ref>. For the numeric results, refer to <ref type="table" target="#tab_8">Table 4</ref> in the main paper (for the output stride backbone 8). This examples show the visual comparison results between our proposed head, PyConvPH (PyConv parsing head), with ASPP (Atrous Spatial Pyramid Pooling) of <ref type="bibr" target="#b23">[24]</ref> and PPM head (Pyramid Pooling Module) of <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Qualitative examples on image segmentation</head><p>Very suggestive is the last row example of <ref type="figure" target="#fig_1">Fig. 10</ref>, where we can clearly notice the difference in segmentation details. It is remarkable that our proposed head can compete at a high level with other state-of-the art approaches for image segmentation while having significantly less requirements in terms of number of parameters and computational complexity. For instance, in comparison with our PyConSegNet, PSPNet <ref type="bibr" target="#b22">[23]</ref> requires over 40% more parameters and FLOPs, while DeepLabv3 <ref type="bibr" target="#b23">[24]</ref> requires over 20% more parameters and close to 30% more FLOPs.</p><p>In the second row example of <ref type="figure" target="#fig_1">Fig. 10</ref> we can also notice a failure case of our approach, which confuses the door with a window. However, this case is quite difficult and confusing even for a human eye. <ref type="figure" target="#fig_1">Fig. 11</ref> shows some visual results of our approach, PyConSegNet, using 50-, 101-, 152-layers for the PyConvResNet backbone. For the exact number, refer to <ref type="table" target="#tab_9">Table 5</ref> in the main paper (multi-scale inference). Note in the second row of <ref type="figure" target="#fig_1">Fig. 11</ref> how the quality of the segmentation for the fan (ceiling mount air fan) is improving while increasing the depth of our PyConvResNet backbone.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2006.11538v1 [cs.CV] 20 Jun 2020 (a) Image (b) Scene Parsing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Scene parsing examples. An outdoor and indoor images with their associated pixel-level semantic category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(a) Standard conv; (b) Proposed PyConv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Grouped Convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 4: PyConv bottleneck building block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>PyConv blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>ImageNet training curves for ResNet and PyConvResNet on 50, 101 and 152 layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>PyConvSSD framework for object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Training and validation curves on Kinetics-400 dataset (these results are computed during training over independent clips).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Visual comparison results of our approach PyConSegNet (with PyConvPH head) with state-of-the-art approaches: PSPNet<ref type="bibr" target="#b22">[23]</ref> (with PPM head) and DeepLabv3<ref type="bibr" target="#b23">[24]</ref> (with ASPP head). The images are from ADE20K dataset<ref type="bibr" target="#b19">[20]</ref> validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Visual results of our approach, PyConSegNet, on 50-, 101-, 152-layers deep backbone PyConvResNet. The images are from ADE20K dataset<ref type="bibr" target="#b19">[20]</ref> validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>4</head><label></label><figDesc>PyConv Networks for Image Classification</figDesc><table><row><cell>FM i = 256</cell></row><row><cell>1x1, 64</cell></row><row><cell>BN</cell></row><row><cell>ReLU</cell></row><row><cell>PyConv4, 64:</cell></row><row><cell>9x9, 16, G=16</cell></row><row><cell>7x7, 16, G=8</cell></row><row><cell>5x5, 16, G=4</cell></row><row><cell>3x3, 16, G=1</cell></row><row><cell>BN</cell></row><row><cell>ReLU</cell></row><row><cell>1x1, 256</cell></row><row><cell>BN</cell></row><row><cell>+</cell></row><row><cell>ReLU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>PyConvResNet and PyConvHGResNet.</figDesc><table><row><cell>stage output</cell><cell>ResNet-50</cell><cell>PyConvResNet-50</cell><cell>PyConvHGResNet-50</cell></row><row><cell cols="2">112×112 7×7, 64, s=2</cell><cell>7×7, 64, s=2</cell><cell>7×7, 64, s=2</cell></row><row><cell></cell><cell>3×3 max pool,s=2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1×1, 64</cell><cell></cell><cell></cell></row><row><cell>1 56×56</cell><cell>3×3, 64</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1×1, 256</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>ImageNet ablation experiments of PyConvResNet.</figDesc><table><row><cell>PyConv levels</cell><cell cols="2">top-1(%) top-5(%) params GFLOPs</cell></row><row><cell cols="2">(1, 1, 1, 1)baseline 23.88</cell><cell>7.06 25.56 4.14</cell></row><row><cell>(2, 2, 2, 1)</cell><cell>23.12</cell><cell>6.58 24.91 3.91</cell></row><row><cell>(3, 3, 2, 1)</cell><cell>22.98</cell><cell>6.62 24.85 3.85</cell></row><row><cell>(4, 3, 2, 1)</cell><cell>22.97</cell><cell>6.56 24.85 3.84</cell></row><row><cell>top(4, 3, 2, 1)</cell><cell>23.18</cell><cell>6.60 24.24 3.63</cell></row><row><cell>(5, 4, 3, 2)</cell><cell>23.03</cell><cell>6.56 23.45 3.71</cell></row><row><cell>(4, 3, 2, 1) max</cell><cell>22.46</cell><cell>6.24 24.85 3.88</cell></row><row><cell>(4, 3, 2, 1) final</cell><cell>22.12</cell><cell>6.20 24.85 3.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Validation error rates comparison results of PyConv on ImageNet with other architectures. ] 23.88 7.06 25.56 4.14 22.00 6.10 44.55 7.88 21.55 5.74 60.19 11.62 pre-act. ResNet [8] 23.77 7.04 25.56 4.14 22.11 6.26 44.55 7.88 21.41 5.78 60.19 11.62 iResNet [26] 22.69 6.46 25.56 4.18 21.36 5.63 44.55 7.92 20.66 5.43 60.19 11.65 NL-ResNet [22] 22.91 6.42 36.72 6.18 21.40 5.83 55.71 9.91 21.91 6.11 71.35 13.66 SE-ResNet [17] 22.74 6.37 28.07 4.15 21.31 5.79 49.29 7.90 21.38 5.80 66.77 11.65 ResNeXt [16] 22.44 6.25 25.03 4.30 21.03 5.66 44.18 8.07 20.98 5.48 59.95 11.84 PyConvHGResNet 21.52 5.94 25.23 4.61 20.78 5.57 44.63 8.42 20.64 5.27 60.66 12.29 PyConvResNet 22.12 6.20 24.85 3.88 20.99 5.53 42.31 7.31 20.48 5.27 56.64 10.72</figDesc><table><row><cell>Network</cell><cell>network depth: 50 top-1 top-5 params GFLOPs top-1 top-5 params GFLOPs top-1 top-5 params GFLOPs network depth: 101 network depth: 152</cell></row><row><cell>ResNet (baseline)[7</cell><cell></cell></row></table><note>less than 10 epochs to outperform the best results of ResNet on all first 30 epochs. Thus, because of improved learning capabilities, our PyConvResNet can require significantly less epochs for training to outperform the baseline. Table 3 presents the comparison results of our proposed networks with other state-of-the-art networks on 50, 101, 152 layers. Our networks outperform the baseline ResNet [7] by a large margin on all depths. For instance, our PyConvResNet improves the top-1 error rate from 23.88 to 22.12 on 50 layers, while having lower number of parameters and FLOPs. Remarkably, our PyConvHGResNet with 50 layers outperforms ResNet with 152 layers on top-1 error rate. Besides providing better results than pre-activation ResNet</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Validation error rates comparison results of PyConvResNet on ImageNet with different training settings, for network depth 50 and 101 ( † on the already trained model with 224×224 crop, just perform the test on 320×320).</figDesc><table><row><cell>Network</cell><cell>top-1</cell><cell cols="2">test crop: 224×224 top-5 GFLOPs</cell><cell cols="3">test crop: 320×320  † top-1 top-5 GFLOPs</cell><cell>params</cell></row><row><cell>PyConvResNet-50</cell><cell>22.12</cell><cell>6.20</cell><cell>3.88</cell><cell>21.10</cell><cell>5.55</cell><cell>7.91</cell><cell>24.85</cell></row><row><cell>PyConvResNet-50 + augment</cell><cell>20.56</cell><cell>5.31</cell><cell>3.88</cell><cell>19.41</cell><cell>4.75</cell><cell>7.91</cell><cell>24.85</cell></row><row><cell>PyConvResNet-101</cell><cell>20.99</cell><cell>5.53</cell><cell>7.31</cell><cell>20.03</cell><cell>4.82</cell><cell>14.92</cell><cell>42.31</cell></row><row><cell>PyConvResNet-101 + augment</cell><cell>19.42</cell><cell>4.87</cell><cell>7.31</cell><cell>18.51</cell><cell>4.28</cell><cell>14.92</cell><cell>42.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Head-to-Head comparison on image segmentation (ResNet-50 as backbone) on ADE20K.</figDesc><table><row><cell>Head</cell><cell cols="3">output stride backbone: 8 mean IoU pixel Acc. params</cell><cell>GFLOPs</cell><cell cols="3">output stride backbone: 16 mean IoU pixel Acc. params GFLOPs</cell></row><row><cell>baseline [23]: 3×3 conv</cell><cell>37.87</cell><cell>78.17</cell><cell cols="2">35.42 131.37</cell><cell>36.84</cell><cell>77.84</cell><cell>35.42 39.52</cell></row><row><cell>DeepLabv3 [24]: ASPP</cell><cell>40.91</cell><cell>79.92</cell><cell cols="2">41.48 151.17</cell><cell>40.34</cell><cell>79.44</cell><cell>41.48 44.47</cell></row><row><cell>PSPNet [23]: PPM</cell><cell>41.24</cell><cell>80.01</cell><cell cols="2">49.06 165.42</cell><cell>39.75</cell><cell>79.17</cell><cell>49.06 48.08</cell></row><row><cell>PyConvSegNet: PyConvPH</cell><cell>41.54</cell><cell>80.18</cell><cell cols="2">34.40 116.84</cell><cell>40.43</cell><cell>79.45</cell><cell>34.40 36.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>PyConvSegNet results with different backbones.</figDesc><table><row><cell>Backbone</cell><cell>mean IoU(%) SS MS</cell><cell>pixel Acc.(%) SS MS</cell><cell cols="2">params GFLOPs</cell></row><row><cell>ResNet-50</cell><cell cols="2">41.54 42.88 80.18 80.97</cell><cell>34.40</cell><cell>116.84</cell></row><row><cell>PyConvResNet-50</cell><cell cols="2">42.08 43.31 80.31 81.18</cell><cell>33.69</cell><cell>114.18</cell></row><row><cell>ResNet-101</cell><cell cols="2">42.88 44.39 80.75 81.60</cell><cell>53.39</cell><cell>185.47</cell></row><row><cell cols="3">PyConvResNet-101 42.93 44.58 80.91 81.77</cell><cell>51.15</cell><cell>177.29</cell></row><row><cell>ResNet-152</cell><cell cols="2">44.04 45.28 81.18 81.89</cell><cell>69.03</cell><cell>242.00</cell></row><row><cell cols="3">PyConvResNet-152 44.36 45.64 81.54 82.36</cell><cell>65.48</cell><cell>229.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>State-of-the-art comparison on ADE20K (single model). ( † increase the crop size just for inference from 473×473 to 617×617; ♣ just increase training epochs from 100 to 120 and train over training+validation sets; the results on testing set are provided by the official evaluation server, as the labels are not publicly available. The score is the average of mean IoU and pixel Acc. results.)</figDesc><table><row><cell>Method</cell><cell cols="5">Validation set mIoU pAcc. mIoU pAcc. Score Testing set</cell></row><row><cell>FCN [38]</cell><cell cols="2">29.39 71.32</cell><cell>-</cell><cell cols="2">-44.80</cell></row><row><cell>DilatedNet [39]</cell><cell cols="2">32.31 73.55</cell><cell>-</cell><cell cols="2">-45.67</cell></row><row><cell>SegNet [40]</cell><cell cols="2">21.64 71.00</cell><cell>-</cell><cell cols="2">-40.79</cell></row><row><cell>RefineNet [41]</cell><cell>40.70</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UperNet [42]</cell><cell cols="2">41.22 79.98</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PSANet [43]</cell><cell cols="2">43.77 81.51</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KE-GAN [44]</cell><cell cols="2">37.10 80.50</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CFNet [45]</cell><cell>44.89</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CiSS-Net [46]</cell><cell cols="2">42.56 80.77</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EncNet [47]</cell><cell cols="2">44.65 81.69</cell><cell>-</cell><cell cols="2">-55.67</cell></row><row><cell>PSPNet-152 [23]</cell><cell cols="2">43.51 81.38</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PSPNet-269 [23]</cell><cell cols="2">44.94 81.69</cell><cell>-</cell><cell cols="2">-55.38</cell></row><row><cell cols="6">PyConvSegNet-152 45.64 82.36 37.75 73.61 55.68</cell></row><row><cell cols="3">PyConvSegNet-152  † 45.99 82.49</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">PyConvSegNet-152 ♣ -</cell><cell>-</cell><cell cols="3">39.13 73.91 56.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>For each of the six output feature maps selected for detection {S3 F M , H F M 1 , H F M 2 , H F M 3 , H F M 4 , H F M 5 } the framework performs the detection using a coresponding number of default boxes (anchor boxes) {4, 6, 6, 6, 4, 4} for each spatial location. For instance, for (S3 F M ) output feature maps with the spatial resolution 38×38, using the four default boxes on each location results in 5776 detections. For localizing each bounding box, there are four values that network should predict (loc: ∆(cx, cy, w, h), where cx and cy represent the center point of the bounding box, w and h the width and height of the bounding box). This bounding box offset output values are measured relative to a</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>PyConvSSD with 300×300 input image size (results on COCO val2017).<ref type="bibr" target="#b49">50</ref> 26.20 43.97 26.96 8.12 28.22 42.64 24.50 35.41 37.07 12.61 40.76 57.25 22.89 20.92 PyConvSSD-50 29.16 47.26 30.24 9.31 31.21 47.79 26.14 37.81 39.61 13.79 43.87 60.98 21.55 19.71 Baseline SSD-101 29.58 47.69 30.80 9.38 31.96 47.64 26.47 38.00 39.64 14.09 43.54 61.03 41.89 48.45 PyConvSSD-101 31.27 50.00 32.67 10.65 33.76 51.75 27.33 39.33 41.07 15.48 45.53 63.44 39.01 45.02</figDesc><table><row><cell>Architecture</cell><cell>Avg. Precision, IoU: Avg. Precision, Area: Avg. Recall, #Dets: Avg. Recall, Area: params GFLOPs 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L</cell></row><row><cell>Baseline SSD-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Video recognition networks.</figDesc><table><row><cell>stage</cell><cell>output</cell><cell cols="2">ResNet3D-50</cell><cell></cell><cell></cell><cell cols="3">PyConvResNet3D-50</cell></row><row><cell></cell><cell>16×112×112</cell><cell cols="2">5×7×7, 64 stride (1,2,2)</cell><cell></cell><cell></cell><cell>5×7×7, 64 stride (1,2,2)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">1×3×3 max pool</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>stride (1,2,2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1×1×1, 64</cell><cell cols="2"></cell></row><row><cell>1</cell><cell>16×56×56</cell><cell>1×1×1, 64 3×3×3, 64 1×1×1, 256</cell><cell>×3</cell><cell>      </cell><cell cols="2">PyConv4, 64:    7×9×9, 16, G=16 5×7×7, 16, G=8 3×5×5, 16, G=4 3×3×3, 16, G=1</cell><cell cols="2">         </cell><cell>×3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1×1×1, 256</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2"></cell><cell>1×1×1, 128</cell><cell></cell></row><row><cell>2</cell><cell>16×28×28</cell><cell>1×1×1, 128 3×3×3, 128 1×1×1, 512</cell><cell>×4</cell><cell cols="2">    </cell><cell>PyConv3, 128: 5×7×7, 64, G=8 3×5×5, 32, G=4 3×3×3, 32, G=1</cell><cell cols="2">     ×4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1×1×1, 512</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3"> 1×1×1, 256</cell><cell></cell></row><row><cell>3</cell><cell>8×14×14</cell><cell>1×1×1, 256 3×3×3, 256 1×1×1, 1024</cell><cell>×6</cell><cell cols="3">    PyConv2, 256: 3×5×5, 128, G=4 3×3×3, 128, G=1</cell><cell>   </cell><cell>×6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1×1×1, 1024</cell><cell></cell></row><row><cell>4</cell><cell>4×7×7</cell><cell>1×1×1, 512 3×3×3, 512 1×1×1, 2048</cell><cell>×3</cell><cell cols="5"> 1×1×1, 512 1×1×1, 2048   PyConv1, 512: [3×3×3, 512, G=1]   ×3</cell></row><row><cell></cell><cell>1×1×1</cell><cell cols="2">global avg pool 400-d fc</cell><cell></cell><cell></cell><cell cols="2">global avg pool 400-d fc</cell></row><row><cell></cell><cell># params</cell><cell>47.00 × 10 6</cell><cell></cell><cell></cell><cell></cell><cell>44.91 × 10 6</cell><cell></cell></row><row><cell></cell><cell>FLOPs</cell><cell>93.26 × 10 9</cell><cell></cell><cell></cell><cell></cell><cell>91.81 × 10 9</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Video recognition error rates (%). Architecture top-1 top-5 params GFLOPs ResNet3D-50 [7] 37.01 15.41 47.00 93.26 PyConvResNet3D-50 34.56 13.34 44.91 91.81</figDesc><table><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>top-1 error (%)</cell><cell>50 60 70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell>1</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40 epochs 50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ArXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ICML</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>ArXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improved residual networks for image and video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Duta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno>ArXiv:2004.04989</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>ArXiv:1706.02677</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno>ArXiv:1710.03740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ke-gan: Knowledge embedded generative adversarial networks for semi-supervised scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context-reinforced semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno>ArXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>ArXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
