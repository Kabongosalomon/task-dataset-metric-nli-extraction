<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
							<email>hershey@merl.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MERL</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Le Roux</surname></persName>
							<email>leroux@merl.com</email>
							<affiliation key="aff2">
								<orgName type="institution">MERL</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<email>watanabe@merl.com</email>
							<affiliation key="aff3">
								<orgName type="institution">MERL</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of acoustic source separation in a deep learning framework we call "deep clustering". Rather than directly estimating signals or masking functions, we train a deep network to produce spectrogram embeddings that are discriminative for partition labels given in training data. Previous deep network approaches provide great advantages in terms of learning power and speed, but previously it has been unclear how to use them to separate signals in a classindependent way. In contrast, spectral clustering approaches are flexible with respect to the classes and number of items to be segmented, but it has been unclear how to leverage the learning power and speed of deep networks. To obtain the best of both worlds, we use an objective function that to train embeddings that yield a low-rank approximation to an ideal pairwise affinity matrix, in a classindependent way. This avoids the high cost of spectral factorization and instead produces compact clusters that are amenable to simple clustering methods. The segmentations are therefore implicitly encoded in the embeddings, and can be "decoded" by clustering. Preliminary experiments show that the proposed method can separate speech: when trained on spectrogram features containing mixtures of two speakers, and tested on mixtures of a held-out set of speakers, it can infer masking functions that improve signal quality by around 6dB. We show that the model can generalize to three-speaker mixtures despite training only on twospeaker mixtures. The framework can be used without class labels, and therefore has the potential to be trained on a diverse set of sound types, and to generalize to novel sources. We hope that future work will lead to segmentation of arbitrary sounds, with extensions to microphone array methods as well as image segmentation and other domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In real world perception, we are often confronted with the problem of selectively attending to objects whose features are intermingled with one another in the incoming sensory signal. In computer vision, the problem of scene analysis is to partition an image or video into regions attributed to the visible objects present in the scene. In audio there is a corresponding problem known as auditory scene analysis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, which seeks to identify the components of audio signals corresponding to individual sound sources in a mixture signal. Both of these problems can be approached as segmentation problems, where we formulate a set of "elements" in the signal via an indexed set of features, each of which carries (typically multi-dimensional) information about part of the signal. For images, these elements are typically defined spatially in terms of pixels, whereas for audio signals they may be defined in terms of time-frequency coordinates. The segmentation problem is then solved by segmenting elements into groups or partitions, for example by assigning a group label to each element. Note that although clustering methods can be applied to segmentation problems, the segmentation problem is technically different in that clustering is classically formulated as a domain-independent problem based on simple objective functions defined on pairwise point relations, whereas partitioning may depend on complex processing of the whole input, and the task objective may be arbitrarily defined via training examples with given segment labels.</p><p>Segmentation problems can be broadly categorized into class-based segmentation problems where the goal is to learn from training class labels to label known object classes, versus more general partition-based segmentation problems where the task is to learn from labels of partitions, without requiring object class labels, to segment the input. Solving the partition-based problem has the advantage that unknown objects could then be segmented. In this paper, we propose a new partitionbased approach which learns embeddings for each input elements, such that the correct labeling can be determined by simple clustering methods. We focus on the single-channel audio domain, although our methods are applicable to other domains such as images and multi-channel audio. The motivation for segmenting in this domain, as we shall describe later, is that using the segmentation as a mask, we can extract parts of the target signals that are not corrupted by other signals.</p><p>Since class-based approaches are relatively straightforward, and have been tremendously successful at their task, we first briefly mention this general approach. In class based vision models, such as <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, a hierarchical classification scheme is trained to estimate the class label of each pixel or superpixel region. In the audio domain, single-channel speech separation methods, for example, segment the time-frequency elements of the spectrogram into regions dominated by a target speaker, either based on classifiers <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, or generative models <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. In recent years, the success of deep neural networks for classification problems has naturally inspired their use in class-based segmentation problems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>, where they have proven very successful.</p><p>However class-based approaches have some important limitations. First, of course, the assumed task of labeling known classes fundamentally does not address the general problem in real world signals that there may be a large number of possible classes, and many objects may not have a well-defined class. It is also not clear how to directly apply current class-based approaches to the more general problem. Class-based deep network models for separating sources require explicitly representing output classes and object instances in the output nodes, which leads to complexities in the general case. Although generative model-based methods can in theory be flexible with respect to the number of model types and instances after training, inference typically cannot scale computationally to the potentially larger problem posed by more general segmentation tasks.</p><p>In contrast, humans seem to solve the partition-based problem, since they can apparently segment well even with novel objects and sounds. This observation is the basis of Gestalt theories of perception, which attempt to explain perceptual grouping in terms of features such as proximity and similarity <ref type="bibr" target="#b12">[13]</ref>. The partition-based segmentation task is closely related, and follows from a tradition of work in image segmentation and audio separation. Application of the perceptual grouping theory to audio segmentation is generally known as computational auditory scene analysis (CASA) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Spectral clustering is an active area of machine learning research with application to both image and audio segmentation. It uses local affinity measures between features of elements of the signal, and optimizes various objective functions using spectral decomposition of the normalized affinity matrix <ref type="bibr" target="#b15">[16]</ref>. In contrast to conventional central clustering algorithms such as k-means, spectral clustering has the advantage that it does not require points to be tightly clustered around a central prototype, and can find clusters of arbitrary topology, provided that they form a connected sub-graph. Because of the local form of the pairwise kernel functions used, in difficult spectral clustering problems the affinity matrix has a sparse block-diagonal structure that is not directly amenable to central clustering, which works well when the block diagonal affinity structure is dense. The powerful but computationally expensive eigenspace transformation step of spectral clustering addresses this, in effect, by "fattening" the block structure, so that connected components become dense blocks, prior to central clustering <ref type="bibr" target="#b16">[17]</ref>.</p><p>Although affinity-based methods were originally unsupervised inference methods, multiple-kernel learning methods such as <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> were later introduced to train weights used to combine separate affinity measures. This allows us to consider using them for partition-based segmentation tasks in which partition labels are available, but without requiring specific class labels. In <ref type="bibr" target="#b16">[17]</ref>, this was applied to speech separation by including a variety of complex features developed to implement various auditory scene analysis grouping principles, such as similarity of onset/offset, pitch, spectral envelope, and so on, as affinities between time-frequency regions of the spectrogram. The input features included a dual pitch-tracking model in order to improve upon the relative simplicity of kernel-based features, at the expense of generality.</p><p>Rather than using specially designed features and relying on the strength of the spectral clustering framework to find clusters, we propose to use deep learning to derive embedding features that make the segmentation problem amenable to simple and computationally efficient clustering algorithms such as k-means, using the partition-based training approach. Learned feature transformations known as embeddings have recently been gaining significant interest in many fields. Unsupervised embeddings obtained by auto-associative deep networks, used with relatively simple clustering algorithms, have recently been shown to outperform spectral clustering methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> in some cases. Embeddings trained using pairwise metric learning, such as word2vec <ref type="bibr" target="#b20">[21]</ref> using neighborhood-based partition labels, have also been shown to have interesting invariance properties. We present below an objective function that minimizes the distances between embeddings of elements within a partition, while maximizing the distances between embeddings for elements in different partitions. This appears to be an appropriate criterion for central clustering methods. The proposed embedding approach has the attractive property that all partitions and their permutations can be represented implicitly using the fixed-dimensional output of the network.</p><p>The experiments described below show that the proposed method can separate speech using a speaker-independent model with an open set of speakers at test time. As in <ref type="bibr" target="#b16">[17]</ref>, we derive partition labels by mixing signals together and observing their spectral dominance patterns. After training on a database of mixtures of speakers trained in this way, we show that without any modification the model shows a promising ability to separate three-speaker mixtures despite training only on two-speaker mixtures. Although results are preliminary, the hope is that this work leads to methods that can achieve class-independent segmentation of arbitrary sounds, with additional application to image segmentation and other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning deep embeddings for clustering</head><p>We define as x a raw input signal, such as an image or a time-domain waveform, and as X n = g n (x), n ∈ {1, . . . , N }, a feature vector indexed by an element n. In the case of images, n typically may be a superpixel index and X n some vector-valued features of that superpixel; in the case of audio signals, n may be a time-frequency index (t, f ), where t indexes frames of the signal and f frequencies, and X n = X t,f the value of the complex spectrogram at the corresponding timefrequency bin. We assume that there exists a reasonable partition of the elements n into regions, which we would like to find, for example to further process the features X n separately for each region. In the case of audio source separation, for example, these regions could be defined as the sets of time-frequency bins in which each source dominates, and estimating such a partition would enable us to build time-frequency masks to be applied to X n , leading to time-frequency representations that can be inverted to obtain isolated sources.</p><p>To estimate the partition, we seek a K-dimensional embedding V = f θ (x) ∈ R N ×K , parameterized by θ, such that performing some simple clustering in the embedding space will likely lead to a partition of {1, . . . , N } that is close to the target one. In this work, V = f θ (x) is based on a deep neural network that is a global function of the entire input signal x (we allow for a feature extraction step to create the network input; in general, the input features may be completely different from X n ). Thus our transformation can take into account global properties of the input, and the embedding can be considered a permutation-and cardinality-independent encoding of the network's estimate of the signal partition. Here we consider a unit-norm embedding, so that |v n | 2 = k v 2 n,k = 1, ∀n, where v n = {v n,k } and v n,k is the value of the k-th dimension of the embedding for element n. We omit the dependency of V on θ to simplify notations.</p><p>The partition-based training requires a reference label indicator Y = {y n,c }, mapping each element n to each of c arbitrary partition classes, so that y n,c = 1 if element n is in partition c. For a training objective, we seek embeddings that enable accurate clustering according to the partition labels. To do this, we need a convenient expression that is invariant to the number and permutations of the partition labels from one training example to the next. One such objective for minimization is</p><formula xml:id="formula_0">C(θ) = |V V T − Y Y T | 2 W = i,j:yi=yj ( v i , v j − 1) 2 d i + i,j:yi =yj ( v i , v j − 0) 2 d i d j ,<label>(1)</label></formula><formula xml:id="formula_1">= i,j:yi=yj |v i − v j | 2 d i + i,j |v i − v j | 2 − 2 2 4 d i d j − N,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">|A| 2 W = i,j w i,j a 2 i,j is a weighted Frobenius norm, with W = d − 1 2 d − T 2 , where d i = Y Y T 1 is an (N × 1)</formula><p>vector of partition sizes: that is, d i = |{j : y i = y j }|. In the above we use the fact that |v n | 2 = 1, ∀n. Intuitively, this objective pushes the inner product v i , v j to 1 when i and j are in the same partition, and to 0 when they are in different partitions. Alternately, we see from <ref type="formula" target="#formula_1">(2)</ref> that it pulls the squared distance |v i − v j | 2 to 0 for elements within the same partition, while preventing the embeddings from trivially collapsing into the same point. Note that the first term is the objective function minimized by k-means, as a function of cluster assignments, and in this context the second term is a constant. So the objective reasonably tries to lower the k-means score for the labeled cluster assignments at training time.</p><p>This formulation can be related to spectral clustering as follows. We can define an ideal affinity matrix A * = Y Y T , that is block diagonal up to permutation and use an inner-product kernel, so that A = V V T is our affinity matrix. Our objective becomes C = |A − A * | 2 F , which measures the deviation of the model's affinity matrix from the ideal affinity. Note that although this function ostensibly sums over all pairs of data points i, j, the low-rank nature of the objective leads to an efficient implementation, defining D = diag(Y Y T 1):</p><formula xml:id="formula_3">C = |V V T − Y Y T | 2 W = |V T D − 1 2 V | 2 F − 2|V T D − 1 2 Y | 2 F + |Y T D − 1 2 Y | 2 F ,<label>(3)</label></formula><p>which avoids explicitly constructing the N ×N affinity matrix. In practice, N is orders of magnitude greater than K, leading to a significant speedup. To optimize a deep network, we typically need to use first-order methods. Fortunately derivatives of our objective function with respect to V are also efficiently obtained due to the low-rank structure:</p><formula xml:id="formula_4">∂C ∂V T = 4D − 1 2 V V T D − 1 2 V − 4D − 1 2 Y Y T D − 1 2 V.<label>(4)</label></formula><p>This low-rank formulation also relates to spectral clustering in that the latter typically requires the Nyström low-rank approximation to the affinity matrix, <ref type="bibr" target="#b21">[22]</ref> for efficiency, so that the singular value decomposition (SVD) of an N × K matrix can be substituted for the much more expensive eigenvalue decomposition of the K × K normalized affinity matrix. Rather than following spectral clustering in making a low-rank approximation of a full-rank model, our method can be thought of as directly optimizing a low-rank affinity matrix so that processing is more efficient and parameters are tuned to the low-rank structure.</p><p>At test time, we compute the embeddings V on the test signal, and cluster the rows v i ∈ R K , for example using k-means. We also alternately perform a spectral-clustering style dimensionality reduction before clustering, starting with a singular value decomposition (SVD),Ṽ = U SR T , of </p><formula xml:id="formula_5">normalizedṼ = D − 1 2 V , where D = V V T 1 N ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Speech separation experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>We evaluate the proposed model on a speech separation task: the goal is to separate each speech signal from a mixture of multiple speakers. While separating speech from non-stationary noise is in general considered to be a difficult problem, separating speech from other speech signals is particularly challenging because all sources belong to the same class, and share similar characteristics. Mixtures involving speech from same gender speakers are the most difficult since the pitch of the voice is in the same range. We here consider mixtures of two speakers and three speakers (the latter always containing at least two speakers of the same gender). However, our method is not limited in the number of sources it can handle or the vocabulary and discourse style of the speakers. To investigate the effectiveness of our proposed model, we built a new dataset of speech mixtures based on the Wall Street Journal (WSJ0) corpus, leading to a more challenging task than in existing datasets. Existing datasets are too limited for evaluation of our model because, for example, the speech separation challenge <ref type="bibr" target="#b23">[24]</ref> only contains a mixture of two speakers, with a limited vocabulary and insufficient training data. The SISEC challenge (e.g., <ref type="bibr" target="#b24">[25]</ref>) is limited in size and designed for the evaluation of multi-channel separation, which can be easier than single-channel separation in general.</p><p>A training set consisting of 30 hours of two-speaker mixtures was generated by randomly selecting utterances by different speakers from the WSJ0 training set si_tr_s, and by mixing them at various signal-to-noise ratios (SNR) between 0 dB and 5 dB. We also designed the two training subsets from the above whole training set (whole), one considered the balance of the mixture of the genders (balanced, 22.5 hours), and the other only used the mixture of female speakers (female, 7.5 hours). 10 hours of cross validation set were generated similarly from the WSJ0 training set, which is used to optimize some tuning parameters, and to evaluate the source separation performance of the closed speaker experiments (closed speaker set). 5 hours of evaluation data was generated similarly using utterances from sixteen speakers from the WSJ0 development set si_dt_05 and evaluation set si_et_05, which are based on the different speakers from our training and closed speaker sets (open speaker set). Note that many existing speech separation methods (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>) cannot handle the open speaker problem without special adaptation procedures, and generally require knowledge of the speakers in the evaluation. For the evaluation data, we also created 100 utterances of three-speaker mixtures for each closed and open speaker set as an advanced setup. All data were downsampled to 8 kHz before processing to reduce computational and memory costs.</p><p>The input features X were the log short-time Fourier spectral magnitudes of the mixture speech, computed with a 32 ms window length, 8 ms window shift, and the square root of the hann window. To ensure the local coherency, the mixture speech was segmented with the length of 100 frames, roughly the length of one word in speech, and processed separately to output embedding V based on the proposed model. The ideal binary mask was used to build the target Y when training our network. The ideal binary mask gives ownership of a time-frequency bin to the source whose magnitude is maximum among all sources in that bin. The mask values were assigned with 1 for active and 0 otherwise (binary), making Y Y T as the ideal affinity matrix for the mixture.</p><p>To avoid problems due to the silence regions during separation, a binary weight for each timefrequency bin was used during the training process, only retaining those bins such that each source's magnitude at that bin is greater than some ratio (arbitrarily set to -40 dB) of the source's maximum magnitude. Intuitively, this binary weight guides the neural network to ignore bins that are not important to all sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training procedure</head><p>Networks in the proposed model were trained given the above input X and the ideal affinity matrix Y Y T . The network structure used in our experiments has two bi-directional long short-term memory (BLSTM) layers, followed with one feedforward layer. Each BLSTM layer has 600 hidden cells and the feedforward layer corresponds with the embedding dimension (i.e., K). Stochastic gradient descent with momentum 0.9 and fixed learning rate 10 − 5 was used for training. In each updating step, a Gaussian noise with zero mean and 0.6 variance was added to the weight. We prepared several networks used in the speech separation experiments using different embedding dimensions from 5 to 60. In addition, two different activation functions (logistic and tanh) were explored to form the embedding V with different ranges of v n,k . For each embedding dimension, the weights for the corresponding network were initialized randomly from the scratch according to a normal distribution with zero mean and 0.1 variance with the tanh activation and whole training set. In the experiments of a different activation (logistic) and different training subsets (balanced and female), the network was initialized with the one with the tanh activation and whole training set. The implementation was  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Speech separation procedure</head><p>In the test stage, the speech separation was performed by constructing a time-domain speech signal based on time-frequency masks for each speaker. The time-frequency masks for each source speaker were obtained by clustering the row vectors of embedding V , where V was outputted from the proposed model for each segment (100 frames), similarly to the training stage. The number of clusters corresponds to the number of speakers in the mixture. We evaluated various types of clustering methods: k-means on the whole utterance by concatenating the embeddings V for all segments; k-means clustering within each segment; spectral clustering within each segment. For the withinsegment clusterings, one needs to solve a permutation problem, as clusters are not guaranteed to be consistent across segments. For those cases, we report oracle permutation results (i.e., permutations that minimize the L 2 distance between the masked mixture and each source's complex spectrogram) as an upper bound on performance.</p><p>One interesting property of the proposed model is that it can potentially generalize to the case of three-speaker mixtures without changing the training procedure in Section 3.2. To verify this, speech separation experiments on three-speaker mixtures were conducted using the network trained with two speaker mixtures, simply changing the above clustering step from 2 to 3 clusters. Of course, training the network including mixtures involving more than two speakers should improve performance further, but we shall see that the method does surprisingly well even without retraining.</p><p>As a standard speech separation method, supervised sparse non-negative matrix factorization (SNMF) was used as a baseline <ref type="bibr" target="#b25">[26]</ref>. While SNMF may stand a chance separating speakers in male-female mixtures when using a concatenation of bases trained separately on speech by other speakers of each gender, it would not make sense to use it in the case of same-gender mixtures. To give SNMF the best possible advantage, we use an oracle: at test time we give it the basis functions trained on the actual speaker in the mixture. For each speaker, 256 bases were learned on the clean training utterances of that speaker. Magnitude spectra with 8 consecutive frames of left context were used as input features. At test time, the basis functions for the two speakers in the test mixture were concatenated, and their corresponding activations computed on the mixture. The estimated models for each speaker were then used to build a Wiener-filter like mask applied to the mixture, and the corresponding signals reconstructed by inverse STFT.</p><p>For all the experiment, performance was evaluated in terms of averaged signal-to-distortion ratio (SDR) using the bss_eval toolbox <ref type="bibr" target="#b26">[27]</ref>. The initial SDR averaged over the mixtures was 0.16 dB for two speaker mixtures and −2.95 dB for three speaker mixtures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and discussion</head><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, both the oracle and non-oracle clustering methods for proposed system significantly outperform the oracle NMF baseline, even though the oracle NMF is a strong model with the important advantage of knowing the speaker identity and has speaker-dependent models. For the proposed system the open speaker performance is similar to the closed speaker results, indicating that the system can generalize well to unknown speakers, without any explicit adaptation methods. For different clustering methods, the oracle k-means outperforms the oracle "spectral clustering" by 0.19 dB showing that the embedding represents centralized clusters. To be fair, what we call spectral clustering here is using our outer product kernel instead of a local kernel function such as a Gaussian, as commonly used in spectral clustering. However a Gaussian kernel could not be used here due to computational complexity. Also note that the oracle clustering method in our experiment resolves the permutation of two (or three in <ref type="table" target="#tab_3">Table 4</ref>) speakers in each segment. In the dataset, each utterance usually contains 6∼8 segments so the permutation search space is relatively small for each utterance. Hence this problem may have an easy solution to be explored in future work. For the non-oracle experiments, the whole utterance clustering also performs relatively well compared to baseline. Given the fact that the system was only trained with individual segments, the effectiveness of the whole utterance clustering suggests that the network learns features that are globally important, such us pitch, timbre etc.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, the K = 5 system completely fails, either because optimization of the current network architecture fails, or the embedding fundamentally requires more dimensions. The performance of K = 20, K = 40, K = 60 are similar, showing that the system can operate in a wide range of parameter values. We arbitrarily used tanh networks in most of the experiments because the tanh network has larger embedding space than logistic network. However, in <ref type="table" target="#tab_1">Table 2</ref>, we show that in retrospect the logistic network performs slightly better than the tanh one.</p><p>In <ref type="table" target="#tab_2">Table 3</ref>, since the female and male mixture is an intrinsically easier segmentation problem, the performance of mixture between female and male is significantly better than the same gender mixtures for all situations. As mentioned in Section 3, the random selection of speaker would also be a factor for the large gap. With more balanced training data, the system has better performance for the same gender separation with a sacrifice of its performance for different gender mixture. If we only focus on female mixtures, the performance is still better. <ref type="figure" target="#fig_2">Figure 2</ref> shows an example of embeddings for two different mixtures (female-female and malefemale), in which a few embedding dimensions are plotted for each time-frequency bin in order to show how they are sensitive to different aspects of each signal.</p><p>In <ref type="table" target="#tab_3">Table 4</ref>, the proposed system can also separate the mixture of three speakers, even though it is only trained on two-speaker mixtures. As discussed in previous sections, unlike many separation algorithms, deep clustering can naturally scale up to more sources, and thus make it suitable for many real world tasks when the number of sources is not available or fixed. example of the separation for three speaker mixture in the open speaker set case. Note that we also did experiments with mixtures of three fixed speakers for the training and testing data, and the SDR improvement of the proposed system is 6.15.</p><p>Deep clustering has been evaluated in a variety of conditions and parameter regimes, on a challenging speech separation problem. Since these are just preliminary results, we expect that further refinement of the model will lead to significant improvement. For example, by combining the clustering step into the embedding BLSTM network using the deep unfolding technique <ref type="bibr" target="#b27">[28]</ref>, the separation could be jointly trained with embedding and lead to potential better result. Also in this work, the BLSTM network has a relatively uniform structure. Alternative architectures with different time and frequency dependencies, such as deep convolutional neural networks <ref type="bibr" target="#b2">[3]</ref>, or hierarchical recursive embedding networks <ref type="bibr" target="#b3">[4]</ref>, could also be helpful in terms of learning and regularization. Finally, scaling up training on databases of more disparate audio types, as well as applications to other domains such as image segmentation, would be prime candidates for future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>sorted by decreasing eigenvalue, and clustering the normalized rows of the matrix of m principal left singular vectors, with the i'th row given bỹ u i,r = u i,r / m r =1 u i,r : r ∈ [1, m], similar to<ref type="bibr" target="#b22">[23]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 Figure 1 :</head><label>11</label><figDesc>An example of three-speaker separation. Top: log spectrogram of the input mixture. Middle: ideal binary mask for three speakers. The dark blue shows the silence part of the mixture. Bottom: output mask from the proposed system trained on two-speaker mixtures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Examples of embeddings for two mixtures: f+f (left) and f+m (right). 1st row: spectrogram; 2nd row: ideal binary mask; 3rd-5th row: embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>SDR improvements (in dB) for different clustering methods.</figDesc><table><row><cell>method</cell><cell cols="2">closed speaker set open speaker set</cell></row><row><cell>oracle NMF</cell><cell>5.06</cell><cell>-</cell></row><row><cell>DC oracle k-means</cell><cell>6.54</cell><cell>6.45</cell></row><row><cell>DC oracle spectral</cell><cell>6.35</cell><cell>6.26</cell></row><row><cell>DC global k-means</cell><cell>5.87</cell><cell>5.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>SDR improvements (in dB) for different embedding dimensions K and activation functions</figDesc><table><row><cell></cell><cell cols="2">closed speaker set</cell><cell cols="2">open speaker set</cell></row><row><cell>model</cell><cell cols="4">DC oracle DC global DC oracle DC global</cell></row><row><cell>K = 5</cell><cell>-0.77</cell><cell>-0.96</cell><cell>-0.74</cell><cell>-1.07</cell></row><row><cell>K = 10</cell><cell>5.15</cell><cell>4.52</cell><cell>5.29</cell><cell>4.64</cell></row><row><cell>K = 20</cell><cell>6.25</cell><cell>5.56</cell><cell>6.38</cell><cell>5.69</cell></row><row><cell>K = 40</cell><cell>6.54</cell><cell>5.87</cell><cell>6.45</cell><cell>5.81</cell></row><row><cell>K = 60</cell><cell>6.00</cell><cell>5.19</cell><cell>6.08</cell><cell>5.28</cell></row><row><cell>K = 40 logistic</cell><cell>6.59</cell><cell>5.86</cell><cell>6.61</cell><cell>5.95</cell></row><row><cell cols="5">based on CURRENNT, a publicly available training software for DNN and (B)LSTM networks with</cell></row><row><cell cols="3">GPU support (https://sourceforge.net/p/currennt).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>SDR improvement (in dB) for each type of mixture. Scores averaged over male-male (m+m), female-female (f+f), female-male (f+m), or all mixtures.</figDesc><table><row><cell></cell><cell>training gender</cell><cell></cell><cell cols="2">closed speaker set</cell><cell></cell><cell>open speaker set</cell></row><row><cell>method</cell><cell>distribution</cell><cell cols="2">m+m f+f</cell><cell>f+m all</cell><cell cols="2">m+m f+f</cell><cell>f+m all</cell></row><row><cell cols="3">oracle NMF speaker dependent 3.25</cell><cell cols="3">3.31 6.53 4.90 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DC oracle permute</cell><cell>whole balanced female</cell><cell>3.79 3.89 -</cell><cell cols="3">4.29 9.04 6.54 4.49 4.35 8.74 6.42 4.61 5.03 ---</cell><cell>3.21 8.69 6.45 3.49 8.27 6.41 4.04 --</cell></row><row><cell>DC global k-means</cell><cell>whole balanced female</cell><cell>2.54 2.78 -</cell><cell cols="3">2.85 9.07 5.87 3.51 2.87 8.63 5.72 3.89 3.88 ---</cell><cell>1.42 8.57 5.80 1.74 8.27 5.83 2.56 --</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>SDR improvement (in dB) for three speaker mixture</figDesc><table><row><cell>method</cell><cell cols="2">closed speaker set open speaker set</cell></row><row><cell>oracle NMF</cell><cell>4.42</cell><cell>-</cell></row><row><cell>DC oracle</cell><cell>3.50</cell><cell>2.81</cell></row><row><cell>DC global</cell><cell>2.74</cell><cell>2.22</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Auditory scene analysis: The perceptual organization of sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Auditory grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Darwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Carlyon</surname></persName>
		</author>
		<editor>Hearing, B. Moore</editor>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recursive context propagation network for semantic scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2447" to="2455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutive speech bases and their application to supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating single-channel source separation masks: Relevance vector machine classifiers vs. pitch-based masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SAPA</title>
		<meeting>SAPA</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An algorithm that improves speech intelligibility in noise for normal-hearing listeners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1486" to="1494" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring monaural features for classification-based speech segregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Factorial models and refiltering for speech separation and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1009" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single-channel speech separation using sparse non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Super-human multi-talker speech recognition: A graphical modeling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Kristjansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards scaling up classification-based speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1381" to="1390" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Laws of organization in perceptual forms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wertheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Source book of Gestalt psychology</title>
		<editor>W. A. Ellis</editor>
		<imprint>
			<date type="published" when="1938" />
			<biblScope unit="page" from="71" to="88" />
		</imprint>
		<respStmt>
			<orgName>Routledge and Kegan Paul</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Modelling auditory processing and organisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Cooke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Univ. of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Prediction-Driven Computational Auditory Scene Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis, MIT</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning spectral clustering, with application to speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning affinity functions for image segmentation: Combining patch-based and gradient-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep embedding network for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spectral grouping using the nyström method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="225" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monaural speech separation and recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bofill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gowreesunker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Q</forename><surname>Duong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Achievements and remaining challenges</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="1928" to="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sparse NMF -half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno>TR2015-023</idno>
		<imprint>
			<date type="published" when="2015-03" />
			<pubPlace>MERL, Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep unfolding: Model-based inspiration of novel deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2574</idno>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
