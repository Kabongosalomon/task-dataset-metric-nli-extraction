<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Video Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Rakhimov</surname></persName>
							<email>ruslan.rakhimov@skoltech.ru</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Volkhonskiy</surname></persName>
							<email>denis.volkhonskiy@skoltech.ru</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
							<email>a.artemov@skoltech.ru</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Zorin</surname></persName>
							<email>dzorin@cs.nyu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
							<email>e.burnaev@skoltech.ru</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Skolkovo Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Skolkovo Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Skolkovo Institute of Science and Technology</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Skolkovo Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Video Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The video generation task can be formulated as a prediction of future video frames given some past frames. Recent generative models for videos face the problem of high computational requirements. Some models require up to 512 Tensor Processing Units for parallel training. In this work, we address this problem via modeling the dynamics in a latent space. After the transformation of frames into the latent space, our model predicts latent representation for the next frames in an autoregressive manner. We demonstrate the performance of our approach on BAIR Robot Pushing and Kinetics-600 datasets. The approach tends to reduce requirements to 8 Graphical Processing Units for training the models while maintaining comparable generation quality. * Equal contribution 2 Source code is available at https://github.com/rakhimovv/lvt Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video prediction and generation is an important problem with a lot of down-stream applications: self-driving, anomaly detection, timelapse generation <ref type="bibr" target="#b28">[29]</ref>, animating landscape <ref type="bibr" target="#b13">[14]</ref> etc. The task is to generate the most probable future frames given several initial ones.</p><p>Recent advances in generative learning allow generation of realistic objects with high quality: images, text, and speech. However, video generation is still a very challenging task. Even for short videos (16 frames) of low resolution, neural networks require up to 512 Tensor Processing Units (TPUs) <ref type="bibr" target="#b25">[26]</ref> for parallel training. Despite this, the quality of the generated video remains low.</p><p>In this work, we introduce a Latent Video Transformer 2 . We combine the idea of representation learning and recurrent video generation. Instead of working in pixel space, we conduct the generation process in the latent space. Our model tends to significantly reduce computational requirements without significant deterioration in quality.</p><p>The key novelty in our model is the usage of a discrete latent space <ref type="bibr" target="#b41">[42]</ref>. It allows us to represent each frame as a set of indices. Thanks to discrete representation we can use autoregressive generative models and other approaches from natural language processing.</p><p>We analyzed the results of our model on two datasets: BAIR Robot Pushing <ref type="bibr" target="#b12">[13]</ref> and Kinetics 600 <ref type="bibr" target="#b4">[5]</ref>. On both datasets, we obtained quality comparable to state-of-the-art methods.</p><p>To summarize, our contributions are as follows:</p><p>• We proposed a new autoregressive model for video generation, that works in the latent space rather than pixel space; • We reduced computational requirements comparing to previously proposed methods.</p><p>2 Related work <ref type="bibr" target="#b1">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.1 Video Generation</head><p>Video generation is a long-standing problem. One can formulate it in different ways. Future video prediction, unconditional video synthesis or video to video translation <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45]</ref>. There exist other rare setups like generating video from one image <ref type="bibr" target="#b37">[38]</ref>. Video prediction and unconditional video synthesis have been addressed for a long time and the solutions include recurrent models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b3">4]</ref>, VAE-based models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9]</ref>, autoregressive models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b18">19]</ref>, normalizing flows <ref type="bibr" target="#b23">[24]</ref>, GANs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref> and optical flow <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>In the first attempts, fully deterministic models have been used. Later generative models were applied. Similar to image generation, video generative models inherited similar benefits and drawbacks. Variational autoencoder (VAE)-based models try to model videos in latent space but produce blurry results. Later GANs were applied to address those issues, but they suffer from mode-dropping behavior. Some models <ref type="bibr" target="#b24">[25]</ref> try to combine VAE and GANs.</p><p>The recent state-of-the-art approaches DVD-GAN-FP <ref type="bibr" target="#b6">[7]</ref> and its modification TRIVD-GAN-FP <ref type="bibr" target="#b25">[26]</ref> follow the success of BigGAN <ref type="bibr" target="#b2">[3]</ref>. They use 2D residuals blocks for independent frames prediction with Convolutional Gated Recurrent units between frames.</p><p>Another branch of generative models is autoregressive (AR) models. PixelCNN <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b36">37]</ref> generates new images by producing a new pixel value conditioning on previous (seen, already generated) ones in the raster-scan order. Later, PixelSnail <ref type="bibr" target="#b5">[6]</ref> increased the quality of generated samples by utilizing an attention mechanism. Recently, such an approach was applied to video generation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b48">49]</ref>. The latest work, Video Transformer <ref type="bibr" target="#b48">[49]</ref>, utilizes autoregressive video generation along with subscaling <ref type="bibr" target="#b27">[28]</ref> and attention mechanism <ref type="bibr" target="#b42">[43]</ref>.</p><p>The main challenge of AR models is a generation speed. Even though the latest AR model (Video-Transformer <ref type="bibr" target="#b48">[49]</ref>) applied the subscaling mechanism <ref type="bibr" target="#b27">[28]</ref>, introduced block-local attention, the generation speed is still quite slow.</p><p>Also, DVD-GAN-FP, TRIVD-GAN-FP, Video Transformer (VT) -they are all suffering from significant resource requirements, even for generating low-resolution video with frames of size 64x64. For instance, VT needs 128 TPUs and 1M steps for training.</p><p>Our work is in the field of autoregressive models and follows the setup of VideoTransformer <ref type="bibr" target="#b48">[49]</ref>.</p><p>The key novelty is that we mitigate GPU memory consumption and accelerate inference speed by working in a discrete latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discrete latent space</head><p>Autoencoder is a neural network trained in a self-supervised manner. Autoencoder takes an input (image, text, audio, etc.) and transfers (encodes) it into a more compact latent representation. The learning consists of finding such an encoder and decoder so that we can encode and decode the input as closely as possible.</p><p>Usually, latent space is continuous. However, some works like VQ-VAE <ref type="bibr" target="#b41">[42]</ref>, VQ-VAE2 <ref type="bibr" target="#b33">[34]</ref> model it as discrete with a categorical distribution inside. They demonstrated good reconstruction and generation quality. As generating from uniform distribution directly produced inferior results, autoregressive models were applied to learn the prior inside the latent space.</p><p>We follow this pipeline but for video modeling. First, encoding conditioning frames to discrete latent space, generate new (latent) frames using an autoregressive model, and decode the generated frames back to pixel space. Parallel to this work, a similar pipeline was applied to audio generation (Jukebox <ref type="bibr" target="#b11">[12]</ref>).</p><p>Discrete latent space also occurred to be useful in other works. Discrete quantization was added to a discriminator in GAN <ref type="bibr" target="#b50">[51]</ref>. <ref type="bibr" target="#b20">[21]</ref> uses discrete variables to increase the speed of the autoregressive model for neural machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Latent Video Transformer</head><p>Consider a video X to be a sequence of T frames {x t } T t=1 . Each frame x t ∈ R H×W ×3 has height H, width W and 3 RGB channels. Given the first T 0 frames, the goal is to generate the remaining T − T 0 frames. For this purpose, we propose a model: Latent Video Transformer (LVT). In general, it consists of two parts: a frame autoencoder and an autoregressive generative model.</p><p>We use the frame autoencoder to learn a compact latent representation so that we can transfer the task of video modeling from the pixel space to the latent space. The recurrent model is then used for generating new frames. The key novelty compared to existing models that operate in the latent space is a discrete structure of the latent space. Discrete representation helps us to use autoregressive generative models and other approaches, tailored for working for discrete data, e.g. those used for natural language processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Frame Autoencoder</head><p>We train a frame autoencoder to transfer individual images (frames) to latent space. The particular choice of the autoencoder is VQ-VAE [42] -variational autoencoder with discrete latent space.  <ref type="figure">Figure 1</ref>: Frame autoencoder architecture. An input image is passed through the encoder and split along the channel dimension into n c = 4 parts. Then we map pixels in each part to the corresponding nearest embeddings in the codebook. These nearest embeddings are then passed as an input to the decoder.</p><p>VQ-VAE (see <ref type="figure">Fig. 1</ref>) learns to encode an input image x ∈ R H×W ×3 using a codebook e ∈ R K×D , where K denotes the codebook size (i.e., latent space is K-way categorical) and D represents the size of an embedding in the codebook.</p><p>In general VQ-VAE consists of an encoder which encodes the image into more compact representation E(x) = z e (x) ∈ R h×w×D ; a bottleneck, that discretizes each pixel by mapping it to its nearest embedding e i from the codebook and produces z(x) ∈ [K] h×w×1 ; a decoder D takes as input discrete latent codes z(x), maps indexes to corresponding embeddings, and decodes the result of mapping z q (x) ∈ R h×w×D back to input pixel space.</p><p>VQ-VAE is trained with the following objective:</p><formula xml:id="formula_0">L = x − D(z q (x)) 2 + z e (x) − sg[e] 2 ,<label>(1)</label></formula><p>where sg[] is the stop gradient operator, which returns its argument during the forward pass and zero gradients during backward pass. The first term is a reconstruction loss, and the second term is regularization term to make the encodings less volatile. We use EMA updates over the codebook variables.</p><p>Decomposed Vector Quantization. If the size K of the codebook e is large, then the model tends to index collapse. It means that some embedding vector e i is close to a lot of encoders outputs. In this case, it receives a strong update signal <ref type="bibr" target="#b20">[21]</ref>. As a result, the model would use only a limited number of vectors from e.</p><p>In order to overcome this issue, we exploit Sliced Vector Quantization <ref type="bibr" target="#b20">[21]</ref>. We introduce several codebooks {e j ∈ R K×D/nc } nc j=1 and split the output of encoder z e (x) along the channel dimension into n c parts with individual codebook per each part (see <ref type="figure">Figure 1</ref>). The output from discretization bottleneck in this case is z ∈ [K] h×w×nc .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Latent Video Generator</head><p>Frame encoder transforms the first T 0 frames to a discrete representation Z</p><formula xml:id="formula_1">0 ∈ [K] T0×h×w×nc .</formula><p>The autoregressive model is used to generate new T − T 0 frames conditioned on Z 0 . As such model, we use the Video Transformer <ref type="bibr" target="#b48">[49]</ref>, autoregressive video generative model, but apply it in the latent space in contrast to the pixel space in the original paper. Next, we describe the architecture of a video transformer. We abuse notation and refer to a latent representation of a video also as video and individual elements of it as frames and pixels. For exhaustive architecture details, we refer the reader to the original paper <ref type="bibr" target="#b48">[49]</ref>.  The model takes as input a tensor Z ∈ [K] T ×h×w×nc and primes the generation process on first T 0 given frames, i.e. Z :T0,:,:,: = Z 0 . The other frames could be randomly filled as the generation process is conditioned only on already generated or priming pixels. First, the model utilizes the idea of subscaling <ref type="bibr" target="#b27">[28]</ref>: let's generate a video as a sequence of nonoverlapping slices. After defining a subscale factor s = (s t , s h , s w ), it divides video into s = s t s h s w slices of size T /s t × h/s h × w/s w . The generation process happens slice by slice, pixel by pixel inside one slice, channel by channel for one pixel:</p><formula xml:id="formula_2">p(Z) = T hw−1 i=0 nc−1 k=0 p Z k π(i) |Z π(&lt;i) , Z &lt;k π(i)</formula><p>Pixels in each slice Z (a,b,c) are generated in raster-scan order and slices are generated in the subscale order: Z (0,0,0) , Z (0,0,1) , . . . , Z (st−1,s h −1,sw−1) .</p><p>The model follows the original Transformer <ref type="bibr" target="#b42">[43]</ref> and consists of an encoder and a decoder. To generate a new pixel value inside slice Z (a,b,c) , firstly, the encoder outputs the representation of already generated slices Z &lt;(a,b,c) . This representation goes to the decoder, which mixes it with a representation of already generated pixels inside a current slice Z (a,b,c) . This autoregressive order is preserved by padding input video inside the encoder, and masking used in convolutions and attention inside the decoder. After generating a new pixel value, we replace the respective padding with the generated output and repeat the generation process recursively. The generation process in case of spatiotemporal (s t &gt; 0, s h &gt; 0, s w &gt; 0) subscaling can be seen at <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Finally, when the generation process is done, the frame decoder takes as input Z ∈ [K] T ×h×w×nc (now all values are valid), maps it to already learned embeddings Z q ∈ R T ×h×w×D and decodes it back frame by frame to an original pixel space X ∈ R T ×H×W ×3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>We model the videos of length T = 16 and spatial size 64 × 64 similar to the setup of prior works in this field <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>Measures of quality. Video prediction is a challenging problem, as there are many possible future outcomes for given conditioning frames. Therefore conventional metrics as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) that require ground truth correspondence were later displaced by the better-suited metric -Fréchet Video Distance (FVD) <ref type="bibr" target="#b17">[18]</ref>. FVD applies idea from Fréchet Inception Distance <ref type="bibr" target="#b17">[18]</ref> for videos and computes Fréchet distance between real and generated samples based on statistics calculated on logits from action-recognition Inception3D network trained on Kinetics-400 <ref type="bibr" target="#b22">[23]</ref> dataset. The metric was shown to better correlate with human perception, than previously used ones.</p><p>We also report bits per dimension 3 (bits/dim) -negative log 2 -probability averaged across all generated (latent) pixels and channels.</p><p>Frame autoencoder. The encoder contains two strided convolutional layers with ReLU activation function, stride 2 and kernel size 4 × 4, followed by a convolution layer with kernel size 3 × 3, the same padding, followed by two residual blocks (ReLU, 3 × 3 conv, ReLU, 1 × 1 conv). The decoder has a symmetrical structure containing two residual blocks, followed by two transposed convolutions with stride 2 and window size 4 × 4. For Kinetics-600 dataset, we use four residual blocks instead of two both in encoder and decoder.</p><p>In our experiments we explore two setups for the codebook structure: (the default one) n c = 1, K = 512 and n c = 4, K = 2048. The embedding dimension inside codebook is D = 256. The encoder and discretization bottleneck converts 64 × 64 × 3 RGB image into 16 × 16 × n c discrete latent codes indices.</p><p>We trained VQ-VAE using Adam optimizer with learning rate 0.0003 for 500K steps in case of BAIR Robot pushing dataset and 1M steps in case of Kinetics-600 dataset using a batch of 32 images.</p><p>Latent Video Generator. As no public code was available, Video Transformer implementation was written from scratch using the setup of a medium size model and following the implementation and training details from the original paper <ref type="bibr" target="#b48">[49]</ref>.</p><p>Different from the original transformer, an attention block in the encoder and the decoder is not block-local and spans across the whole input. It is possible due to the reduction of the input size via VQ-VAE. In almost all our experiments we also compare different subscaling types: (i) spatiotemporal (s = (4, 2, 2)), (ii) spatial (s = (1, 2, 2)), and (iii) single frame (s = (T, 1, 1)).</p><p>Each model was trained on 8 Nvidia V100 GPUs for two days for 600K steps. Sampling one video of 16 frames with 5 priming frames takes about 30 seconds on 1 Nvidia V100 GPU, compared to one minute used by the original Video Transformer <ref type="bibr" target="#b48">[49]</ref>. The approximate size of a latent video generator is 50M parameters.</p><p>We provide quantitative and qualitative results on two datasets: BAIR Robot Pushing <ref type="bibr" target="#b12">[13]</ref> and Kinetics 600 <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BAIR Robot Pushing</head><p>BAIR Robot Pushing <ref type="bibr" target="#b12">[13]</ref> dataset consists of 40K training and 256 test videos of robotic arm motion with a fixed camera position. First, we evaluate the VQ-VAE's reconstruction error with varying number of codebooks used inside the discretization bottleneck. We provide mean squared error (MSE) and FVD for reconstructed videos of 16 frames length (see <ref type="table" target="#tab_1">Table 1</ref>). In terms of video prediction, following the setup of previous approaches <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>, we train video generator conditioning on one frame and report metrics for videos of 16 frames. FVD and bits/dim are computed on videos with five priming frames and one priming frame accordingly (see <ref type="table" target="#tab_2">Table 2</ref>). We report the mean and standard deviations of 10 runs. It can bee seen that both VQ-VAE and Video Transformer demonstrate better accuracy when using four codebooks inside the discretization bottleneck. Preliminary experiments showed that a further increase of the number of codebooks would lead to overfitting. Finally, we compare our approach to others (see <ref type="table" target="#tab_3">Table 3</ref>). We also provide the baseline solution: what if we take the last ground truth frame and use it as a prediction for all future frames. We achieve comparable performance in comparison to other methods. We also provide samples for qualitative assessment (see <ref type="figure" target="#fig_2">Fig 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Kinetics-600</head><p>Kinetics-600 <ref type="bibr" target="#b4">[5]</ref> dataset consists of 350k train and 50k test videos. There are 600 classes presented. Following the setup of previous approaches <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>, we cropped each video to the size of the smallest side. Then we resized them to 64 × 64 using Lanczos filter. First, we evaluate the VQ-VAE's reconstruction error with varying number of codebooks used inside the discretization bottleneck. For that, we provide mean squared error (MSE) and FVD for reconstructed videos of 16 frames (see <ref type="table" target="#tab_4">Table 4</ref>). VQ-VAE with four codebooks outperforms in terms of both metrics, and therefore later, we conduct the experiments under only this setup as the evaluation time for Kinetics-600 is particularly significant due to the large size of test data.</p><p>In terms of video prediction, following the setup of previous approaches <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>, we train the video generator conditioning on one frame and report metrics for videos of 16 frames. FVD and bits/dim are computed on videos with five priming frames and one priming frame accordingly (see <ref type="table" target="#tab_5">Table 5</ref>). We compare our approach to others (see <ref type="table" target="#tab_6">Table 6</ref>). Here the baseline is the prediction of the next frame by the previous known frame. Our results are inferior to others on this dataset. We conclude that it is caused by error accumulation inside the Transformer model. We link it to the high complexity and diversity of the Kinetics-600 dataset. We want to emphasize that only four other approaches tried to model videos from this dataset, and all of them use six times bigger generative models (up to 350M parameters) than ours. In the meantime, increasing the size of our model led to a very slow convergence.</p><p>We also provide samples from our model for qualitative assessment (see <ref type="figure" target="#fig_3">Fig. 4</ref>). One can notice artifacts in the second video. We found approximately half of the videos to be good and half of the videos to have artifacts. We provide more visualization results in the Appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study of Attention</head><p>The Video Transformer contains memory and time costly operation of multi-head attention. In general case multi-head attention computes feature y q as:</p><formula xml:id="formula_3">y q = M m=1 W m   k∈Ωq A m (x q, x k ) V m x k   ,</formula><p>where m, q, k are the indexes of attention head, query and key elements respectively. W m and V m are learnable matrices. A m computes the attention weight for an each key element. We also normalize attention weights, s.t. k∈Ωq A m (q, k, z q , x k ) = 1.</p><p>In our default setup, the attention weights are computed as:</p><formula xml:id="formula_4">A m (x q , x k ) ∝ exp x q Q m K m x k + b kq ,</formula><p>where Q m , K m are learnable matrices for retrieving key and content embeddings, and b kq is computed as the sum of per-dimension relative distance biases between pixels k and q. We would refer to this type of attention as "query-key + relative distance".</p><p>We also explore two other variants:</p><p>• "key + relative distance":</p><formula xml:id="formula_5">A m (x q , x k ) ∝ exp u m K m x k + b kq , where u m is a lernable vector, • "relative distance only": A m (x q , x k ) ∝ exp (b kq ).</formula><p>The empirical comparison of these different types of attention can be seen at <ref type="table" target="#tab_7">Table 7</ref>. Similar to <ref type="bibr" target="#b51">[52]</ref> we find that query-key term inside self-attention module does not play a crucial role in the success of Latent Video Generator and can be replaced with cheaper variants with a cost of slight quality reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we tackled the video generation problem. Given several first frames, the goal was to predict the continuation of a video. Modern methods for video generation requires up to 512 Tensor Processing Units for parallel training. We were focused on the reduction of the computational requirements of the model. We showed that one could achieve comparable results on video prediction by training a model using the usual research setup -8 V100 GPUs. To achieve such a result, we moved the video generation process from pixel space to a latent space. We demonstrated decent results on the dataset BAIR Robot Pushing. In the meantime, in some cases, we observe visual artifacts on the Kinetics-600 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>In our work, we present a new model for video generation. Potentially, our model could be applied in cases where we need to predict the future. For example, prediction of further pedestrian movement on the photo, made by a camera on a self-driving car. Or animating photos of landscapes. One can use our model for animating pictures in their mobile phones, creating GIF animations.</p><p>We encourage researchers to understand and mitigate the risks of wrong future predictions. When the model is used in a self-driving car for pedestrian movement prediction, the error cost is high. Also, in some cases, there are long-tail events. If there is no such training data, the model could fail then. We suggest additional research to be done in terms of the reliability of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>A Adaptive Input and Adaptive Softmax.</p><p>We analyzed how often each latent code from the codebook was used for encoding images from train videos in BAIR Robot Pushing dataset. We found that 218 latent codes out of 512 constitute the 80% of probability mass (see <ref type="figure" target="#fig_4">Fig. 5</ref>). Based on this fact, we tried to improve metrics using Adaptive Input <ref type="bibr" target="#b1">[2]</ref> and Adaptive Softmax <ref type="bibr" target="#b16">[17]</ref>. Neither of them brings an improvement to quality (see <ref type="table" target="#tab_8">Table 8</ref>), despite their successful applications in natural language processing. For an interested reader, we refer to the original papers for particular details.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B BAIR visualizations</head><p>We present visualizations of BAIR robot pushing dataset in <ref type="figure">Fig. 6</ref> and <ref type="figure">Fig. 7</ref>. At the first row of each figure there is a video: 5 real and 11 generated frames. Rows 2-5 contain codes visualizations. Each code row corresponds to one codebook. Since a single code is just a matrix of indexes, we decode it with a technique called indexed color. In other words, we assigned each index in a code to a specific color. Rows 6-9 represent binary mask denoting whether the latent code between consecutive frames changes or not (yellow means a change).</p><p>One can see that results on the BAIR dataset are quite realistic. Also, it is important to note that some of the codes stay the same from frame to frame. The static background causes it. Codes that are responsible for non-static objects change from frame to frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Kinetics-600 visualizations</head><p>We present good samples along with codes on the Kinetics-600 dataset in <ref type="figure">Figures 8 and 9</ref>. Bad samples with codes are presented in <ref type="figure">Fig. 10 and 11</ref>. One can see that bad samples and good samples are different in their codes and code differences. For latent transformer, it is easier to predict the latent code of the next frame if it is similar to the latent code of the current frame. <ref type="figure">Figure 6</ref>: Sample from BAIR robot pushing dataset. The first row represents a single video with the first five frames being real and others generated. Rows 2-5 represent four latent codes, one row for each codebook. Rows 6-9 represent binary mask denoting whether the latent code between consecutive frames changes or not (yellow means a change). <ref type="figure">Figure 7</ref>: Sample from BAIR robot pushing dataset. The first row represents a single video with the first five frames being real and others generated. Rows 2-5 represent four latent codes, one row for each codebook. Rows 6-9 represent binary mask denoting whether the latent code between consecutive frames changes or not (yellow means a change). <ref type="figure">Figure 8</ref>: Good sample from Kinetics-600 dataset. The first row represents a single video with the first five frames being real and others generated. Rows 2-5 represent four latent codes for real video, one row for each codebook. Rows 6-9 represent binary mask denoting whether the latent code between consecutive frames changes or not (yellow means a change). Rows 10-13 represent four latent codes for generated video, one row for each codebook. Rows 14-17 represent binary mask denoting whether the latent code between consecutive frames changes or not (yellow means a change). <ref type="figure">Figure 9</ref>: Good sample from Kinetics-600 dataset. The first row represents a single video with the first five frames being real and others generated. Rows 2-5 represent four latent codes for real video, one row for each codebook. Rows 6-9 represent binary mask denoting whether the latent code between consecutive frames changes or not (yellow means a change). Rows 10-13 represent four latent codes for generated video, one row for each codebook. Rows 14-17 represent binary mask denoting whether the latent code between consecutive frames changes or not (yellow means a change). <ref type="figure">Figure 10</ref>: Bad sample from Kinetics-600 dataset. The first row represents a single video with the first five frames being real and others generated. Rows 2-5 represent four latent codes for real video, one row for each codebook. Rows 6-9 represent binary mask denoting whether the latent code between consecutive frames changes or not (yellow means a change). Rows 10-13 represent four latent codes for generated video, one row for each codebook. Rows 14-17 represent binary mask denoting whether the latent code between consecutive frames changes or not (yellow means a change). <ref type="figure">Figure 11</ref>: Bad sample from Kinetics-600 dataset. The first row represents a single video with the first five frames being real and others generated. Rows 2-5 represent four latent codes for real video, one row for each codebook. Rows 6-9 represent binary mask denoting whether the latent code between consecutive frames changed or not (yellow means changed). Rows 10-13 represent four latent codes for generated video, one row for each codebook. Rows 14-17 represent binary mask denoting whether the latent code between consecutive frames changes or not (yellow means a change). <ref type="figure" target="#fig_1">Figure 12</ref>: Samples from Kinetics-600 dataset. Each row represents a single video with first five frames being real and others generated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Video Transformer adapted to latent codes. Numbers represent generation order. Pixels are colored if they are already generated. White-colored pixels are zero-padded. Pixels with the same color belong to the same slice. The example represents the generation of the last pixel of slice Z (1,0,1) for a video of size (t, h, w) = (4, 4, 4) and (s t , s h , s w ) = (2, 2, 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Samples from BAIR Robot Pushing dataset. Each row represents a single video with first 5 frames being real and others generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Samples from Kinetics-600 dataset. Each row represents a single video with first five frames being real and others generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Sorted codes frequencies for BAIR Robot Pushing dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>VQ-VAE performance on BAIR Robot Pushing dataset.</figDesc><table><row><cell cols="3">n c MSE(↓) FVD(↓)</cell></row><row><cell>1</cell><cell>0.0016</cell><cell>222.71</cell></row><row><cell>4</cell><cell>0.0004</cell><cell>47.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Subscaling type n c Bits/dim(↓)</cell><cell>FVD(↓)</cell></row><row><cell>Single Frame</cell><cell>1</cell><cell>1.28</cell><cell>258.89 ± 2.85</cell></row><row><cell>Spatial</cell><cell>1</cell><cell>1.79</cell><cell>524.43 ± 9.41</cell></row><row><cell>Spatiotemporal</cell><cell>1</cell><cell>1.25</cell><cell>275.71 ± 5.41</cell></row><row><cell>Single Frame</cell><cell>4</cell><cell>1.53</cell><cell>125.76 ± 2.90</cell></row><row><cell>Spatial</cell><cell>4</cell><cell>2.99</cell><cell>920.37 ± 7.71</cell></row><row><cell>Spatiotemporal</cell><cell>4</cell><cell>1.62</cell><cell>145.85 ± 1.68</cell></row></table><note>Video prediction performance on BAIR Robot Pushing dataset. Best results in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different methods for video prediction on BAIR Robot Pushing dataset.</figDesc><table><row><cell>Method</cell><cell>bits/dim(↓)</cell><cell>FVD(↓)</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>320.90</cell></row><row><cell>VideoFlow [24]</cell><cell>1.87</cell><cell>-</cell></row><row><cell>SVP-FP [10]</cell><cell>-</cell><cell>315.5</cell></row><row><cell>CDNA [16]</cell><cell>-</cell><cell>296.5</cell></row><row><cell>LVT (ours, n c = 1)</cell><cell>1.25</cell><cell>275.71 ± 5.41</cell></row><row><cell>SV2P [8]</cell><cell>-</cell><cell>262.5</cell></row><row><cell>LVT (ours, n c = 4)</cell><cell>1.53</cell><cell>125.8 ± 2.9</cell></row><row><cell>SAVP [25]</cell><cell>-</cell><cell>116.4</cell></row><row><cell>DVD-GAN-FP [7]</cell><cell>-</cell><cell>109.8</cell></row><row><cell>TriVD-GAN-FP [26]</cell><cell>-</cell><cell>103.3</cell></row><row><cell>Axial Transformer [19]</cell><cell>1.29</cell><cell>-</cell></row><row><cell>Video Transformer [49]</cell><cell>1.35</cell><cell>94 ± 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>VQ-VAE performance on Kinetics-600 dataset.</figDesc><table><row><cell cols="3">n c MSE(↓) FVD(↓)</cell></row><row><cell>1</cell><cell>0.002</cell><cell>396.58</cell></row><row><cell>4</cell><cell>0.0004</cell><cell>25.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Video prediction performance on Kinetics-600 dataset. Best results in bold.</figDesc><table><row><cell cols="3">Subscaling type n c Bits/dim(↓)</cell><cell>FVD(↓)</cell></row><row><cell>Single Frame</cell><cell>4</cell><cell>2.14</cell><cell>224.73</cell></row><row><cell>Spatial</cell><cell>4</cell><cell>4.22</cell><cell>2845.06 ± 612.07</cell></row><row><cell>Spatiotemporal</cell><cell>4</cell><cell>2.47</cell><cell>338.39 ± 0.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different methods for video prediction on Kinetics-600 dataset.</figDesc><table><row><cell>Method</cell><cell>Bits/dim(↓)</cell><cell>FVD(↓)</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>271.00</cell></row><row><cell>LVT (ours)</cell><cell>2.14</cell><cell>224.73</cell></row><row><cell>Video Transformer [49]</cell><cell>1.19</cell><cell>170 ± 5</cell></row><row><cell>DVD-GAN-FP [7]</cell><cell>-</cell><cell>69.15 ± 1.16</cell></row><row><cell>TriVD-GAN-FP [26]</cell><cell>-</cell><cell>25.74 ± 0.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Attention comparison on BAIR Robot Pushing dataset. The results are obtained using the model with a single frame subscaling type modeling the latent space with n c = 4 codebooks.</figDesc><table><row><cell>attention type</cell><cell cols="2">Bits/dim(↓) FVD(↓)</cell><cell cols="2">params (M) inference time (sec)</cell></row><row><cell cols="2">query-key + relative distance 1.53</cell><cell>125.76 ± 2.90</cell><cell>49.87</cell><cell>35</cell></row><row><cell>key-only + relative distance</cell><cell>1.57</cell><cell>130.27 ± 4.26</cell><cell>41.50</cell><cell>32</cell></row><row><cell>relative distance only</cell><cell>1.58</cell><cell>141.62 ± 4.34</cell><cell>33.09</cell><cell>30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Effects of applying adaptive input / softmax in decoder architecture. The results are obtained using the model with a single frame subscaling. Latent space is modelled with n c = 1.</figDesc><table><row><cell cols="2">Subscaling type Decoder Input</cell><cell>Decoder Output</cell><cell>Bits/dim(↓)</cell><cell>FVD(↓)</cell></row><row><cell>Single Frame</cell><cell cols="2">128d Embedding 512d Softmax</cell><cell>1.28</cell><cell>258.89 ± 2.85</cell></row><row><cell>Spatial</cell><cell cols="2">128d Embedding 512d Softmax</cell><cell>1.79</cell><cell>524.43 ± 9.41</cell></row><row><cell>Spatiotemporal</cell><cell cols="2">128d Embedding 512d Softmax</cell><cell>1.25</cell><cell>275.71 ± 5.41</cell></row><row><cell>Single Frame</cell><cell cols="2">128d Embedding 512d Softmax (tied emb)</cell><cell>1.27</cell><cell>265.10 ± 3.85</cell></row><row><cell>Single Frame</cell><cell>Adaptive Input</cell><cell>512d Softmax</cell><cell>1.26</cell><cell>259.73 ± 6.35</cell></row><row><cell>Single Frame</cell><cell cols="2">128d Embedding Adaptive Softmax</cell><cell>1.37</cell><cell>265.99 ± 4.16</cell></row><row><cell>Single Frame</cell><cell>Adaptive Input</cell><cell>Adaptive Softmax</cell><cell>1.36</cell><cell>259.88 ± 5.25</cell></row><row><cell>Single Frame</cell><cell>Adaptive Input</cell><cell>Adaptive Softmax (tied emb)</cell><cell>1.35</cell><cell>259.55 ± 8.50</cell></row><row><cell>Single Frame</cell><cell>Adaptive Input</cell><cell>Adaptive Softmax (tied emb/proj)</cell><cell>1.34</cell><cell>264.79 ± 4.27</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Due to the nature of frame autoencoder, it is not possible to compute bits/dim directly. We provide bits/dim in the latent space. Other works provide this metric for the pixel space. One can consider bits/dim in the latent space as the lower bound to real bits/dim on images.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">More generated samples are presented inFig. 12. For each video, first five frames are real, others are generated.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>The authors acknowledge the usage of the Skoltech CDISE HPC cluster Zhores for obtaining the results presented in this paper. The authors were supported by the Russian Science Foundation under Grant 19-41-04109. They also acknowledge Vage Egiazarian for thoughtful discussions of the model and the experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards high resolution video generation with progressive growing of sliced wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danda</forename><surname>Pani Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02419</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10853</idno>
		<title level="m">Adaptive input representations for neural language modeling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Contextvp: Fully context-aware video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Koumoutsakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="753" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A short note about kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09763</idno>
		<title level="m">Pixelsnail: An improved autoregressive generative model</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial video generation on complex datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06571</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07687</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1174" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07687</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emily L Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4414" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Self-supervised visual planning with temporal skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05268</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Animating landscape: self-supervised learning of decoupled motion and appearance for single-image video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeru</forename><surname>Kuriyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07192</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ting</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fast decoding in sequence models using discrete latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03382</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durk</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01434</idno>
		<title level="m">Videoflow: A flow-based generative model for video</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Diego de Las Casas, Yotam Doron, Albin Cassirer, and Karen Simonyan. Transformation-based adversarial video prediction on large-scale data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04035</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01608</idno>
		<title level="m">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-toend time-lapse video synthesis from a single outdoor image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghyeon</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical video generation from orthogonal information: Optical flow and texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsunori</forename><surname>Ohnishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video generation from single semantic label map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Viorica Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06309</idno>
		<title level="m">Spatio-temporal video autoencoder with differentiable memory</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14837" to="14847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2830" to="2839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Tganv2: Efficient training of large models for video generation with multiple subsampling layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09245</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><forename type="middle">Rott</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4570" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6306" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Few-shot video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12713</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06601</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Video-to-video synthesis. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06300</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Eidetic 3d lstm: A model for video prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Scaling autoregressive video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02634</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02088</idno>
		<title level="m">Feature quantization improves gan training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An empirical study of spatial attention mechanisms in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6688" to="6697" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
