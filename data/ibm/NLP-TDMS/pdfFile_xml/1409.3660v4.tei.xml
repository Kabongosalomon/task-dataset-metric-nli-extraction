<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">0,000+ Times Accelerated Robust Subset Selection (ARSS)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinliang</forename><surname>Zhu</surname></persName>
							<email>zhuxinliang2012@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
							<email>chpan@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">0,000+ Times Accelerated Robust Subset Selection (ARSS)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Subset selection from massive data with noised information is increasingly popular for various applications. This problem is still highly challenging as current methods are generally slow in speed and sensitive to outliers. To address the above two issues, we propose an accelerated robust subset selection (ARSS) method. Specifically in the subset selection area, this is the first attempt to employ the p (0 &lt; p ≤ 1)-norm based measure for the representation loss, preventing large errors from dominating our objective. As a result, the robustness against outlier elements is greatly enhanced. Actually, data size is generally much larger than feature length, i.e. N L. Based on this observation, we propose a speedup solver (via ALM and equivalent derivations) to highly reduce the computational cost, theoretically from O N 4 to O N 2 L . Extensive experiments on ten benchmark datasets verify that our method not only outperforms state of the art methods, but also runs 10,000+ times faster than the most related method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Due to the explosive growth of data <ref type="bibr" target="#b17">(Wang, Kumar, and Chang 2012)</ref>, subset selection methods are increasingly popular for a wide range of machine learning and computer vision applications <ref type="bibr" target="#b5">(Frey and Dueck 2007;</ref><ref type="bibr" target="#b7">Jenatton, Audibert, and Bach 2011)</ref>. This kind of methods offer the potential to select a few highly representative samples or exemplars to describe the entire dataset. By analyzing a few, we can roughly know all. Such case is very important to summarize and visualize huge datasets of texts, images and videos etc. <ref type="bibr" target="#b0">(Bien and Tibshirani 2011;</ref><ref type="bibr" target="#b4">Elhamifar et al. 2012b)</ref>. Besides, by only using the selected exemplars for succeeding tasks, the cost of memories and computational time will be greatly reduced <ref type="bibr" target="#b6">(Garcia et al. 2012)</ref>. Additionally, as outliers are generally less representative, the side effect of outliers will be reduced, thus boosting the performance of subsequent applications <ref type="bibr" target="#b3">(Elhamifar et al. 2012a</ref>).</p><p>There have been several subset selection methods. The most intuitional method is to randomly select a fixed number of samples. Although highly efficient, there is no guarantee for an effective selection. For the other methods, depending on the mechanism of representative exemplars, there are mainly three categories of selection methods. One category Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Classifiers Classification Accuracy <ref type="bibr">(%)</ref> Performance TED RRSSNie RRSSour ARSSour <ref type="figure">Figure 1</ref>: Comparisons of four algorithms on Optdigit. Two conclusions can be drawn. First, our method (ARSSour) is highly faster than all others; with the help of an elegant new theorem, RRSSour is significantly faster than the authorial algorithm RRSSNie. Second, ARSSour achieves highly promising prediction accuracies.</p><p>relies on the assumption that the data points lie in one or multiple low-dimensional subspaces. Specifically, the Rank Revealing QR (RRQR) <ref type="bibr" target="#b2">(Chan 1987;</ref><ref type="bibr" target="#b1">Boutsidis, Mahoney, and Drineas 2009)</ref> selects the subsets that give the best conditional sub-matrix. Unfortunately, this method has suboptimal properties, as it is not assured to find the globally optimum in polynomial time.</p><p>Another category assumes that the samples are distributed around centers <ref type="bibr" target="#b5">(Frey and Dueck 2007;</ref><ref type="bibr" target="#b10">Liu et al. 2010)</ref>. The center or its nearest neighbour are selected as exemplars. Perhaps, Kmeans and Kmedoids are the most typical methods (Kmedoids is a variant of Kmeans). Both methods employ an EM-like algorithm. Thus, the results depend tightly on the initialization, and they are highly unstable for large K (i.e. the number of centers or selected samples).</p><p>Recently, there are a few methods that assume exemplars are the samples that can best represent the whole dataset. However, for <ref type="bibr" target="#b20">(Yu, Bi, and Tresp 2006)</ref>, the optimization is a combinatorial problem (NP-hard) <ref type="bibr" target="#b13">(Nie et al. 2013;</ref><ref type="bibr" target="#b19">Yu et al. 2008)</ref>, which is computationally intractable to solve. Besides, the representation loss is measured by the least square measure, which is sensitive to outliers in data <ref type="bibr" target="#b21">Zhu et al. 2014;</ref><ref type="bibr" target="#b13">Nie et al. 2013)</ref>.</p><p>Then <ref type="bibr" target="#b13">(Nie et al. 2013)</ref> improves <ref type="bibr" target="#b20">(Yu, Bi, and Tresp 2006)</ref> by employing a robust loss via the 2,1 -norm; the 1 -norm is applied to samples, and the 2 -norm is used for features. In this way, the side effect of outlier samples is relieved. The solver of <ref type="bibr" target="#b13">(Nie et al. 2013</ref>) is theoretically perfect due to its ability of convergence to global optima. Unfortunately, in terms of computational costs, the solver is highly complex. It takes O N 4 for one iteration as shown in <ref type="table" target="#tab_1">Table 1</ref>. This is infeasible for the case of large N (e.g. it takes 2000+ hours for a case of N = 13000). Moreover, the representation loss is only robust against outlier samples. Such case is worth improvement, as there may exist outlier elements in real data.</p><p>Contributions. In this paper, we propose an accelerated robust subset selection method to highly raise the speed on the one hand, and to boost the robustness on the other. To this end, we use the p (0 &lt; p ≤ 1)-norm based robust measure for the representation loss, preventing large errors from dominating our objective. As a result, the robustness against outliers is greatly boosted. Then, based on the observation that data size is generally much larger than feature length, i.e. N L, we propose a speedup solver. The main acceleration is owing to the Augmented Lagrange Multiplier (ALM) and an equivalent derivation. Via them, we reduce the computational complexity from O N 4 to O N 2 L . Extensive results on ten benchmark datasets demonstrate that in average, our method is 10,000+ times faster than Nie's method. The selection quality is highly encouraging as shown in <ref type="figure">Fig. 1</ref>. Additionally, via another equivalent derivation, we give an accelerated solver for Nie's method, theoretically reducing the computational complexity from O N 4 to O N 2 L + N L 3 as listed in <ref type="table" target="#tab_1">Table 1</ref>, empirically obtaining a 500+ times speedup compared with the authorial solver.</p><p>Notations. We use boldface uppercase letters to denote matrices and boldface lowercase letters to represent vectors. For a matrix Y = [Y ln ] ∈ R L×N , we denote its l th row and n th column as y l and y n respectively. The 2,1 -norm of a matrix is defined as</p><formula xml:id="formula_0">Y 2,1 = L l N n Y 2 ln = L l y l 2 . The p (0 &lt; p ≤ 1)-norm of a matrix is defined as Y p = N n L l |Y ln | p 1 p</formula><p>; thus, we have Y p p = l,n |Y ln | p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset Selection via Self-Representation</head><p>In the problem of subset selection, we are often given a set of N unlabelled points X = x 1 , x 2 ,· · ·, x N | x n ∈ R L , where L is the feature length. The goal is to select the top K (K N ) most representative and informative samples (i.e. exemplars) to effectively describe the entire dataset X. By solely using these K exemplars for subsequent tasks, we could greatly reduce the computational costs and largely alleviate the side effects of outlier elements in data. Such a motivation could be formulated as the Transductive Experimental Design (TED) model <ref type="bibr" target="#b20">(Yu, Bi, and Tresp 2006)</ref>:</p><formula xml:id="formula_1">min Q,A N n=1 x n − Qa n 2 2 + α a n 2 2 ,<label>(1)</label></formula><p>where Q ∈ R L×K is the selected subset matrix, whose column vectors all come from X, i.e. q k ∈ X, ∀k ∈ </p><formula xml:id="formula_2">O N 2 L + N L 3 O N 2 L</formula><p>{1, · · · , K}; A = [a 1 , · · · , a N ] ∈ R K×N is the corresponding linear combination coefficients. By minimizing (1), TED could select the highly informative and representative samples, as they have to well represent all the samples in X.</p><p>Although TED (1) is well modeled-very accurate and intuitive, there are two bottlenecks. First, the objective is a combinatorial optimization problem. It is NP-hard to exhaustively search the optimal subset Q from X. For this reason, the author approximate (1) via a sequential optimization problem, which is solved by an inefficient greedy optimization algorithm. Second, similar to the existing least square loss based models in machine learning and statistics, (1) is sensitive to the presence of outliers  </p><formula xml:id="formula_3">min A∈R N ×N N n=1 x n − Xa n 2 + γ A 2,1 ,<label>(2)</label></formula><p>where γ is a nonnegative parameter; A is constrained to be row-sparse, and thus to select the most representative and informative samples <ref type="bibr" target="#b13">(Nie et al. 2013)</ref>. As the representation loss is accumulated via the 1 -norm among samples, compared with (1), the robustness against outlier samples is enhanced. Equivalently, (2) is rewritten in the matrix format:</p><formula xml:id="formula_4">min A∈R N ×N (X − XA) T 2,1 + γ A 2,1 .<label>(3)</label></formula><p>Since the objective <ref type="formula" target="#formula_4">(3)</ref> is convex in A, the global minimum may be found by differentiating (3) and setting the derivative to zero <ref type="bibr" target="#b8">(Levin et al. 2008)</ref>, resulting in a linear system 1</p><formula xml:id="formula_5">an = Unn UnnX T X + γV −1 X T xn, ∀n = {1,2,· · ·,N },<label>(4)</label></formula><p>where V ∈ R N ×N is a diagonal matrix with the n th diagonal entry as V nn = 1 2 a n 2 and U nn = 1 2 xn−Xan 2 . It seems perfect to use (4) to solve the objective (3), because (4) looks simple and the global optimum is theoretically guaranteed <ref type="bibr" target="#b13">(Nie et al. 2013)</ref>. Unfortunately, in terms of speed, (4) is usually infeasible due to the incredible computational demand in the case of large N (the number of samples). At each iteration, the computational complexity of (4) is up to O N 4 , as analyzed in Remark 1. According to our experiments, the time cost is up to 2088 hours (i.e. 87 days) for a subset selection problem of 13000 samples. 1 To avoid singular failures, we get Vnn = 1 2 √ a n 2 2 + , Unn = 1 2 √ xn−Xan 2 2 + ( &gt; 0). Then the algorithm is to minimize the objective of N n xn − Xan 2 2 + + γ N n a n 2 2 + . When → 0, this objective is reduced to the objective (3).</p><p>Remark 1. Since U nn X T X+γV ∈ R N ×N , the major computational cost of (4) focuses on a N × N linear system. If solved by the Cholesky factorization method, it costs 1 3 N 3 for factorization as well as 2N 2 for forward and backward substitution. This amounts to O N 3 in total. By now, we only solve a n . Once solving all the set of {a n } N n=1 , the total complexity amounts to O N 4 for one iteration step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accelerated Robust Subset Selection (ARSS)</head><p>Due to the huge computational costs, Nie's method is infeasible for the case of large N -the computational time is up to 2088 hours for a case of 13000 samples. Besides, Nie's model <ref type="formula" target="#formula_4">(3)</ref> imposes the 2 -norm among features, which is prone to outliers in features. To tackle the above two issues, we propose a more robust model in the p (0 &lt; p ≤ 1)norm. Although the resulted objective is challenging to solve, a speedup algorithm is proposed to dramatically save the computational costs. For the same task of N = 13000, it costs our method 1.8 minutes, achieving a 68429 times acceleration compared with the speed of Nie's method.</p><p>Modeling. To boost the robustness against outliers in both samples and features, we formulate the discrepancy between X and XA via the p (0 &lt; p &lt; 1)-norm. There are theoretical and empirical evidences to verify that compared with 2 or 1 norms, the p -norm is more able to prevent outlier elements from dominating the objective, enhancing the robustness <ref type="bibr" target="#b12">(Nie et al. 2012</ref>). Thus, we have the following objective min</p><formula xml:id="formula_6">A∈R N×N O = X − XA p p + γ A 2,1 ,<label>(5)</label></formula><p>where γ is a balancing parameter; A is a row sparse matrix, used to select the most informative and representative samples. By minimizing the energy of (5), we could capture the most essential properties of the dataset X.</p><p>After obtaining the optimal A, the row indexes are sorted by the row-sum value of the absolute A in decreasing order. The samples specified by the top K indexes are selected as exemplars. Note that the model (5) could be applied to the unsupervised feature selection problem by only transposing the data matrix X. In this case, A is a L × L row sparse matrix, used to select the most representative features.</p><p>Accelerated Solver for the ARSS Objective in <ref type="formula" target="#formula_6">(5)</ref> Although objective <ref type="formula" target="#formula_6">(5)</ref> is challenging to solve, we propose an effective and highly efficient solver. The acceleration owes to the ALM and an equivalent derivation.</p><p>ALM The most intractable challenge of (5) is that, the p (0 &lt; p ≤ 1)-norm is non-convex, non-smooth and notdifferentiable at the zero point. Therefore, it is beneficial to use the Augmented Lagrangian Method (ALM) <ref type="bibr" target="#b15">(Nocedal and Wright 2006)</ref> to solve (5), resulting in several easily tackled unconstrained subproblems. By solving them iteratively, the solutions of subproblems could eventually converge to a minimum <ref type="bibr" target="#b9">(Li 2011;</ref><ref type="bibr" target="#b11">Meng et al. 2013)</ref>.</p><p>Specifically, we introduce an auxiliary variable E = X − XA ∈ R L×N . Thus, the objective (5) becomes: min</p><formula xml:id="formula_7">A,E=X−XA E p p + γ A 2,1 .<label>(6)</label></formula><p>To deal with the equality constraint in <ref type="formula" target="#formula_7">(6)</ref>, the most convenient method is to add a penalty, resulting in</p><formula xml:id="formula_8">min A E p p + γ A 2,1 + µ 2 E − X + XA 2 F ,<label>(7)</label></formula><p>where µ is a penalty parameter. To guarantee the equality constraint, it requires µ approaching infinity, which may cause bad numerical conditions. Instead, once introducing a Lagrangian multiplier, it is no longer requiring µ → ∞ <ref type="bibr" target="#b9">(Li 2011;</ref><ref type="bibr" target="#b15">Nocedal and Wright 2006)</ref>. Thus, we rewrite <ref type="formula" target="#formula_8">(7)</ref> into the standard ALM formulation as:</p><formula xml:id="formula_9">min A,E,Λ,µ L A = E p p +γ A 2,1 + µ 2 E − X + XA + Λ µ 2 F ,<label>(8)</label></formula><p>where Λ consists of L × N Lagrangian multipliers. In the following, a highly efficient solver will be given.</p><p>The updating rule for Λ Similar to the iterative thresholding (IT) in <ref type="bibr" target="#b18">(Wright et al. 2009;</ref><ref type="bibr" target="#b14">Nie et al. 2014)</ref>, the degree of violations of the L × N equality constraints are used to update the Lagrangian multiplier:</p><formula xml:id="formula_10">Λ ← Λ + µ (E − X + XA) ,<label>(9)</label></formula><p>where µ is a monotonically increasing parameter over iteration steps. For example, µ ← ρµ, where 1 &lt; ρ &lt; 2 is a predefined parameter <ref type="bibr" target="#b15">(Nocedal and Wright 2006)</ref>.</p><p>Efficient solver for E Removing irrelevant terms with E from (8), we have</p><formula xml:id="formula_11">min E E p p + µ 2 E − H 2 F ,<label>(10)</label></formula><p>where H = X − XA − Λ µ ∈ R L×N . According to the definition of the p -norm and the Frobenius-norm, (10) could be decoupled into L × N independent and unconstrained subproblems. The standard form of these subproblems is</p><formula xml:id="formula_12">min y f (y) = λ |y| p + 1 2 (y − c) 2 ,<label>(11)</label></formula><p>where λ = 1 µ is a given positive parameter, y is the scalar variable need to deal with, c is a known scalar constant.</p><p>Zuo et al. <ref type="bibr" target="#b22">(Zuo et al. 2013</ref>) has recently proposed a generalized iterative shrinkage algorithm to solve (11). This algorithm is easy to implement and able to achieve more accurate solutions than current methods. Thus, we use it for our problem as:</p><formula xml:id="formula_13">y * = max (|c| − τ p (λ) , 0) · S p (|c| ; λ) · sign (c) , (12) where τ p (λ) = [2λ (1 − p)] 1 2−p + λp [2λ (1 − p)] p−1 2−p ; S p (|c| ; λ)</formula><p>is obtained by solving the following equation:</p><formula xml:id="formula_14">S p (c; λ) − c + λp (S p (c; λ)) p−1 = 0,</formula><p>which could be solved efficiently via an iterative algorithm. In this manner, (10) could be sovled extremely fast.</p><p>Accelerated solver for A The main acceleration focuses on the solver of A. Removing irrelevant terms with A from (8), we have <ref type="formula" target="#formula_1">(13)</ref> is convex in A, the optimum could be found by differentiating (13) and setting the derivative to zero. This amounts to tackling the following linear system 2 :</p><formula xml:id="formula_15">min A A 2,1 + β 2 Tr (XA − P) T (XA − P) , (13) where β = µ γ is a nonnegative parameter, P = X − E − Λ µ ∈ R L×N . Since</formula><formula xml:id="formula_16">A = β V + βX T X −1 X T P.<label>(14)</label></formula><p>As <ref type="formula" target="#formula_1">(14)</ref> is mainly a N × N linear system. Once solved by the Cholesky factorization, the computational complexity is highly up to O N 3 . This is by no means a good choice for real applications with large N . In the following, an equivalent derivation of (14) will be proposed to significantly save the computational complexity. Theorem 2. The N × N linear system <ref type="formula" target="#formula_1">(14)</ref> is equivalent to the following L × L linear system:</p><formula xml:id="formula_17">V + βX T X ∈ R N ×N ,</formula><formula xml:id="formula_18">A = β XV −1 T I L + βX XV −1 T −1 P,<label>(15)</label></formula><p>where I L is a L × L identity matrix.</p><p>Proof. Note that V is a N ×N diagonal and positive-definite matrix, the exponent of V is efficient to achieve, i.e. V α = {V α nn } N n=1 , ∀α ∈ R. We have the following equations</p><formula xml:id="formula_19">A = β V + βX T X −1 X T P = βV − 1 2 V − 1 2 V + βX T X V − 1 2 −1 V − 1 2 X T P = βV − 1 2 I N + βZ T Z −1 Z T P,<label>(16)</label></formula><p>where Z = XV − 1 2 , I N is a N × N identity matrix. The following equation holds for any conditions</p><formula xml:id="formula_20">I N + βZ T Z Z T = Z T I L + βZZ T .<label>(17)</label></formula><p>Multiplying <ref type="formula" target="#formula_1">(17)</ref> with I N + βZ T Z −1 on the left and I L + βZZ T −1 on the right of both sides of the equal-sign, we have the equation as:</p><formula xml:id="formula_21">Z T I L + βZZ T −1 = I N + βZ T Z −1 Z T .<label>(18)</label></formula><p>Therefore, substituting (18) and Z = XV − 1 2 into (16), we have the simplified updating rule as:</p><formula xml:id="formula_22">A = β XV −1 T I L + βX XV −1 T −1 P.<label>(19)</label></formula><p>When N L, the most complex operation is the matrix multiplications, not the L × L linear system. Corollary 3. We have two equivalent updating rules <ref type="formula" target="#formula_1">(14)</ref> and (15) for the objective (13). If using (14) when N ≤ L, and otherwise using (15) as shown in Algorithm 1, the computational complexity of solvers for (13) is O N 2 L . Due to N L, we have highly reduced the complexity from O N 4 to O N 2 L compared with Nie's method.</p><p>Algorithm 1 for (13): A * = ARSS A (X, V, P, I L , β) Input: X, V, P, I L , β 1: if N ≤ L then 2:</p><p>update A via the updating rule <ref type="formula" target="#formula_1">(14)</ref>, that is 3: </p><formula xml:id="formula_23">A = β V + βX T X −1 X T P.</formula><formula xml:id="formula_24">update V = [V nn ] ∈ R N ×N . 5: P = X−E− Λ µ , β = µ γ ; I L is a L × L identity matrix.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>A = ARSS A (X, V, P, I L , β) via Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>update Λ by the updating rule (9), µ ← ρµ. 8: until convergence Output: A The solver to update A is given in Algorithm 1. The overall solver for our model <ref type="formula" target="#formula_6">(5)</ref> is summarized in Algorithm 2.</p><p>According to Theorem 2 and Corollary 3, the solver for our model <ref type="formula" target="#formula_1">(13)</ref> is highly simplified, as feature length is generally much smaller than data size, i.e L N . Similarly, Nie's method could be highly accelerated by Theorem 4, obtaining 500+ times speedup, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> and <ref type="table">Table 3</ref>. Theorem 4. Nie's N × N solver (20) <ref type="bibr" target="#b13">(Nie et al. 2013</ref>) is equivalent to the following L × L linear system (21) a n = U nn U nn X T X + γV −1 X T x n (20)</p><formula xml:id="formula_25">= U nn XV −1 T U nn X XV −1 T +γI L −1 x n (21) ∀n ∈ {1, 2, · · · , N }, where I L is a L × L identity matrix.</formula><p>Proof. Based on (20), we have the following equalities:</p><formula xml:id="formula_26">a n = U nn U nn X T X + γV −1 X T x n , = U nn V − 1 2 V − 1 2 U nn X T X+γV V − 1 2 −1 V − 1 2 X T x n = U nn V − 1 2 U nn XV − 1 2 T XV − 1 2 +γI N −1 XV − 1 2 T x n = U nn V − 1 2 XV − 1 2 T U nn XV − 1 2 XV − 1 2 T +γI L −1 x n = U nn XV −1 T U nn X XV −1 T +γI L −1 x n .</formula><p>The derivations are equivalent; their results are equal. 2 V ∈ R N ×N is a positive and diagonal matrix with the n th diagonal entry as Vnn = 1 √ a n 2 2 + &gt; 0, where is a small value to avoid singular failures <ref type="bibr" target="#b13">(Nie et al. 2013;</ref><ref type="bibr" target="#b21">Zhu et al. 2014</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Settings</head><p>In this part, the experimental settings are introduced. All experiments are conducted on a server with 64-core Intel Xeon E7-4820 @ 2.00 GHz, 18 Mb Cache and 0.986 TB RAM, using Matlab 2012. Brief descriptions of ten benchmark datasets are summarized in <ref type="table" target="#tab_4">Table 2</ref>, where 'Total(N * )' denotes the total set of samples in each data. Due to the high computational complexity, other methods can only handle small datasets (while our method can handle the total set). Thus, we randomly choose the candidate set from the total set to reduce the sample size, i.e. N &lt; N * (cf. 'Total(N * )' and 'candid.(N )' in <ref type="table" target="#tab_4">Table 2</ref>). The remainder (except candidate set) are used for test. Specifically, to simulate the varying quality of samples, ten percentage of candidate samples from each class are randomly selected and arbitrarily added one of the following three kinds of noise: "Gaussian", "Laplace" and "Salt &amp; pepper" respectively. In a word, all experiment settings are same and fair for all the methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speed Comparisons</head><p>There are two parts of speed comparisons. First, how speed varies with increasing N is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Then the comparison of specific speed is summarized in <ref type="table">Table 3</ref>. Note that TED and RRSS Nie denote the authorial solver (via authorial codes); RRSS our is our accelerated solver for Nie's model via Theorem 4; ARSS is the proposed method.</p><p>Speed vs. increasing N To verify the great superiority of our method over the state-of-the-art methods in speed, three experiments are conducted. The results are illustrated in <ref type="figure" target="#fig_1">Fig.  2</ref>, where there are three sub-figures showing the speed of four methods on the benchmark datasets of Letter, MNIST and Waveform respectively. As we shall see, both selection time of TED <ref type="bibr" target="#b20">(Yu, Bi, and Tresp 2006)</ref> and RRSS Nie <ref type="bibr" target="#b13">(Nie et al. 2013)</ref> increases dramatically as N increases. No surprisingly, RRSS Nie is incredibly time-consuming as N growsthe order of curves looks higher than quadratic. Actually, the theoretical complexity of RRSS Nie is highly up to O N 4 as analyzed in Remark 1. Compared with TED and RRSS Nie , the curve of ARSS is surprisingly lower and highly stable against increasing N ; there is almost no rise of selection time over growing N . This is owing to the speedup techniques of ALM and equivalent derivations. Via them, we reduce the computational cost from O N 4 to O N 2 L , as analyzed in Theorem 2 and Corollary 3. Moreover, with the help of Theorem 4, RRSS our is the second faster algorithm that is significantly accelerated compared with the authorial algorithm RRSS Nie .</p><p>Speed with fixed N The speed of four algorithms is summarized in <ref type="table">Table 3a</ref>, where each row shows the results on one dataset and the last row displays the average results. Four conclusions can be drawn from <ref type="table">Table 3a</ref>. First, ARSS is the fastest algorithm, and RRSS our is the second fastest algorithm. Second, with the help of Theorem 4, RRSS our is highly faster than RRSS Nie , averagely obtaining a 559 times acceleration. Third, ARSS is dramatically faster than <ref type="table">Table 3</ref>: Performances of TED, RRSS and ARSS: (left-a) speed in seconds, <ref type="bibr">(right-b)</ref> prediction accuracies. In terms of speed, with the help of Theorem 4, RRSSour is averagely 559+ times faster than the authorial algorithm, i.e. RRSSNie; ARSS achieves surprisingly 23275+ times acceleration compared with RRSSNie. Due to the more robust loss in the p-norm, the prediction accuracy of ARSS is highly encouraging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Speed 1 'ARSS(N * )' means the task of selecting samples from the whole dataset (with N * samples as shown in the 2 nd column in <ref type="table" target="#tab_4">Table 2</ref>), while 'TED' to 'ARSS' indicate the problem of dealing with the candidate sample sets (with N samples as shown in the 3 rd column in <ref type="table" target="#tab_4">Table 2</ref>). <ref type="table">Table 3a</ref> verify an average acceleration of 23275 times faster than RRSS Nie and 281 times faster than TED. This means that for example if it takes RRSS Nie 100 years to do a subset selection task, it only takes our method 1.6 days to address the same problem. Finally, we apply ARSS to the whole sample set of each data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RRSS Nie and TED; the results in</head><p>The results are displayed in the 6 th column in <ref type="table">Table 3</ref>, showing its capability to process very large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction Accuracy</head><p>Accuracy comparison We conduct experiments on ten benchmark datasets. For each dataset, the top 200 representative samples are selected for training. The prediction accuracies are reported in <ref type="table">Table 3b</ref>, including the results of two popular classifiers. Three observations can be drawn from this table. First, Linear SVM generally outperforms KNN. Second, in general, our method performs the best; for a few cases, our method achieves comparable results with the best performances. Third, compared with TED, both RRSS and ARSS achieve an appreciable advantage. The above analyses are better illustrated in the last row of <ref type="table">Table 3b</ref>. These results demonstrate that the p loss in our model is well suited to select exemplars from the sample sets of various quality.</p><p>Prediction accuracies vs. increasing K To give a more detailed comparison, <ref type="figure">Fig. 3</ref> shows the prediction accuracies versus growing K (the number of selected samples). There are two rows and four columns of sub-figures. The top row shows the results of KNN, and the bottom one shows results of SVM. Each column gives the result on one dataset. As we shall see, the prediction accuracies generally increase as K increases. Such case is consistent with the common view that more training data will boost the prediction accuracy. For each sub-figure, ARSS is generally among the best. This case implies that our robust objective (5) via the p -norm is feasible to select subsets from the data of varying qualities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>To deal with tremendous data of varying quality, we propose an accelerated robust subset selection (ARSS) method. The p -norm is exploited to enhance the robustness against both outlier samples and outlier features. Although the resulted objective is complex to solve, we propose a highly efficient solver via two techniques: ALM and equivalent derivations. Via them, we greatly reduce the computational complexity from O N 4 to O N 2 L . Here feature length L is much smaller than data size N , i.e. L N . Extensive results on ten benchmark datasets verify that our method not only runs 10,000+ times faster than the most related method, but also outperforms state of the art methods. Moreover, we propose an accelerated solver to highly speed up Nie's method, theoretically reducing the computational complexity from O N 4 to O N 2 L + N L 3 . Empirically, our accelerated solver could achieve equal results and 500+ times acceleration compared with the authorial solver.</p><p>Limitation. Our efficient algorithm build on the observation that the number of samples is generally larger than feature length, i.e. N &gt; L. For the case of N ≤ L, the acceleration will be inapparent.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>4: else if N &gt; L then 5: update A via the updating rule (15), that is 6:A = B (I L + XB) −1 P, where B = β XV −1 T . 7: end if Output: AAlgorithm 2 for (5) or (8): A * = ARSS ALM (X, γ, p) Input: X, γ, p 1: Initialize µ &gt; 0, 1 &lt; ρ &lt; 2, = 10 −10 , A = I N , Λ = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Speed vs. increasing N on (a) Letter, (b) MNIST and (c) Waveform. Compared with the authorial solver TED and RRSSNie, our method ARSS and RRSSour dramatically reduce the computational time. The larger data size is, the larger gaps between these methods are. Note that the selection time is not sensitive to the number of selected samples K. (best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Complexity comparison of three algorithms at one iteration step. Generally, data size is much larger than feature length, i.e. N L. Compared with RRSSNie (Nie's model via the authorial solver), RRSSour (Nie's method speeded up by our solver) and ARSSour (ours) are significantly simplified.</figDesc><table><row><cell>Methods RRSS Nie</cell><cell>RRSS our</cell><cell>ARSS our</cell></row><row><cell>Complex. O N 4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Statistics of ten benchmark datasets.</figDesc><table><row><cell>Datasets</cell><cell cols="4">Total(N  *  ) Candid.(N ) Classes Features(L)</cell></row><row><cell>#1 Vehicle</cell><cell>846</cell><cell>700</cell><cell>4</cell><cell>18</cell></row><row><cell>#2 Diabetes</cell><cell>768</cell><cell>600</cell><cell>2</cell><cell>8</cell></row><row><cell>#3 Optdigit</cell><cell>5,620</cell><cell>3,823</cell><cell>10</cell><cell>64</cell></row><row><cell>#4 Waveform</cell><cell>5,000</cell><cell>4,200</cell><cell>3</cell><cell>21</cell></row><row><cell>#5 Satimage</cell><cell>6,435</cell><cell>4,435</cell><cell>7</cell><cell>36</cell></row><row><cell>#6 Coil20</cell><cell>1,440</cell><cell>1,200</cell><cell>20</cell><cell>256</cell></row><row><cell>#7 University</cell><cell>42,776</cell><cell>17,400</cell><cell>9</cell><cell>115</cell></row><row><cell>#8 Center</cell><cell>103,539</cell><cell>4,500</cell><cell>9</cell><cell>115</cell></row><row><cell>#9 MNIST</cell><cell>60,000</cell><cell>5,000</cell><cell>10</cell><cell>196</cell></row><row><cell>#10 Letter</cell><cell>20,000</cell><cell>13,000</cell><cell>26</cell><cell>16</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the editor and the reviewers for their valuable suggestions. Besides, this work is supported by the projects (Grant No. 61272331, 91338202,  61305049  and 61203277) of the National Natural Science Foundation of China.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Accuracy</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Prototype selection for interpretable classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2403" to="2424" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An improved approximation algorithm for the column subset selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rank revealing {QR} factorizations. Linear Algebra and its Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">See all by looking at a few: Sparse modeling for finding representative objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1600" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding exemplars from pairwise dissimilarities via simultaneous sparse recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Clustering by passing messages between data points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="issue">5814</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prototype selection for nearest neighbor classification: Taxonomy and empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Derrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="417" to="435" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structured variable selection with sparsity-inducing norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2777" to="2824" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compressive Sensing for 3D Data Processing Tasks: Applications, Models and Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. Dissertation</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust matrix completion via joint schatten p-norm and lpnorm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICDM</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="566" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Early active learning via robust representation and structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H Q</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1572" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">New primal svm solver with linear computational cost for big data classifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust distance metric learning via simultaneous l1-norm minimization and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1836" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semisupervised hashing for large-scale search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2080" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-greedy active learning for text categorization using convex transductive experimental design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Active learning via transductive experimental design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Effective spectral unmixing via robust representation and learning-based sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0685</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A generalized iterated shrinkage algorithm for nonconvex sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
