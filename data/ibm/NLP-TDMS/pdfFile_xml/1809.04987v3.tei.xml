<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synthetic Occlusion Augmentation with Volumetric Heatmaps for the 2018 ECCV PoseTrack Challenge on 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">István</forename><surname>Sárándi</surname></persName>
							<email>sarandi@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
							<email>timm.linder@de.bosch.com</email>
							<affiliation key="aff1">
								<orgName type="department">Corporate Research</orgName>
								<orgName type="institution">Robert Bosch GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Corporate Research</orgName>
								<orgName type="institution">Robert Bosch GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<email>leibe@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Synthetic Occlusion Augmentation with Volumetric Heatmaps for the 2018 ECCV PoseTrack Challenge on 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present our winning entry at the 2018 ECCV PoseTrack Challenge on 3D human pose estimation. Using a fully-convolutional backbone architecture, we obtain volumetric heatmaps per body joint, which we convert to coordinates using soft-argmax. Absolute person center depth is estimated by a 1D heatmap prediction head. The coordinates are back-projected to 3D camera space, where we minimize the L1 loss. Key to our good results is the training data augmentation with randomly placed occluders from the Pascal VOC dataset. In addition to reaching first place in the Challenge, our method also surpasses the state-of-the-art on the full Hu-man3.6M benchmark when considering methods that use no extra pose datasets in training. Code for applying synthetic occlusions is availabe at https://github.com/ isarandi/synthetic-occlusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The 3D part of the 2018 ECCV PoseTrack Challenge invited participants to tackle the following task. Given an uncropped, static RGB image containing a single person, estimate the position of J body joints in 3D camera space, relative to the root (pelvis) joint position. Predicting human poses in 3D space has several important applications, such as human-robot collaboration and virtual reality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Human Pose Estimation. State-of-the-art 3D pose estimation methods are based on deep convolutional neural networks. We recommend Sarafianos et al.'s survey <ref type="bibr" target="#b21">[22]</ref> for an overview of methods. Recently, building on experience gained from 2D human pose estimation (e.g., <ref type="bibr" target="#b15">[16]</ref>), heatmap-like methods have been introduced for 3D pose es- timation with promising results. This includes volumetric heatmaps <ref type="bibr" target="#b19">[20]</ref> <ref type="bibr" target="#b26">[27]</ref> <ref type="bibr" target="#b12">[13]</ref>, marginal heatmaps <ref type="bibr" target="#b16">[17]</ref> and location maps <ref type="bibr" target="#b14">[15]</ref>.</p><p>Occlusion Augmentation. Erasing or pasting over parts of an image has been successfully used as data augmentation in image classification, object detection, person reidentification <ref type="bibr" target="#b29">[30]</ref>[2] <ref type="bibr" target="#b3">[4]</ref>[3] <ref type="bibr" target="#b5">[6]</ref>, and facial facial landmark localization <ref type="bibr" target="#b28">[29]</ref>. Ke et al. <ref type="bibr" target="#b10">[11]</ref> augment images for 2D pose estimation by copying background patches over some of the body joints. We have recently found that such techniques are also very effective for 3D pose estimation <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>The dataset in this challenge is a subset of Hu-man3.6M <ref type="bibr" target="#b9">[10]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>We present a modified version of the method we recently used for studying occlusion-robustness in 3D pose estimation <ref type="bibr" target="#b22">[23]</ref>, extending it to handle the above-mentioned differences.</p><p>Image Preprocessing. We obtain person bounding boxes using the YOLOv3 detector <ref type="bibr" target="#b20">[21]</ref>. Treating the original camera's focal length f as a global hyperparameter, we reproject the image to be centered on the person box, at a scale where the larger side of the box fills 90% of the output.</p><p>Backbone Network. We feed the cropped and zoomed image (256 × 256 px) into a fully-convolutional backbone network (ResNet v2-50 <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b23">[24]</ref>). We directly obtain volumetric heatmaps from the backbone net by adding a 1x1 convolutional layer on the last spatial feature map of the backbone, producing J · D output channels. The resulting tensor is reshaped to yield J volumes, one per body joint, each with depth D.</p><p>Volumetric Heatmaps. We follow Pavlakos et al. in the interpretation of the volumetric heatmap's axes <ref type="bibr" target="#b19">[20]</ref>: X and Y correspond to image space and the depth axis to camera space, relative to the person center. Relative depths are not sufficient, however, when back-projecting from image to camera space. Pavlakos et al. optimize the root joint depth in post-processing, based on bone-length priors. By contrast, we predict it using a second prediction head on the backbone net (see <ref type="figure">Fig. 2</ref>). This outputs a 1D heatmap discretized to 32 units, representing a 10 meter range in front of the camera.</p><p>Soft-Argmax. We extract coordinate predictions from the heatmaps using soft-argmax <ref type="bibr" target="#b11">[12]</ref> <ref type="bibr" target="#b17">[18]</ref>. Since this operation is differentiable, there is no need to provide groundtruth heatmaps at training time <ref type="bibr" target="#b26">[27]</ref>. Instead, the loss can be computed deeper in the network and backpropagated through the soft-argmax operation. Soft-argmax also reduces the quantization errors inherent in hard argmax and gives fine-grained, continuous results without requiring memory-expensive, high-resolution heatmaps <ref type="bibr" target="#b26">[27]</ref>. Indeed, we use a heatmap resolution as low as 16 <ref type="bibr" target="#b2">3</ref> for the results presented in this paper.</p><p>Camera Intrinsics. Having predicted image coordinates x i , y i , depth coordinates ∆Z i relative to the person center and the absolute depth Z * of the person center by softargmax, we now need camera intrinsics to move from image space to 3D camera space. As mentioned earlier, the original camera's focal length f is treated as a hyperparameter, and we must also take into account the zooming factor s applied in preprocessing.</p><p>To avoid the need for precise hyperparameter tuning of f , we learn an additional, input-independent corrective factor c for the focal length during training, to achieve better alignment of image and heatmap locations. Denoting the image height and width as H and W , back-projection is performed as</p><formula xml:id="formula_0">  X i Y i Z i   =(Z * + ∆Z i )   f sc 0 W/2 0 f sc H/2 0 0 1   −1   x i y i 1   . (1)</formula><p>Loss. After subtracting the root joint coordinates, we compute the L 1 loss in the original camera space w.r.t. the provided root-relative ground truth. No explicit heatmap loss is used. Since all above operations are differentiable the whole network can be trained end-to-end.</p><p>Data Augmentation. In our recent study on the occlusionrobustness of 3D pose estimation <ref type="bibr" target="#b22">[23]</ref>, we found that augmenting training images with synthetic occlusions acts as an effective regularizer. Starting with the objects in the Pascal VOC dataset <ref type="bibr" target="#b4">[5]</ref>, we filter out persons, segments labeled as difficult or truncated and segments with area below 500 px, leaving 2638 objects. With probability p occ , we paste a random number (between 1 and 8) of these objects at random locations in each frame. We also apply standard geometric augmentations (scaling, rotation, translation, horizontal flip) and appearance distortions (blurs and color manipulations). At test time only horizontal flipping augmentation is used.</p><p>Training Details. The backbone net is initialized with ImageNet-pretrained weights from <ref type="bibr" target="#b23">[24]</ref>. We train the final method for 410 epochs on the union of the training and val- <ref type="table" target="#tab_1">Zhu  58  59  64  62  65  60  68 77  92  65  68  62  60  70  59  66  Rhodin  51  53  58  52  64  53  67 94  132  65  64  57  53  67  53  66  Zhou  52  56  55  51  57  53  64 73  81  61  60  57  49  61  53  59  Park  53  52  52  53  55  55  54 71  84  56  60  58  51  64  57  58  Shen  53  54  54  52  56  55  58 70  78  60  59  57  48  61  56  58  Pavlakos  44  46  50  47  56  47  52 63  70  54  54  48  46  58  46  52  Sun  38  43  46  41  46  40  49 65  73  48  49  43  38  52  38  47  Ours  38  40  43  40  43  40  47 58  64  43  48  42  36  50</ref> 38 45  idation set using the Adam optimizer and cyclical (triangular) learning rates <ref type="bibr" target="#b24">[25]</ref>. Our final challenge predictions were produced using a snapshot ensemble <ref type="bibr" target="#b7">[8]</ref>, averaging the predictions of snapshots taken at the last three learning-rateminima of the cyclical schedule. We used f = 1500 and p occ = 0.5 for the submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Direct Discuss Eat Greet Phone Pose Purch. Sit SitD Smoke Photo Wait Walk WalkD WalkT Avg</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>The evaluation metric is the mean per joint position error (MPJPE) over all joints after subtraction of the root joint position. Our method achieves best results for all actions, even ahead of methods using extra 2D pose datasets in training (see <ref type="table" target="#tab_1">Table 1</ref>). The margin is largest for the actions Sitting and Sitting Down, showing that our method is more robust to the presence of a chair, which is the only occluding object in the Human3.6M dataset.</p><p>Effect of Occlusion Augmentation. <ref type="figure" target="#fig_2">Fig. 3</ref> shows how synthetic occlusion augmentation improves results on the Challenge validation set as we vary the probability p occ of applying occlusion augmentation to each frame. Augmenting just 10% of the images already improves MPJPE by 8.2 mm and improvements continue to about p occ = 70%, after which performance is only influenced slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0%</head><p>20% 40% 60% 80% 100%  Full Human3.6M Benchmark. For comparison with prior work, we train and evaluate our method on the full Hu-man3.6M benchmark as well ( <ref type="table" target="#tab_2">Table 2</ref>). Here we use the bounding boxes and camera intrinsics provided with the dataset and minimize the L 1 loss computed on the absolute (i.e. non-root-relative) coordinates in camera space for 40 epochs. The person center depth Z * is estimated as described in Section 4. We follow the common protocol of training on five subjects (S1, S5, S6 S7, S8) and evaluating on two (S9, S11), without Procrustes alignment. We use no snapshot ensembling here, for better comparability.</p><p>The occlusion probability p occ is set to 1. Our method outperforms all prior work on Human3.6M when no additional pose datasets are used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented an architecture and data augmentation method for 3D human pose estimation and have shown that it outperforms other methods both by achieving first place in the 2018 ECCV PoseTrack Challenge and by surpassing the state-of-the-art on the full benchmark among methods using no additional pose datasets in training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of synthetic occlusions with Pascal VOC objects (geometric and color augmentations not depicted).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Effect of the per-frame probability of occlusion augmentation (pocc) evaluated on the Challenge validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b8">[9]</ref>, with 35,832 training, 19,312 validation and 24,416 test examples. There are a few important differences compared to the full benchmark. First, the Challenge version lacks person bounding boxes and camera intrinsics as input. Second, the ground truth labels are more restricted, consisting only of camera-space 3D joint coordinates after</figDesc><table><row><cell>1x1 conv</cell><cell>ResNet</cell><cell>Reshape</cell><cell>Global average pooling heatmaps Volumetric</cell><cell>3D soft argmax 1D soft argmax</cell><cell>x i y i ∆Z i Z  *</cell><cell>Back-project to 3D</cell><cell>Pred. in camera space</cell><cell>Ground truth Subtract root pred.</cell><cell>L1 loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Figure 2. Overview of our architecture.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">subtraction of the root joint. Image-space joint coordinates</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>are not available either.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Mean per joint position errors achieved by participants of the 2018 ECCV PoseTrack Challenge on 3D human pose estimation [1], on a subset of the Human3.6M dataset. In contrast to our method, some participants used extra 2D pose datasets in training (in accorance with the challenge rules).</figDesc><table><row><cell></cell><cell cols="2">Direct Discuss Eat Greet Phone Pose Purch.</cell><cell cols="3">Sit SitD Smoke Photo Wait Walk WalkD WalkT Avg</cell></row><row><cell>* Zhou (2017) [31]</cell><cell>54.8</cell><cell cols="3">60.7 58.2 71.4 62.0 65.5 53.8 55.6 75.2 111.6 64.2 66.0 51.4</cell><cell>63.2 55.3 64.9</cell></row><row><cell>* Martinez (2017) [14]</cell><cell>51.8</cell><cell cols="2">56.2 58.1 59.0 69.5 55.2 58.1 74.0 94.6</cell><cell>62.3 78.4 59.1 65.1</cell><cell>49.5 52.4 62.9</cell></row><row><cell>* Sun (2017) [26]</cell><cell>52.8</cell><cell cols="2">54.8 54.2 54.3 61.8 53.1 53.6 71.7 86.7</cell><cell>61.5 67.2 53.4 47.1</cell><cell>61.6 53.4 59.1</cell></row><row><cell>* Pavlakos (2018) [19]</cell><cell>48.5</cell><cell cols="2">54.4 54.4 52.0 59.4 49.9 52.9 65.8 71.1</cell><cell>56.6 65.3 52.9 60.9</cell><cell>44.7 47.8 56.2</cell></row><row><cell>* Luvizon (single-crop, 2018) [13]</cell><cell>51.5</cell><cell cols="2">53.4 49.0 52.5 53.9 50.3 54.4 63.6 73.5</cell><cell>55.3 61.9 50.1 46.0</cell><cell>60.2 51.0 55.1</cell></row><row><cell>* Luvizon (multi-crop, 2018) [13]</cell><cell>49.2</cell><cell cols="2">51.6 47.6 50.5 51.8 48.5 51.7 61.5 70.9</cell><cell>53.7 60.3 48.9 44.4</cell><cell>57.9 48.9 53.2</cell></row><row><cell>* Sun (2018) [27]</cell><cell>47.5</cell><cell cols="2">47.7 49.5 50.2 51.4 43.8 46.4 58.9 65.7</cell><cell>49.4 55.8 47.8 38.9</cell><cell>49.0 43.8 49.6</cell></row><row><cell>Tekin (2016) [28]</cell><cell cols="5">102.4 147.7 88.8 125.4 118.0 112.4 129.2 138.9 224.9 118.4 182.7 138.8 55.1 126.3 65.8 125.0</cell></row><row><cell>Zhou (2016) [33]</cell><cell cols="5">87.4 109.3 87.1 103.2 116.2 106.9 99.8 124.5 199.2 107.4 139.5 118.1 79.4 114.2 97.7 113.0</cell></row><row><cell>Xingyi (2016) [32]</cell><cell cols="5">91.8 102.4 97.0 98.8 113.4 90.0 93.8 132.2 159.0 106.9 125.2 94.4 79.0 126.0 99.0 107.3</cell></row><row><cell>Sun (2017) [26]</cell><cell>90.2</cell><cell cols="2">95.5 82.3 85.0 87.1 87.9 93.4 100.3 135.4</cell><cell>91.4 94.5 87.3 78.0</cell><cell>90.4 86.5 92.4</cell></row><row><cell>Pavlakos (2017) [20]</cell><cell>67.4</cell><cell cols="2">72.0 66.7 69.1 72.0 65.0 68.3 83.7 96.5</cell><cell>71.7 77.0 65.8 59.1</cell><cell>74.9 63.2 71.9</cell></row><row><cell>Sun (2018) [27]</cell><cell>63.8</cell><cell cols="2">64.0 56.9 64.8 62.1 59.8 60.1 71.6 91.7</cell><cell>60.9 70.4 65.1 51.3</cell><cell>63.2 55.4 64.1</cell></row><row><cell cols="2">Ours (no occlusion augmentation) 63.3</cell><cell cols="2">65.5 56.0 62.1 64.0 60.7 64.8 76.7 93.0</cell><cell>63.3 69.7 62.0 54.1</cell><cell>68.8 61.3 65.7</cell></row><row><cell>Ours (full)</cell><cell>49.1</cell><cell cols="2">54.6 50.4 50.7 54.8 47.4 50.1 67.5 78.4</cell><cell>53.1 57.4 50.7 40.1</cell><cell>54.0 46.1 54.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Mean per joint position error on the full Human3.6M dataset. Results marked with an asterisk (*) were achieved using extra 2D pose dataset(s) in training. Boldface indicates the overall best results, while italic indicates the best when using no extra 2D pose datasets.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>This project has been funded by a grant from the Bosch Research Foundation, by ILIAD (H2020-ICT-2016-732737) and by ERC Consolidator Grant DeeViSe (ERC-2017-COG-773161).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://archive.today/2018.09.12-150455/http://vision.imar.ro/human3.6m/ranking.php.Accessed" />
		<title level="m">3D pose estimation rankings of the 2018 ECCV PoseTrack Challenge</title>
		<imprint>
			<biblScope unit="page" from="2018" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling visual context is key to augmenting object detection datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Synthesizing training data for object detection in indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07836</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00109</idno>
		<title level="m">Snapshot ensembles: Train 1, get M for free</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Hu-man3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multiscale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09894</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1334" to="1373" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">2D/3D pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01484</idno>
		<title level="m">3D human pose estimation with 2D marginal heatmaps</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07372</idno>
		<title level="m">Numerical coordinate regression with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How robust is 3D human pose estimation to occlusion?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sárándi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09316</idno>
	</analytic>
	<monogr>
		<title level="m">IROS Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">TensorFlow-Slim image classification model library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/models/tree/master/research/slim" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Direct prediction of 3D body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An occluded stacked hourglass approach to facial landmark localization and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intel. Veh</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<title level="m">Random erasing data augmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3D human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
