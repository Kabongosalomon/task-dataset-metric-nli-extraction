<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bottom-up Object Detection by Grouping Extreme and Center Points</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
							<email>zhouxy@cs.utexas.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
							<email>jzhuo@cs.utexas.edu</email>
							<affiliation key="aff1">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bottom-up Object Detection by Grouping Extreme and Center Points</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, leftmost, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.7% on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9%, much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6% Mask AP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Top-down approaches have dominated object detection for years. Prevalent detectors convert object detection into rectangular region classification, by either explicitly cropping the region <ref type="bibr" target="#b11">[12]</ref> or region feature <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">41]</ref> (two-stage object detection) or implicitly setting fix-sized anchors for region proxies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref> (one-stage object detection). However, top-down detection is not without limits. A rectangular bounding box is not a natural object representation. Most objects are not axis-aligned boxes, and fitting them inside a box includes many distracting background pixels ( <ref type="figure">Figure. 1</ref>). In addition, top-down object detectors enumerate a large number of possible box locations without truly understanding the compositional visual grammars <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> of objects themselves. This is computationally expensive. Finally, boxes are a bad proxy for the object themselves. They convey little detailed object information, e.g., object shape and pose. <ref type="figure">Figure 1</ref>: We propose to detect objects by finding their extreme points. They directly form a bounding box , but also give a much tighter octagonal approximation of the object.</p><p>In this paper, we propose ExtremeNet, a bottom-up object detection framework that detects four extreme points (top-most, left-most, bottom-most, right-most) of an object. We use a state-of-the-art keypoint estimation framework <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b48">49]</ref> to find extreme points, by predicting four multi-peak heatmaps for each object category. In addition, we use one heatmap per category predicting the object center, as the average of two bounding box edges in both the x and y dimension. We group extreme points into objects with a purely geometry-based approach. We group four extreme points, one from each map, if and only if their geometric center is predicted in the center heatmap with a score higher than a pre-defined threshold. We enumerate all O(n 4 ) combinations of extreme point prediction, and select the valid ones. The number of extreme point prediction n is usually quite small, for COCO <ref type="bibr" target="#b25">[26]</ref> n ≤ 40, and a brute force algorithm implemented on GPU is sufficient. <ref type="figure">Figure 2</ref> shows an overview of the proposed method.</p><p>We are not the first to use deep keypoint prediction for object detection. CornerNet <ref type="bibr" target="#b21">[22]</ref> predicts two opposing corners of a bounding box. They group corner points into bounding boxes using an associative embedding feature <ref type="bibr" target="#b29">[30]</ref>. Our approach differs in two key aspects: key-Figure 2: Illustration of our object detection method. Our network predicts four extreme point heatmaps (Top. We shown the heatmap overlaid on the input image) and one center heatmap (Bottom row left) for each category. We enumerate the combinations of the peaks (Middle left) of four extreme point heatmaps and compute the geometric center of the composed bounding box (Middle right). A bounding box is produced if and only if its geometric center has a high response in the center heatmap (Bottom right). point definition and grouping. A corner is another form of bounding box, and suffers many of the issues top-down detection suffers from. A corner often lies outside an object, without strong appearance features. Extreme points, on the other hand, lie on objects, are visually distinguishable, and have consistent local appearance features. For example, the top-most point of human is often the head, and the bottommost point of a car or airplane will be a wheel. This makes the extreme point detection easier. The second difference to CornerNet is the geometric grouping. Our detection framework is fully appearance-based, without any implicit feature learning. In our experiments, the appearance-based grouping works significantly better.</p><p>Our idea is motivated by Papadopoulos et al. <ref type="bibr" target="#b32">[33]</ref>, who proposed to annotate bounding boxes by clicking the four extreme points. This annotation is roughly four times faster to collect and provides richer information than bounding boxes. Extreme points also have a close connection to object masks. Directly connecting the inflated extreme points offers a more fine-grained object mask than the bounding box. In our experiment, we show that fitting a simple octagon to the extreme points yields a good object mask estimation. Our method can be further combined with Deep Extreme Cut (DEXTR) <ref type="bibr" target="#b28">[29]</ref>, which turns extreme point annotations into a segmentation mask for the indicated object. Directly feeding our extreme point predictions as guidance to DEXTR <ref type="bibr" target="#b28">[29]</ref> leads to close to state-of-the-art instance segmentation results.</p><p>Our proposed method achieves a bounding box AP of 43.7% on COCO test-dev, out-performing all reported onestage object detectors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52]</ref> and on-par with sophisticated two-stage detectors. A Pascal VOC <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref> pre-trained DEXTR <ref type="bibr" target="#b28">[29]</ref> model yields a Mask AP of 34.6%, without using any COCO mask annotations. Code is available at https://github.com/xingyizhou/ ExtremeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Two-stage object detectors Region-CNN family <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref> considers object detection as two sequential problems: first propose a (large) set of category-agnostic bounding box candidates, crop them, and use an image classification module to classify the cropped region or region feature. R-CNN <ref type="bibr" target="#b11">[12]</ref> uses selective search <ref type="bibr" target="#b46">[47]</ref> to generate region proposals and feeds them to an ImageNet classification network. SPP <ref type="bibr" target="#b15">[16]</ref> and Fast RCNN <ref type="bibr" target="#b10">[11]</ref> first feed an image through a convolutional network and crop an intermediate feature map to reduce computation. Faster RCNN <ref type="bibr" target="#b40">[41]</ref> further replaces region proposals <ref type="bibr" target="#b46">[47]</ref> with a Region Proposal Network. The detection-by-classification idea is intuitive and keeps the best performance so far <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>Our method does not require region proposal or region classification. We argue that a region is not a necessary component in object detection. Representing an object by four extreme points is also effective and provides as much information as bounding boxes.</p><p>One-stage object detector One-stage object detectors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref> do not have a region cropping module. They can be considered as category-specific region or anchor proposal networks and directly assign a class label to each positive anchor. SSD <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref> uses different scale anchors in different network layers. YOLOv2 <ref type="bibr" target="#b38">[39]</ref> learns anchor shape priors. RetinaNet <ref type="bibr" target="#b24">[25]</ref> proposes a focal loss to balance the training contribution between positive and negative anchors. RefineDet <ref type="bibr" target="#b51">[52]</ref> learns to early reject negative anchors. Well-designed single-stage object detectors achieve very close performance with two-stage ones at higher efficiency.</p><p>Our method falls in the one-stage detector category. However, instead of setting anchors in an O(h 2 w 2 ) space, we detects five individual parts (four extreme points and one center) of a bounding box in O(hw) space. Instead of setting default scales or aspect-ratios as anchors at each pixel location, we only predict the probability for that location being a keypoint. Our center map can also be seen as a scale and aspect ratio agnostic region proposal network without bounding box regression.</p><p>Deformable Part Model As a bottom-up object detec-  tion method, our idea of grouping center and extreme points is related to Deformable Part Model <ref type="bibr" target="#b8">[9]</ref>. Our center point detector functions similarly with the root filter in DPM <ref type="bibr" target="#b8">[9]</ref>, and our four extreme points can be considered as a universal part decomposition for all categories. Instead of learning the part configuration, our predicted center and four extreme points have a fixed geometry structure. And we use a stateof-the-art keypoint detection network instead of low-level image filters for part detection.</p><p>Grouping in bottom-up human pose estimation Determining which keypoints are from the same person is an important component in bottom-up multi-person pose estimation. There are multiple solutions: Newell et al. <ref type="bibr" target="#b29">[30]</ref> proposes to learn an associative feature for each keypoint, which is trained using an embedding loss. Cao et al. <ref type="bibr" target="#b2">[3]</ref> learns an affinity field which resembles the edge between connected keypoints. Papandreous et al. <ref type="bibr" target="#b33">[34]</ref> learns the displacement to the parent joint on the human skeleton tree, as a 2-d feature for each keypoint. Nie et al. <ref type="bibr" target="#b31">[32]</ref> also learn a feature as the offset with respect to the object center.</p><p>In contrast to all the above methods, our center grouping is pure appearance-based and is easy to learn, by exploiting the geometric structure of extreme points and their center.</p><p>Implicit keypoint detection Prevalent keypoint detection methods work on well-defined semantic keypoints, e.g., human joints. StarMap <ref type="bibr" target="#b52">[53]</ref> mixes all types of keypoints using a single heatmap for general keypoint detection. Our extreme and center points are a kind of such general implicit keypoints, but with more explicit geometry property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Extreme and center points Let (x (tl) , y (tl) , x (br) , y (br) ) denote the four sides of a bounding box. To annotate a bounding box, a user commonly clicks on the top-left (x (tl) , y (tl) ) and bottom-right (x (br) , y (br) ) corners. As both points regularly lie outside an object, these clicks are often inaccuracy and need to be adjusted a few times. The whole process takes 34.5 seconds on average <ref type="bibr" target="#b43">[44]</ref>. Papadopoulos et al. <ref type="bibr" target="#b32">[33]</ref> propose to annotate the bounding box by clicking the four extreme points (</p><formula xml:id="formula_0">x (t) , y (t) ), (x (l) , y (l) ), (x (b) , y (b) ), (x (r) , y (r) ), where the box is (x (l) , y (t) , x (r) , y (b)</formula><p>). An extreme point is a point (x (a) , y (a) ) such that no other point (x, y) on the object lies further along one of the four cardinal directions a: top, bottom, left, right. Extreme click annotation time is 7.2 seconds on average <ref type="bibr" target="#b32">[33]</ref>. The resulting annotation is on-par with the more time-consuming box annotation. Here, we use the extreme click annotations directly and bypass the bounding box. We additionally use the center point of each object as (</p><formula xml:id="formula_1">x (l) +x (r) 2 , y (t) +y (b) 2 ).</formula><p>Keypoint detection Keypoint estimation, e.g., human joint estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49]</ref> or chair corner point estimation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b52">53]</ref>, commonly uses a fully convolutional encoderdecoder network to predict a multi-channel heatmap for each type of keypoint (e.g., one heatmap for human head, another heatmap for human wrist). The network is trained in a fully supervised way, with either an L2 loss to a rendered Gaussian map <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49]</ref> or with a per-pixel logistic regression loss <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. State-of-the-art keypoint estimation networks, e.g., 104-layer HourglassNet <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>, are trained in a fully convolutional manner. They regress to a heatmapŶ ∈ (0, 1) H×W of width W and height H for each output channel. The training is guided by a multi-peak Gaussian heatmap Y ∈ (0, 1) H×W , where each keypoint defines the mean of a Gaussian Kernel. The standard deviation is either fixed, or set proportional to the object size <ref type="bibr" target="#b21">[22]</ref>. The Gaussian heatmap serves as the regression target in the L2 loss case or as the weight map to reduce penalty near a positive location in the logistic regression case <ref type="bibr" target="#b21">[22]</ref>.</p><p>CornerNet CornerNet <ref type="bibr" target="#b21">[22]</ref> uses keypoint estimation with an HourglassNet <ref type="bibr" target="#b30">[31]</ref> as an object detector. They predict two sets of heatmaps for the opposing corners of the box. In order to balance the positive and negative locations they use a modified focal loss <ref type="bibr" target="#b24">[25]</ref> for training:</p><formula xml:id="formula_2">L det = − 1 N H i=1 W j=1 (1 −Ŷ ij ) α log(Ŷ ij ) if Y ij = 1 (1−Y ij ) β (Ŷ ij ) α log(1−Ŷ ij ) o.w. ,<label>(1)</label></formula><p>where α and β are hyper-parameters and fixed to α = 2 and β = 4 during training. N is the number of objects in the image.</p><p>For sub-pixel accuracy of extreme points, CornerNet additionally regresses to category-agnostic keypoint offset ∆ (a) for each corner. This regression recovers part of the information lost in the down-sampling of the hourglass network. The offset map is trained with smooth L1 Loss <ref type="bibr" target="#b10">[11]</ref> SL 1 on ground truth extreme point locations:</p><formula xml:id="formula_3">L of f = 1 N N k=1 SL 1 (∆ (a) , x/s − x/s ),<label>(2)</label></formula><p>where s is the down-sampling factor (s = 4 for Hourglass-Net), x is the coordinate of the keypoint. CornerNet then groups opposing corners into detection using an associative embedding <ref type="bibr" target="#b29">[30]</ref>. Our extreme point estimation uses the CornerNet architecture and loss, but not the associative embedding.</p><p>Deep Extreme Cut Deep Extreme Cut (DEXTR) <ref type="bibr" target="#b28">[29]</ref> is an extreme point guided image segmentation method. It takes four extreme points and the cropped image region surrounding the bounding box spanned by the extreme points as input. From this it produces a category-agnostic foreground segmentation of the indicated object using the semantic segmentation network of Chen et al. <ref type="bibr" target="#b3">[4]</ref>. The network learns to generate the segmentation mask that matches the input extreme point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ExtremeNet for Object detection</head><p>ExtremeNet uses an HourglassNet <ref type="bibr" target="#b30">[31]</ref> to detect five keypoints per class (four extreme points, and one center). We follow the training setup, loss and offset prediction of CornerNet <ref type="bibr" target="#b21">[22]</ref>. The offset prediction is category-agnostic, but extreme-point specific. There is no offset prediction for the center map. The output of our network is thus 5 × C heatmaps and 4 × 2 offset maps, where C is the number of classes (C = 80 for MS COCO <ref type="bibr" target="#b25">[26]</ref>). <ref type="figure" target="#fig_0">Figure 3</ref> shows an overview. Once the extreme points are extracted, we group them into detections in a purely geometric manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Center Grouping</head><p>Input : Center and Extremepoint heatmaps of an image for one category:Ŷ (c) ,Ŷ (t) ,Ŷ (l) ,Ŷ (b) ,Ŷ (r) ∈ (0, 1) H×W Center and peak selection thresholds: τc and τp Output: Bounding box with score // Convert heatmaps into coordinates of keypoints. // T , L, B, R are sets of points. </p><formula xml:id="formula_4">T ← ExtractPeak(Ŷ (t) , τp) L ← ExtractPeak(Ŷ (l) , τp) B ← ExtractPeak(Ŷ (b) , τp) R ← ExtractPeak(Ŷ (r) , τp) for t ∈ T , l ∈ L, b ∈ B, r ∈ R do // If</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Center Grouping</head><p>Extreme points lie on different sides of an object. This complicates grouping. For example, an associative embedding <ref type="bibr" target="#b29">[30]</ref> might not have a global enough view to group these keypoints. Here, we take a different approach that exploits the spread out nature of extreme points.</p><p>The input to our grouping algorithm is five heatmaps per class: one center heatmapŶ (c) ∈ (0, 1) H×W and four extreme heatmapsŶ (t) ,Ŷ (l) ,Ŷ (b) ,Ŷ (r) ∈ (0, 1) H×W for the top, left, bottom, right, respectively. Given a heatmap, we extract the corresponding keypoints by detecting all peaks. A peak is any pixel location with a value greater than τ p , that is locally maximal in a 3 × 3 window surrounding the pixel. We name this procedure as ExtrectPeak.</p><p>Given four extreme points t, b, r, l extracted from heatmapsŶ (t) ,Ŷ (l) ,Ŷ (b) ,Ŷ (r) , we compute their geometric center c = ( lx+tx 2 , ty+by 2 ). If this center is predicted with a high response in the center mapŶ (c) , we commit the extreme points as a valid detection:Ŷ (c) cx,cy ≥ τ c for a threshold τ c . We then enumerate over all quadruples of keypoints t, b, r, l in a brute force manner. We extract detections for each class independently. Algorithm 1 summarizes this procedure. We set τ p = 0.1 and τ c = 0.1 in all experiments.</p><p>This brute force grouping algorithm has a runtime of O(n 4 ), where n is the number of extracted extreme points for each cardinal direction. Supplementary material presents a O(n 2 ) algorithm that is faster on paper. However, then it is harder to accelerate on a GPU and slower in practice for the MS COCO dataset, where n ≤ 40.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ghost box suppression</head><p>Center grouping may give a high-confidence falsepositive detection for three equally spaced colinear objects of the same size. The center object has two choices here, commit to the correct small box, or predict a much larger box containing the extreme points of its neighbors. We call these false-positive detections "ghost" boxes. As we'll show in our experiments, these ghost boxes are infrequent, but nonetheless a consistent error mode of our grouping.</p><p>We present a simple post-processing step to remove ghost boxes. By definition a ghost box contains many other smaller detections. To discourage ghost boxes, we use a form of soft non-maxima suppression <ref type="bibr" target="#b0">[1]</ref>. If the sum of scores of all boxes contained in a certain bounding box exceeds 3 times of the score of itself, we divide its score by 2. This non-maxima suppression is similar to the standard overlap-based non-maxima suppression, but penalizes potential ghost boxes instead of multiple overlapping boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Edge aggregation</head><p>Extreme points are not always uniquely defined. If vertical or horizontal edges of an object form the extreme points (e.g., the top of a car) any point along that edge might be considered an extreme point. As a result, our network produces a weak response along any aligned edges of the object, instead of a single strong peak response. This weak response has two issues: First, the weaker response might be below our peak selection threshold τ p , and we will miss the extreme point entirely. Second, even if we detect the keypoint, its score will be lower than a slightly rotated object with a strong peak response.</p><p>We use edge aggregation to address this issue. For each extreme point, extracted as a local maximum, we aggregate its score in either the vertical direction, for left and right extreme points, or the horizontal direction, for top and bottom keypoints. We aggregate all monotonically de-creasing scores, and stop the aggregation at a local minimum along the aggregation direction. Specifically, let m be an extreme point and N , where λ aggr is the aggregation weight. In our experiments, we set λ aggr = 0.1. See <ref type="figure" target="#fig_2">Fig. 4</ref> for en example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Extreme Instance Segmentation</head><p>Extreme points carry considerable more information about an object, than a simple bounding box, with at least twice as many annotated values <ref type="bibr">(8 vs 4)</ref>. We propose a simple method to approximate the object mask using extreme points by creating an octagon whose edges are centered on the extreme points. Specifically, for an extreme point, we extend it in both directions on its corresponding edge to a segment of 1/4 of the entire edge length. The segment is truncated when it meets a corner. We then connect the end points of the four segments to form the octagon. See <ref type="figure">Figure  1</ref> for an example.</p><p>To further refine the bounding box segmentation, we use Deep Extreme Cut (DEXTR) <ref type="bibr" target="#b28">[29]</ref>, a deep network trained to convert the manually provided extreme points into instance segmentation mask. In this work, we simply replace the manual input of DEXTR <ref type="bibr" target="#b28">[29]</ref> with our extreme point prediction, to perform a 2-stage instance segmentation. Specifically, for each of our predicted bounding box , we crop the bounding box region, render a Gaussian map with our predicted extreme point, and then feed the concatenated image to the pre-trained DEXTR model. DEXTR <ref type="bibr" target="#b28">[29]</ref> is class-agnostic, thus we directly use the detected class and score of ExtremeNet. No further post-processing is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our method on the popular MS COCO dataset <ref type="bibr" target="#b25">[26]</ref>. COCO contains rich bounding box and instance segmentation annotations for 80 categories. We train on the train2017 split, which contains 118k images and 860k annotated objects. We perform all ablation studies on val2017 split, with 5k images and 36k objects, and compare to prior work on the test-dev split with contains 20k images The main evaluation metric is average precision over a dense set of fixed recall threshold We show average precision at IOU threshold 0.5 (AP 50 ), 0.75 (AP 75 ), and averaged over all thresholds between 0.5 and 1 (AP ). We also report AP for small, median and large objects (AP S , AP M , AP L ). The test evaluation is done on the official evaluation server. Qualitative results are shown in <ref type="table">Table.</ref> 4 and can be found more in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Extreme point annotations</head><p>There are no direct extreme point annotation in the COCO <ref type="bibr" target="#b25">[26]</ref>. However, there are complete annotations for object segmentation masks. We thus find extreme points as extrema in the polygonal mask annotations. In cases where an edge is parallel to an axis or within a 3 • angle, we place the extreme point at the center of the edge. Although our training data is derived from the more expensive segmentation annotation, the extreme point data itself is 4× cheaper to collect than the standard bounding box <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training details</head><p>Our implementation is based on the public implementation of CornerNet <ref type="bibr" target="#b21">[22]</ref>. We strictly follow CornerNets hyper-parameters: we set the input resolution to 511 × 511 and output resolution to 128×128. Data augmentation consists of flipping, random scaling between 0.6 and 1.3, random cropping, and random color jittering. The network is optimized with Adam <ref type="bibr" target="#b20">[21]</ref> with learning rate 2.5e − 4. Cor-nerNet <ref type="bibr" target="#b21">[22]</ref> was originally trained on 10 GPUs for 500k iterations, and an equivalent of over 140 GPU days. Due to limited GPU resources, the self-comparison experiments (Table. 1) are finetuned from a pre-trained CornerNet model with randomly initialized head layers on 5 GPUs for 250k iterations with a batch size of 24. Learning rate is dropped 10× at the 200k iteration. The state-of-the-art comparison experiment is trained from scratch on 5 GPUs for 500k iterations with learning rate dropped at the 450k iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Testing details</head><p>For each input image, our network produces four C-channel heatmaps for extreme points, one C-channel heatmap for center points, and four 2-channel offset maps. We apply edge aggregation (Section. 4.3) to each extreme point heatmap, and multiply the center heatmap by 2 to correct for the overall scale change. We then apply the center grouping algorithm (Section. 4.1) to the heatmaps. At most 40 top points are extracted in ExtrectPeak to keep the enumerating efficiency. The predicted bounding box coordinates are refined by adding an offset at the corresponding location of offsetmaps.</p><p>Following CornerNet <ref type="bibr" target="#b21">[22]</ref>, we keep the original image resolution instead of resizing it to a fixed size. We use flip augmentation for testing. In our main comparison, we use additional 5× multi-scale (0.5, 0.75, 1, 1.25, 1.5) augmentation. Finally, Soft-NMS <ref type="bibr" target="#b0">[1]</ref> filters all augmented detection results. Testing on one image takes 322ms (3.1FPS), with 168ms on network forwarding, 130ms on decoding and rest time on image pre-and post-processing (NMS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation studies</head><p>Center Grouping vs. Associative Embedding Our Ex-tremeNet can also be trained with an Associative Embed-AP AP <ref type="bibr" target="#b49">50</ref>  ding <ref type="bibr" target="#b29">[30]</ref> similar to CornerNet <ref type="bibr" target="#b21">[22]</ref>, instead of our geometric center point grouping. We tried this idea and replaced the center map with a four-channel associative embedding feature map trained with a Hinge Loss <ref type="bibr" target="#b21">[22]</ref>. <ref type="table">Table 1</ref> shows the result. We observe a 2.1% AP drop when using the associative embedding. While associative embeddings work well for human pose estimation and CornerNet, our extreme points lie on the very side of objects. Learning the identity and appearance of entire objects from the vantage point of its extreme points might simply be too hard. While it might work well for small objects, where the entire object easily fits into the effective receptive field of a keypoint, it fails for medium and large objects as shown in <ref type="table">Table 1</ref>. Furthermore, extreme points often lie at the intersection between overlapping objects, which further confuses the identity feature. Our geometric grouping method gracefully deals with these issues, as it only needs to reason about appearance.</p><p>Edge aggregation Edge aggregation (Section 4.3) gives a decent AP improvement of 0.7%. It proofs more effective for larger objects, that are more likely to have a long axis aligned edges without a single well defined extreme point. Removing edge aggregation improves the decoding time to 76ms and overall speed to 4.1 FPS.</p><p>Ghost box suppression Our simple ghost bounding box suppression (Section 4.2) yields 0.3% AP improvement. This suggests that ghost boxes are not a significant practical issue in MS COCO. A more sophisticated false-positive removal algorithm, e.g., learn NMS <ref type="bibr" target="#b17">[18]</ref>, might yield a slightly better result.</p><p>Error Analysis To better understand where the error comes from and how well each of our components is trained, we provide error analysis by replacing each output component with its ground truth. <ref type="table">Table 1</ref> shows the result. A ground truth center heatmap alone does not increase AP much. This indicates that our center heatmap is trained quite well, and shows that the implicit object center is learnable. Replacing the extreme point heatmap with ground truth gives 16.3% AP improvement. When replacing both extreme point heatmap and center heatmap, the result comes to 79.8%, much higher than replacing one of them. This is due to that our center grouping is very strict in the keypoint location and a high performance requires to improve both extreme point heatmap and center heatmap. Adding the ground truth offsets further increases the AP to AP AP 50 AP 75 AP S AP M AP L   <ref type="table" target="#tab_3">Table 2</ref> compares ExtremeNet to other state-of-the-art methods on COCO test-dev. Our model with multi-scale testing achieves an AP of 43.7, outperforming all reported one-stage object detectors and on-par with popular twostage detectors. Notable, it performs 1.6% higher than Cor-nerNet, which shows the advantage of detecting extreme and center points over detecting corners with associative features. In single scale setting, our performance is 0.3% AP below CornerNet <ref type="bibr" target="#b21">[22]</ref>. However, our method has higher AP for small and median objects than CornerNet, which is known to be more challenging. For larger objects our center response map might not be accurate enough to perform well, as a few pixel shift might make the difference between a detection and a false-negative. Further, note that we used the half number of GPUs to train our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">State-of-the-art comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Instance Segmentation</head><p>Finally, we compare our instance segmentation results with/ without DEXTR <ref type="bibr" target="#b28">[29]</ref> to other baselines in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>As a dummy baseline, we directly assign all pixels inside the rectangular bounding box as the segmentation mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extreme point heatmap</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Center heatmap</head><p>Octagon mask Extreme points+DEXTR <ref type="bibr" target="#b28">[29]</ref>  <ref type="table">Table 4</ref>: Qualitative results on COCO val2017. First and second column: our predicted (combined four) extreme point heatmap and center heatmap, respectively. We show them overlaid on the input image. We show heatmaps of different categories in different colors. Third column: our predicted bounding box and the octagon mask formed by extreme points. Fourth column: resulting masks of feeding our extreme point predictions to DEXTR <ref type="bibr" target="#b28">[29]</ref>.</p><p>The result on our best-model (with 43.3% bounding box AP) is 12.1% Mask AP. The simple octagon mask (Section. 4.4) based on our predicted extreme points gets a mask AP of 18.9%, much better than the bounding box baseline. This shows that this simple octagon mask can give a relatively reasonable object mask without additional cost. Note that directly using the quadrangle of the four extreme points yields a too-small mask, with a lower IoU.</p><p>When combined with DEXTR <ref type="bibr" target="#b28">[29]</ref>, our method achieves a mask AP of 34.6% on COCO val2017. To put this result in a context, the state-of-the-art Mask RCNN <ref type="bibr" target="#b14">[15]</ref> gets a mask AP of 37.5% with ResNeXt-101-FPN <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">50]</ref> backbone and 34.0% AP with Res50-FPN. Considering the fact that our model has not been trained on the COCO segmentation annotation, or any class specific segmentations at all, our result which is on-par with Res50 [17] and 2.9% AP below ResNeXt-101 is very competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In conclusion, we present a novel object detection framework based on bottom-up extreme points estimation. Our framework extracts four extreme points and groups them in a purely geometric manner. The presented framework yields state-of-the-art detection results and produces competitive instance segmentation results on MSCOCO, without seeing any COCO training instance segmentations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of our framework. Our network takes an image as input and produces four C-channel heatmaps, one Cchannel heatmap, and four 2-channel category-agnostic offset map. The heatmaps are trained by weighted pixel-wise logistic regression, where the weight is used to reduce false-positive penalty near the ground truth location. And the offset map is trained with Smooth L1 loss applied at ground truth peak locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the purpose of edge aggregation. In the case of multiple points being the extreme point on one edge, our model predicts a segment of low confident responses (a). Edge aggregation enhances the confidence of the middle pixel (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>mx+i,my be the vertical or horizontal line segment at that point. Let i 0 &lt; 0 and 0 &lt; i 1 be the two closest local minima N i1+1 . Edge aggregation updates the keypoint score asỸ m =Ŷ m + λ aggr i1 i=i0 N (m) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the bounding box is valid if ty ≤ ly, ry ≤ by and lx ≤ tx, bx ≤ rx then</figDesc><table><row><cell cols="2">// compute geometry center</cell></row><row><cell cols="2">cx ← (lx + rx)/2</cell></row><row><cell cols="2">cy ← (ty + by)/2</cell></row><row><cell cols="2">// If the center is detected ifŶ (c) cx,cy ≥ τc then</cell></row><row><cell cols="2">Add Bounding box (lx, ty, rx, by) with score</cell></row><row><cell>(Ŷ</cell><cell>(t) tx,ty +Ŷ lx,ly +Ŷ (l) bx,by +Ŷ (b) rx,ry +Ŷ (r) cx,cy )/5. (c)</cell></row><row><cell>end</cell><cell></cell></row><row><cell>end</cell><cell></cell></row><row><cell>end</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>AP 75 AP S AP M AP L 40.3 55.1 43.7 21.6 44.0 56.1 w/ multi-scale testing 43.3 59.6 46.8 25.7 46.6 59.4 w/o Center grouping 38.2 53.8 40.4 20.6 41.5 52.</figDesc><table><row><cell></cell><cell>9</cell></row><row><cell cols="2">w/o Edge aggregation 39.6 54.7 43.0 22.0 43.0 54.1</cell></row><row><cell>w/o Ghost removal</cell><cell>40.0 54.7 43.3 21.6 44.2 54.1</cell></row><row><cell>w/ gt center</cell><cell>48.6 62.1 53.9 26.3 53.7 66.7</cell></row><row><cell>w/ gt extreme</cell><cell>56.3 67.2 60.0 40.9 62.0s 64.0</cell></row><row><cell cols="2">w/ gt extreme + center 79.8 94.5 86.2 65.5 88.7 95.5</cell></row><row><cell cols="2">w/ gt ex. + ct. + offset 86.0 94.0 91.3 73.4 95.7 98.4</cell></row><row><cell cols="2">Table 1: Ablation study and error analysis on COCO</cell></row><row><cell cols="2">val2017. We show AP(%) after removing each component</cell></row><row><cell cols="2">or replacing it with its ground truth.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>State-of-the-art comparison on COCO test-dev. SS/ MS are short for single-scale/ multi-scale tesing, respectively. It shows that our ExtremeNet in on-par with state-of-the-art region-based object detectors.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Instance segmentation evaluation on COCO val2017. The results are shown in Mask AP. 86.0%. The rest error is from the ghost box (Section 4.2).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement We thank Chao-Yuan Wu, Dian Chen, and Chia-Wen Cheng for helpful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Softnmsimproving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.2" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dssd: Deconvolutional single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection with grammar models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning nonmaximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Lighthead r-cnn: In defense of two-stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pose partition networks for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Derpanis, and K. Daniilidis. 6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection-snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Crowdsourcing annotations for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAIW</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Denet: Scalable realtime object detection with directed sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tychsen-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10295</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improving object localization with fitness nms and bounded iou loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tychsen-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Point linking network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03646</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep regionlets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Starmap for category-agnostic keypoint and viewpoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Couplenet: Coupling global structure with local parts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
