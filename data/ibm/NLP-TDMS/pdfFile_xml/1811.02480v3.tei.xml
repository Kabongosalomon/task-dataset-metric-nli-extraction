<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FACE LANDMARK-BASED SPEAKER-INDEPENDENT AUDIO-VISUAL SPEECH ENHANCEMENT IN MULTI-TALKER ENVIRONMENTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Morrone</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Pasa</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Istituto Italiano di Tecnologia</orgName>
								<address>
									<settlement>Ferrara</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Tikhanoff</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Istituto Italiano di Tecnologia</orgName>
								<address>
									<settlement>Ferrara</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Bergamaschi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Fadiga</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Istituto Italiano di Tecnologia</orgName>
								<address>
									<settlement>Ferrara</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Badino</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Istituto Italiano di Tecnologia</orgName>
								<address>
									<settlement>Ferrara</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering &quot;Enzo Ferrari&quot;</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<settlement>Modena</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FACE LANDMARK-BASED SPEAKER-INDEPENDENT AUDIO-VISUAL SPEECH ENHANCEMENT IN MULTI-TALKER ENVIRONMENTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-audio-visual speech enhancement</term>
					<term>cock- tail party problem</term>
					<term>time-frequency mask</term>
					<term>LSTM</term>
					<term>face land- marks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the problem of enhancing the speech of a speaker of interest in a cocktail party scenario when visual information of the speaker of interest is available.</p><p>Contrary to most previous studies, we do not learn visual features on the typically small audio-visual datasets, but use an already available face landmark detector (trained on a separate image dataset).</p><p>The landmarks are used by LSTM-based models to generate time-frequency masks which are applied to the acoustic mixed-speech spectrogram. Results show that: (i) landmark motion features are very effective features for this task, (ii) similarly to previous work, reconstruction of the target speaker's spectrogram mediated by masking is significantly more accurate than direct spectrogram reconstruction, and (iii) the best masks depend on both motion landmark features and the input mixed-speech spectrogram.</p><p>To the best of our knowledge, our proposed models are the first models trained and evaluated on the limited size GRID and TCD-TIMIT datasets, that achieve speaker-independent speech enhancement in a multi-talker setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the context of speech perception, the cocktail party effect <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> is the ability of the brain to recognize speech in complex and adverse listening conditions where the attended speech is mixed with competing sounds/speech. Speech perception studies have shown that watching speaker's face movements could dramatically improve our ability at recognizing the speech of a target speaker in a multi-talker environment <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>This work aims at extracting the speech of a target speaker from single channel audio of several people talking simultaneously. This is an ill-posed problem in that many different hypotheses about what the target speaker says are con-sistent with the mixture signal. Yet, it can be solved by exploiting some additional information associated to the speaker of interest and/or by leveraging some prior knowledge about speech signal properties (e.g., <ref type="bibr" target="#b4">[5]</ref>). In this work we use face movements of the target speaker as additional information.</p><p>This paper (i) proposes the use of face landmark's movements, extracted using Dlib <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and (ii) compares different ways of mapping such visual features into time-frequency (T-F) masks, then applied to clean the acoustic mixed-speech spectrogram.</p><p>By using Dlib extracted landmarks we relieve our models from the task of learning useful visual features from raw pixels. That aspect is particularly relevant when the training audio-visual datasets are small.</p><p>The analysis of landmark-dependent masking strategies is motivated by the fact that speech enhancement mediated by an explicit masking is often more effective than mask-free enhancement <ref type="bibr" target="#b7">[8]</ref>.</p><p>All our models were trained and evaluated on the GRID <ref type="bibr" target="#b8">[9]</ref> and TCD-TIMIT <ref type="bibr" target="#b9">[10]</ref> datasets in a speaker-independent setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Speech enhancement aims at extracting the voice of a target speaker, while speech separation refers to the problem of separating each sound source in a mixture. Recently proposed audio-only single-channel methods have achieved very promising results <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. However the task still remains challenging. Additionally, audio-only systems need separate models in order to associate the estimated separated audio sources to each speaker, while vision easily allow that in a unified model.</p><p>Regarding audio-visual speech enhancement and separation methods an extensive review is provided in <ref type="bibr" target="#b13">[14]</ref>. Here we focus on the deep-learning methods that are most related to the present work.</p><p>Our first architecture (Section 2.1) is inspired by <ref type="bibr" target="#b14">[15]</ref>, where a pre-trained convolutional neural network (CNN) is used to generate a clean spectrogram from silent video <ref type="bibr" target="#b15">[16]</ref>. Rather than directly computing a time-frequency (T-F) mask, the mask is computed by thresholding the estimated clean spectrogram. This approach is not very effective since the pre-trained CNN is designed for a different task (video-tospeech synthesis). In <ref type="bibr" target="#b16">[17]</ref> a CNN is trained to directly estimate clean speech from noisy audio and input video. A similar model is used in <ref type="bibr" target="#b17">[18]</ref>, where the model jointly generates clean speech and input video in a denoising-autoender architecture. <ref type="bibr" target="#b18">[19]</ref> shows that using information about lip positions can help to improve speech enhancement. The video feature vector is obtained computing pair-wise distances between any mouth landmarks. Similarly to our approach their visual features are not learned on the audio-visual dataset but are provided by a system trained on different dataset. Contrary to our approach, <ref type="bibr" target="#b18">[19]</ref> uses position-based features while we use motion features (of the whole face) that in our experiments turned out to be much more effective than positional features.</p><p>Although the aforementioned audio-visual methods work well, they have only been evaluated in a speaker-dependent setting. Only the availability of new large and heterogeneous audio-visual datasets has allowed the training of deep neural network-based speaker-independent speech enhancement models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>The present work shows that huge audio-visual datasets are not a necessary requirement for speaker-independent audio-visual speech enhancement. Although we have only considered datasets with simple visual scenarios (i.e., the target speaker is always facing the camera), we expect our methods to perform well in more complex scenarios thanks to the robust landmark extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODEL ARCHITECTURES</head><p>We experimented with the four models shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. All models receive in input the target speaker's landmark motion vectors and the power-law compressed spectrogram of the single-channel mixed-speech signal. All of them perform some kind of masking operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">VL2M model</head><p>At each time frame, the video-landmark to mask (VL2M) model ( <ref type="figure" target="#fig_0">Fig. 1a</ref>) estimates a T-F mask from visual features only (of the target speaker). Formally, given a video sequence</p><formula xml:id="formula_0">v = [v 1 , . . . , v T ], v t ∈ R n and a target mask sequence m = [m 1 , . . . , m T ], m t ∈ R d , VL2M perform a function F vl2m (v) =m, wherem is the estimated mask.</formula><p>The training objective for VL2M is a Target Binary Mask (TBM) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, computed using the spectrogram of the target speaker only. This is motivated by our goal of extracting the speech of a target speaker as much as possible independently of the concurrent speakers, so that, e.g., we do not need to estimate their number. An additional motivations is that the model takes as only input the visual features of the target speaker, and a target TBM that only depends on the target speaker allows VL2M to learn a function (rather than approximating an ill-posed one-to-many mapping).</p><p>Given a clean speech spectrogram of a speaker s = [s 1 , . . . , s T ], s t ∈ R d , the TBM is defined by comparing, at each frequency bin f ∈ [1, . . . , d], the target speaker value s t [f ] vs. a reference threshold τ [f ]. As in <ref type="bibr" target="#b14">[15]</ref>, we use a function of long-term average speech spectrum (LTASS) as reference threshold. This threshold indicates if a T-F unit is generated by the speaker or refers to silence or noise. The process to compute the speaker's TBM is as follows:</p><p>1. The mean π[f ] and the standard deviation σ[f ] are computed for all frequency bins of all seen spectrograms in speaker's data.</p><formula xml:id="formula_1">2. The threshold τ [f ] is defined as τ [f ] = π[f ]+0.6·σ[f ]</formula><p>where 0.6 is a value selected by manual inspection of several spectrogram-TBM pairs.</p><p>3. The threshold is applied to every speaker's speech spectrogram s.</p><formula xml:id="formula_2">m t [f ] = 1, if s t [f ] ≥ τ [f ], 0, otherwise.</formula><p>The mapping F vl2m (·) is carried out by a stacked bidirectional Long Short-Term Memory (BLSTM) network <ref type="bibr" target="#b24">[25]</ref>. The BLSTM outputs are then forced to lay within the [0, 1] range. Finally the computed TBMm and the noisy spectrogram y are element-wise multiplied to obtain the estimated clean spectrogramŝ m =m • y, where</p><formula xml:id="formula_3">y = [y 1 , . . . y T ], y t ∈ R d .</formula><p>The model parameters are estimated to minimize the loss:</p><formula xml:id="formula_4">J vl2m = T t=1 d f=1 −m t [f] · log(m t [f]) − (1 − m t [f]) · log(1 −m t [f])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">VL2M ref model</head><p>VL2M generates T-F masks that are independent of the acoustic context. We may want to refine the masking by including such context. This is what the novel VL2M ref does <ref type="figure" target="#fig_0">(Fig.  1b)</ref>. The computed TBMm and the input spectrogram y are the input to a function that outputs an Ideal Amplitude Mask (IAM) p (known as FFT-MASK in <ref type="bibr" target="#b7">[8]</ref>). Given the target clean spectrogram s and the noisy spectrogram y, the IAM is defined as:</p><formula xml:id="formula_5">p t [f ] = s t [f ] y t [f ]</formula><p>Note that although IAM generation requires the mixed-speech spectrogram, separate spectrograms for each concurrent speakers are not required. The target speaker's spectrogram s is reconstructed by multiplying the input spectrogram with the estimated IAM. Values greater than 10 in the IAM are clipped to 10 in order to obtain better numerical stability as suggested in <ref type="bibr" target="#b7">[8]</ref>. </p><formula xml:id="formula_6">J mr = T t=1 d f =1 (p t [f ] · y t [f ] − s t [f ]) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Audio-Visual concat model</head><p>The third model <ref type="figure" target="#fig_0">(Fig. 1c)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Audio-Visual concat-ref model</head><p>The fourth model <ref type="figure" target="#fig_0">(Fig. 1d)</ref> is an improved version of the model described in section 2.3. The only difference is the input of the stacked BLSTM that is replaced by [ŝ m , y] wherê s m is the denoised spectrogram returned by VL2M operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>All experiments were carried out using the GRID <ref type="bibr" target="#b8">[9]</ref> and TCD-TIMIT <ref type="bibr" target="#b9">[10]</ref> audio-visual datasets. For each of them, we created a mixed-speech version.</p><p>Regarding the GRID corpus, for each of the 33 speakers (one had to be discarded) we first randomly selected 200 utterances (out of 1000). Then, for each utterance, we created 3 different audio-mixed samples. Each audio-mixed sample was created by mixing the chosen utterance with one utterance from a different speaker.</p><p>That resulted in 600 audio-mixed samples per speaker.</p><p>The resulting dataset was split into disjoint sets of 25/4/4 speakers for training/validation/testing respectively.</p><p>The TCD-TIMIT corpus consists of 59 speakers (we excluded 3 professionally-trained lipspeakers) and 98 utterances per speaker. The mixed-speech version was created following the same procedure as for GRID, with one difference. Contrary to GRID, TCD-TIMIT utterances have different duration. Thus 2 utterances were mixed only if their duration difference did not exceed 2 seconds. For each utterance pair, we forced the non-target speaker's utterance to match the duration of the target speaker utterance. If it was longer, the utterance was cut at its end, whereas if it was shorter, silence samples were equally added at its start and end.</p><p>The resulting dataset was split into disjoint sets of 51/4/4 speakers for training/validation/testing respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LSTM training</head><p>In all experiments, the models were trained using the Adam optimizer <ref type="bibr" target="#b25">[26]</ref>. Early stopping was applied when the error on the validation set did not decrease over 5 consecutive epochs.</p><p>VL2M, AV concat and AV concat-ref had 5, 3 and 3 stacked BLSTM layers respectively. All BLSTMs had 250 units. Hyper-parameters selection was performed by using random search with a limited number of samples, therefore all the reported results may improve through a deeper hyperparameters validation phase.</p><p>VL2M ref and AV concat-ref training was performed in 2 steps. We first pre-trained the models using the oracle TBM m. Then we substituted the oracle masks with the VL2M component and retrained the models while freezing the parameters of the VL2M component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Audio pre-and post-processing</head><p>The original waveforms were resampled to 16 kHz. Short-Time Fourier Transform (STFT) x was computed using FFT size of 512, Hann window of length 25 ms (400 samples), and hop length of 10 ms (160 samples). The input spectrogram was obtained taking the STFT magnitude and performing power-law compression |x| p with p = 0.3. Finally we applied per-speaker 0-mean 1-std normalization.</p><p>In the post-processing stage, the enhanced waveform generated by the speech enhancement models was reconstructed SDR PESQ ViSQOL  <ref type="table">Table 2</ref>. GRID results -speaker-independent.</p><p>by applying the inverse STFT to the estimated clean spectrogram and using the phase of the noisy input signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Video pre-processing</head><p>Face landmarks were extracted from video using the Dlib <ref type="bibr" target="#b6">[7]</ref> implementation of the face landmark estimator described in <ref type="bibr" target="#b5">[6]</ref>. It returns 68 x-y points, for an overall 136 values. We upsampled from 25/29.97 fps (GRID/TCD-TIMIT) to 100 fps to match the frame rate of the audio spectrogram. Upsampling was carried out through linear interpolation over time.</p><p>The final video feature vector v was obtained by computing the per-speaker normalized motion vector of the face landmarks by simply subtracting every frame with the previous one. The motion vector of the first frame was set to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>In order to compare our models to previous works in both speech enhancement and separation, we evaluated the performance of the proposed models using both speech separation  <ref type="table">Table 3</ref>. TCD-TIMIT results -speaker-independent. and enhancement metrics. Specifically, we measured the capability of separating the target utterance from the concurrent utterance with the source-to-distortion ratio (SDR) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. While the quality of estimated target speech was measured with the perceptual PESQ <ref type="bibr" target="#b28">[29]</ref> and ViSQOL <ref type="bibr" target="#b29">[30]</ref> metrics. For PESQ we used the narrow band mode while for ViSQOL we used the wide band mode. As a very first experiment we compared landmark position vs. landmark motion vectors. It turned out that landmark positions performed poorly, thus all results reported here refer to landmark motion vectors only.</p><p>We then carried out some speaker-dependent experiments to compare our models with previous studies as, to the best of our knowledge, there are no reported results of speakerindependent systems trained and tested on GRID and TCD-TIMIT to compare with. <ref type="table">Table 1</ref> reports the test-set evaluation of speaker-dependent models on the GRID corpus with landmark motion vectors. Results are comparable with previous state-of-the-art studies in an almost identical setting <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>. <ref type="table">Table 2 and 3</ref> show speaker-independent test-set results on the GRID and TCD-TIMIT datasets respectively. V2ML performs significantly worse than the other three models indicating that a successful mask generation has to depend on the acoustic context. The performance of the three models in the speaker-independent setting is comparable to that in the speaker-dependent setting.</p><p>AV concat-ref outperforms V2ML ref and AV concat for both datasets. This supports the utility of a refinement strategy and suggests that the refinement is more effective when it directly refines the estimated clean spectrogram, rather than refining the estimated mask.</p><p>Finally, we evaluated the systems in a more challenging testing condition where the target utterance was mixed with 2 utterances from 2 competing speakers. Despite the model was trained with mixtures of two speakers, the decrease of performance was not dramatic.</p><p>Code and some testing examples of our models are available at https://goo.gl/3h1NgE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>This paper proposes the use of face landmark motion vectors for audio-visual speech enhancement in a single-channel multi-talker scenario. Different models are tested where landmark motion vectors are used to generate time-frequency (T-F) masks that extract the target speaker's spectrogram from the acoustic mixed-speech spectrogram.</p><p>To the best of our knowledge, some of the proposed models are the first models trained and evaluated on the limited size GRID and TCD-TIMIT datasets that accomplish speakerindependent speech enhancement in the multi-talker setting, with a quality of enhancement comparable to that achieved in a speaker-dependent setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Model architectures. The model performs a function F mr (v, y) =p that consists of a VL2M component plus three different BLSTMs G m , G y and H . G m (F vl2m (v)) = r m receives the VL2M maskm as input, and G y (y) = r y is fed with the noisy spectrogram. Their output r m , r y ∈ R z are fused in a joint audio-visual representation h = [h 1 , . . . , h T ], where h t is a linear combination of r mt and r yt : h t = W hm · r mt + W hy · r yt + b h . h is the input of the third BLSTM H (h) =p, wherep lays in the [0,10] range. The loss function is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>performs early fusion of audiovisual features. This model consists of a single stacked BLSTM that computes the IAM maskp from the concatenated [v, y]. The training loss is the same J mr used to train VL2M ref. This model can be regarded as a simplification of VL2M ref, where the VL2M operation is not performed.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some experiments on the recognition of speech, with one and with two ears</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="975" to="979" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cocktail party problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Josh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="1024" to="1027" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual input enhances selective speech envelope tracking in auditory cortex at a &quot;cocktail party</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">B</forename><surname>Elana Zion Golumbic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Cogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poeppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1417" to="1426" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lip-reading aids word recognition most in moderate noise: A bayesian explanation using high-dimensional feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Foxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><forename type="middle">C</forename><surname>Parra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Auditory scene analysis: The perceptual organization of sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Albert S Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On Training Targets for Supervised Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TCD-TIMIT: An Audio-Visual Corpus of Continuous Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Harte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eoin</forename><surname>Gillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="615" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017-03" />
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech and Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Audiovisual Speech Source Separation: An overview of key methodologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Rivet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Syed Mohsen Naqvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="125" to="134" />
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Seeing through noise: Visually driven speaker separation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tavi</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="3051" to="3055" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved speech reconstruction from silent video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tavi</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2017 Workshop on Computer Vision for Audio-Visual Media</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaph</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="1170" to="1174" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Audio-Visual Speech Enhancement Using Multimodal Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Cheng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syu-Siang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Hui</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiu-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio-visual speech enhancement using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Cheng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syu-Siang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Hui</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Chun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiu-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)</title>
		<meeting><address><addrLine>Jeju, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinatan</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03619</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The conversation: Deep audio-visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Determination of the potential benefit of timefrequency gain manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Anzalone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">A</forename><surname>Calandruccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurel</forename><forename type="middle">H</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">16957499</biblScope>
			<date type="published" when="2006-10" />
		</imprint>
	</monogr>
	<note>pmid</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Role of mask pattern in intelligibility of ideal binary-masked noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrik</forename><surname>Kjems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jesper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Boldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Lunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1415" to="1426" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fevotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">mir eval: A transparent implementation of common mir metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Society for Music Information Retrieval Conference</title>
		<meeting>the 15th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ViSQOL: The Virtual Speech Quality Objective Listener</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWAENC 2012; International Workshop on Acoustic Signal Enhancement</title>
		<imprint>
			<date type="published" when="2012-09" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
