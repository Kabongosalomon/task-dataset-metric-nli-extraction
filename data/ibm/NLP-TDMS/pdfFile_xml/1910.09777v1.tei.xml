<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Correction for Human Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peike</forename><surname>Li</surname></persName>
							<email>peike.li@yahoo.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ReLER Lab, Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqiu</forename><surname>Xu</surname></persName>
							<email>imyunqiuxu@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ReLER Lab, Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ReLER Lab, Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Correction for Human Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Labeling pixel-level masks for fine-grained semantic segmentation tasks, e.g. human parsing, remains a challenging task. The ambiguous boundary between different semantic parts and those categories with similar appearance usually are confusing, leading to unexpected noises in ground truth masks. To tackle the problem of learning with label noises, this work introduces a purification strategy, called Self-Correction for Human Parsing (SCHP), to progressively promote the reliability of the supervised labels as well as the learned models. In particular, starting from a model trained with inaccurate annotations as initialization, we design a cyclically learning scheduler to infer more reliable pseudomasks by iteratively aggregating the current learned model with the former optimal one in an online manner. Besides, those correspondingly corrected labels can in turn to further boost the model performance. In this way, the models and the labels will reciprocally become more robust and accurate during the self-correction learning cycles. Benefiting from the superiority of SCHP, we achieve the best performance on two popular single-person human parsing benchmarks, including LIP and Pascal-Person-Part datasets. Our overall system ranks 1st in CVPR2019 LIP Challenge. Code is available at this url.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human parsing, as a fine-grained semantic segmentation task, aims to assign each image pixel from the human body to a semantic category, e.g. arm, leg, dress, skirt. Understanding the detailed semantic parts of human is crucial in several potential application scenarios, including image editing, human analysis, virtual try-on and virtual reality. Recent advances on fully convolutional neural networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3]</ref> achieves various of well-performing methods for the human parsing task <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>To learn reliable models for human parsing, a large amount of pixel-level masks are required for supervision. However, labeling pixel-level annotations for human pars- ing is much harder than those traditional pixel-level understanding tasks. In particular, for those traditional semantic segmentation tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3]</ref>, all the pixels belonging to one instance share the same semantic label, which is usually easy to be identified by annotators. Differently, the human parsing task requires annotators to carefully distinguish those semantic parts of one person. Moreover, the situation will become even more challenging when the annotator got confused by the ambiguous boundaries between different semantic parts. Due to these factors, there inevitably exists different types of label noises (as illustrated in <ref type="figure" target="#fig_0">Figure 1a</ref>) caused by the careless observations by annotators. This incomplete and low quality of the annotation labels will set a significant obstacle, which is usually ignored and prevents the performance of human parsing from increasing to a higher level. In this work, we investigate the problem of learning with noise in the human parsing task. Our target is to improve the model performance and generalization by progressively refining the noisy labels during the training stage.</p><p>In this paper, we introduce a purification strategy named Self-Correction for Human Parsing (SCHP), which can progressively promote the reliability of the supervised labels, as well as the learned models during the training process. Concretely, the whole SCHP pipeline can be divided into two sub-procedures, i.e. the model aggregation and the label refinement procedure. Starting from a model trained on inaccurate annotations as initialization, we design a cyclically learning scheduler to infer more reliable pseudomasks by iteratively aggregating the current learned model with the former optimal one in an online manner. Besides, those corrected labels can in turn to boost the model performance, simultaneously. In this way, the self-correction mechanism will enable the model or labels to mutually promote its counterpart, leading to a more robust model and accurate label masks as training goes on.</p><p>Besides, to tackle the problem of ambiguous boundaries between different semantic parts, we introduce a network architecture called Augmented Context Embedding with Edge Perceiving (A-CE2P). In principle, our network architecture is an intuitive generalization and augmentation of the CE2P <ref type="bibr" target="#b26">[27]</ref> framework. We introduce a consistency constraint term to augment the CE2P, so that the edge information is not only implicitly facilitated the parsing result by feature map level fusion, but also explicitly constrained between the parsing and edge prediction. Note that we do not claim any novelty of our architecture structure, but only the superiority of the performance.</p><p>On the whole, our major contributions can be summarized as follows,</p><p>• We propose a simple yet effective self-correction strategy SCHP for the human parsing task by online model aggregating and label refining which could mutually promote the model performance and label accuracy.</p><p>• We introduce a general architecture framework A-CE2P for human parsing that both implicitly and explicitly captures the boundary information along with the parsing information.</p><p>• We extensively investigate our SCHP on two popular human parsing benchmarks. Our method achieves the new state-of-the-art. In particular, we achieve the mIoU score of 59.36 on the large scale benchmark LIP, which outperforms the previous closest approach by 6.2 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human Parsing. Several different aspects of the human parsing task have been studied. Some early works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b17">18]</ref> utilized pose estimation together with the human parsing simultaneously as a multi-task learning problem. In <ref type="bibr" target="#b26">[27]</ref>, they cooperated the edge prediction with human parsing to accurately predict the boundary area. Most of the prior works assumed the fact that ground truth labels are correct and well-annotated. However, due to time and cost consuming, there inevitably exists lots of different label noises (as shown in <ref type="figure" target="#fig_0">Figure 1a</ref>). Meanwhile, it is impracticable to clean the pixel-level labels manually. Guided by this intuition, we try to tackle this problem via a novel self-correction mechanism in this paper.</p><p>Pseudo-Labeling. Pseudo-labeling <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref> is a typical technique used in semi-supervised learning. In semisupervised learning setting, they assign pseudo-labels to the unlabeled data. However, in our fully supervised learning scheme, we are unable to locate the label noises, thus all ground truth labels are treated equally. From the perspective of distillation, the generated pseudo-label data contains much so-called dark knowledge <ref type="bibr" target="#b12">[13]</ref> which could serve as a purification signal. Inspired by these findings, we design a cyclically learning scheduler to infer more reliable pseudomasks by iteratively aggregating the current learned model with the former optimal one in an online manner. Also those corrected labels can in turn to boost the model performance, simultaneously.</p><p>Self-Ensembling. There is a line of researches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b14">15]</ref> that exploit self-ensembling methods in various scenarios. For example, <ref type="bibr" target="#b28">[29]</ref> averaged model weights as selfensembling and adopted in the semi-supervised learning task. In <ref type="bibr" target="#b14">[15]</ref>, they averaged the model weight and led to better generalization. Different from their method, our proposed self-correction approach is to correct the noisy training label via a model and label mutually promoting process. By an online manner, we average both model weights and the predictions simultaneously. To the best of our knowledge, we make a first attempt to formulate the label noise problem as the mutual model and label optimization in fine-grained semantic segmenting to boost the performance. Furthermore, our proposed method is online training with a cyclical learning scheduler and only exhaust little extra computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting CE2P</head><p>CE2P <ref type="bibr" target="#b26">[27]</ref> is a well-performing framework for the human parsing task. In the CE2P network, they cooperate the edge prediction with human parsing to accurately predict the boundary area. Concretely, CE2P consists of three key branches, i.e. parsing branch, edge branch and fusion branch. In particular, the edge branch is employed to generate class-agnostic boundary maps. In the fusion branch, both semantic-aware feature representations from the parsing branch and the boundary-aware feature representations from the edge branch are concatenated to further produce a refined human parsing prediction.</p><p>Although CE2P is a framework that has already incorporated the most useful functions from the semantic segmentation community. However, there are still some aspects that could be further strengthened. First, the conventional cross-entropy loss indirectly optimizes mean intersectionover-union (mIoU) metric, which is a crucial metric to reveal the comprehensive performance of the model. Second, CE2P only implicitly facilitates the parsing results with the edge predictions by feature-level fusion. There is no explicit constraint to ensure the parsing results maintaining the same geometry shape of the boundary predictions.</p><p>Moreover, few efforts have been made to investigate the versatility of the CE2P framework i.e. the ability to accommodate other modules. Based on the key function, the parsing branch can be divided into three modules, i.e. backbone module, context encoding module and decoder module. Concretely, the backbone module could be plugged in with any fully-convolutional structure backbone such as ResNet-based <ref type="bibr" target="#b11">[12]</ref> semantic segmentation network. The context encoding module utilizes the global context information to distinguish the fine-grained categories information. This module could be any effective context discovering module, e.g. feature pyramid based approaches like PSP <ref type="bibr" target="#b32">[33]</ref>, ASPP <ref type="bibr" target="#b3">[4]</ref>, or attention-based modules like OC-Net <ref type="bibr" target="#b31">[32]</ref>. More detailed network architecture could refer to our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Augmented-CE2P</head><p>The Augmented-CE2P (A-CE2P) is an intuitive generalization and augmentation of the CE2P, which can yield increased performance gain by augmenting additional powerful modules. In this work, our self-correction training employs the A-CE2P as the basic framework for conduct-ing human parsing. We demonstrate the overview of the A-CE2P framework in <ref type="figure" target="#fig_1">Figure 2</ref>. Notably, several unique characteristics of A-CE2P are described as follows.</p><p>Targeted Learning Objectives. For an image I, suppose the human parsing ground truth label isŷ n k and the parsing prediction is y n k , where n is the number of pixels for the k-th class. We define the pixel-level supervised objective using conventional cross-entropy loss as:</p><formula xml:id="formula_0">L cls = − 1 N k nŷ n k log p(y n k ).<label>(1)</label></formula><p>here N is the number of pixels, K is the number of classes. It is known that conventional cross-entropy loss is usually convenient to train a neural network, but it facilitates mean intersection-over-union (mIoU) indirectly. To tackle this issue, following by <ref type="bibr" target="#b0">[1]</ref>, we additionally introduce a tractable surrogate loss function for optimizing the mIoU directly. The final parsing loss function can be defined as a combination of the cross-entropy loss and the mIoU loss L miou ,</p><formula xml:id="formula_1">L parsing = L cls + L miou .<label>(2)</label></formula><p>Consistency Constraint. In the CE2P, the balanced cross-entropy loss L edge is adopted to optimize the edge prediction, so that the learned edge-aware features can help distinguish human parts and facilitate human parsing via the fusion branch indirectly.</p><p>In the A-CE2P, we propose to further exploit the predicted boundary information by explicitly maintaining the consistency between the parsing prediction and the boundary prediction, i.e. ensure that the predicted parsing result matches the predicted edge as exact as possible. Intuitively, we add a constraint term to penalized the mismatch:</p><formula xml:id="formula_2">L consistent = 1 |N + | n∈N + |ẽ n − e n |,<label>(3)</label></formula><p>where e n is the edge maps predicted from the edge branch andẽ n is the edge maps generated from the parsing result y n k . To prevent the non-edge pixels dominate the loss, we only allow the positive edge pixels n ∈ N + for contributing the consistency constraint term.</p><p>In brief, the overall learning objective of our framework is</p><formula xml:id="formula_3">L = λ 1 L edge + λ 2 L parsing + λ 3 L consistent ,<label>(4)</label></formula><p>where λ 1 , λ 2 and λ 3 are hyper-parameters to control the contribution among these three losses. We jointly train the model in an end-to-end fashion by minimizing L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning with Noise via Self-Correction</head><p>Based on the A-CE2P, we proposed the self-correction method that allows us to refine the label and get a robust   <ref type="figure">Figure 3</ref>: Illustration of the SCHP Pipeline. The self-correction mechanism will enable the model or labels to mutually promote its counterpart, leading to a more robust model and accurate label masks as training goes on. Label noises are specially marked with white boxes. model via an online mutual improvement process, illustrated in <ref type="figure">Figure 3</ref>.</p><p>Training Strategy. Our proposed self-correction training strategy is a model and label aggregating process, which can promote the model performance and refine the ground truth labels iteratively. This promotion relies on the initial performance of the model. In other words, if intermediate results generated by the network are not accurate enough, they may potentially harm the iteration process. Therefore, we start to run our proposed self-correction algorithm after a good initialization, i.e. when the training loss starts to flatten with the original noisy labels. To make a fair comparison with other methods, we shorten the initial stage and keep the total training epochs as the same. After the initialization stage, we adopt a cyclically learning scheduler with warm restarts. Each cycle totally contains T epochs. In practice, we use a cosine annealing learning rate scheduler with cyclical restart <ref type="bibr" target="#b22">[23]</ref>. Formally, η max and η min are set to the initial learning rate and final learning rate, while T cur is the number of epochs since the last restart. Thus, the overall learning rate can be formulated as,</p><formula xml:id="formula_4">η = η min + 1 2 (η max − η min )(1 + cos( T cur T π)). (5)</formula><p>Online Model Aggregation. We aim to discover all the potential information from the past optimal models to improve the performance of the future model. In our cyclical training strategy, intuitively, the model will converge to a local-minimum at the end of each cycle. And there has great model disparity among these sub-optimal models. Here we denote the set of all the sub-optimal model we get after each cycle as Ω = {ω 0 ,ω 1 , ...,ω M } and M is the total number of cycles.</p><p>At the end of each cycle, we aggregate the current model weightω with the former sub-optimal oneω m−1 to achieve a new model weightω m ,</p><formula xml:id="formula_5">ω m = m m + 1ω m−1 + 1 m + 1ω ,<label>(6)</label></formula><p>where m denotes the current cycle number and 0 ≤ m ≤ M .</p><p>After updating the current model weight with the former optimal one from the last cycle, we forward all the training data for one epoch to re-estimate the statistics of the parameters (i.e. moving average and standard deviation) in all batch normalization <ref type="bibr" target="#b13">[14]</ref> layers. During these successive cycles of model aggregation, the network leads to wider model optima as well as improved model's generalization ability.</p><p>Online Label Refinement. It is known that soft, multiclass labels may contain dark information <ref type="bibr" target="#b12">[13]</ref>. We aim to explore all these dark information to improve the performance and alleviate the label noises. After updating the model weight as mentioned in Eq. (6), we also update the ground truth of training labels. These generated pseudomasks are more unambiguous, smooth and have the relational information between the fine-grain categories, which are taken as the supervised signal for the next cycle's optimization. During successive cycles of pseudo-label refinement, this improves the network performance as well as the generalization ability of the model. Also, these pseudomasks potentially alleviate or eliminate the noise in the original ground truth. Here we denote the predicted label after each cycles as Y = {ŷ 0 ,ŷ 1 , ...,ŷ M }. Same as the model weight averaging process, we update the ground truth label  </p><formula xml:id="formula_6">= m m + 1ŷ m−1 + 1 m + 1ŷ ,<label>(7)</label></formula><p>whereŷ is the generated pseudo-mask by modelω m . The detail of our proposed self-correction procedure is summarized in Algorithm 1.</p><p>Note that the model and label are mutual improved stepby-step after each cyclical training process. The whole process is training in an online manner and does not need any extra training epochs. In addition, there is barely no extra computation required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>Can SCHP generalize to other tasks? Our approach has no assumption for the data type. But the self-correction is base on the soft pseudo-label generated during the process. Thus our method could be applied in some other task such as classification and segmentation, but may be not applicable to regression tasks like detection.</p><p>Can SCHP still benefit with clean data? Although we could achieve more performance gain with our proposed self-correction process on noisy datasets. However, when the ground truth is relatively clean, the online model aggregation process could serve as a self-model ensembling, which could lead to better performance and generalization. Still, the online label refinement process benefits from discovering the dark knowledge using pseudo-mask instead of one-hot ground truth pixel-level label.</p><p>Transduction vs. Induction. In this work, we mainly focus on the supervised training scheme. Nevertheless, our approach can also be operated under semi-supervised learning manner, i.e. we assume that all the test images are available at once and utilize all the test samples for selfcorrection process together with the training images jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we perform a comprehensive comparison of our SCHP with other single-person human parsing state-of-the-art methods along with thorough ablation experiments to demonstrate the contribution of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>Datasets. We evaluate our proposed method on two single human parsing benchmarks, including LIP <ref type="bibr" target="#b17">[18]</ref> and PASCAL-Person-Part <ref type="bibr" target="#b5">[6]</ref>. Method hat hair glove s-glass u-clot dress coat sock pant j-suit scarf skirt face l-arm r-arm l-leg r-leg l-shoe r-shoe bkg mIoU Attention <ref type="bibr" target="#b4">[5]</ref> 58 -  LIP <ref type="bibr" target="#b17">[18]</ref> is the largest human parsing dataset, which contains 50,462 images with elaborated pixel-wise annotations with 19 semantic human part labels. The images collected from the real-world scenarios contain human appearing with challenging poses and views, heavily occlusions, various appearances and low-resolutions. The datasets are divided images into 30,462 images for train set, 10,000 images for validation set and 10,000 for test set.</p><formula xml:id="formula_7">- - - - - - - - - - - - - - - - - - -</formula><p>PASCAL-Person-Part [6] is a relatively small dataset annotated from PASCAL VOC 2010, which contains 1,716 train images, 1,817 validation images. The ground truth label consists of six semantic parts including head, torso, upper/lower arms, upper/lower legs and one background class. This dataset is challenging due to large variations in scale.</p><p>Metrics. We report three standard metrics for the human parsing task, including pixel accuracy, mean accuracy, mean intersection over union (mIoU). Note the mIoU metric generally represents the overall parsing performance of the method. Implementation Details. We choose the ResNet-  101 <ref type="bibr" target="#b11">[12]</ref> as the backbone of the feature extractor and use an ImageNet <ref type="bibr" target="#b7">[8]</ref> pre-trained weights. Specifically, we fix the first three residual layers and set the stride size of last residual layer to 1 with a dilation rate of 2. In this way, the final output is enlarged to 1/16 resolution size w.r.t the original image. We adopt pyramid scene parsing network <ref type="bibr" target="#b32">[33]</ref> as the context encoding module. We use 473 × 473 as the input resolution. Training is done with a total batch size of 36. For our joint loss function, we set the weight of each term as λ 1 = 1, λ 2 = 1, λ 3 = 0.1. The initial learning rate is set as 7e-3 with a linear increasing warm-up strategy for 10 epochs. We train our network for 150 epochs in total for a fair comparison, the first 100 epochs as initialization following 5 cycles each contains 10 epochs of the self-correction process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with state-of-the-arts</head><p>In <ref type="table" target="#tab_3">Table 1</ref>, we compare the performance of our network with other state-of-the-art methods on the LIP. It can be observed that even our baseline model outperforms all the   <ref type="table">Table 4</ref>: The effect of our proposed model aggregation (MA) and label refinement (LR) strategy is evaluated on LIP validation set.</p><p>other state-of-the-art methods, which illustrates the effectiveness of the A-CE2P framework. In particular, we also apply test-time augmentation with multi-scale and horizontal flipping to make a fair comparison with others. Our SCHP outperforms the others with a large gain, achieving mIoU improvement by 6.26%, which is a significant boost considering the performance at this level. Our proposed approach achieves a large gain especially for some categories with few samples like scarf, sunglasses and some confusing categories such as dress, skirt and left-right confusion. The gains are mainly from using both model aggregation and label refinement for our self-correction process. Furthermore, the qualitative comparison between the predicted results of SCHP and ground truth annotations is shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>We can see that our SCHP can achieve even better parsing results than the ground truth ones.</p><p>To validate the generalization ability of our method, we further report the comparison with other state-of-the-arts on PASCAL-Person-Part in <ref type="table" target="#tab_4">Table 2</ref>. It can be observed that our SCHP outperforms all the previous approaches. Particularly, our SCHP beats the DPC <ref type="bibr" target="#b1">[2]</ref>. DPC is a network architecture search (NAS)-based method which easily achieves optimal results on the small dataset. Besides, instead of using more powerful backbone models such as Xception <ref type="bibr" target="#b6">[7]</ref> in DPC, we only adopt ResNet-101 as the backbone. In addition, DPC leverages MS COCO <ref type="bibr" target="#b20">[21]</ref> as additional data for pre-training, while our model is only pre-trained on Im-ageNet. All these results well demonstrate the superiority and generalization of our proposed approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We perform extensive ablation experiments to illustrate the effect of each component in our SCHP. All experiments are conducted on LIP benchmark.</p><p>Alternatives Architectures. Since our proposed SHCP is a generic framework, we could merely plug-and-play with various backbones and context encoding modules. <ref type="figure" target="#fig_4">Figure 5a</ref> shows SHCP with different backbones from lightweight model MobileNet-V2 <ref type="bibr" target="#b27">[28]</ref> to relatively heavy backbone ResNet-101 <ref type="bibr" target="#b11">[12]</ref>. Interestingly, the lightweight MobileNet-V2 achieves the mIoU score of 52.06, with the benefit of SCHP leads to 53.13. This result is even better than some previous results <ref type="bibr" target="#b26">[27]</ref> achieved by ResNet-101. We note that deeper network (18 vs. 50 vs. 101) tends to perform better. Regardless of different backbones, our SCHP consistently brings consistent positive gains, 1.08, 1.40 and 1.70, respectively in terms of mIoU. It suggests the robustness of our proposed method. As shown in <ref type="figure" target="#fig_4">Figure 5b</ref>, we further examine the robustness of our SCHP by varying the context encoding module. In particular, we choose three different types of modules, including multi-level global average pooling based module pyramid scene parsing network (PSP) <ref type="bibr" target="#b32">[33]</ref>, multi-level atrous convolutional pooling based module atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b3">[4]</ref> and attention-mechanism based module OCNet <ref type="bibr" target="#b31">[32]</ref>. Despite the similar basic performance of these three modules, our SCHP unfailingly obtains mIoU increased by 1.70, 1.30, 1.00 points for PSP, ASPP and OCNet respectively. This further highlights the effectiveness of self-correction mechanism in our approach. Note that although we could achieve even better results with these advanced modules, the network structure modification is not the focus of this study. In our baseline model, we choose to use ResNet-101 as backbone and PSP as context encoding module.</p><p>Influence of Learning Objectives. Our network is  trained in an end-to-end manner with composite learning objectives describes as Eq. <ref type="formula" target="#formula_3">(4)</ref>. An evaluation of different learning objectives is shown in <ref type="table" target="#tab_7">Table 3</ref>. In this table, E denotes the binary cross-entropy loss to optimize the boundary prediction. I denotes the tractable surrogate function optimizing the mIoU metric. C denotes the consistency constraint term for maintain the consistency between parsing result and boundary result. Without all these three terms, only the basic cross-entropy loss function for parsing takes effect. By introducing the edge information, the performance improves mIoU by about 0.4. This gain is mainly due to the accurate prediction at the boundary area between semantic parts. Additionally, we compare the result further adding the IoU loss. As can be seen, the IoU loss significantly boosts the mean accuracy by 2.8 points and mIoU by 2.2 points, but the pixel accuracy almost remains the same level. This highlights that IoU loss largely resolves the challenge of prediction accuracy especially at some small area and infrequent categories. Finally, the result shows a gain of 0.59 mean accuracy and 0.55 mIoU when applying the consistency between parsing segmentation and edge prediction. Effect of Self-Correction. In <ref type="table">Table 4</ref>, we validate the effect of each component in our proposed SCHP, including the model aggregation (MA) process and the label refinement (LR) process. All experiments are conducted upon with A-CE2P framework. When there are no MA and LR involved, our method reduces to the conventional training process. By employing the MA process, the result shows a gain of mIoU by 1.1 points. While only benefit from the LR process, we achieve 0.6 point improvement. We achieve the best performance by simultaneously introducing these two processes. We can observe that the model aggregation and label refinement mutually promote each other in our SCHP. To better qualitatively describe our SCHP, <ref type="figure" target="#fig_0">Figure 1b</ref> shows the visualization of the generated pseudo-masks during the self-correction cycles. Note that all these pseudomasks are up-sampled to the original size and applied argmax operation for better illustration. Label noises like inaccurate boundary, confused fine-grained categories, confused mirror categories, multi-person occlusion are alleviated and partly resolved during the self-correction cycles. Unsurprisingly, some of the boundaries of our corrected labels are prone to be more smooth than the ground truth labels. All these results demonstrate the effectiveness of our proposed self-correction method. Intuitively, our selfcorrection process is a mutual promoting process benefiting both model aggregation and label refinement. During the self-correction cycles, the model gets increasingly more robust, while by exploring the dark information from pseudomasks produced by the enhanced model, the label noises are corrected in an implicit manner. The fact that corrected labels are smooth than the ground truth also illustrates the effectiveness of our model architecture design for combining the edge information. Influence of Self-Correction Cycles. We achieve the goal of self-correction by a cyclically learning scheduler. The number of cycles is a virtual hyper-parameter for this process. To make a fair comparison with other methods <ref type="bibr" target="#b26">[27]</ref>, we maintain the entire training epoch unchanged.</p><p>The performance curves are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. It is evident that the performance consistently improves during the process, with the largest improvement after the first cycle and tendency saturates at the end. Our method may achieve even higher performance when extending more training epochs. It is noteworthy the performance of MA, LR and SCHP is not same at cycle 0. This small gap is caused due to the re-estimation of BatchNorm parameters. From the performance curve, We also intelligibly demonstrate the mutual benefit of the model aggregation and the label refinement process. More visualization of the self-correction process is illustrated in <ref type="figure" target="#fig_6">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we propose an effective self-correction strategy to deal with the label noises for the human parsing task. Our proposed method achieves the new state-of-theart with a large margin gain. Moreover, the self-correction mechanism is a general strategy for training and can be incorporated into any frameworks to make further performance improvement. In the future, we would like to extend our method to multiple-person human parsing and video multiple-person human parsing tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Different types of label noises exist in the ground truth. (b) Our self-correction mechanism progressively promotes the reliability of the supervised labels. Label noises are emphasized by white dotted boxes. Better zoom in to see the details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the Augmented-CE2P framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of SCHP results on LIP validation set. Note in most cases, our SCHP human parsing prediction is even better than the ground truth label. Zoom in to see details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Effect of SCHP with different backbones and context encoding modules. All experiments are conducted on LIP validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Performance curves w.r.t different training cycles. All experiments are conducted on LIP validation set. The mIoU, pixel accuracy, mean accuracy are reported, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of our self-correction process in LIP train set. Label noises are emphasized with white dotted boxes. Better zoom in to see the details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Legend Cycle 1 mIoU=58.13 Cycle 2 mIoU=58.35 Cycle 3 mIoU=58.48 Cycle 4 mIoU=58.55 Cycle 5 mIoU=58.62 update weight update label Left Arm Right Arm Upper Cloth</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Self-Correction</cell></row><row><cell></cell><cell>final</cell></row><row><cell></cell><cell>prediction</cell></row><row><cell></cell><cell>final</cell></row><row><cell></cell><cell>model</cell></row><row><cell>upsample</cell><cell></cell></row><row><cell>&amp;</cell><cell></cell></row><row><cell>argmax</cell><cell>Cycle 0</cell></row><row><cell></cell><cell>mIoU=57.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.87 66.78 23.32 19.48 63.20 29.63 49.70 35.23 66.04 24.73 12.84 20.41 70.58 50.17 54.03 38.35 37.70 26.20 27.09 84.00 42.92 DeepLab [3] 59.76 66.22 28.76 23.91 64.95 33.68 52.86 37.67 68.05 26.15 17.44 25.23 70.00 50.42 53.89 39.36 38.27 26.95 28.36 84.09 44.80 SSL [11] 58.21 67.17 31.20 23.65 63.66 28.31 52.35 39.58 69.40 28.61 13.70 22.52 74.84 52.83 55.67 48.22 47.49 31.80 29.97 84.64 46.19 MMAN [24] 57.66 65.63 30.07 20.02 64.15 28.39 51.98 41.46 71.03 23.61 9.65 23.20 69.54 55.30 58.13 51.90 52.17 38.58 39.05 84.75 46.81 MuLA<ref type="bibr" target="#b24">[25]</ref> </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>49.<ref type="bibr" target="#b29">30</ref> JPPNet<ref type="bibr" target="#b17">[18]</ref> 63.55 70.20 36.<ref type="bibr" target="#b15">16</ref> 23.48 68.15 31.42 55.65 44.56 72.19 28.39 18.76 25.14 73.36 61.97 63.88 58.21 57.99 44.02 44.09 86.26 51.37 CE2P [27] 65.29 72.54 39.09 32.73 69.46 32.52 56.28 49.67 74.11 27.23 14.19 22.51 75.50 65.14 66.59 60.10 58.59 46.63 46.12 87.67 53.10 A-CE2P w/o SCHP 69.59 73.02 45.21 35.59 69.85 35.97 56.96 51.06 75.79 30.41 22.00 27.07 75.79 68.54 70.30 67.83 66.90 53.53 54.08 88.11 56.88 A-CE2P w/ SCHP 69.96 73.55 50.46 40.72 69.93 39.02 57.45 54.27 76.01 32.88 26.29 31.68 76.19 68.65 70.92 67.28 66.56 55.76 56.50 88.36 58.62 A-CE2P w/ SCHP † 70.63 74.09 51.40 41.70 70.56 40.06 58.17 55.17 76.57 33.78 26.63 32.83 76.63 69.33 71.76 67.93 67.42 56.56 57.55 88.40 59.36 Comparison with state-of-the-arts on LIP validation set. † designates the test time augmentation. 72.28 57.07 56.21 52.43 50.36 97.72 67.60 PGN [10] 90.89 75.12 55.83 64.61 55.42 41.57 95.33 68.40 DPC [2] 88.81 74.54 63.85 63.73 57.24 54.55 96.66 71.34 A-CE2P w/ SCHP 87.00 72.27 64.10 63.44 56.57 55.00 96.07 70.63 A-CE2P w/ SCHP † 87.41 73.80 64.98 64.70 57.43 55.62 96.26 71.46</figDesc><table><row><cell>Method</cell><cell cols="7">head torso u-arm l-arm u-leg l-leg bkg mIoU</cell></row><row><cell>Attention [5]</cell><cell cols="7">81.47 59.06 44.15 42.50 38.28 35.62 93.65 56.39</cell></row><row><cell>HAZN [30]</cell><cell cols="7">80.76 60.50 45.65 43.11 41.21 37.74 93.78 57.54</cell></row><row><cell>LG-LSTM [20]</cell><cell cols="7">82.72 60.99 45.40 47.76 42.33 37.96 88.63 57.97</cell></row><row><cell>SS-JPPNet [18]</cell><cell cols="7">83.26 62.40 47.80 45.58 42.32 39.48 94.68 59.36</cell></row><row><cell>MMAN [24]</cell><cell cols="7">82.58 62.83 48.49 47.37 42.80 40.40 94.92 59.91</cell></row><row><cell>G-LSTM [19]</cell><cell cols="7">82.69 62.68 46.88 47.71 45.66 40.93 94.59 60.16</cell></row><row><cell>Part FCN [31]</cell><cell cols="7">85.50 67.87 54.72 54.30 48.25 44.76 95.32 64.39</cell></row><row><cell>Deeplab [3]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-64.94</cell></row><row><cell>WSHP [9]</cell><cell>87.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison with state-of-the-arts on PASCAL- Person-Part validation set.† designates the test time augmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">Each component of our loss function is evaluated</cell></row><row><cell cols="5">on LIP validation set, including edge loss (E), IoU loss</cell></row><row><cell cols="3">(I) and consistency constraint (C).</cell><cell></cell><cell></cell></row><row><cell cols="2">Method MA LR</cell><cell cols="3">Pixel Acc Mean Acc mIoU</cell></row><row><cell>-</cell><cell>-</cell><cell>87.68</cell><cell>68.79</cell><cell>56.88</cell></row><row><cell></cell><cell>-</cell><cell>87.90</cell><cell>71.27</cell><cell>57.94</cell></row><row><cell>-</cell><cell></cell><cell>87.86</cell><cell>70.88</cell><cell>57.44</cell></row><row><cell></cell><cell></cell><cell>88.10</cell><cell>72.76</cell><cell>58.62</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>We thank Ting Liu and Tao Ruan for providing insights and expertise to improve this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The lovász-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Weakly and semi supervised human body part parsing via pose-guided knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04310</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance-level human parsing via part grouping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="770" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look into person: Self-supervised structuresensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="932" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Look into person: Joint body parsing &amp; pose estimation network and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="871" to="885" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic object parsing with local-global long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3185" to="3193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Macro-micro adversarial network for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mutual learning to adapt for joint human parsing and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="502" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Devil in the details: Towards accurate single and multiple human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4814" to="4821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Zoom better to see clearer: Human part segmentation with auto zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangting</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="648" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint multi-person pose estimation and semantic part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangting</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
