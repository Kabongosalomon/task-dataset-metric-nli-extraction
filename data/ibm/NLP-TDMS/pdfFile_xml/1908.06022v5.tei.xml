<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
							<email>chuxiangxiang@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
							<email>zhangbo1@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
							<email>lijixiang@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyuan</forename><surname>Li</surname></persName>
							<email>liqingyuan@xiaomi.com</email>
							<affiliation key="aff1">
								<orgName type="department">Xiaomi IoT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
							<email>xuruijun@xiaomi.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To discover powerful yet compact models is an important goal of neural architecture search. Previous two-stage one-shot approaches are limited by search space with a fixed depth. It seems handy to include an additional skip connection in the search space to make depths variable. However, it creates a large range of perturbation during supernet training and it has difficulty giving a confident ranking for subnetworks. In this paper, we discover that skip connections bring about significant feature inconsistency compared with other operations, which potentially degrades the supernet performance. Based on this observation, we tackle the problem by imposing an equivariant learnable stabilizer to homogenize such disparities (see <ref type="figure">Fig.1</ref>). Experiments show that our proposed stabilizer helps to improve the supernet's convergence as well as ranking performance. With an evolutionary search backend that incorporates the stabilized supernet as an evaluator, we derive a family of state-ofthe-art architectures, the SCARLET 3 series of several depths, especially SCARLET-A obtains 76.9% top-1 accuracy on ImageNet 4 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Incorporating scalability into neural architecture search is crucial to exploring efficient networks. The handcrafted way of scaling models up and down is to stack more or fewer cells <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37]</ref>. However, model scaling is nontrivial which involves tuning width, depth, and resolution altogether. To this end, a compound scaling method is proposed in <ref type="bibr" target="#b28">[29]</ref>, it starts with a searched mobile baseline EfficientNet-B0 and 'grid-search' the combination of these three factors to achieve larger models. In this paper, we are mainly concerned about finding models of varying depths, while the input resolution is kept fixed since it can be simply scaled manually.</p><p>To achieve such scalability, we first need to construct a search space of variable depths. To this end, skip connections are commonly used in differentiable approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref>, but they face a common issue of undesired skip connection aggregation as noted by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref>, which yields non-optimal results. Recent advances  <ref type="figure" target="#fig_4">Fig. 1</ref>. ELS helps to calibrate feature inconsistencies in scalable supernets with boosted cosine similarity (compared to <ref type="figure" target="#fig_3">Fig.4</ref>) and reduced angles among feature vectors (from blue shade to red). The cosine similarity is calculated on the third layer's output feature maps from 7 paralleled choice blocks (MobileNetV2's blocks numbered 0 to 5 and a skip connection). Each block's feature vectors are projected to 2-dimensional space (x1, x2) to draw their relative angles (shaded in color). Other layers also have similar results. Left Pair: Single Path One-Shot <ref type="bibr" target="#b11">[12]</ref>, Right Pair: FairNAS <ref type="bibr" target="#b5">[6]</ref>.</p><p>in one-shot approaches take a two-stage mechanism: single-path supernet optimization and searching <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>. A supernet is an embodiment of the search space, whose single path is a candidate model. Their single-path paradigm is more efficient and less error-prone, which also potentially avoids the aggregation problem but they carefully removed skip connections from search space. In this light, we integrate skip connections in their search space under the same single-path setting for a comprehensive investigation. We name the supernet in this new search space as a scalable supernet.</p><p>Our contributions can be summarized as follows,</p><p>First, we are the first to thoroughly investigate scalability in one-shot neural architecture search. We discover that a vanilla training of scalable supernets suffers from instability (see <ref type="figure">Fig. 3</ref>) and leads to weak evaluation performance. As FairNAS <ref type="bibr" target="#b5">[6]</ref> suggests that feature similarity is critical for single-path training, we find that this requirement is rigorously broken by skip connections <ref type="figure" target="#fig_3">(Fig. 4)</ref>.</p><p>Second, based on the above observation, we propose a simple learnable stabilizer to calibrate feature deviation (see <ref type="figure" target="#fig_4">Fig.1</ref>). It is proved effective to restore stability (see <ref type="figure">Fig. 3</ref>) while all submodels still have invariant representational power. Experiments on NAS-Bench-101 <ref type="bibr" target="#b32">[33]</ref> testify that it also substantially improves the ranking performance which is crucial for the second searching stage. Our pipeline is exemplified in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Last but not the least, we perform a single proxyless evolutionary search on ImageNet after training the scalable supernet. The total cost is 10 GPU days. Three new state-of-the-art models of different depths are generated. Specifically, SCARLET-A obtains 76.9% top-1 accuracy on ImageNet with 25M fewer FLOPS than EfficientNet-B0 (76.3%) <ref type="bibr" target="#b4">5</ref> . Moreover, we manually upscale the searched models with zero cost to have comparable FLOPS with EfficientNet variants and we also achieve competitive results.  Our proposed SCARLET-NAS pipeline where the scalable supernet is stabilized by ELS, which also provides good ranking ability compared with the vanilla approach. ELS is removed from the final subnetwork to train from scratch. Note SC and ELS can appear on each row (layer), only one is drawn for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single-Path Supernet Training</head><p>The weight-sharing mechanism is now widely applied in neural architecture search as it saves a tremendous amount of computation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b0">1]</ref>. It is usually embodied as a supernet that incorporates all subnetworks. The supernet is trained till convergence only once, from which all subnetworks can inherit weights for evaluation (so-called one-shot models) without extra fine-tuning. It is thus named the one-shot approach, as opposed to those who train each child network independently <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28]</ref>. Methods vary on how to train the supernet. In this paper, we concentrate on the single-path way <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>, which is more memoryfriendly and efficient.</p><p>Single Path One-Shot <ref type="bibr" target="#b11">[12]</ref> utilizes a supernet A with 20 layers, and there are 4 choice blocks per layer based on ShuffleNet <ref type="bibr" target="#b34">[35]</ref>. The total size of the search space reaches 4 20 . It uniformly samples a single-path model (say a with weights W a ) to train at each step, after which only this activated path in the supernet gets its weights W a updated. Formally, this process is to reduce the overall training loss L train of the supernet,</p><formula xml:id="formula_0">W A = argmin W E a∼Γ A [L train (A(a, W a ))]<label>(1)</label></formula><p>Notice that it differs from the nested manner in differential approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b9">10]</ref> where Γ is not fixed but used as a representation for variable architectural weights.</p><p>FairNAS <ref type="bibr" target="#b5">[6]</ref> rephrases each supernet training step as training m single-path models either sequentially or in parallel. These models are built on choice blocks uniformly sampled without replacement (denoted as a ∼ Ψ A ). During each step, all blocks in the supernet are trained once. The weights are aggregated and also updated once in a single step. It can be formulated as,</p><formula xml:id="formula_1">W A = argmin W E a∼Ψ A [ 1 m m i L train (A(a i , W ai ))]<label>(2)</label></formula><p>By ensuring the same amount of training for each block, FairNAS achieves a notable improvement in supernet performance. Interestingly enough, features learned by each block (of the same layer) in thus-trained supernet have high channel-wise similarities. This will be later proved a useful hint to restore training stability when skip connections are involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Ranking</head><p>Searching is essentially based on ranking. Incomplete training can give a rough guess <ref type="bibr" target="#b36">[37]</ref> but it is too costly. Differentiable methods <ref type="bibr" target="#b21">[22]</ref> consider the magnitude of architectural coefficients as each operation's importance. However, there is a large discrepancy when discretizing such continuous encodings. As we are focusing on the two-stage weight-sharing neural architecture search method, we rely on the supernet to evaluate models. It is thus of uttermost importance for it to have a good model ranking ability. FairNAS <ref type="bibr" target="#b5">[6]</ref> has shown that strict fairness during supernet training has a strong impact on it. In particular, they adopted Kendall Tau <ref type="bibr" target="#b16">[17]</ref> to measure the correlation between the performance of oneshot models (predicted by the supernet) and stand-alone models (trained from scratch). Tau value ranges from -1 to 1, meaning the order is completely inverted or identical. Ideally, we would like a tau of 1, which gives the exact ground truth ranking of submodels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Instability of Scalable Supernet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Degraded Supernet Performance</head><p>The skip connection plays a role in changing depths for architectures in Mo-bileNetV2's block-level search space <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref>. We detail it as S 1 and its variant S 2 in C.1. To investigate scalability in one-shot approaches, we train the supernet in the previously discussed single-path fashion (Section 2.1) in search space S 1 . Surprisingly, we find them suffering from severe training instability, which is illustrated in <ref type="figure">Fig. 3</ref>. Unlike the reported stable training process for the supernets without skip connections <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>, we instead observe much higher variances (shadowed in blue at the top of <ref type="figure">Fig. 3</ref>) and lower training accuracies (solid line in blue).</p><p>Training instability also deteriorates one-shot model performance. We sample 1024 models to measure their accuracies on the ImageNet validation dataset. Training supernet with Single Path One-Shot <ref type="bibr" target="#b11">[12]</ref> and FairNAS <ref type="bibr" target="#b5">[6]</ref> on Ima-geNet with and without Equivariant Learnable Stabilizer (ELS) in search space S1. Top: The supernets with ELS enjoy better convergence (red thick lines) and small variance (red shaded area). Bottom: Histogram of randomly sampled 1k one-shot models' accuracies. Supernets with ELS have an improved estimation of subnetworks.</p><p>In particular, we need to neither overestimate nor underestimate the sampled submodels. This is hard for the scalable supernet trained so far. We can easily draw an example in <ref type="table">Table 1</ref>, where model A is underestimated with only 1% accuracy and B overestimated (49%, much better than A). The ground truth is just the opposite, A has 74% which is better than B with 73.3%. We later show how we design an ELS for the supernet training to rectify this mistake. <ref type="table">Table 1</ref>. ImageNet performance of model A and B (denoted by the choice block IDs) in S1. Both are mistakenly estimated by the supernet trained w/o ELS. Instead, enabling ELS gives the right ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Top-1 (%) Top-1 (%) Top-1 (%) (in S1) (est. w/o ELS) (est. w/ ELS) (standalone) A(0,5,0,6,3,2,3,0,2,1,3,5,2,4,4,4,5,3,6) 1.0 53.1 74.0 B(5,0,1,0,2,6,6,4,3,1,5,1,0,2,4,4,1,1,2) 49.5 49.6 73.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Skip Connections Break Feature Similarity</head><p>A well-trained supernet matters for one-shot models' ranking. We are thus driven to unveil what causes such a phenomenon to find a cure for stabilizing the training process. Inspired by the analysis of the underlying working mechanism in the singlepath training <ref type="bibr" target="#b5">[6]</ref>, we pick the outputs of the third layer (for an example) in the formerly trained supernets to calculate their cosine similarities across different choice blocks, which are depicted as 7 × 7 similarity matrices in <ref type="figure" target="#fig_3">Fig. 4</ref>. The first six inverted bottlenecks of different configurations yield quite similar highdimensional features (with a shape of 32 × 28 × 28) and their cosine similarities are high (all above 0.85). Meanwhile, the feature maps from the skip connection (the last choice block) are quite distinct from other blocks and the average cosine similarity is below 0.6. This disparity is observed in both training methods. . Cosine similarity matrices of the third layer's outputs (averaged on 32 channels of 28 × 28 feature maps) from 7 choice blocks of supernets trained without ELS. The average similarity is shown as x-axis at the top. The skip connection yields different feature maps from others. Left: Single Path One-Shot <ref type="bibr" target="#b11">[12]</ref>, Right: FairNAS <ref type="bibr" target="#b5">[6]</ref> Feature disparity troubles the training for the next layer and consequently the whole supernet. As the fourth layer randomly selects one output from the third layer, the unique skip connection disrupts feature similarities. This discrepancy of inflowing features (occurs in other layers too) will get magnified layer by layer and finally deteriorate supernet training. This is shown on the top of <ref type="figure">Fig. 3</ref>. What's worse, it makes big trouble for the supernet to predict submodels' performance. Such a supernet becomes nearly useless because it severely underestimates or overestimates candidate architectures, shown at the bottom of <ref type="figure">Fig. 3</ref>. Therefore, we attribute the instability to low similarities of features across different paralleled choices, mainly from skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Scalable Neural Architecture Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Improve Supernet Training with a Learnable Stabilizer</head><p>Based on the previous discussion, one direct approach to stabilize the training process is to boost the cross-block similarities by replacing the parameter-free skip connection with a learnable stabilizer. Ideally, the stabilizer will deliver similar features as other choice blocks. What's more important, the stabilizer must be equivariant in terms of representational capacity since we want to remove it eventually (see the third step in <ref type="figure" target="#fig_1">Figure 2</ref>). This is detailed as Definition 1. For a search space S like S 1 with n choices per layer, we denote x c l l as the input with c l channels to layer l, and f o l the o-th operation function in that layer. Without loss of generality, we put the skip connection as the last choice, while other choices all start with a convolution operation. The equivalence requirement for an equivariant learnable stabilizer function f ELS l can then be formulated as,</p><formula xml:id="formula_2">f o l+1 (x c l l ) = f o l+1 (f ELS l (x c l l )), ∀o ∈ {0, 1, 2, ..., n − 1}.<label>(3)</label></formula><p>As for S, we can utilize the property of matrix multiplication to find a simple ELS function: a 1 × 1 convolution without batch normalization or activation. This is given as Lemma 1 and proven in the A.</p><formula xml:id="formula_3">Lemma 1. Let f ELS l = Conv (c l ,c l+1 ,1,1) , then Equation 3 holds.</formula><p>By adopting the learnable 1 × 1 convolution as an ELS, we observe improved stability in supernet training and better evaluation of subnetworks ( <ref type="figure">Fig. 3</ref>). We still maintain scalability since we can remove ELS based on Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Architecture Search with the Scalable Supernet</head><p>Being a two-stage one-shot method like <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6]</ref>, we have so far focused on supernet training. For the second searching stage, evolutionary algorithms are mostly used. For instance, FairNAS <ref type="bibr" target="#b5">[6]</ref> utilizes the well-known NSGA-II algorithm <ref type="bibr" target="#b7">[8]</ref> where they examine three objectives: classification accuracies, multiply-adds and the number of parameters. In practice, they are of different importance. We are more concerned about accuracies (performance) and multiply-adds (speed) than the number of parameters (memory cost), which calls for a weighted solution like <ref type="bibr" target="#b4">[5]</ref>. It is however nontrivial for our scalable search space. First of all, models with too many skip connections are easily sorted as frontiers because of low multiply-adds. Although such a model dominates others but it usually comes with a low accuracy which is not desired. So we set a minimum accuracy requirement acc min . Second, we are searching models for mobile deployment, where we should encourage increasing the number of parameters to prevent underfitting rather than overfitting <ref type="bibr" target="#b34">[35]</ref>. Last, for practical reasons, we also need to set maximum multiply-adds madds max . Formally, we describe our searching process as a constrained multi-objective optimization as follows, </p><p>Specifically, we adopt a similar evolutionary searching algorithm based on NSGA-II <ref type="bibr" target="#b7">[8]</ref> as in FairNAS <ref type="bibr" target="#b5">[6]</ref> with some modifications. For handling weights of different objectives, we make use of weighted crowding distance <ref type="bibr" target="#b10">[11]</ref> for nondominated sorting. We set w acc = 0.4, w madds = 0.4, w params = 0.2. The constraints are set to madds max = 500M and acc min = 0.4. Notice that we treat these two constraints in sequential order to reduce cost. As calculating multiplyadds is much faster than accuracies, models violating madds max are immediately removed for further evaluation. The whole search pipeline is presented in Algorithm 1 and <ref type="figure" target="#fig_4">Fig. 10 (both in B)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset, Training, and Searching</head><p>Dataset. For training and searching, we use the ILSVRC2012 dataset <ref type="bibr" target="#b8">[9]</ref>. To be consistent with previous work <ref type="bibr" target="#b27">[28]</ref>, the validation set consists of 50k images selected from the training set. The original validation set serves as the test set.</p><p>Supernet Training. For FairNAS experiments in S 1 , we follow <ref type="bibr" target="#b5">[6]</ref> except that we train the supernet for 60 epochs. It costs nearly 8 GPU days. For SPOS, we train it for 360 epochs to have the same amount of weight updates per block. As for S 2 with more choices, we use the same setting except for a smaller batch size of 256, which results in higher top-1 accuracy on average.  Evolutionary Searching. We search proxylessly in S 2 on ImageNet. The evolution covered 8400 models (a population of 70 models evolved for 120 generations). It costs 2 GPU days on a Tesla V100. The final architectures SCARLET-A, B and C (shown in <ref type="figure" target="#fig_6">Fig. 5</ref>) are sampled from the Pareto front at equal distance and are trained from scratch. Due to the equivalence requirement, we remove ELS to achieve two competitive models with shorter depths, SCARLET-B and C.</p><p>Single Model Training. To train the selected single model, we follow Mnas-Net <ref type="bibr" target="#b27">[28]</ref> with vanilla Inception preprocessing <ref type="bibr" target="#b26">[27]</ref>. We train EfficientNet and SCARLET models without AutoAugment <ref type="bibr" target="#b6">[7]</ref> to have a fair comparison with state-of-the-art architectures. The batch size is 4096. The initial learning rate is 0.256 and it decays at an amount of 0.01 every 2.4 epochs. The dropout with a rate 0.2 <ref type="bibr" target="#b24">[25]</ref> is put before the last FC layer. The weight decay rate (l 2 ) is 1.0 × 10 −5 . The RMSProp optimizer has a momentum of 0.9.  <ref type="bibr" target="#b29">[30]</ref>, which has 76.6% accuracy when we trained it with the same tricks. We further give a closer examination of the SCARLET series in C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ImageNet Classification</head><p>Comparison of Models at a Larger Scale Higher accuracy requirements beyond mobile settings are also considered. To be comparable with EfficientNet's scaled variants, we simply manually upscale our SCARLET baseline models to have the same resolution and FLOPS without any extra tuning cost. We compare the results with other state-of-the-art methods in <ref type="table" target="#tab_3">Table 3</ref>. At the level of 1 billion FLOPS, while EfficientNet-B2 is based on grid search at a very high cost on GPUs <ref type="bibr" target="#b28">[29]</ref>, our SCARLET-A2 (79.5%) is from upscaling with zero cost. No AutoAugment tricks are applied for a fair comparison. Moreover, Xception <ref type="bibr" target="#b3">[4]</ref> uses 8 times more FLOPS to reach 79.0%. Notably, our SCARLET-A4 achieves new state-of-the-art top-1 accuracy 82.3% again without extra costs using only 4.2 billion FLOPS. By contrast, SENet <ref type="bibr" target="#b14">[15]</ref> uses 10× FLOPS. <ref type="table" target="#tab_4">Table 4</ref> shows our transfer results on CIFAR-10 dataset <ref type="bibr" target="#b18">[19]</ref>. We utilize similar training settings from <ref type="bibr" target="#b17">[18]</ref>. In particular, each model is loaded with ImageNet pre-trained weights and finetuned for 200 epochs with a batch size of 128. The initial learning rate is set to 0.025 with a cosine decay strategy. We also adopted AutoAugment policy for CIFAR-10 <ref type="bibr" target="#b6">[7]</ref>. The dropout rate is 0.3. To achieve comparable top-1 accuracy as NASNet-A Large, our SCARLET-A only uses 33× fewer FLOPS. SCARLET-B doesn't utilize the mixed convolution but it is still comparable to MixNet <ref type="bibr" target="#b29">[30]</ref>. In particular, our smallest model SCARLET-C is close to MixNet-M, saving 22% FLOPS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Transferability to CIFAR-10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Object Detection</head><p>To verify the transferability of our models on the object detection task, we utilize drop-in replacements of backbones of the RetinaNet framework (Res101+FPN) <ref type="bibr" target="#b19">[20]</ref>. To make fair comparisons, we focus on the mobile settings of the backbone. All methods are trained for 12 epochs on COCO dataset <ref type="bibr" target="#b20">[21]</ref> with a batch size of 16 (train2017 for training and val2017 for reporting results). The initial learning rate is 0.01 and decayed by 0.1 on epoch 8 and 11. Compared with recent NAS models in <ref type="table" target="#tab_5">Table 5</ref>, we utilize fewer FLOPS to have better results, suggesting a better transferability. 6 Ablation Study and Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Training Stability</head><p>Compared with skip connection, ELS can help stabilize the training process of a scalable supernet, shown in <ref type="figure">Fig. 3</ref>. We believe it is due to boosted crossblock features similarities (increased by 0.3 compared with pure skip connection). Interestingly enough, ELS is also able to close up the feature angle discrepancy. This phenomenon is depicted in <ref type="figure" target="#fig_4">Fig. 1</ref>. Informally, ELS plays an important role in rectifying the features' phase gap between skip connections and other homogeneous choices. Essentially, ELS is a near-homogeneous to an inverted bottleneck block, while a skip connection is instead heterogeneous. As a result, for both one-shot approaches, supernets with ELS enjoy higher training accuracies (red solid line) and lower variances (red shaded area). Although there is a small proportion of one-shot models with low accuracy, they can be easily excluded from the proposed constrained optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ranking Ability with and without ELS</head><p>The most important role of the supernet in the two-stage approach is to evaluate the performance of the subnetworks, so-called 'ranking ability'. To find out the contribution of ELS, we perform experiments on a common NAS benchmark NAS-Bench-101 <ref type="bibr" target="#b32">[33]</ref> with some adaptations. Specifically, we construct a supernet to have a stack of 9 cells, each cell has at most 5 sequential internal nodes, each node has 3 optional operations: 1 × 1 Conv, 3 × 3 Conv and a skip connection. The first node is preceded by a 1x1 Conv projection. The designed cell and node choices are shown in <ref type="figure" target="#fig_7">Fig. 6</ref>.  <ref type="figure">Fig. 7</ref>. Comparison of ground-truth vs. estimated accuracy correlation between the supernets trained with and without ELS, based on 100 sampled models from NAS-Bench-101 <ref type="bibr" target="#b32">[33]</ref>. ELS substantially boosts the ranking ability of the supernet.</p><p>We train such a supernet with and without ELS on CIFAR-10. For both experiments, we train for 100 epochs with a batch size of 96, and a learning rate of 0.025. We randomly sample 100 models to lookup their ground-truth accuracies from NAS-Bench-101. We calculate the ranking ability of each supernet using Kendall Tau <ref type="bibr" target="#b16">[17]</ref>, shown in <ref type="figure">Fig 7.</ref> The one with ELS reaches a tau value of 0.421, indicating much higher correlation.</p><p>ELS vs. Skip Connection. To give a clearer comparison between the skip connection (SC) and the proposed ELS, we illustrate their functionality in <ref type="table" target="#tab_6">Table 6</ref>. Both operations are foldable, meaning they are used in supernet training, but later removed (folded) to build the corresponding subnetworks as they both creates an identity transformation before folding and after (see also <ref type="figure" target="#fig_1">Fig. 2)</ref>. The difference is that, ELS is learnable so that it gives more consistent feature maps in each layer. This is crucial to improve the supernet ranking, attested by  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Equivariant vs. Non-equivariant Stabilizer</head><p>The equivalence requirement for the stabilizer <ref type="figure">(Equation 3</ref>) plays a pivotal role in our approach. We evaluate a subnetwork with ELS as it is an identical proxy to the one without it. A stabilizer that violates the equivalence requirement will give wrong evaluation. For example, we make a simple modification by adding a ReLU function to ELS, this makes the stabilizer non-equivariant because of non-linearity. Can we use a supernet with this stabilizer to correctly evaluate a model? Given a model denoted by choice indices: M (1,3,1,0,12,0,0,0,12,12,12,12,12,0,0,0, <ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9)</ref> , when we train it respectively with ELS and with ELS-ReLU on the same settings, we find them have different representational power, as shown in <ref type="figure">Fig. 8</ref>. The one with ELS-ReLU overestimates the model compared to the one with ELS-no-ReLU, which reflects its truth by Lemma 1. Training Loss ELS-ReLU ELS <ref type="figure">Fig. 8</ref>. Adding ReLU to ELS gives a wrong evaluation (overestimation) of a subnetwork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Constrained Optimization</head><p>For multi-objective evolutionary search in scalable search space, to limit the minimum accuracy is more than necessary. In a standard NSGA-II <ref type="bibr" target="#b7">[8]</ref> process, models with many skip connections are easily picked for the high-ranking nondominated sets. For an extreme example, a model consisting of skip connections for all 19 layers has minimum multiply-adds, it will always stay as a boundary node. This brings in gene contamination for the evolution process as this poorperforming gene never dies out.</p><p>To demonstrate the necessity of a minimum accuracy constraint, we compare the case with acc min = 0 and acc min = 0.4 in <ref type="figure" target="#fig_10">Fig. 9</ref>. We can observe the number of skip connections has been greatly reduced (red line in the right <ref type="figure">figure)</ref>. As a consequence, the evolution converges to a better Pareto Front (red line in the left <ref type="figure">figure)</ref>: higher validation accuracies at the same level of multiply-adds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Component Analysis</head><p>A supernet trained without ELS can't deliver good search results. Using the same searching strategy NSGA-II, its best model found below 380M FLOPS obtains 71.3% top-1 accuracy on ImageNet, while SCARLET-A (365M) has 76.9%. Therefore, the contribution to the final performance comes mainly from ELS in the first stage. The searching strategy heavily depends on ranking ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we expose a critical failure in single-path one-shot neural architecture search when scalability is considered. We discover the underlying feature dissimilarity hinders supernet training. The proposed equivariant learnable stabilizer is effective to rectify such discrepancy while maintaining the same representational power for subnetworks. We also employ a weighted multi-objective evolutionary search to find a series of state-of-the-art SCARLET architectures. Good transferability is achieved on various vision tasks. Compared with unnecessarily costly EfficientNet, our method is a step forward towards more efficient and flexible neural architecture search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof</head><p>Proof. First, we prove that Equation 3 (main text) holds for ∀o ∈ {0, 1, ..., n−2}. In this case, it's sufficient to prove the output of the first convolution Conv (c l ,m,k,k) can be exactly matched by adding Conv (c l ,c l+1 ,1,1) before Conv (c l+1 ,m,k,k) . Let W 1 c l ,c l+1 ,1,1 and W 2 c l ,m,k,k be the weight tensors of Conv (c l ,c l+1 ,1,1) and Conv (c l+1 ,m,k,1) respectively. Let W 3 c l ,m,k,k be the weight tensors of Conv (c l ,m,k,1) . Let w be one element of the tensor. We have</p><formula xml:id="formula_5">y = Conv (c l ,c l+1 ,1,1) (x c l l ), z = Conv (c l+1 ,m,k,1) (y)<label>(5)</label></formula><formula xml:id="formula_6">y(i, j, c) = c l p=1 w 1 p,c,1,1 x(i, j, p)<label>(6)</label></formula><p>Also,</p><formula xml:id="formula_7">z(i, j, c) = k q=1 c l+1 p=1 w 2 p,c,q,q y(i + q, j + q, p) = k q=1 c l+1 p=1 w 2 p,c,q,q ( c l u=1 w 1 u,p,1,1 x(i + q, j + q, u)) = k q=1 c l+1 p=1 c l u=1 w 2 p,c,q,q w 1 u,p,1,1 x(i + q, j + q, u) = k q=1 c l u=1 w 3 u,c,q,q x(i + q, j + q, u)<label>(7)</label></formula><p>Therefore, the first part is proved by setting</p><formula xml:id="formula_8">w 3 u,c,q,q = c l+1 p=1 w 2 p,c,q,q w 1 u,p,1,1 .<label>(8)</label></formula><p>For o = n − 1, we replace a skip connection with an ELS. We can iteratively apply the first part of the proof till the end of searchable layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Algorithm</head><p>Our constrained and weighted NAS pipeline is listed in Algorithm 1 and <ref type="figure" target="#fig_4">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Search Space</head><p>For later experiments, we add skip connections to commonly used search space to construct S 1 and S 2 . They are described as follows,  <ref type="bibr" target="#b23">[24]</ref> is adopted as its backbone. In particular, S 1 is represented as a block-level supernet with L = 19 layers of N = 7 choices each. Its total size is 7 <ref type="bibr" target="#b18">19</ref> . The choices are, -MobileNetV2's inverted bottleneck blocks <ref type="bibr" target="#b23">[24]</ref> of two expansion rates (x) in <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6)</ref>, three kernel sizes (y) in <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7)</ref>, labelled as MBExKy 6 , -skip connection (the 6th choice 7 ).</p><p>Search Space S 2 . On top of S 1 , we give each inverted bottleneck a squeezeand-excitation <ref type="bibr" target="#b14">[15]</ref> option (e.g., ExKy, ExKy SE), similar to MnasNet <ref type="bibr" target="#b27">[28]</ref>. Its total size thus becomes 13 <ref type="bibr" target="#b18">19</ref> .</p><p>We have to notice that skip connections are commonly used <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b0">1]</ref>, but meticulously neglected in recent single-path one-shot methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>.  <ref type="figure" target="#fig_4">Fig. 10</ref>. Constrained and weighted NSGA-II Pipeline. It starts with a uniform initialization (top left) with some constraints (red) to generate the initial population. The trained scalable supernet serves as a fast evaluator to decide the relative performance of each model so that they can be grouped into several Fronts (F1, F2, . . .) by weighted non-dominated sorting (right). Only the top n of them make up the next generation Pi+1, based on which Qi+1 is produced with tournament selection, crossover and mutation (blue) under the same constraints (bottom left). The whole evolution loops until we reach Pareto-optimality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 NSGA-II Hyperparameters</head><p>The hyperparameters for the weighted NSGA-II approach are given in <ref type="table" target="#tab_8">Table 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 More Details about Scalable Supernet with ELS</head><p>Given an input of a chickadee 8 image from ImageNet, we illustrate both highlevel and low-level feature maps of the trained supernet with our proposed improvements in <ref type="figure" target="#fig_4">Figure 11</ref>. Pure skip connection easily interferes with the training process as it causes incongruence with other choice blocks. Note the channel size of feature map after Choice 6 in <ref type="figure" target="#fig_4">Figure 11</ref> (a) is half of others because the previous channel size is <ref type="bibr" target="#b15">16</ref>, while other choice blocks output 32 channels. This effect is largely attenuated by ELS. As it goes deeper, we still observe consistent high-level features. Specifically, when ELS is not enforced, high-level features of deeper channels easily get blurred out, while the supernet with ELS enabled continues to learn useful features in deeper channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Search Space Evaluation</head><p>NAS results can benefit from good search space. To prove the validity of the proposed method, we show our search space has a wide range and is not particularly designed. We pick two extreme cases, one with all identity blocks (only the stem and the tail remains), the other with all K7E6s. They have the minimum and the maximum FLOPS respectively. We list their evaluation result in <ref type="table" target="#tab_9">Table 8</ref>. The former has 24.1% top-1 accuracy on ImageNet, and the latter 76.8% at a cost of 557M FLOPs. Both are infeasible solutions as they violate either acc min or madds max . It's thus a challenging task to deal with such search space for ordinary search techniques. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Analysis of SCARLET Models</head><p>SCARLET-A makes full use of large kernels (five 5 × 5 and seven 7×7 kernels) to enlarge receptive field. Besides it activates many squeezing and excitation (12 out of 19) blocks to improve its classification performance. At the early stage, it appreciates either large kernels and small expansion ratios or small kernels and large expansion ratios to balance the trade-off between accuracy and FLOPs. SCARLET-B chooses two identity operations. Compared with A, it shortens network depth at the last stages. Besides, it utilizes squeezing and excitation block extensively (14 out of 17). It places a large expansion block with large kernels at the tail stage.</p><p>SCARLET-C uses three identity operations and utilizes small expansion ratio extensively to cut down the FLOPs, large expansion ratio at the tail stage whose resolution is 7 × 7. It prefers large kernels before the downsampling layers. Besides, it makes an extensive use of squeeze and excitation to boost accuracy.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3</head><label></label><figDesc>SCAlable supeRnet with Learnable Equivariant sTablizer 4 Models: https://github.com/xiaomi-automl/SCARLET-NAS arXiv:1908.06022v5 [cs.LG] 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our proposed SCARLET-NAS pipeline where the scalable supernet is stabilized by ELS, which also provides good ranking ability compared with the vanilla approach. ELS is removed from the final subnetwork to train from scratch. Note SC and ELS can appear on each row (layer), only one is drawn for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 3demonstrates that the majority of one-shot models from both SPOS (bottom left in blue) and FairNAS (bottom right in blue) are underestimated, which are mainly close to 0. This phenomenon hasn't been observed in reduced S 1 (without skip connections) by previous work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4. Cosine similarity matrices of the third layer's outputs (averaged on 32 channels of 28 × 28 feature maps) from 7 choice blocks of supernets trained without ELS. The average similarity is shown as x-axis at the top. The skip connection yields different feature maps from others. Left: Single Path One-Shot [12], Right: FairNAS [6]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Definition 1 .</head><label>1</label><figDesc>Equivariant Learnable Stabilizer. A plug-in learnable stabilizer is called an Equivariant Learnable Stabilizer (ELS) iff a model with such a stabilizer is exactly equivalent to the one without it in terms of representational capacity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>max {acc(m), −madds(m), params(m)}, m ∈ search space S s.t. w acc + w madds + w params = 1, ∀w &gt;= 0 acc(m) &gt; acc min , madds(m) &lt; madds max .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>The architectures of SCARLET-A, B and C (from top to bottom). Downsampling points are indicated by dashed lines. The stem and tail parts are omitted for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Ranking analysis is based on a subspace of NAS-Bench-101. (a) A cell is a stack of 5 nodes. An additional 1 × 1 conv projection is added before the first one to avoid channel mismatch. (b) For each node, we can select one operation from 3 choices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Ablation study on constrained optimization. Left: Pareto front of MultAdds vs. Accuracy. Right: The ratios of skip connections per epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>High-level choice blocks' feature maps without ELS High-level choice blocks' feature maps with ELS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Learned low-level and high-level features for the supernet with and without ELS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-art architectures on ImageNet classification task. ‡ : model trained from scratch by us without AutoAugment.</figDesc><table><row><cell>Models</cell><cell cols="2">×+ Params</cell><cell cols="2">Top-1 Top-5</cell></row><row><cell></cell><cell>(M)</cell><cell>(M)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>MobileNetV2 [24]</cell><cell>300</cell><cell>3.4</cell><cell>72.0</cell><cell>91.0</cell></row><row><cell>MobileNetV3 [14]</cell><cell>219</cell><cell>5.4</cell><cell>75.2</cell><cell>92.2</cell></row><row><cell>MnasNet-A1 [28]</cell><cell>312</cell><cell>3.9</cell><cell>75.2</cell><cell>92.5</cell></row><row><cell>MnasNet-A2 [28]</cell><cell>340</cell><cell>4.8</cell><cell>75.6</cell><cell>92.7</cell></row><row><cell>FBNet-B [31]</cell><cell>295</cell><cell>4.5</cell><cell>74.1</cell><cell>-</cell></row><row><cell>Proxyless-R [2]</cell><cell>320</cell><cell>4.0</cell><cell>74.6</cell><cell>92.2</cell></row><row><cell>Proxyless GPU [2]</cell><cell>465</cell><cell>7.1</cell><cell>75.1</cell><cell>-</cell></row><row><cell>Single-Path [26]</cell><cell>365</cell><cell>4.3</cell><cell>75.0</cell><cell>92.2</cell></row><row><cell cols="2">Single Path One-Shot [12] 328</cell><cell>3.4</cell><cell>74.9</cell><cell>92.0</cell></row><row><cell>FairNAS-A [6]</cell><cell>388</cell><cell>4.6</cell><cell>75.3</cell><cell>92.4</cell></row><row><cell>MixNet-M [30]</cell><cell>360</cell><cell cols="2">5.0 76.6  † (77)</cell><cell>93.2</cell></row><row><cell>EfficientNet B0 [29]</cell><cell>390</cell><cell>5.3</cell><cell>76.3</cell><cell>93.2</cell></row><row><cell>SCARLET-A (Ours)</cell><cell>365</cell><cell>6.7</cell><cell cols="2">76.9 93.4</cell></row><row><cell>SCARLET-B (Ours)</cell><cell>329</cell><cell>6.5</cell><cell>76.3</cell><cell>93.0</cell></row><row><cell>SCARLET-C (Ours)</cell><cell>280</cell><cell>6.0</cell><cell>75.6</cell><cell>92.6</cell></row><row><cell cols="5">Comparison of State-of-the-art Mobile Architectures We give full train</cell></row><row><cell cols="5">results of the SCARLET series on ImageNet dataset in Table 2. Although in ab-</cell></row><row><cell cols="5">sence of AutoAugment tricks [7], SCARLET-A still clearly surpasses EfficientNet-</cell></row><row><cell cols="5">B0 (+0.6% higher accuracy) using fewer FLOPS. The shallower model SCARLET-</cell></row><row><cell cols="5">B achieves 76.3% top-1 accuracy with 329M FLOPS, which exceeds several</cell></row><row><cell cols="5">models of a similar size by a clear margin: MnasNet-A1 (+1.1%), Proxyless-</cell></row><row><cell cols="5">R (+1.7%). Notably, to be comparable to our shallowest model SCARLET-C</cell></row><row><cell cols="5">(75.6%), MnasNet-A1 comes with 21% more FLOPS at the cost of 200× GPU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Single-crop results of scaled architectures on ImageNet validation set.</figDesc><table><row><cell>:</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Transferring SCARLET models to CIFAR-10. † : Reported by<ref type="bibr" target="#b17">[18]</ref>.</figDesc><table><row><cell>Models</cell><cell cols="2">Input Size ×+ (M) Top-1 (%)</cell></row><row><cell cols="2">NASNet-A Large [37]  † 331×331</cell><cell>12030 98.00</cell></row><row><cell>MixNet-M [30]</cell><cell>224×224</cell><cell>352 97.92</cell></row><row><cell>SCARLET-A</cell><cell>224×224</cell><cell>364 98.05</cell></row><row><cell>SCARLET-B</cell><cell>224×224</cell><cell>328 97.93</cell></row><row><cell>SCARLET-C</cell><cell>224×224</cell><cell>279 97.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Object detection result of various drop-in backbones on the COCO dataset.</figDesc><table><row><cell>Backbones</cell><cell>×+ Acc AP</cell><cell cols="3">AP50 AP75 APS APM APL</cell></row><row><cell></cell><cell cols="2">(M) (%) (%) (%)</cell><cell>(%)</cell><cell>(%) (%)</cell><cell>(%)</cell></row><row><cell>MnasNet-A2 [28]</cell><cell cols="4">340 75.6 30.5 50.2 32.0 16.6 34.1 41.1</cell></row><row><cell>MobileNetV3 [14]</cell><cell cols="4">219 75.2 29.9 49.3 30.8 14.9 33.3 41.1</cell></row><row><cell cols="5">SingPath NAS [26] 365 75.0 30.7 49.8 32.2 15.4 33.9 41.6</cell></row><row><cell>SCARLET-A</cell><cell cols="4">365 76.9 31.4 51.2 33.0 16.3 35.1 41.8</cell></row><row><cell>SCARLET-B</cell><cell cols="4">329 76.3 31.2 51.2 32.6 17.0 34.7 41.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Comparison of Skip Connections (SC) and ELS as per foldability and ranking.</figDesc><table><row><cell cols="3">Operation Foldable Identity Mapping Learnable Feature Similarity Ranking</cell></row><row><cell>SC</cell><cell>Low</cell><cell>Poor</cell></row><row><cell>ELS</cell><cell>High</cell><cell>Good</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Algorithm 1 The constrained and weighted NAS pipeline. Supernet S, the number of generations N , population size n, validation dataset D, constraints C, objective weights w Output: A set of K individuals on the Pareto front. Train supernet S defined on the scalable search space. Uniformly generate the populations P0 and Q0 until each has n individuals satisfying CFLOPS, CAccuracy. Evaluate model qi+1 with S on D {Check the accuracy constraint (It takes Select K equispaced models near Pareto-front from PN Search Space S 1 . It is similar to ProxylessNAS [2], where MobileNetV2</figDesc><table><row><cell>for i = 0 to N − 1 do</cell></row><row><cell>Ri = Pi ∪ Qi</cell></row><row><cell>F = non-dominated-sorting(Ri)</cell></row><row><cell>Pick n individuals to form Pi+1 by ranks and the crowding distance weighted by</cell></row><row><cell>w.</cell></row><row><cell>Qi+1 = ∅</cell></row><row><cell>while size(Qi+1) &lt; n do</cell></row><row><cell>M = tournament-selection(Pi+1)</cell></row><row><cell>qi+1 = crossover(M )∪hierarchical-mutation(M ) {Check the FLOPS constraint</cell></row><row><cell>at first (It takes &lt; 1ms).}</cell></row><row><cell>if F LOP S(qi+1) &gt; F LOP Smax then</cell></row><row><cell>continue</cell></row><row><cell>end if</cell></row><row><cell>≈ 60s).}</cell></row><row><cell>if Accuracy(qi+1) &gt; Accmin then</cell></row><row><cell>Add qi+1 to Qi+1</cell></row><row><cell>end if</cell></row><row><cell>end while</cell></row><row><cell>end for</cell></row></table><note>Input:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Hyperparameters for the weighted NSGA-II approach.</figDesc><table><row><cell>Item</cell><cell cols="2">value Item</cell><cell>value</cell></row><row><cell>Population N</cell><cell>70</cell><cell cols="2">Mutation Ratio 0.8</cell></row><row><cell>prm</cell><cell cols="2">0.2 pre</cell><cell>0.65</cell></row><row><cell>ppr</cell><cell cols="2">0.15 pM</cell><cell>0.7</cell></row><row><cell>pK−M</cell><cell>0.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Full train results of models with minimal and maximal FLOPS.</figDesc><table><row><cell>Models</cell><cell cols="5">FLOPS (M) &gt; maddsmax Top-1 (%) Top-5 (%) &lt; accmin</cell></row><row><cell>All Identity</cell><cell>23</cell><cell>No</cell><cell>24.1</cell><cell>45.0</cell><cell>Yes</cell></row><row><cell>All K7E6</cell><cell>557</cell><cell>Yes</cell><cell>76.8</cell><cell>93.3</cell><cell>No</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Searching EfficientNet-B0 is similar to MnasNet<ref type="bibr" target="#b27">[28]</ref> which takes 2304 TPU days.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The order of numbering o = (x − 3) + (y − 3)/2. 7 zero-based numbering</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">ImageNet ID: n01592084 7680</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Understanding and Simplifying One-Shot Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<title level="m">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<title level="m">Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation</title>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">MoGA: Searching Beyond MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICASSP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">AutoAugment: Learning Augmentation Policies from Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Searching for a Robust Neural Architecture in Four GPU Hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Weighted Preferences in Evolutionary Multi-Objective Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Single Path One-Shot Neural Architecture Search with Uniform Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00420</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<title level="m">Searching for MobileNetV3</title>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A New Measure of Rank Correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Do Better Imagenet Models Transfer Better? In: CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2661" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common Objects in Context. In: ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">DARTS: Differentiable Architecture Search</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Overfitting. JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ECMLPKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mnasnet: Platform-Aware Neural Architecture Search for Mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">MixConv: Mixed Depthwise Convolutional Kernels. BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<title level="m">FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Understanding and Robustifying Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PolyNet: A Pursuit of Structural Diversity in Very Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="718" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Transferable Architectures for Scalable Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
