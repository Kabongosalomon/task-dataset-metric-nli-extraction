<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DELTAS: Depth Estimation by Learning Triangulation And densification of Sparse points</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-25">25 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Sinha</surname></persName>
							<email>asinha@magicleap.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Magic Leap Inc</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
							<email>zmurez@magicleap.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Magic Leap Inc</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bartolozzi</surname></persName>
							<email>bartolozzij@gmail.com2wayve.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
							<email>andrew@insideiq.team</email>
							<affiliation key="aff1">
								<orgName type="department">InsideIQ Inc</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DELTAS: Depth Estimation by Learning Triangulation And densification of Sparse points</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-25">25 Aug 2020</date>
						</imprint>
					</monogr>
					<note>*Work done at Magic Leap</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> Multi-view stereo (MVS)  <p>is the golden mean between the accuracy of active depth sensing and the practicality of monocular depth estimation. Cost volume based approaches employing 3D convolutional neural networks (CNNs) have considerably improved the accuracy of MVS systems. However, this accuracy comes at a high computational cost which impedes practical adoption. Distinct from cost volume approaches, we propose an efficient depth estimation approach by first (a) detecting and evaluating descriptors for interest points, then (b) learning to match and triangulate a small set of interest points, and finally (c) densifying this sparse set of 3D points using CNNs. An endto-end network efficiently performs all three steps within a deep learning framework and trained with intermediate 2D image and 3D geometric supervision, along with depth supervision. Crucially, our first step complements pose estimation using interest point detection and descriptor learning. We demonstrate state-of-the-art results on depth estimation with lower compute for different scene lengths. Furthermore, our method generalizes to newer environments and the descriptors output by our network compare favorably to strong baselines. Code is available at https://github.com/magicleap/DELTAS Keywords: 3D from Multi-view and Sensors, Stereo Depth Estimation, Multi-task learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation</head><p>Depth sensing is crucial for a wide range of applications ranging from Augmented Reality (AR)/ Virtual Reality (VR) to autonomous driving. Estimating depth can be broadly divided into classes: active and passive sensing. Active sensing techniques include LiDAR, structured-light and time-of-flight (ToF) cameras, whereas depth estimation using a monocular camera or stereopsis of an array of cameras is termed passive sensing. Active sensors are currently the de-facto standard of applications requiring depth sensing due to good accuracy and low latency in varied environments <ref type="bibr" target="#b28">[48]</ref>. However, active sensors have their own of limitation. LiDARs are prohibitively expensive and provide sparse measurements. arXiv:2003.08933v2 [cs.CV] Depth Image SPARSE TO DENSE DEPTH IMAGE FEATURES <ref type="figure">Fig. 1</ref>. End-to-end network for detection and description of interest points, matching and triangulation of the points and densification of 3D points for depth estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structured-light and ToF depth cameras have limited range and completeness due to the physics of light transport. Furthermore, they are power hungry and inhibit mobility critical for AR/VR applications on wearables. Consequently, computer vision researchers have pursued passive sensing techniques as a ubiquitous, cost-effective and energy-efficient alternative to active sensors <ref type="bibr">[31]</ref>. Passive depth sensing using a stereo cameras requires a large baseline and careful calibration for accurate depth estimation <ref type="bibr" target="#b2">[3]</ref>. A large baseline is infeasible for mobile devices like phones and wearables. An alternative is to use MVS techniques for a moving monocular camera to estimate depth. MVS generally refers to the problem of reconstructing 3D scene structure from multiple images with known camera poses and intrinsics <ref type="bibr" target="#b13">[14]</ref>. The unconstrained nature of camera motion alleviates the baseline limitation of stereo-rigs, and the algorithm benefits from multiple observations of the same scene from continuously varying viewpoints <ref type="bibr" target="#b16">[17]</ref>. However, camera motion also makes depth estimation more challenging relative to rigid stereo-rigs due to pose uncertainty and added complexity of motion artifacts. Most MVS approaches involve building a 3D cost volume, usually with a plane sweep stereo approach <ref type="bibr" target="#b25">[45,</ref><ref type="bibr">18]</ref>. Accurate depth estimation using MVS rely on 3D convolutions on the cost volume, which is both memory as well as computationally expensive, scaling cubically with the resolution. Furthermore, redundant compute is added by ignoring useful image-level properties such as interest points and their descriptors, which are a necessary precursor to camera pose estimation, and hence, any MVS technique. This increases the overall cost and energy requirements for passive sensing.</p><p>Passive sensing using a single image is fundamentally unreliable due to scale ambiguity in 2D images. Deep learning based monocular depth estimation approaches formulate the problem as depth regression <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> and have reduced the performance gap to those of active sensors <ref type="bibr">[26,</ref><ref type="bibr">24]</ref>, but still far from being practical. Recently, sparse-to-dense depth estimation approaches have been proposed to remove the scale ambiguity and improve robustness of monocular depth estimation <ref type="bibr">[31]</ref>. Indeed, recent sparse-to-dense approaches with less than 0.5% depth samples have accuracy comparable to active sensors, with higher range and completeness <ref type="bibr" target="#b5">[6]</ref> . However, these approaches assume accurate or seed depth samples from an active sensor which is limiting. The alternative is to use the sparse 3D landmarks output from the best performing algorithms for Simultaneous Localization and Mapping (SLAM) <ref type="bibr">[32]</ref> or Visual Inertial Odometry (VIO) <ref type="bibr">[34]</ref>. However, using depth evaluated from these sparse landmarks in lieu of depth from active sensors, significantly degrades performance <ref type="bibr" target="#b27">[47]</ref>. This is not surprising as the learnt sparse-to-dense network ignores potentially useful cues, structured noise and biases present in SLAM or VIO algorithm.</p><p>Here we propose to learn the sparse 3D landmarks in conjunction with the sparse to dense formulation in an end-to-end manner so as to (a) remove dependence on a cost volume in the MVS technique,thus, significantly reducing compute, (b) complement camera pose estimation using sparse VIO or SLAM by reusing detected interest points and descriptors, (c) utilize geometry-based MVS concepts to guide the algorithm and improve the interpretability, and (d) benefit from the accuracy and efficiency of sparse-to-dense techniques. Our network is a multitask model <ref type="bibr">[22]</ref>, comprised of an encoder-decoder structure composed on two encoders, one for RGB image and one for sparse depth image, and three decoders: one for interest point detection, one for descriptors and one for the dense depth prediction. We also contribute a differentiable module that efficiently triangulates points using geometric priors and forms the critical link between the interest point decoder, descriptor decoder, and the sparse depth encoder enabling end-to-end training.</p><p>The rest of the paper is organized as follows. Section 2 discussed related work and Section 3 describes our approach. We perform experimental evaluation in Section 4, and finally conclusions and future work are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Interest point detection and description: Sparse feature based methods are standard for SLAM or VIO techniques due to their high speed and accuracy. The detect-then-describe approach is the most common approach to sparse feature extraction, wherein, interest points are detected and then described for a patch around the point. The descriptor encapsulates higher level information, which are missed by typical low-level interest points such as corners, blobs, etc. Prior to the deep learning revolution, classical systems like SIFT [28] and ORB <ref type="bibr" target="#b18">[38]</ref> were ubiquitously used as descriptors for feature matching for low level vision tasks. Deep neural networks directly optimizing for the objective at hand have now replaced these hand engineered features across a wide array of applications. However, such an end-to-end network has remained elusive for SLAM [33] due to the components being non-differentiable. General purpose descriptors learnt by methods such as SuperPoint <ref type="bibr" target="#b8">[9]</ref>, LIFT <ref type="bibr" target="#b26">[46]</ref>, <ref type="bibr">GIFT [27]</ref> aim to bridge the gap towards differentiable SLAM.</p><p>MVS: MVS approaches either directly reconstruct a 3D volume or output a depth map which can be flexibly used for 3D reconstruction or other applications. Methods reconstructing 3D volumes <ref type="bibr" target="#b25">[45,</ref><ref type="bibr" target="#b4">5]</ref> are restricted to small spaces or isolated objects either due to the high memory load of operating in a 3D voxelized space <ref type="bibr">[36,</ref><ref type="bibr" target="#b20">40]</ref>, or due to the difficulty of learning point representations in complex environments <ref type="bibr">[35]</ref>. Here, we use multi-view images captured in indoor environments for depth estimation due to the versatility of depth map representation. This area has lately seen a lot of progress starting with DeepMVS [18] which proposed a learnt patch matching approach. MVDepthNet <ref type="bibr" target="#b24">[44]</ref>, and DP-SNet [19] build a cost volume for depth estimation. GP-MVSNet <ref type="bibr" target="#b16">[17]</ref> built upon MVDepthNet to coherently fuse temporal information using gaussian processes. All these methods utilize the plane sweep algorithm during some stage of depth estimation, resulting in an accuracy vs efficiency trade-off.</p><p>Sparse to Dense Depth prediction: Sparse-to-dense depth estimation has recently emerged as a way to supplement active depth sensors due to their range limitations when operating on a power budget, and to fill in depth in hard to detect regions such as dark or reflective objects. The first such approach was proposed by Ma et.al <ref type="bibr">[31]</ref>, and following work by Chen et. al. <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b27">[47]</ref> introduced innovations in the representation and network architecture. A convolutional spatial propagation module is proposed in <ref type="bibr" target="#b6">[7]</ref> to in-fill the missing depth values. Self-supervised approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref> have concurrently been explored for the sparse-to-dense problem <ref type="bibr">[30]</ref>. Recently, a learnable triangulation technique was proposed to learn human pose key-points <ref type="bibr">[21]</ref>. We leverage their algebraic triangulation module for the purpose of sparse reconstruction of 3D points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our method can be broadly sub-divided into three steps as illustrated in <ref type="figure" target="#fig_5">Figure  1</ref> for a prototypical target image and two view-points. In the first step, the target or anchor image and the multi-view images are passed through a shared RGB encoder and descriptor decoder to output a descriptor field for each image. Interest points are also detected for the target or the anchor image. In the second step, the interest points in the anchor image in conjunction with the relative poses are used to determine the search space in the reference or auxiliary images from alternate view-points. Descriptors are sampled in the search space and are matched with descriptors for the interest points. Then, the matched keypoints are triangulated using SVD and the output 3D points are used to create a sparse depth image. In the third and final step, the output feature maps for the sparse depth encoder and intermediate feature maps from the RGB encoder are collectively used to inform the depth decoder and output a dense depth image. Each of the three steps are described in greater detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Interest point detector and descriptor</head><p>We adopt SuperPoint-like <ref type="bibr" target="#b8">[9]</ref> formulation of a fully-convolutional neural network architecture which operates on a full-resolution image and produces in-  terest point detection accompanied by fixed length descriptors. The model has a single, shared encoder to process and reduce the input image dimensionality. The feature maps from the encoder feed into two task-specific decoder heads, which learn weights for interest point detection and interest point description. This joint formulation of interest point detection and description in SuperPoint enables sharing compute for the detection and description tasks, as well as the down stream task of depth estimation. However, SuperPoint was trained on grayscale images with focus on interest point detection and description for continuous pose estimation on high frame rate video streams, and hence, has a relatively shallow encoder. On the contrary, we are interested in image sequences with sufficient baseline, and consequently longer intervals between subsequent frames. Furthermore, SuperPoint's shallow backbone suitable for sparse point analysis has limited capacity for our downstream task of dense depth estimation. Hence, we replace the shallow backbone with a ResNet-50 <ref type="bibr" target="#b15">[16]</ref> encoder which balances efficiency and performance. The output resolution of the interest point detector decoder is identical to that of SuperPoint. In order to fuse fine and coarse level image information critical for point matching, we use a U-Net <ref type="bibr" target="#b17">[37]</ref> like architecture for the descriptor decoder. This decoder outputs an N-dimensional descriptor tensor at 1/8 th the image resolution, similar to SuperPoint. The architecture is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. We train the interest point detector network by distilling the output of the original SuperPoint network and the descriptors are trained by the matching formulation described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Point matching and triangulation</head><p>The previous step provides interest points for the anchor image and descriptors for all images, i.e., the anchor image and full set of auxiliary images. A naive approach will be to match descriptors of the interest points sampled from the descriptor field of the anchor image to all possible positions in each auxiliary image. However, this is computationally prohibitive. Hence, we invoke geometrical constraints to restrict the search space and improve efficiency. Using concepts from multi-view geometry, we only search along the epipolar line in the auxiliary images <ref type="bibr" target="#b13">[14]</ref>. The epipolar line is determined using the fundamental matrix, F , using the relation xF x T = 0, where x is the set of points in the image. The matched point is guaranteed to lie on the epipolar line in an ideal scenario as illustrated in <ref type="figure">Figure 3</ref> (Left). However, practical limitations to obtain perfect pose lead us to search along the epipolar line with a small fixed offset on either side; <ref type="figure">Figure 3</ref> (Middle). Furthermore, the epipolar line stretches for depth values from −∞ to ∞. We clamp the epipolar line to lie within feasible depth sensing range, and vary the sampling rate within this restricted range in order to obtain descriptor fields with the same output shape for implementation purposes, shown in <ref type="figure">Figure 3</ref> (Right). We use bilinear sampling to obtain the descriptors at the desired points in the descriptor field. The descriptor of each interest point is convolved with the descriptor field along its corresponding epipolar line for each image view-point:</p><formula xml:id="formula_0">C j,k =D j * D k j , ∀x ∈ E,<label>(1)</label></formula><p>whereD is the descriptor field of the anchor image, D k is the descriptor field of the k th auxiliary image, and convolved over all sampled points x along the clamped epipolar line E for point j. This effectively provides a cross-correlation map <ref type="bibr" target="#b1">[2]</ref> between the descriptor field and interest point descriptors. High values in this map indicate potential key-point matches in the auxiliary images to the interest points in the anchor image. In practice, we add batch normalization <ref type="bibr">[20]</ref> and ReLU non-linearity [23] to output C j,k in order to ease training. To obtain the 3D points, we follow the algebraic triangulation approach proposed in <ref type="bibr">[21]</ref>. We process each interest point j independently of each other. The approach is built upon triangulating the 2D interest points along with the 2D positions obtained from the peak value in each cross correlation map. To estimate the 2D positions we first compute the softmax across the spatial axes:</p><formula xml:id="formula_1">C j,k = exp(C j,k )/( W rx=1 H ry=1 exp(C j,k (r x , r y )),<label>(2)</label></formula><p>where, C j,k indicates the cross-correlation map for the j th inter-point and k th view, and W, H are spatial dimensions of the epipolar search line. Then we calculate the 2D positions of the points as the center of mass of the corresponding cross-correlation maps, also termed soft-argmax operation:</p><formula xml:id="formula_2">x j,k = W rx=1 H ry=1 r(x, y)(C j,k (r(x, y))).<label>(3)</label></formula><p>The soft-argmax operation enables differentiable routing between the 2D position of the matched points x j,k and the cross-correlation maps C j,k . We use the linear algebraic triangulation approach proposed in [21] to estimate the 3D points from the matched 2D points x j,k . Their method reduces the finding of the 3D coordinates of a point z j to solving the over-determined system of equations on homogeneous 3D coordinate vector of the pointz:</p><formula xml:id="formula_3">A jzj = 0,<label>(4)</label></formula><p>where A j ∈ R 2k,4 is a matrix composed of the components from the full projection matrices and x j,k . Different view-points may contribute unequally to the triangulation of a point due to occlusions and motion artifacts. Weighing the contributions equally leads to sub-optimal performance. The problem is solved in a differentiable way by adding weights w k to the coefficients of the matrix corresponding to different views:</p><formula xml:id="formula_4">(w j A j )z j = 0.<label>(5)</label></formula><p>The weights w are set to be the max value in each cross-correlation map. This allows the contribution of the each camera view to be controlled by the quality of match, and low-confidence matches to be weighted less while triangulating the interest point. Note the confidence value of the interest points are set to be 1. The above equation is solved via differentiable Singular Value Decomposition (SVD) of the matrix B = U DV T , from whichz is set as the last column of V . The final non-homogeneous value of z is obtained by dividing the homogeneous 3D coordinate vectorz by its fourth coordinate: z =z/(z) <ref type="bibr" target="#b3">4</ref> [21].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Densification of sparse depth points</head><p>The interest-point detector network provides the 2D position of the points. The z coordinate of the triangulated points provides the depth. We impute a sparse depth image of the same resolution as the input image with depth of these sparse points. Note that the gradients can propagate from the sparse depth image back to the 3D key-points all the way to the input image. This is akin to switch unpooling in SegNet <ref type="bibr" target="#b0">[1]</ref>. We pass the sparse depth image through an encoder network which is a narrower version of the image encoder network.  the intermediate feature maps of the same resolution in the decoder, similar to <ref type="bibr" target="#b5">[6]</ref>. We provide deep supervision over 4 scales <ref type="bibr">[25]</ref>. We also include a spatial pyramid pooling block to encourage feature mixing at different receptive field sizes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4]</ref>. The details of the architecture are shown in the <ref type="figure" target="#fig_1">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall training objective</head><p>The entire network is trained with a combination of (a) cross entropy loss between the output tensor of the interest point detector decoder and ground truth interest point locations obtained from SuperPoint, (b) a smooth-L1 loss between the 2D points output after soft argmax and ground truth 2D point matches, (c) a smooth-L1 loss between the 3D points output after SVD triangulation and ground truth 3D points, (d) an edge aware smoothness loss on the output dense depth map, and (e) a smooth-L1 loss over multiple scales between the predicted dense depth map output and ground truth 3D depth map. The overall training objective is:</p><formula xml:id="formula_5">L = w ip L ip + w 2d L 2d + w 3d L 3d + w sm L sm + i w d,i L d,i ,<label>(6)</label></formula><p>where L ip is the interest point detection loss, L 2d is the 2D matching loss, L 3d is the 3D triangulation loss, L sm is the smoothness loss, and L d,i is the depth estimation loss at scale i for 4 different scales ranging from original image resolution to 1/16 th the image resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Training: Most MVS approaches are trained on the DEMON dataset <ref type="bibr" target="#b23">[43]</ref>. However, the DEMON dataset mostly contains pairs of images with the associated depth and pose information. Relative confidence estimation is crucial to accurate triangulation in our algorithm, and needs sequences of length three or greater in order to estimate the confidence accurately and holistically triangulate an interest point. Hence, we diverge from traditional datasets for MVS depth estimation, and instead use ScanNet <ref type="bibr" target="#b7">[8]</ref>. ScanNet is an RGB-D video dataset containing 2.5 million views in more than 1500 scans, annotated with 3D camera poses, surface reconstructions, and instance-level semantic segmentations. Three views from a scan at a fixed interval of 20 frames along with the pose and depth information forms a training data point in our method. The target frame is passed through SuperPoint in order to detect interest points, which are then distilled using the loss L ip while training our network. We use the depth images to determine ground truth 2D matches, and unproject the depth to determine the ground truth 3D points. We train our model for 100K iterations using PyTorch framework with batch-size of 24 and ADAM optimizer with learning rate 0.0001 (β 1 = 0.9, β 2 = 0.999), which takes about 3 days across 4 Nvidia Titan RTX GPUs. . We fix the resolution of the image to be qVGA (240×320) and number of interest points to be 512 in each image with at most half the interest points chosen from the interest point detector thresholded at 0.0005, and the rest of the points chosen randomly from the image. Choosing random points ensures uniform distribution of sparse points in the image and helps the densification process. We set the length of the sampled descriptors along the epipolar line to be 100, albeit, we found that the matching is robust even for lengths as small as 25. We set the range of depth estimation to be between 0.5 and 10 meters, as common for indoor environments. We empirically set the weights to be [0.1,1.0,2.0,1.0,2.0] for w ip , w 2d , w 3d , w sm , w d,1 , respectively. We damp w d,1 by a factor of 0.7 for each subsequent scale.</p><p>Evaluation: The ScanNet test set consists of 100 scans of unique scenes different for the 707 scenes in the training dataset. We first evaluate the performance of our detector and descriptor decoder for the purpose of pose estimation on Scan-Net. We use the evaluation protocol and metrics proposed in SuperPoint, namely the mean localization error (MLE), the matching score (MScore), repeatability (Rep) and the fraction of correct pose estimated using descriptor matches and PnP algorithm at 5 • threshold for rotation and and 5 cm for translation. We compare against SuperPoint, SIFT, ORB and SURF at a NMS threshold of 3 pixels for Rep, MLE, and MScore as suggested in the SuperPoint paper. Next, we use standard metrics to quantitatively measure the quality of our estimated depth: : absolute relative error (Abs Rel), absolute difference error (Abs diff), square relative error (Sq Rel), root mean square error and its log scale (RMSE and RMSE log) and inlier ratios (δ &lt; 1.25 i where i ∈ 1, 2, 3). Note higher values for inlier ratios are desirable, whereas all other metrics warrant lower values. We compare our method to recent deep learning approaches for MVS: (a) DP-SNet: Deep plane sweep approach, (b) MVDepthNet: Multi-view depth net, and (c) GPMVSNet temporal non-parametric fusion approach using Gaussian processes. Note that these methods perform much better than traditional geometrybased stereo algorithms. Our primary results are on sequences of length 3, but we also report numbers on sequences of length 2,4,5 and 7 in order to understand the performance as a function of scene length. We evaluate the methods on Sun3D dataset, in order to understand the generalization of our approach to other indoor scenes. We also discuss the multiply-accumuate operations (MACs) for the different methods to understand the operating efficiency at run-time. <ref type="table" target="#tab_3">Table 1</ref> shows the results of the our detector and descriptor evaluation. Note that MLE and repeatability are detector metrics, MScore is a descriptor metric, and rotation@5 • and translation@5cm are combined metrics. We set the threshold for our detector at 0.0005, the same as that used during training. This results in a large number of interest points being detected (Num) which artificially inflates the repeatability score (Rep) in our favour, but has poor localization performance as indicated by MLE metric. However, our MScore is comparable to SuperPoint although we trained our network to only match along the epipolar line, and not for the full image. Furthermore, we have the best rotation@5 • and translation@5cm metric indicating that the matches found using our descriptors help accurately determine rotation and translation, i.e., pose. These results are indicative that our training procedure can complement the homographic adaptation technique of SuperPoint and boost the overall performance. Incorporation of evaluated pose using ideas discussed in <ref type="bibr" target="#b19">[39]</ref>, in lieu of ground truth pose to train our network is left for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detector and Descriptor Quality</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Depth Results</head><p>We set the same hyper-parameters for evaluating our network for all scenarios and across all datasets, i.e., fix the number of points detected to be 512, length of the sampled descriptors to be 100, and the detector threshold to be 5e-4. In order to ensure uniform distribution of the interest points and avoid clusters, we set a high NMS value of 9 as suggested in <ref type="bibr" target="#b8">[9]</ref>. The supplement has analysis of the sparse depth output from our network and ablation study over different choices of hyper parameters. <ref type="table" target="#tab_4">Table 2</ref> shows the performance of depth estimation on sequences of length 3 and gap 20 as used in the training set. For fair comparison, we evaluate two versions of the competing approaches <ref type="formula" target="#formula_0">(1)</ref> The author provided open source trained model, <ref type="bibr">(</ref>2) The trained model fine-tuned on Scan-Net for 100K iterations with the default training parameters as suggested in the manuscript or made available by the authors. We use a gap of 20 frames to train each network, similar to ours. The fine-tuned models are indicated by the suffix FT in the table. Unsurprisingly, the fine-tuned models fare much better than the original models on ScanNet evaluation. MVDepthNet has least improvement after fine-tuning, which can be attributed to the heavy geometric and photometric augmentation used during training, hence making it generalize well. DPSNet benefits maximally from fine-tuning with over 25% drop in absolute error. However, our network outperforms all methods across all metrics. <ref type="figure" target="#fig_4">Figure 6</ref> shows qualitative comparison between the different methods and <ref type="figure" target="#fig_3">Figure 5</ref> show sample 3D reconstructions of the scene from the estimated depth maps. In <ref type="figure" target="#fig_4">Figure 6</ref>, we see that MVDepthNet has gridding artifacts, which are removed by GPMVS-Net. However, GPMVSNet has poor metric performance. DPSNet washes away finer details and also suffers from gridding artifacts. Our method preserves finer details while maintaining global coherence compared to all other methods. As we use geometry to estimate sparse depth, and the network in-fills the missing values, we retain metric performance while leveraging the generative ability of CNNs with sparse priors. In <ref type="figure" target="#fig_3">Figure 5</ref> we see our method consistently output less noisy scene reconstructions compared to MVDepthNet and DPSNet. Moreover, we see planes and corners being respected better than the other methods. An important feature of any multiview stereo method is the ability to improve with more views. <ref type="table" target="#tab_7">Table 3</ref> shows the performance for different number of images. We set the frame gap to be 20, 15, 12 and 10 for 2,4,5 and 7 frames respectively. These gaps ensure that each set approximately span similar volumes in 3D space, and any performance improvement emerges from the network better using the available information as opposed to acquiring new information. We again see <ref type="table" target="#tab_7">Table 3</ref>. Performance of depth estimation on ScanNet. Results on sequences of various lengths are presented. GPN: GPMVSNet, MVN: MVDepthNet, DPS: DPSNet. AbR: Abolute Relative, Abs: Absolute difference, SqR: Square Relative.  that our method outperforms all other methods on all three metrics for different sequence lengths. Closer inspection of the values indicate that the DPSNet and GPMVSNet do not benefit from additional views, whereas, MVDepthNet benefits from a small number of additional views but stagnates for more than 4 frames. On the contrary, we show steady improvement in all three metrics with additional views. This can be attributed to our point matcher and triangulation module which naturally benefits from additional views. As a final experiment, we test our network on Sun3D test dataset consisting of 80 pairs of images. Sun3D also captures indoor environments, albeit at a much smaller scale compared to ScanNet. <ref type="table" target="#tab_5">Table 4</ref> shows the performance for the two versions of DPSNet and MVDepthNet discussed previously, and our network. Note DPSNet and MVDepthNet were originally trained on the Sun3D training database. The fine-tuned version of DPSNet performs better than the original network on the Sun3D test set owing to the greater diversity in ScanNet training database. MVDepthNet on the contrary performs worse, indicating that it overfit to ScanNet and the original network was sufficiently trained and generalized well. Remarkably, we again outperform both methods although our trained network has never seen any image from the Sun3D database. This indicates that our principled way of determining sparse depth, and then densifying has good generalizability. The supplement shows additional qualitative results.</p><p>We evaluate the total number of multiply-accumulate operations (MACs) needed for our approach. For a 2 image sequence, we perform 16.57 Giga Macs (GMacs) for the point detector and descriptor module, less than 0.002 GMacs for the matcher and triangulation module, and 67.90 GMacs for the sparse-todense module. A large fraction of this is due to the U-Net style feature tensors connecting the image and sparse depth encoder to the decoder. We perform a total of 84.48 GMacs to estimate the depth for a 2 image sequence. This is considerably lower than DPSNet which performs 295.63 GMacs for a 2 image sequence, and also less than the real-time MVDepthNet which performs 134.8 GMacs for a pair of images to estimate depth. It takes 90 milliseconds to estimate depth on Nvidia Titan RTX GPU, which we evaluated to be 2.5 times faster than DPSNet. Inference time for MVDepthNet and GPMVSNet is ≈ 60 milliseconds. We believe our method can be further sped up by replacing Pytorch's native SVD with a custom implementation for triangulation. Furthermore, as we do not depend on a cost volume, compound scaling laws as those derived for image <ref type="bibr" target="#b21">[41]</ref> and object <ref type="bibr" target="#b22">[42]</ref> recognition can be straightforwardly extended to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work we developed an efficient depth estimation algorithm by learning to triangulate and densify sparse points in a multi-view stereo scenario. On all of the existing benchmarks, we have exceeded the state-of-the-art results, and demonstrated computation efficiency over competitive methods. In future work, we will expand on incorporating more effective attention mechanisms for interest point matching, and more anchor supporting view selection. Jointly learning depth and the full scene holistically using truncated signed distance function (TSDF) or similar representations is another promising direction. Video depth estimation approaches such as <ref type="bibr">[29]</ref> are closely related to MVS, and our approach can be readily extended to predict consistent and efficient depth for videos. Finally, we look forward to deeper integration with the SLAM problem, as depth estimation and SLAM are duals of each other. Overall, we believe that our approach of coupling geometry with the power of conventional 2D CNNs is a promising direction for learning 3D Vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DPSNet MVDepthNet</head><p>Ours Ground Truth  1 Ablation Studies</p><p>We first analyze the sparse triangulated points output from our network and perform ablation studies on our critical hyper-parameters and its influence on the depth estimation performance of our trained model, i.e., we only investigate the parameter values at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse Depth Analysis:</head><p>We first validate the need to triangulate points in a differentiable manner as opposed to directly using sparse points output by a standard SLAM systems for the task of dense depth estimation. <ref type="table" target="#tab_3">Table 1</ref> lists the performance of sparse and dense depth estimations using COLMAP. We see that the sparse depth is of very poor quality, and the dense depth calculated by COLMAP is able to reduce the performance gap due to additional post processing steps like block matching etc. However, the best performance is that of using sparse map predicted by COLMAP as input to a sparse-to-dense depth estimation network, illustrating the power of deep networks. The sparse-to-dense network is identical to the network structure described in the main manuscript minus the triangulation module, and the descriptor and detector heads. Note that the performance of sparse-to-dense network is significantly worse than that of our approach end-to-end approach described in the main manuscript. <ref type="table" target="#tab_4">Table 2</ref> shows the performance of the sparse depth output by the triangulation module. We see that the performance is significantly better than that of sparse points output by COLMAP, and robust accross different ratios of interest points and random points. This indicates that the network learns context around a point to circumvent the hardness of triangulating non-interest points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Points:</head><p>We first study the influence of the number of sampled points in the target image on the final depth estimation. In <ref type="table" target="#tab_7">Table 3</ref> we see that the performance of our approach is fairly robust in the range of 256 to 512 points. Performance slightly degrades for more than 512 points. Unsurprisingly, the performance significantly degrades when no triangulated points are considered for depth estimation which would be equivalent to monocular depth estimation. However,even as few as 32 points greatly improves the performance of depth estimation. If we were to swap the depth of triangulated points with ground truth depth, we see that the performance is significantly better. Consequently, our method can be used in conjunction with an active sensor when available without requiring any retraining of the networks. A hybrid system consisting of an active sensor and our passive sensing approach is useful towards reducing the frame rate of the active sensor, and hence, reducing the power consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ratio of points:</head><p>We investigated the influence of the ratio of the number of interest points from the interest point detector to the total number of points which are a combination of those detected by the detector and points sampled randomly from the image. For e.g., 0.75 indicates 3/4 th points sampled from the detector and the rest chosen randomly. We see in <ref type="table" target="#tab_5">Table 4</ref> that the performance of our approach is robust across all ratios. This indicates that the network is not biased towards corner points, but can robustly match points across the image.</p><p>NMS Radius: Next we investigate the influence of the non-maximum suppression (NMS) radius value for the interest point detector on the performance. Note that small values of NMS result in interest points being sampled predominantly from high texture regions and being clustered together, whereas high values of NMS encourage the points to be well distributed. In <ref type="table">Table 5</ref> we see that small values of NMS hurt performance, with the performance improving till NMS value of 9 and then again degrading for value of 11. This indicates that the network prefers well separated, uniformly sampled points across the image.</p><p>Threshold: We also investigated the performance of depth estimation for different thresholds on the interest point detector. In <ref type="table">Table 6</ref> we see that threshold values of 0.0001 and 0.0005 result in similar performance. The performance degrades for higher values of 0.001 and 0.005. This suggests that the network does not particularly favour high quality interest points, but a large number of them, which are made available when the threshold is low.</p><p>Epipolar Length: In <ref type="table">Table 7</ref> we investigate the influence of the length of the sampled descriptors along the epipolar line on depth estimation. We see that the performance is robust across all values of length ranging from 25 pixels to 150 pixels. This observation can further reduce the training time and inference time for depth estimation.</p><p>Offset value: We investigated the performance of our trained network for 1 pixel and 2 pixel offsets to compensate for pose error. We see in <ref type="table">Table 8</ref> that 2 pixel offset does not improve performance, suggesting that the pose in ScanNet is sufficiently reliable. This parameter however might be of greater influence in cases wherein pose estimation is unreliable. Model Architecture: Finally, we explore the performance of our approach on model architecture. We swap our ResNet-50 backbone with a VGG-9 backbone similar to that of SuperPoint. We use the same training procedure as that of the ResNet-50 architecture mentioned in the main manuscript. In <ref type="table">Table 9</ref> we that the extremely light-weight VGG-9 architecture performs much better than MVDepthNet and some values are comparable or even better than those of DP-SNet. Furthermore, the total number of GMACs is only 16.9, which is ≈ 18x more efficient that DPSNet and 8x more efficient that real-time MVDepthNet.</p><p>In <ref type="table" target="#tab_3">Table 10</ref> we see that we observe only a slight degradation in pose performance (rotation and translation) compared to SuperPoint. This reinforces our conclusion in the main manuscript that our supervision can complement that of SuperPoint. Overall, the robust performance of our network with extremely low compute is a promising first step to derive scaling laws as done in EfficientNet.</p><p>Qualitative results: In <ref type="figure" target="#fig_5">Figure 1</ref> we see that our depth maps are more consistent with respect to ground truth, and respect the geometry of the scene better. For e.g., the lamp in the second row, the chair at the back in the fourth row, the phone in the seventh row and the cabinet in the eight row are qualitatively better than all other methods. Furthermore, we are also able to coherently reconstruct depth where the active depth sensor fails, for e.g. the windows in the second and seventh row and the transparent glass side-table in the sixth row.   <ref type="table" target="#tab_5">Table 4</ref>. Performance of depth estimation on ScanNet for different ratios of interest points and random points. We use sequences of length 3 and sample every 20 frames.   <ref type="table">Table 7</ref>. Performance of depth estimation on ScanNet for different lengths of the sampled descriptors. We use sequences of length 3 and sample every 20 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image GT Depth MVDepthNet GPMVSNet DPSNet Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ratio Abs Rel Abs</head><p>Length Abs Rel Abs Sq Rel δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 25 0.0934 0.1542 0.0508 0.9287 0.9767 0.9893 50 0.0933 0.1540 0.0507 0.9287 0.9767 0.9893 100 0.0932 0.1540 0.0506 0.9287 0.9767 0.9893 150 0.0932 0.1540 0.0506 0.9286 0.9767 0.9893 <ref type="table">Table 8</ref>. Performance of depth estimation on ScanNet for different sampling offsets. We use sequences of length 3 and sample every 20 frames.</p><p>Offsets Abs Rel Abs Sq Rel δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 1 pix 0.0932 0.1540 0.0506 0.9287 0.9767 0.9893 2 pix 0.0933 0.1541 0.0507 0.9285 0.9766 0.9893 <ref type="table">Table 9</ref>. Performance of depth estimation on ScanNet for different architectures. We use sequences of length 3 and sample every 20 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arch</head><p>Abs Rel Abs Sq Rel δ &lt; </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>SuperPoint-like network with detector and descriptor heads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Proposed sparse-to-dense network architecture showing the concatenation of image and sparse depth features. We use deep supervision over 4 image scales. The blocks below illustrate the upsampling and the altrous spatial pyramid pooling (ASPP) block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>AbR Abs SqR AbR Abs SqR AbR Abs SqR GPN 0.112 0.233 0.101 0.109 0.226 0.100 0.107 0.226 0.112 0.109 0.230 0.116 MVN 0.126 0.238 0.471 0.105 0.191 0.078 0.106 0.192 0.071 0.108 0.195 0.067 DPS 0.099 0.181 0.062 0.102 0.168 0.057 0.102 0.168 0.057 0.102 0.167 0.057 Ours 0.106 0.173 0.057 0.090 0.150 0.049 0.088 0.147 0.048 0.087 0.144 0.043</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>3D scene reconstruction using predicted depth over the full sequence.Image GT Depth MVDepthNet GPMVSNet DPSNet Ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative Performance of our networks on sampled images from ScanNet. 18. Huang, P.H., Matzen, K., Kopf, J., Ahuja, N., Huang, J.B.: Deepmvs: Learning multi-view stereopsis. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2821-2830 (2018) 19. Im, S., Jeon, H.G., Lin, S., Kweon, I.S.: Dpsnet: End-to-end deep plane sweep stereo. In: 7th International Conference on Learning Representations, ICLR 2019. International Conference on Learning Representations, ICLR (2019) 20. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015) 21. Iskakov, K., Burkov, E., Lempitsky, V., Malkov, Y.: Learnable triangulation of human pose. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 7718-7727 (2019) 22. Kendall, A., Gal, Y., Cipolla, R.: Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7482-7491 (2018) 23. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097-1105 (2012) 24. Lasinger, K., Ranftl, R., Schindler, K., Koltun, V.: Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. arXiv preprint arXiv:1907.01341 (2019) 25. Lee, C.Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z.: Deeply-supervised nets. In: Artificial intelligence and statistics. pp. 562-570 (2015) 26. Lee, J.H., Han, M.K., Ko, D.W., Suh, I.H.: From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326 (2019) 27. Liu, Y., Shen, Z., Lin, Z., Peng, S., Bao, H., Zhou, X.: Gift: Learning transformation-invariant dense visual descriptors via group cnns. In: Advances in Neural Information Processing Systems. pp. 6990-7001 (2019) 28. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. International journal of computer vision 60(2), 91-110 (2004) 29. Luo, X., Huang, J., Szeliski, R., Matzen, K., Kopf, J.: Consistent video depth estimation 39(4) (2020) 30. Ma, F., Cavalheiro, G.V., Karaman, S.: Self-supervised sparse-to-dense: Selfsupervised depth completion from lidar and monocular camera. In: 2019 International Conference on Robotics and Automation (ICRA). pp. 3288-3295. IEEE (2019) 31. Ma, F., Karaman, S.: Sparse-to-dense: Depth prediction from sparse depth samples and a single image (2018) 32. Mur-Artal, R., Montiel, J.M.M., Tardos, J.D.: Orb-slam: a versatile and accurate monocular slam system. IEEE transactions on robotics 31(5), 1147-1163 (2015) 33. Murthy Jatavallabhula, K., Iyer, G., Paull, L.: gradslam: Dense slam meets automatic differentiation. arXiv preprint arXiv:1910.10672 (2019) 34. Nistér, D., Naroditsky, O., Bergen, J.: Visual odometry. In: Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004. vol. 1, pp. I-I. Ieee (2004) 35. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d classification and segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 652-660 (2017) 36. Riegler, G., Osman Ulusoy, A., Geiger, A.: Octnet: Learning deep 3d representations at high resolutions. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3577-3586 (2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 .</head><label>1</label><figDesc>Qualitative Performance of our networks on sampled images from ScanNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Sq Rel δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 0.0 0.0935 0.1544 0.0508 0.9283 0.9766 0.9893 0.25 0.0933 0.1540 0.0507 0.9286 0.9766 0.9893 0.5 0.0932 0.1540 0.0506 0.9287 0.9767 0.9893 0.75 0.0933 0.1540 0.0507 0.9286 0.9766 0.9893 1.0 0.0933 0.1540 0.0507 0.9286 0.9766 0.9893</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Image Features Dilated Block Rate = 6 Dilated Block Rate = 12 Dilated Block Rate = 18 Dilated Block Rate = 24 conv+bn+relu conv+bn+relu conv+bn+relu Image Features Depth Features conv+bn+relu conv+bn conv+bn Up Project Block Altrous Spatial Pyramid Pooling Block</head><label></label><figDesc>Specifically, we use a ResNet-50 encoder with the channel widths after each layer to be 1/4 th of the image encoder. We concatenate these features with the features obtained from the image encoder. We use a U-net style decoder with intermediate feature maps from both the image as well as sparse depth encoder concatenated with</figDesc><table><row><cell></cell><cell cols="4">Image Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Key</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>= [H/2, W/2, d]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>= [H/4, W/4, d]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>= [H/8, W/8, d]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>= [H/16,W/16,d]</cell></row><row><cell>H</cell><cell>16</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>2048</cell><cell>512</cell><cell>256</cell><cell>1024</cell><cell>256</cell><cell>128</cell><cell>512</cell><cell>256</cell><cell>128</cell><cell>64</cell><cell>256</cell><cell>64</cell><cell>16</cell><cell>64</cell><cell>= [H/32,W/32,d] = Addition = Addition = Concatination</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dilated</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Block</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rate = 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Performance of different descriptors on ScanNet.</figDesc><table><row><cell></cell><cell cols="3">MLE MScore Num Rep rot@5 • trans@5cm</cell></row><row><cell>ORB</cell><cell>2.584 0.194</cell><cell>401 0.613 0.142</cell><cell>0.064</cell></row><row><cell>SIFT</cell><cell>2.327 0.201</cell><cell>203 0.496 0.311</cell><cell>0.148</cell></row><row><cell>SURF</cell><cell>2.577 0.198</cell><cell>268 0.460 0.303</cell><cell>0.134</cell></row><row><cell cols="3">SuperPoint 2.545 0.375 129 0.519 0.489</cell><cell>0.244</cell></row><row><cell>Ours</cell><cell cols="2">3.101 0.329 1511 0.738 0.518</cell><cell>0.254</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Performance of depth estimation on ScanNet. We use sequences of length 3 and sample every 20 frames. FT indicates fine-tuned on ScanNet.</figDesc><table><row><cell></cell><cell cols="3">Abs Rel Abs Sq Rel RMSE RMSE log δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</cell></row><row><cell>GPMVS</cell><cell>0.1306 0.2600 0.0944 0.3451 0.1881</cell><cell>0.8481 0.9462</cell><cell>0.9753</cell></row><row><cell cols="2">GPMVS-FT 0.1079 0.2255 0.0960 0.4659 0.1998</cell><cell>0.8905 0.9591</cell><cell>0.9789</cell></row><row><cell>MVDepth</cell><cell>0.1191 0.2096 0.0910 0.3048 0.1597</cell><cell>0.8690 0.9599</cell><cell>0.9851</cell></row><row><cell cols="2">MVDepth-FT 0.1054 0.1911 0.0970 0.3053 0.1553</cell><cell>0.8952 0.9707</cell><cell>0.9895</cell></row><row><cell>DPS</cell><cell>0.1470 0.2248 0.1035 0.3468 0.1952</cell><cell>0.8486 0.9474</cell><cell>0.9761</cell></row><row><cell>DPS-FT</cell><cell>0.1025 0.1675 0.0574 0.2679 0.1531</cell><cell>0.9102 0.9708</cell><cell>0.9872</cell></row><row><cell>Ours</cell><cell cols="3">0.0932 0.1540 0.0506 0.2505 0.1426 0.9287 0.9767 0.9893</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Performance of depth estimation on Sun3D. We use sequences of length 2.Abs Rel Abs Sq Rel RMSE RMSE log δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</figDesc><table><row><cell>MVDepth</cell><cell cols="2">0.1377 0.3199 0.1564 0.4523 0.1853</cell><cell>0.8245 0.9601</cell><cell>0.9851</cell></row><row><cell cols="2">MVDepth-FT 0.3092 0.7209 4.4899 1.718</cell><cell>0.319</cell><cell>0.7873 0.9117</cell><cell>0.9387</cell></row><row><cell>DPS</cell><cell cols="2">0.1590 0.3341 0.1564 0.4516 0.1958</cell><cell>0.8087 0.9363</cell><cell>0.9787</cell></row><row><cell>DPS-FT</cell><cell cols="2">0.1274 0.2858 0.0855 0.3815 0.1768</cell><cell>0.8396 0.9459</cell><cell>0.9866</cell></row><row><cell>Ours</cell><cell cols="4">0.1245 0.2662 0.0741 0.3602 0.1666 0.8551 0.9728 0.9902</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Performance of depth estimation on ScanNet using COLMAP. Sparse refers to the sparse map predicted by COLMAP, Dense refers to the dense depth map predicted by COLMAP, and Sparse +DNN refers to densification of the sparse map predicted by COLMAP using a deep neural network. Performance of sparse depth estimation on ScanNet for different ratios of interest points and random points. We use sequences of length 3 and sample every 20 frames.</figDesc><table><row><cell>Approach</cell><cell cols="3">Abs Rel Abs Sq Rel δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</cell></row><row><cell>Sparse</cell><cell cols="2">0.2629 0.4618 0.3882 0.5713 0.7498</cell><cell>0.8322</cell></row><row><cell>Dense</cell><cell cols="2">0.1371 0.2643 0.1379 0.8344 0.9080</cell><cell>0.9383</cell></row><row><cell cols="4">Sparse + DNN 0.1242 0.1990 0.0658 0.8756 0.9649 0.9878</cell></row><row><cell cols="4">Ratio Abs Rel Abs Sq Rel δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</cell></row><row><cell cols="4">0.0 0.0993 0.1899 0.0503 0.8856 0.9701 0.9906</cell></row><row><cell cols="4">0.25 0.0986 0.1891 0.0502 0.8869 0.9703 0.9906</cell></row><row><cell cols="2">0.5 0.0988 0.1893 0.0503 0.8866 0.9702</cell><cell cols="2">0.9905</cell></row><row><cell cols="2">0.75 0.0988 0.1893 0.0503 0.8866 0.9702</cell><cell cols="2">0.9905</cell></row><row><cell cols="2">1.0 0.0988 0.1893 0.0503 0.8866 0.9702</cell><cell cols="2">0.9905</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Performance of depth estimation on ScanNet for different number of sparse points. We use sequences of length 3 and sample every 20 frames.Num Points Abs Rel Abs Sq Rel δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</figDesc><table><row><cell>0</cell><cell>0.2203 0.3049 0.1375 0.7198 0.9022</cell><cell>0.9650</cell></row><row><cell>32</cell><cell>0.1105 0.1793 0.0582 0.9002 0.9722</cell><cell>0.9886</cell></row><row><cell>128</cell><cell cols="2">0.0960 0.1591 0.0514 0.9232 0.9760 0.9895</cell></row><row><cell>256</cell><cell cols="2">0.0934 0.1550 0.0505 0.9276 0.9766 0.9895</cell></row><row><cell>384</cell><cell cols="2">0.0931 0.1541 0.0505 0.9285 0.9767 0.9894</cell></row><row><cell>512</cell><cell cols="2">0.0932 0.1540 0.0506 0.9287 0.9767 0.9893</cell></row><row><cell>640</cell><cell>0.0936 0.1543 0.0509 0.9285 0.9766</cell><cell>0.9892</cell></row><row><cell>768</cell><cell>0.0942 0.1549 0.0512 0.9282 0.9766</cell><cell>0.9891</cell></row><row><cell cols="3">512 (GT) 0.0680 0.1111 0.0406 0.9562 0.9800 0.9903</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Performance of depth estimation on ScanNet for different radius for nonmaximum suppression (NMS Rad). We use sequences of length 3 and sample every 20 frames.NMS Rad Abs Rel Abs Sq Rel δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 Performance of depth estimation on ScanNet for different thresholds for the detector. We use sequences of length 3 and sample every 20 frames.Thresh Abs Rel Abs Sq Rel δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 0.0001 0.0932 0.1539 0.0506 0.9287 0.9767 0.9893 0.0005 0.0932 0.1540 0.0506 0.9287 0.9767 0.9893 0.001 0.0933 0.1540 0.0507 0.9286 0.9767 0.9893 0.005 0.0934 0.1544 0.0507 0.9283 0.9766 0.9893</figDesc><table><row><cell>3</cell><cell>0.0942 0.1554 0.0512 0.9278 0.9765</cell><cell>0.9892</cell></row><row><cell>5</cell><cell>0.0937 0.1545 0.0508 0.9283 0.9766</cell><cell>0.9892</cell></row><row><cell>7</cell><cell>0.0937 0.1545 0.0510 0.9284 0.9766</cell><cell>0.9892</cell></row><row><cell>9</cell><cell cols="2">0.0932 0.1540 0.0506 0.9287 0.9767 0.9893</cell></row><row><cell>11</cell><cell>0.0938 0.1546 0.0511 0.9285 0.9766</cell><cell>0.9891</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 GMACs Performance of different descriptors on ScanNet.</figDesc><table><row><cell cols="2">MVDepth 0.1054 0.1911 0.0970 0.8952 0.9707</cell><cell>0.9895</cell><cell>134.8</cell></row><row><cell>DPS</cell><cell>0.1025 0.1675 0.0574 0.9102 0.9708</cell><cell>0.9872</cell><cell>295.6</cell></row><row><cell cols="2">VGG-9 0.1073 0.1815 0.0581 0.9023 0.9719</cell><cell>0.9890</cell><cell>16.9</cell></row><row><cell cols="3">ResNet-50 0.0932 0.1540 0.0506 0.9287 0.9767 0.9893</cell><cell>84.4</cell></row><row><cell></cell><cell cols="3">MLE MScore Num Rep rot@5 • trans@5cm</cell></row><row><cell cols="2">SuperPoint 2.545 0.375 129 0.519 0.489</cell><cell>0.244</cell><cell></cell></row><row><cell cols="2">VGG-9 3.057 0.325 1619 0.751 0.472</cell><cell>0.228</cell><cell></cell></row><row><cell cols="2">ResNet-50 3.101 0.329 1511 0.738 0.518</cell><cell>0.254</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Point-based multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1538" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating depth from rgb and sparse sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="167" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2018.00060</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2018.00060" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="337" to="33712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-view stereo by temporal nonparametric fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2651" to="2660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11763</idno>
		<title level="m">Superglue: Learning feature matching with graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Surfnet: Generating 3d shape surfaces using deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Unmesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6040" to="6049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09070</idno>
		<title level="m">Efficientdet: Scalable and efficient object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5038" to="5047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mvdepthnet: real-time multiview depth estimation neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="248" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="767" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
