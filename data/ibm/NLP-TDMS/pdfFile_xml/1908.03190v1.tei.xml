<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NeuPDE: Neural Network Based Ordinary and Partial Differential Equations for Modeling Time-Dependent Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayden</forename><surname>Schaeffer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NeuPDE: Neural Network Based Ordinary and Partial Differential Equations for Modeling Time-Dependent Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>partial differential equations</term>
					<term>data-driven models</term>
					<term>data-discovery</term>
					<term>multilayer percep- trons</term>
					<term>image classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a neural network based approach for extracting models from dynamic data using ordinary and partial differential equations. In particular, given a time-series or spatio-temporal dataset, we seek to identify an accurate governing system which respects the intrinsic differential structure. The unknown governing model is parameterized by using both (shallow) multilayer perceptrons and nonlinear differential terms, in order to incorporate relevant correlations between spatio-temporal samples. We demonstrate the approach on several examples where the data is sampled from various dynamical systems and give a comparison to recurrent networks and other data-discovery methods. In addition, we show that for MNIST and Fashion MNIST, our approach lowers the parameter cost as compared to other deep neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling and extracting governing equations from complex time-series can provide useful information for analyzing data. An accurate governing system could be used for making data-driven predictions, extracting large-scale patterns, and uncovering hidden structures in the data. In this work, we present an approach for modeling time-dependent data using differential equations which are parameterized by shallow neural networks, but retain their intrinsic (continuous) differential structure.</p><p>For time-series data, recurrent neural networks (RNN) is often employed for encoding temporal data and forecasting future states. Part of the success of RNN are due to the internal memory architecture which allows these networks to better incorporate state information over the length of a given sequence. Although widely successful for language modeling, translation, and speech recognition, their use in high-fidelity scientific computing applications is limited. One can observe that a sequence generated by an RNN may not preserve temporal regularity of the underlying signals (see, for example <ref type="bibr" target="#b4">[5]</ref> or <ref type="figure">Figure 2</ref>.3) and thus may not represent the true continuous dynamics.</p><p>For imaging tasks, deep neural networks (DNN) such as ResNet <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, FractalNet <ref type="bibr" target="#b18">[19]</ref>, and DenseNet <ref type="bibr" target="#b13">[14]</ref> have been successful in extracting complex hierarchical spatial information. These networks utilize intra-layer connectivity to preserve feature information over the network depth. For example, the ResNet architecture uses convolutional layers and skip connections. The hidden layers take the form x n+1 = x n + F (x n , θ) where x n represents the features at layer n and F is a convolutional neural network (or more generally, any universal approximator) with trainable parameters θ. The evolution of the features over the network depth is equivalent to applying the forward Euler method to the ordinary differential equation (ODE):ẋ = F (x, θ). The connection between ResNet's architecture, numerical integrators for differential equations, and optimal control has been presented in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Recently, DNN-based approaches related to differential equations have been proposed for data mining, forecasting, and approximation. Examples of algorithms which use DNN for learning ODE and PDE include: learning from data using a PDE-based network <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, deep learning for advection equations <ref type="bibr" target="#b5">[6]</ref>, approximating dynamics using ResNet with recurrent layers <ref type="bibr" target="#b24">[25]</ref>, and learning and modeling solutions of PDE using networks <ref type="bibr" target="#b27">[28]</ref>. Other approaches for learning governing systems and dynamics involve sparse regularizers ( 0 or hard-thresholding approaches in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> and 1 problems in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>) or models based on Gaussian processes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Note that in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> it was shown that adding more blocks of the PDE-based network improved (experimentally) the model's predictive capabilities. Using ODEs to represent the network connectivity, <ref type="bibr" target="#b4">[5]</ref> proposed a 'continuous-depth' neural network called ODE-Net. Their approach essentially replaces the layers in ResNet-like architectures with a trainable ODE. In <ref type="bibr" target="#b4">[5]</ref>, the authors state that their approach has several advantages, including the ability to better connect 'layers' due to the continuity of the model and a lower memory cost when training the parameters using the adjoint method. The adjoint method proposed in <ref type="bibr" target="#b4">[5]</ref> may not be stable for a general problem. In <ref type="bibr" target="#b7">[8]</ref>, a memory efficient and stable approach for training a neural ODE was given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions of this Work.</head><p>We present a machine learning approach for constructing approximations to governing equations of time-dependent systems that blends physics-informed candidate functions with neural networks. In particular, we construct a network approximation to an ODE which takes into account the connectivity between components (using a dictionary of monomials) and the differential structure of spatial terms (using finite difference kernels). If the user has prior knowledge on the structure or source of the data, i.e. fluids, mechanics, etc., one can incorporate commonly used physical models into the dictionary. We show that our approach can be used to extract ODE or PDE models from time-dependent data, improve the spatial accuracy of reduced order models, and reduce the parameter cost for image classification (for the MNIST and Fashion MNIST datasets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modeling Via Ordinary Differential Equations</head><p>Given discrete-time measurements generated from an unknown dynamic process, we model the time-series using a (first-order) ordinary differential equation,ẋ(t) = f (t, x(t)), x ∈ R d with d ≥ 1. The problem is to construct an approximation to the unknown generating function f , i.e. we will learn networks net(t, x) such thatẋ ≈ net(t, x). Essentially, we are learning a neural network approximation to the velocity field. Following the approach in <ref type="bibr" target="#b4">[5]</ref>, the system is trained by a 'continuous' model and the function f is parameterized by multilayer perceptrons (MLP). Since a two-layer MLP may require a large width to approximate a generic (nonlinear) function f , we purpose a different parameterization. Specifically, to better capture higher-order correlations between components of the data and to lower the number of parameters needed in the MLP (see for example, <ref type="figure">Figure 2</ref>.2), a dictionary of candidate inputs is added. Let D(t, x; p) be the collection (as a matrix) of the pth order monomial terms depending on t and x, i.e. each element in D can be written as:</p><formula xml:id="formula_0">t k x 1 1 · · · x d d , for 0 &lt; k + i i ≤ p.</formula><p>One has freedom to determine the particular dictionary elements; however, the choice of monomial terms provides a model for the interactions between each of the components of the time-series and is used for model identification of dynamical systems in the general setting <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>. For simplicity, we will suppress the (user-defined) parameter p.</p><p>In <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>, regularized optimization with polynomial dictionaries is used to approximate the generating function of some unknown dynamic process. When the dictionary is large enough so that the 'true' function is in the span of the candidate space, the solutions produced by sparse optimization are guaranteed to be accurate. To avoid defining a sufficiently rich dictionary, we propose using an MLP (with a non-polynomial activation function) in combination with the monomial dictionary, so that general functions may be well-approximated by the network. Note that the idea of using products of the inputs appears in other network architectures for example, the high-order neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>In many DNN architectures, batch normalization <ref type="bibr" target="#b14">[15]</ref> or weight normalization <ref type="bibr" target="#b30">[31]</ref> are used to improve the performance and stability during training. For the training of NeuPDE, a simple (uniform) normalization layer, N (x), is added between the input and dictionary layers, which maps x to a vector in [−1, 1] d (using the range over the all components). Specifically, let M and m be the maximum (and minimum) value of the data, over all components and samples and define the vector N (x) as:</p><formula xml:id="formula_1">N (x) := 2 x − m 1 d M − m − 1 d ∈ [−1, 1] d</formula><p>This normalization is applied to each component uniformly and enforces that each component of the dictionary is bounded by one (in magnitude). We found that this normalization was sufficient for stabilizing training and speeding up optimization in the regression examples. Without this step, divergence in the training phase was observed.</p><p>To train the network: let θ be the vector of learnable parameters in the MLP layer, then the optimization problem is:</p><formula xml:id="formula_2">min θ N i=1 L(x(t i )) + β 1 r(θ) + β 2 2 t N t 0 ẋ(τ ) 2 2 dτ (2.1) s.t. x(t 0 ) = x 0 ,ẋ = F (D(N (x)), θ)</formula><p>where β 1 , β 2 &gt; 0 are regularization parameters set by the user and F is an MLP. Specifically, let σ be a smooth activation function, for example, the exponential linear unit (ELU)</p><formula xml:id="formula_3">σ ELU (x) = e x − 1, x ≥ 0 x, x &lt; 0</formula><p>or the hyperbolic tangent, tanh, which will be sufficiently smooth for integration using Runge-Kutta schemes. The right-hand side of the ODE is parameterized by a fully connected layer -activation layer -fully connected layer, i.e.</p><formula xml:id="formula_4">F (z, θ) := A 2 σ( A 1 z + b 1 ) + b 2 where θ = vect(A 1 , A 2 , b 1 , b 2 ), i.e.</formula><p>the vectorization of all components of the matrices A 1 and A 2 and biases b 1 and b 2 . Therefore, the first layer of the MLP in the form F (D(N (x)), θ) takes a linear combination of candidate functions (applied to normalized data). Note that the dictionary does not include the constant term since we include a bias in the first fully connected layer. The function r is a regularizer on the parameters (for example, the 1 norm) and the time-derivative is penalized by the L 2 norm. When used, the parameters are set to β 1 = 10 −4 and β 2 = 10 −5 (no tuning is performed). The constraints in Eqn. (2.1) are written in continuous-time, i.e. the value of x(t) is defined by the ODE and thus can be evaluated at any time t ∈ [t 0 , t N ]. For a given set of parameters θ, the values x(t i ) are obtained by numerical integration (for example, using a Runge-Kutta scheme). To optimize Eqn. (2.1) using a gradient-based method, the back-propagation algorithm or the adjoint method (following <ref type="bibr" target="#b4">[5]</ref>) can be used. The adjoint method requires solving the ODE (and its adjoint) backward-in-time, which can lead to numerical instabilities. Following the approach in <ref type="bibr" target="#b7">[8]</ref>, checkpointing can be used to mitigate this issue.</p><p>For all experiments, we take the 'discretize-then-optimize' approach. The objective function, Eqn. (2.1), is discretized as follows:</p><formula xml:id="formula_5">min θ N i=1 L(x(t i )) + β 1 r(θ) +β 2 2 N −1 i=0 x(t i+1 ) − x(t i ) 2 2 (2.2) s.t. x(t 0 ) = x 0 , x(t i ) = Φ (i) (x(t 0 ), F (D(N (−)), θ))</formula><p>where Φ (i) is an ODE solver (i.e. a Runge-Kutta scheme) applied i-times,β 2 is β 2 rescaled by the time-step, and the time-derivative is discretized on the time-interval with the integral approximated by piece-wise constant quadrature. The constraint that the ODEẋ = F (D(N (x)), θ) is satisfied at every time-stamp has been transformed to the constraint that the sequence x(t i ) for 0 ≤ i ≤ N is generated by the forward evolution of an ODE solver. The ODE solver takes (as its inputs) the initial data x(t 0 ) and the function F that defines the RHS of the ODE. Note that the ODE solver can be 'sub-grid' in the sense that, over a time interval [t i , t i+1 ], we can set the solver to take multiple (smaller) time-steps. This will increase storage cost needed for back-propagation; however, taking multiple time-steps can better resolve complex dynamics embedded by F • D (see examples below). Additionally, the time-derivative regularizer helps to control the growth of the generative model, which yields control over the solution x(t) and its regularity.</p><p>Eqn. (2.2) is solved using the Adam algorithm <ref type="bibr" target="#b15">[16]</ref> with temporal mini-batches. Each minibatch is constructed by taking a random sampling of the time-stamps t i for i = 1, · · · , N − k and then collecting all data-points from x(t i ) to x(t i+k ) for some k &gt; 0. This can be easily extended to multiple time-series, by sampling over each trajectory as well. Note that, in experiments, taking non-overlapping sub-intervals did not change the results. The back-propagation algorithm <ref type="bibr" target="#b23">[24]</ref> applied to each of the subintervals [t i , t i+k ] can be done in parallel. For all of the regression  </p><formula xml:id="formula_6">examples, we set L(x(t i )) := ||x(t i ) −x i || 2 2 where {x i } N i=0</formula><p>is the given (sequential) data. For the image classification examples, we used the standard cross-entropy for L.</p><p>Remark 2.1. Layers: The right-hand side of the ODE is parameterized by one set of parameters θ. Therefore, in terms of DNN layers, we are technically only training one "layer". However, changes in the structure between time-steps are taken into account by the time-dependence, i.e. the dictionary terms D that contain t. Thus, we are embedding multiple-layers into the time-dependent dictionary.</p><formula xml:id="formula_7">Remark 2.2. 'Continuous-depth': Eqn. (2.</formula><p>2) is fully discrete when using a Runge-Kutta solver for Φ and its gradient can be calculated using the back-propagation algorithm. If we used an adaptive ODE solver, such as Runge-Kutta 45, the forward propagation would generate a new set of time-stamps (which always contain the time-stamps {t i } N i=0 ) in order to approximate the forward evolutionẋ = F (D(N (x)), θ), given an error tolerance and a set of parameters θ. We tested the continuous-depth versions, using back-propagation and the adjoint method (see Appendix A). Although the backward evolution of the ODE constraint may not be well-posed (i.e. numerical unstable or unbounded), our experiments lead to similar results. This could be due to the temporal mini-batches which enforce short-integration windows. Following <ref type="bibr" target="#b7">[8]</ref>, a checkpointing approach should be used, which would also control the memory footprint. It should be noted that, the adjoint approach was tested for the ODE examples but not for PDE examples, since the PDE examples are not time-reversible and will lead to backward-integration issues.</p><p>In addition, when the network is discrete, one may still consider it as a 'continuous-depth' approximation, since the underlying approximation can be refined by adding more time-stamps, without the need to retrain the MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Autonomous ODE.</head><p>When the time-series data is known to be autonomous, i.e. the ODE takes the formẋ = f (x), one can drop the t-dependency in the dictionary. In this case, the monomials take the form x <ref type="bibr" target="#b0">1</ref> 1 · · · x d d . We train this model by minimizing:</p><formula xml:id="formula_8">min θ N −1 i=0 ||x(t i ) −x i || 2 2 + β 1 θ 1 +β 2 2 N i=1 x(t i+1 ) − x(t i ) 2 2 (2.3) s.t. x(t 0 ) =x 0 , x(t i ) = Φ (i) (x(t 0 ), F (D(N (−), θ))</formula><p>wherex i is the given data (corrupted by noise) over the time-stamps t i for 0 ≤ i ≤ N . The true governing equation is given by the 3d Lorenz system:</p><formula xml:id="formula_9">    ẋ (t) = 10(y − x) y(t) = x(28 − z) − ẏ z(t) = xy − 8z/3 (2.4)</formula><p>which emits chaotic trajectories.</p><p>In <ref type="figure">Figure 2.1(a)</ref>, we train the model with 20 hidden nodes per layer using a quadratic dictionary, i.e. there are 9 terms in the dictionary, A 1 ∈ R 20×9 with 20 bias parameters, A 2 ∈ R 3×20 with 3 bias parameters, for a total of 263 trainable parameters. The solid curves are the time-series generated by a forward pass of the trained model. The learned system generates a high-fidelity trajectory for the first part of the time-interval. In <ref type="figure">Figure 2</ref>.1(b-c), we investigate the effect of the degree in the dictionary. In <ref type="figure">Figure 2</ref>.1(b), using a degree 1 monomial dictionary with 38 hidden nodes per layers, i.e. 3 terms in the dictionary, A 1 ∈ R 38×3 with 38 bias parameters, A 2 ∈ R 3×38 with 3 bias parameters (for a total of 269 trainable parameters), the generated curves trace a similar part of phase space, but are point-wise inaccurate. By increasing the hidden nodes to 100 per layer (3 terms in the dictionary, A 1 ∈ R 100×3 with 100 bias parameters, A 2 ∈ R 3×100 with 3 bias parameter, for a total of 703 trainable parameters), we see in <ref type="figure">Figure 2</ref>.1(c) that the method (using a degree 1 dictionary) is able to capture the correct point-wise information (on the same order of accuracy as <ref type="figure">Figure 2</ref>.1(a)) but requires more than double the number of parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Non-Autonomous ODE and Noise.</head><p>To investigate the effects of noise and regularization, we fit the data to a non-linear spiral:</p><formula xml:id="formula_10">    ẋ (t) = 2y(t) 3 y(t) = −2x(t) 3 z(t) = 1 4 + 1 2 sin(π t) (2.5)</formula><p>corrupted by noise. The third coordinate of Eqn. (2.5) is time-dependent, which can be challenging for many recovery algorithms. This is partly due to the redundancy introduced into the dictionary by the time-dependent terms. To generate <ref type="figure">Figure 2</ref>.2, we set:</p><p>(a) the dictionary degree to 1, with 4 terms, A 1 ∈ R 46×4 with 46 bias parameters, A 2 ∈ R 3×46 with 3 bias parameters (371 trainable parameters in total);</p><p>(b) the degree to 4, with 69 terms, A 1 ∈ R 4×69 with 4 bias parameter, A 2 ∈ R 3×4 with 3 bias parameters (295 trainable parameters in total), and the regularization parameter tõ β 2 = 10 −5 ;</p><p>(c) the degree to 4, with 69 terms, A 1 ∈ R 5×69 with 5 bias parameter, A 2 ∈ R 3×5 with 3 bias parameters (368 trainable parameters in total).</p><p>For cases (b-c), we set the degree of the dictionary to be larger than the known degree of the governing ODE in order to verify that we do not overfit using a higher-order dictionary and that we are not tailoring the dictionary to the problem. In <ref type="figure">Figure 2.2(a)</ref>, the dictionary of linear monomials with a moderately sized MLP seems to be insufficient for capturing the true nonlinear dynamics. This can be observed by the over-smoothing caused by the linear-like dynamics. In <ref type="figure">Figure 2</ref>.2(c), a nonlinear dictionary can fit the data and extract the correct pattern (the 'squared'off corners). <ref type="figure">Figure 2</ref>.2(b) shows that we are able to decrease the total number of parameters and fit the trajectory within the same tolerance as (c) by penalizing the derivative. Both (b) and (c) have achieved a mean-squared loss under 0.015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparison for Extracting Governing Models.</head><p>Comparison with SINDy. We compare the results of <ref type="figure">Figure 2</ref>.2 with an approximation using the SINDy algorithm from <ref type="bibr" target="#b3">[4]</ref> (theoretical results of convergence and relationship to the 0 problem appear in <ref type="bibr" target="#b40">[41]</ref>). These approaches differ, since the SINDy algorithm seeks to recover a sparse approximation to the governing system given one tuning parameter and is restricted to the span of the dictionary elements. To make the dictionary sufficiently rich, the degree is set to 4 as was done for <ref type="figure">Figure 2.2 (b-c)</ref>. Since the sparsity of the first two components is equal to one, we search over all parameter-space (up to 6 decimals) that yields the smallest non-zero sparsity. The smallest non-zero sparsity for the first component is 12 and for the second component is 3 with: which is independent of x and y and does not lead to an accurate approximation to the nonlinear spiral. This is likely due to the level of noise present in the data and the incorporation of the time-component. Comparison with LASSO-based methods. We compare the results of <ref type="figure">Figure 2</ref>.2 with LASSO-based approximations for learning governing equations <ref type="bibr" target="#b31">[32]</ref>. The LASSO parameter is chosen so that the sparsity of the solution matches the sparsity of the true dynamics (with respect to a dictionary of degree 4). In addition, the coefficients are 'debiased' following the approach in <ref type="bibr" target="#b31">[32]</ref>. The learned system is:</p><formula xml:id="formula_11">             </formula><formula xml:id="formula_12">    ẋ (t) = 1.8398y 3 y(t) = −1.9071x 3 z(t) = −0.1749x 2 y − 0.0058t 2 x − 0.0008t 2 x 2 (2.7)</formula><p>which matches the profile of the data in the (x, y)-plane; however, it does not predict the correct dynamics for z (emitting seemingly periodic orbits). While the LASSO-based approach better resolves the state-space dependence, it does not correctly identify the time-component.</p><p>Comparison with RNN. In <ref type="figure">Figure 2</ref>.3 the Lorenz system (see <ref type="figure">Figure 2</ref>.1) is approximated by our proposed approach and a standard LSTM (RNN), with the same number of parameters. Although the RNN learns internal hidden states, the RNN does not learn the correct regularity of the trajectories thus leading to sharp corners. It is worth noting that, in experiments, as the number of parameters increases, both the RNN and our network will produce sequences that approach the true time-series. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">ODE from Low-Rank Approximations</head><p>For certain spatio-temporal systems, reduced-order models can be used to transform complex dynamics into low-dimensional time-series (with stationary spatial modes). One of the popular methods for extracting the spatial modes and identifying the corresponding temporal-dynamics is the dynamic mode decomposition (DMD) introduced in <ref type="bibr" target="#b35">[36]</ref>. The projected DMD method <ref type="bibr" target="#b36">[37]</ref> makes use of the SVD approximation to construct the modes and the linear dynamical system. Another reduced-order model, known as the proper orthogonal decomposition (POD) <ref type="bibr" target="#b12">[13]</ref>, can be used to construct spatial modes which best represent a given spatio-temporal dataset. The projected DMD and the POD methods leverage low-rank approximations to reduce the dimension of the system and to construct a linear approximation to the dynamics (related to the spectral analysis of the Koopman operator), see <ref type="bibr" target="#b16">[17]</ref> and the citations within.</p><p>We apply our approach to construct a neural network approximation to the time-series generated by a low-rank approximation of the von Kármán vortex sheet. We explain the construction for this example here but for more details, see <ref type="bibr" target="#b16">[17]</ref>. Given a collection of measurements {u(x, y, t i )} N −1 i=0 , where (x, y) ∈ Ω ⊂ R 2 are the spatial coordinates and t i are the time-stamps, define X as the matrix whose columns are the vectorization of each u(x, y, t i ), i.e. X −,i := vect(u(x, y, t i )) and X ∈ R m×N where m is the number of grid points used to discretize Ω. The SVD of the data is given by X = U ΣV * , where U ∈ R m×m and V ∈ R N ×N are unitary matrices and Σ ∈ R m×N is a diagonal matrix. The best r-rank approximation of X is given by X r := U r Σ r V * r where Σ r ∈ R r×r is the restriction of Σ to the top r singular values and U r ∈ R m×r and V r ∈ R N ×r are the corresponding singular vectors. The columns of the matrix U r represent the r spatial modes that can be used as a low-dimensional representation of the data. In particular, we define the vector α ∈ R r by the projection of the data (i.e. the columns of X) onto the span of U r , that is:</p><formula xml:id="formula_13">α(t i ) := U * r X −,i+1 .</formula><p>Thus, we can construct the time-stampsα(t i ) from the measurements X and can train the system using a version Eqn. (2.1) with the constraint that the ODE is of the form:</p><formula xml:id="formula_14">α = A 0 α + f (α).</formula><p>The additional matrix A 0 ∈ R r×r resembles the standard linear structure from the DMD approximation and the function f can be seen as a nonlinear closure for the linear dynamics. The function f is approximated, as before, by F (D(N (−), θ). To train the model, we minimize:</p><formula xml:id="formula_15">min θ N −1 i=1 ||α(t i ) − U * r X −,i+1 2 2 + β 1 θ 1 +β 2 2 N −2 i=0 α(t i+1 ) − α(t i ) 2 2 (2.8) s.t. α(t 0 ) = U * r X −,1 , α(t i ) = Φ (i) (α(t 0 ), G(D(N (−)), θ))</formula><p>where G(D <ref type="figure">(N (α)</ref>, θ) = A 0 α + F (D <ref type="figure">(N (α)</ref>, θ) and θ also includes the trainable parameters from A 0 . Note that, to recover an approximation to the original measurements u(x, y, t i ), the vector U r α(t i ) is mapped back to the correct spatial ordering (inverting the vectorization process). In <ref type="figure">Figure 2</ref>.4, our approach with an 8 mode decomposition is compared to an 8 mode DMD approximation. The DMD approximation in <ref type="figure">Figure 2</ref>.4(a) introduces two erroneous vortices near the bottom boundary. Our approach matches the test data with higher accuracy, specifically, the relative L 2 error between our generated solution at the terminal time is 0.049 compared to DMD's relative error of 0.060. It is worth noting that this example shows the benefit of the additional term f (α) in the low-mode limit; however, using more modes, the DMD method becomes a very accurate approximation. Unlike the standard DMD method, our model does not require the data to be uniformly spaced in time.</p><p>(a) DMD method with 8 modes <ref type="bibr" target="#b17">[18]</ref> (b) Our method with 8 modes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Partial Differential Equations</head><p>A general form for a first-order in time, a-th order in space, nonlinear PDE is:</p><formula xml:id="formula_16">u t = G(t, x, u, Du, D 2 u, · · · , D a u),</formula><p>where D i u denotes the collection of all i-th order spatial partial derivatives of u for 1 ≤ i ≤ a.</p><p>We form a dictionary D([t, x, u, Du, D 2 u, · · · , D a u]) as done in Sec. 2, where the monomial terms now apply to t, x, u, and D i u for 1 ≤ i ≤ a. The spatial derivatives D i u as well as u t can be calculated numerically from data using finite differences. We then use an MLP, F , to parametrize the governing equation:</p><formula xml:id="formula_17">u t = F D([t,</formula><p>x, u, Du, D 2 u, · · · , D α u]), θ , (3.1) see also <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref>. In particular, the function F can be written as:</p><formula xml:id="formula_18">F (z, θ) = K 2 (σ(K 1 (z) + b 1 )) + b 2 (3.2)</formula><p>where K 1 and K 2 are collections of 1 × 1 convolutions, b 1 and b 2 are biases, θ are all the parameters from K and b , and σ is ELU activation function. The input channels are the monomials determined by t, x, u, and D i u, where t is extended to a constant 2d array. The first linear layer maps the dictionary terms to multiple hidden channels, each defined by their own 1 × 1 convolution. Thus, each hidden channel is a linear combination of input layers. Then we apply the ELU activation, followed by a 1 × 1 convolution, which is equivalent to taking linear combinations of the activated hidden channels. Note that this differs from <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref> in several ways. In the first linear layer, our network uses multiple linear combinations rather than the single combination as in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref>. Additionally, by using a (nonlinear) MLP we can approximate a general function on the coordinates and derivative; however, previous work defined approximations that model functions within the span of the dictionary elements.</p><p>To illustrate this approach, we apply the method to two examples: a regression problem using data from a 2d Burgers' simulation (with noise) and the image classification problem using the MNIST and MNIST-Fashion datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Burgers' Equation</head><p>We consider the 2d Burgers' equation,</p><formula xml:id="formula_19">u t + 0.5 div u 2 = 0.01∆u.</formula><p>The training and test data are generated on (t, x, y) ∈ [0, 0.015] × [0, 1] 2 , with time-step ∆t = 1.5 × 10 −5 and a 32 × 32 uniform grid. To make the problem challenging, the training data is generated using a sine function in x as the initial condition, while the test data uses a sine function in y as the initial condition. We generate 5 training trajectories by adding noise to the initial condition. Our training set is of size <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">100,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b31">32]</ref> and our test data is of size <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">100,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b31">32]</ref>. To train the parameters we minimize:</p><formula xml:id="formula_20">min θ N −1 i=0 u(x, y, t i ) −ũ(x, y, t i ) 2 2 (Ω d ) + β 1 θ 1 +β 2 2 N i=1 u(x, y, t i+1 ) − u(x, y, t i ) 2 2 (Ω d ) (3.3) s.t. u(x, y, t 0 ) =ũ(x, y, t 0 ), u(x, y, t i ) = Φ (i) (u(x, y, t 0 ), F (D(N (−)), θ)</formula><p>where Ω d is a discretization of Ω.</p><p>Training, Mini-batching, and Results. The mini-batches used during training are constructed with mini-batches in time and the full-batch in space. For our experiment, we set a (temporal) batch size of 16 with a length of 3, i.e. each batch is of size <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b31">32]</ref> containing 16 short trajectories. The points are chosen at random, without overlapping. The initial points of each mini-batch are treated as the initial conditions for the batch, and our predictions are performed over the length of the trajectory. This is done at each iteration of the Adam optimizer with a learning rate of 0.1.</p><p>In <ref type="figure">Figure 3</ref>.1, we take 2000 iterations of training, and evaluate our results on both the training and test sets. Each of the 1 × 1 convolutional layers have 50 hidden units, for a total of 2301 learnable parameters. For visualization, we plot the learned solution at the terminal time on both the training and test set. The mean-squared error on the full training set is 0.005 and on the test set is 3.6 (for reference, the mean-squared value of the test data is over 1000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image Classification: MNIST Data.</head><p>Another application of our approach is in reducing the number of parameters in convolutional networks for image classification. We consider a linear (spatially-invariant) dictionary for Eqn. (3.1). In particular, the right-hand side of the PDE is in the form of normalization, ReLU activation, two convolutions, and then a final normalization step. Each convolutional layer uses a 3 × 3 kernel of the form 6 i=1 a i k i , with 6 trainable parameters, where k i are 3 × 3 kernels that represent the identity and the five finite difference approximations to the partial derivatives D x , D y , D xx , D xy , and D yy . In CNN <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>, the early features (relative to the network depth) typically appear  to be images that have been filtered by edge detectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40]</ref>. The early/mid-level trained kernels often represent edge and shape filters, which are connected to second-order spatial derivatives. This motivates us to replace the 3 × 3 convolutions in ODE-Net <ref type="bibr" target="#b4">[5]</ref> by finite difference kernels.</p><p>Result shows that even though the trainable set of parameters are decreased by a third (each kernel has 6 trainable parameters, rather than 9), the overall accuracy is preserved (see <ref type="table" target="#tab_1">Table 3</ref>.1). We follow the same experimental setup as in <ref type="bibr" target="#b4">[5]</ref>, except that the convolutional layers are replaced by finite differences. We first downsample the input data using a downsampling block with 3 convolutional layers. Specifically, we take a 3 × 3 convolutional layer with 64 output channels and then apply two 3 × 3 convolutional layers with 64 output channels and a stride of 2. Between each convolutional layer, we apply batch-normalization and a ReLU activation function. The output of the downsampling block is of size 8 × 8 with 64 channels. We then construct our PDE block using 6 'PDE' layers, taking the form:</p><formula xml:id="formula_21">u(t) = G(t, u, θ i ) i ≤ t ≤ i + 1, i ∈ {0, · · · , 5}. (3.4)</formula><p>We call each subinterval (indexed by i) a PDE layer since it is the evolution of a semi-discete approximation of a coupled system of PDE (the particular form of the convolutions act as approximations to differential operators). The function G takes the form:</p><formula xml:id="formula_22">G(u, θ) = BN (K 2 ([t, BN (K 1 ([t, σ(N (u))]))])) (3.5)</formula><p>where BN (x) is batch-normalization, K is a collection of 3 × 3 kernels of the form 6 i=1 a i k i , θ contains all the learnable parameters, and σ(x) is the ReLU activation function. The PDE block is followed by batch-normalization, the ReLU activation function, and a 2d pooling layer. Lastly, a 64 × 10 fully connected layer is used to transform the terminal state (activated and averaged) of the PDE blocks to a 10 component vector.</p><p>For the optimization, the cross-entropy loss is used to compare the predicted outputs and the true label. We use the SGD optimizer with momentum set to 0.9. There are 160 total training epochs; we set the learning rate to 0.1 and decrease it by 1/10 after epoch 60, 100 and 140. The training is stopped after 160 epochs. All of the convolutions performed after the downsampling block are linear combinations of the 6 finite difference operators rather than the traditional 3 × 3 convolution.  <ref type="bibr" target="#b2">[3]</ref>. The test results are presented in <ref type="table" target="#tab_1">Table 3</ref>.2 and all algorithms are tested without the use of data augmentation. In both the MNIST and Fashion MNIST experiments, we show that the NeuPDE approach allows for less parameters by enforcing (continuous) structure through the network itself. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>We propose a method for learning approximations to nonlinear dynamical systems (ODE and PDE) using DNN. The network we use has an architecture similar to ResNet and ODENet, in the sense that it approximates the forward integration of a first-order in time differential equation. However, we replace the forcing function (i.e. the layers) by an MLP with higher-order correlations between the spatio-temporal coordinates, the states, and derivatives of the states. In terms of convolution neural networks, this is equivalent to enforcing that the kernels approximate differential operators (up to some degree). This was shown to produce more accurate approximations to complex timeseries and spatio-temporal dynamics. As an additional application, we showed that when applied to image classification problems, our approach reduced the number of parameters needed while maintaining the accuracy. In scientific applications, there is more emphasis on accuracy and models that can incorporate physical structures. We plan to continue to investigate this approach for physical systems.</p><p>In imaging, one should consider the computational cost for training the networks versus the number of parameters used. While we argue that our architecture and structural conditions could lead to models with fewer parameter, it could be potentially slower in terms of training (due to the trainable nonlinear layer defined by the dictionary). Additionally, we leave the scalability of our approach for larger imaging data set, such as ImageNet, to future work. For larger classification problems, we suspect that higher-order derivatives (beyond second-order) may be needed. Also, while higher-order integration methods (Runge-Kutta 4 or 45) may be better at capturing features in the ODE/PDE examples, more testing is required to investigate their benefits for imaging classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Derivation of Adjoint Equations</head><p>Let θ be the vector of learnable parameters (that parameterizes the unknown function g, which embeds all network features), then the training problem is:</p><formula xml:id="formula_23">min θ N i=1</formula><p>L(x(t i )) + β 1 r(θ) + where β 1 , β 2 &gt; 0 are regularization parameters set by the user. All subscripts with respect to a variable denote a partial derivative. We use the dot-notation for time-derivative for simplicity of exposition. The function r is a regularizer on the parameters (for example, the p norm) and the time-derivative is penalized by the L 2 norm. Define the Lagrangian L by:</p><formula xml:id="formula_24">L := N i=1 L(x(t i )) + β 1 r(θ) + β 2 2 t N t 0 |ẋ| 2 dτ − t N t 0 λ T (ẋ − g(x, τ ; θ))dτ</formula><p>where λ(t) ∈ BV [t 0 , t N ] is a time-dependent Lagrange multiplier. To apply gradient-based algorithms, the total derivative of the Lagrangian with respect to the trainable parameter must be calculated. Using integration by parts after differentiating with respect to θ yields:</p><formula xml:id="formula_25">dL dθ = N i=1 L x(t i ) (x(t i )) x θ (t i ) + β 1 r θ (θ) + β 2 t N t 0ẋ Tẋ θ dτ − N i=1 t i t i−1 λ T (ẋ θ − g x (x, τ ; θ)x θ − g θ (x, τ ; θ)) dτ = N i=1 L x(t i ) (x(t i )) x θ (t i ) + β 1 r θ (θ) − β 2 t N t 0ẍ T x θ dτ + β 2ẋ T x θ t N t 0 + N i=1 t i t i−1λ T x θ + λ T g x (x, τ ; θ)x θ + λ T g θ (x, τ ; θ) dτ − N i=1 λ T x θ t i t i−1</formula><p>The initial condition x(t 0 ) is independent of θ, so x θ (t 0 ) = 0. Define the evolution for λ between any two time-stamps [t i−1 , t i ] by:λ</p><formula xml:id="formula_26">T (t) = −λ T g x (x, t; θ) + β 2ẍ T then dL dθ = N i=1 L x(t i ) (x(t i )) x θ (t i ) + β 1 r θ (θ) + β 2ẋ T (t N )x θ (t N ) + t N t 0 λ T g θ (x, τ ; θ) dτ − N i=1 λ T x θ t i t i−1</formula><p>To determine λ at t N , we set λ T (t N ) = L x(t N ) (x(t N )) + β 2ẋ T (t N ) and at the right-endpoints of [t i−1 , t i ], we set: λ(t + i ) T = λ(t − i ) T + L x(t i ) (x(t i )). The derivative of the Lagrangian with respect to θ becomes:</p><formula xml:id="formula_27">dL dθ = β 1 r θ (θ) + t N t 0 λ T g θ (x, τ ; θ) dτ.</formula><p>Altogether, the evolution of λ is define by:</p><formula xml:id="formula_28">    λ T (t) = −λ T g x (x, t; θ) + β 2ẍ T , in [t i−1 , t i ] λ T (t N ) = L x(t N ) (x(t N )) + β 2ẋ T (t N ) λ T (t + i ) = λ T (t − i ) + L x(t i ) (x(t i )</formula><p>), for i = 1, · · · , N − 1 which can be re-written as:</p><formula xml:id="formula_29">    λ T (t) = −λ T f x (x, t) + β 2 (g x (x, t; θ)g(x, t; θ) + g t (x, t; θ)) T , in [t i−1 , t i ] λ T (t N ) = L x(t N ) (x(t N )) + β 2 g(x(t N ), t N ; θ) T λ T (t + i ) = λ T (t − i ) + L x(t i ) (x(t i )</formula><p>), for i = 1, · · · , N − 1</p><p>We augment the evolution for λ(t) with x(t), starting at t = t N and integrating backwards. The code follows the structure found in <ref type="bibr" target="#b4">[5]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 . 1 :</head><label>21</label><figDesc>Time-series data generated by the 3d Lorenz system and the corresponding learned processes using our approach. The original data (dashed) and the learned series (solid) are plotted, where red, green, and blue curves correspond to the x 1 , x 2 , and x 3 components, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Linear dictionary (b) Nonlinear dictionary with regularization and fewer parameters (c) Nonlinear dictionary without regularization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 . 2 :</head><label>22</label><figDesc>Extracting and modeling of a nonlinear time-dependent spiral. Noisy data is given in green and learned model is given in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 . 3 :</head><label>23</label><figDesc>Comparing dynamics between the true (green) time-series, the solution generated by our network (red), and a solution generated by an RNN (blue) with the same number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 . 4 :</head><label>24</label><figDesc>Learning reduced-order dynamics from fluid simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Training data.(b) Learned surface, test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 . 1 :</head><label>31</label><figDesc>Burgers' Equation Example. The surfaces at the terminal time simulated by the NeuPDE, with (a) training data and (b) the learned surface on the test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>β 2 2 t N t 0 |ẋ| 2</head><label>22</label><figDesc>dτ s.t. x(t 0 ) = x 0 ,ẋ = g(x, t; θ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ẋ (t) = −4278.0 + 9426.6z − 2204.6t − 7594.0z 2 + 3381.8tz − 351.1t 2 + ... 2650.6z 3 − 1659.3tz 2 + 285.5t 2 z − 339.0z 4 + 264.1tz 3 − 58.4t 2 z 2 y(t) = −79.1527 + 68.0904z − 14.4623z 2 z(t) = −53.0629 + 220.7608z − 168.9863t − 266.8949z 2 + 289.1066tz − 32.6971t 2 ...</figDesc><table /><note>+127.4432z 3 − 161.9778tz 2 + 31.9400t 2 z − 21.1582z 4 + 29.5282tz 3 − 7.6042t 2 z 2 (2.6)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>For a second test, we apply our network to the Fashion MNIST dataset. The Fashion MNIST dataset has 50000 training figures and 10000 test figures that can be classified into 10 different classes. Each data is of size 32 × 32. For our experiment, we did not use any data augmentation, except for normalization before training. The structure of NeuPDE in this example differs from the MNIST experiment as follows: (1) we downsample the data once instead of twice and (2) after the 1st PDE block, which has 64 hidden units, we use a 3 × 3 kernel with 64 input channels and 128 output channel, and then use one more PDE block with 128 hidden units followed by a 128 × 10 fully connected layer. There are numerous reported test results on the Fashion MNIST dataset<ref type="bibr" target="#b0">[1]</ref>; we compare our result to ResNet18<ref type="bibr" target="#b1">[2]</ref> and a simple MLP</figDesc><table><row><cell cols="3">1: Comparison Between Networks on MNIST</cell></row><row><cell></cell><cell>Method</cell><cell></cell></row><row><cell>Name</cell><cell cols="2">#Params. (M) Error(%)</cell></row><row><cell cols="2">MLP [20] 0.24</cell><cell>1.6</cell></row><row><cell>ResNet</cell><cell>0.60</cell><cell>0.41</cell></row><row><cell cols="2">ODENet 0.22</cell><cell>0.51</cell></row><row><cell>Our</cell><cell>0.18</cell><cell>0.51</cell></row><row><cell cols="2">3.3 Image Classification: Fashion MNIST.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">2: Comparison on Fashion MNIST</cell></row><row><cell></cell><cell>Method</cell><cell></cell></row><row><cell>Name</cell><cell cols="2">#Params. (M) Error (%)</cell></row><row><cell cols="2">MLP 256-128-100 0.248</cell><cell>11.67 [3]</cell></row><row><cell>ResNet18</cell><cell>2.78</cell><cell>8.0 [2]</cell></row><row><cell>Our</cell><cell>0.38</cell><cell>7.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors would like to acknowledge the support of AFOSR, FA9550-17-1-0125 and the support of NSF CAREER grant #1752116. We would like to thank Scott McCalla for providing feedback on this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mnist</forename><surname>Fashion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<ptr target="https://github.com/zalandoresearch/fashion-mnist" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mnist</forename><surname>Fashion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnet18</surname></persName>
		</author>
		<ptr target="https://github.com/kefth/fashion-mnist" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<ptr target="https://github.com/heitorrapela/fashion-mnist-mlp" />
	</analytic>
	<monogr>
		<title level="j">MLP for Fashion MNIST</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discovering governing equations from data by sparse identification of nonlinear dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">L</forename><surname>Steven L Brunton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Nathan</forename><surname>Proctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="3932" to="3937" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep learning for physical processes: Incorporating prior scientific knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>De Bezenac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Pajot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07970</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A proposal on machine learning via dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematics and Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">ANODE: Unconditionally accurate memoryefficient gradients for neural ODEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Biros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10298</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning, invariance, and generalization in high-order neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied optics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="4972" to="4978" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Turbulence, coherent structures, dynamical systems and symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gahl</forename><surname>Lumley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clarence</forename><forename type="middle">W</forename><surname>Berkooz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rowley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dynamic Mode Decomposition: Data-driven modeling, Equation-free modeling of Complex systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Kutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bingni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">L</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Proctor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic mode decomposition: data-driven modeling of complex systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nathan Kutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bingni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">L</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Proctor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">FractalNet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pde-net 2.0: Learning PDEs from data with a numeric-symbolic hybrid deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09668</idno>
		<title level="m">Pde-net: Learning PDEs from data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxiao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10121</idno>
		<title level="m">Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Autograd: Reverse-mode differentiation of native python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop on Automatic Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailiang</forename><surname>Tong Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05537</idno>
		<title level="m">Data driven governing equations approximation using deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hidden physics models: Machine learning of nonlinear partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Machine learning of linear differential equations using Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="page" from="683" to="693" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10566</idno>
	</analytic>
	<monogr>
		<title level="m">Data-driven discovery of nonlinear partial differential equations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>part ii</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Data-driven discovery of partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">L</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Nathan</forename><surname>Proctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1602614</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep neural networks motivated by partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning partial differential equations via data discovery and sparse optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayden</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">473</biblScope>
			<biblScope unit="page">20160446</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse model selection via integral terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayden</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccalla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">23302</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning dynamical systems and bifurcation via group sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayden</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giang</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Ward</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01558</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Extracting sparse high-dimensional dynamics from limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayden</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giang</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3279" to="3295" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dynamic Mode Decomposition of numerical and experimental data. Bulletin of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Sesterhenn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>American Physical Society</publisher>
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic mode decomposition of numerical and experimental data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of fluid mechanics</title>
		<imprint>
			<biblScope unit="volume">656</biblScope>
			<biblScope unit="page" from="5" to="28" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The pi-sigma network: An efficient higher-order neural network for pattern classification and function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoan</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN-91-Seattle International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1991" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exact recovery of chaotic systems from highly corrupted data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giang</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1108" to="1129" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayden</forename><surname>Schaeffer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09885</idno>
		<title level="m">Forward stability of resnet and its variants</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the convergence of the sindy algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayden</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="948" to="972" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
