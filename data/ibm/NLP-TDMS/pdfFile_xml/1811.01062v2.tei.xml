<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmenting Compositional Models for Knowledge Base Completion Using Gradient Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Lalisse</surname></persName>
							<email>lalisse@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept of Cognitive Science</orgName>
								<orgName type="institution">Johns Hopkins University Baltimore</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
							<email>smolensky@jhu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept of Cognitive Science</orgName>
								<orgName type="institution">Johns Hopkins University &amp; Microsoft Research AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Augmenting Compositional Models for Knowledge Base Completion Using Gradient Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural models of Knowledge Base data typically employ compositional representations of graph objects: entity and relation embeddings are systematically combined to evaluate the truth of a candidate knowledge base entry. Using a model inspired by Harmonic Grammar, we propose to tokenize triplet embeddings by subjecting them to a process of optimization with respect to learned well-formedness conditions on knowledge base entries. The resulting model, known as Gradient Graphs, leads to sizable improvements when implemented as a companion to compositional models. The "supracompositional" triplet token embeddings it produces have interpretable properties that prove helpful in performing inference on the resulting representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As they are conventionally analyzed, representations of semantic or linguistic data are "compositional": the meanings of complex representations are built up from the meanings of their constituent parts. This idea has motivated numerous models of graph data deployed in knowledge base completion (KBC), in which embeddings of entities and relations are combined into composite representations-pairs of entities in a particular relation with one another-that are built up systematically from the constituent parts. But what happens when the whole is not a simple function of the parts? A natural case arises in the interpretation of Noun-Noun compounds. The contrasting senses of vampire cat (a-cat-that-is-a-vampire) and vampire stake (a-stake-used-to-kill-a-vampire) has as much to do with the compatibility of the contituent nouns occurring in a given relation than with the meanings of the individual constituents.</p><p>Pursuing this line of thought, we propose Gradient Graphs, a neural network model for KBC built on the principle that compositionally-obtained representations of semantic objects can be optimized to reflect context-specific aspects of the meanings of their constituents. The issue of context-conditioned, tokenized semantic representations has received little explicit attention in the KBC literature. However, precedents do exist. <ref type="bibr" target="#b5">Bordes et al. (2011)</ref> model context-sensitive entity senses by embedding relations as pairs of matrices (R lhs , R rhs ) that linearly transform entity embeddings into pairs of embeddings defined by the relation and the entities' positions within it (the lefthand-side or right-hand-side). The distances of the resulting embeddings are then compared. <ref type="bibr" target="#b18">Socher et al. (2013)</ref> cope with the context-sensitivity of relation meanings by learning a k ×d×d-dimensional tensor embeddings for each relation, letting their model represent polysemy by learning k versions of the relation represented in the k slices of its embedding tensor. The intuition underlying this approach is that, for instance, the relation has part has a different sense when applied to a biological organism than when predicated of a company. While the former has parts like organs and limbs, the latter has parts like subsidiaries and workers, which occupy very different parts of the semantic space. Each relational slice is then responsible for learning the compatibility of arguments within particular semantic subspaces.</p><p>In contrast to these other works, our approach is more radical in the sense that our context-sensitive representations of knowledge base entries are not just computed from the entries' constituent elements (entity and relation embeddings), but are instead the result of a representation-optimization procedure that balances compositionally-derived representations with general knowledge about the characteristics of well-formed semantic structures. We show that this additional "supracompositional" processing, in addition to yielding sizable accuracy improvements over the compositional models we apply it to, leads to embeddings of entity tokens with interpretable characteristics. arXiv:1811.01062v2 [cs.CL] 12 Aug 2019 1.1 Layout of the paper Section 2 lays out the general framework, which is compatible with a variety of implementations. Section 3 presents two compositional embedding models proposed in the literature. We adapt these models to construct compositional embeddings, and in Section 4 report evaluations of Gradient versions of these models. Section 5 discusses the characteristics of the resulting semantic representations in greater detail, as well as their role in assisting inference. Section 6 concludes. Technical details about the model and the implementations are given in the Appendices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Optimization of semantic tokens</head><formula xml:id="formula_0">λ λ λ λ λ fcomp fcomp</formula><p>x h e e r r <ref type="figure">Figure 1</ref>: Gradient Graph as a recurrent neural network. In addition to bias terms (omitted in the figure) and self-connections, hidden units are densely connected to one another via a layer of connections with symmetric (undirected) weights, and receive constant input weighted by λ from a single unit in the input layer. The composition function f comp (e , r, e r ), which differs between implementations, computes a compositional embedding x, which is fed into a hidden layer h of the network. The continuous-time dynamics of this network compute an internal representationĥ of the input triplet that is optimal with respect to the Harmony (1)-a measure of the triplet's semantic well-formedness.</p><p>The hypothesis underlying the approach we propose is that noncompositional effects in knowledge base data can be modeled by subjecting candidate facts to a process of optimization with respect to a set of learned semantic coherence conditions. These semantic coherence conditions, encoded in a symmetric matrix, map out the covariance structure of the semantic space, indicating which semantic features are likely to co-occur with one another. The embedding of a given triplet is then the vector obtained by optimizing the semantic coherence of the the triplet embedding.</p><p>We first lay out the model in abstract form, before introducing particular implementations. Let x ∈ R d be a d-dimensional embedding of a knowledge base triplet (e , r, e r ) obtained as some function f comp -the composition function-of the embeddings of the left and right entities as well as the relation r. Section 3 provides several models for constructing the triplet embedding x. Also, let h be a d-dimensional vector giving the internal ("hidden") state of the network. The Harmony of an internal state h of the network with respect to the triplet embedding x is</p><formula xml:id="formula_1">(1) H(h, x) = 1 2 h Wh + b h−λ(h − x) (h − x)</formula><p>where W is a d × d weight matrix with W = W and b is a bias vector, both learned. (1) is composed of two terms: Core Harmony, a measure of the semantic coherence of the state vector h, and Faithfulness, a penalty incurred due to the state h's deviation from the compositional triplet embedding x. λ is a hyperparameter that controls the magnitude of the penalty incurred for straying from x. H(h, x) may be rewritten as (2).</p><p>(2)</p><formula xml:id="formula_2">H(h, x) = 1 2 [h (W − λI) h + (b + 2λx) h −λx x)]</formula><p>If λ is greater than the largest eigenvalue of W, then V = W − λI is negative-definite, and H(h, x) has a unique global optimumĥ = argmax h H(h, x) for each x. In closed form, this global optimum is</p><formula xml:id="formula_3">(3) µ(x) = −V −1 1 2 b + λx</formula><p>which depends only on the network parameters and on x. The expression µ(x) comes from observing thatĥ is the mean of a Gaussian distribution with inverse covariance matrix V , which implies thatĥ is the most probable state h of the network with respect to the probability distribution over the state space defined by p(h|x) ∝ exp{H(h, x)} (see Appendix A). We take the token embedding for a triplet x to be µ(x), which is the most semantically coherent triplet embedding given the compositional triplet x. In the limit as λ → ∞, µ(x)</p><p>is just x itself. Let λ W denote the largest eigenvalue of W; then as λ → λ W , µ(x) may become arbitrarily far from the triplet embedding x.</p><p>A Gradient Graph may be viewed as a neural network with weight matrix W and bias vector b 2 , where the synaptic weights W specify a feedback layer through which the values of the hidden state units affect one another. The construction is as follows. We stipulate that the hidden state of the network follows the gradient of Harmony over time:</p><formula xml:id="formula_4">(4) dh dt = ∂H(x,h) ∂h</formula><p>Therefore,</p><formula xml:id="formula_5">dh i dt = d dh i 1 2 h Wh + b h − λ x − h 2 = d dh i 1 2   jk h j W jk h k + b i h i − λ(x i − h i ) 2   = 1 2   j h j W ji + k W ik h k   + b i 2 + λx i − λh i</formula><p>The above specifies the connectivity of a network whose hidden units have the linear transfer function (f (input) = input), bias b 2 and external input x (weighted by λ). Each h i also receives selfinhibitory input weighted by −λ, as well as inputs W ij h j from each h j . The symmetry of W implies that each term W ij h j = h j W ji occurs twice, so that the factor of 1 2 cancels. This connectivity structure is illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relation to Harmonic Grammar</head><p>In addition to being globally optimal with respect to the Harmony function H(h, x) conditioned on a particular input x, µ(x) is the unique fixed point of this network's state-evolution dynamics. It is interesting to note that such networks are the connectionist foundation for Harmonic Grammar (HG) and Optimality Theory (OT) in <ref type="bibr">Linguistics (Smolensky and Legendre, 2006)</ref>, where the dynamics of a neural network perform optimization over internal representations of an input structure. Appropriate output representations are then selected in accordance with well-formedness constraints encoded in the network parameters. There, the output representation balances Faithfulness to the input (an Underlying Form) and the network's knowledge about the characteristics of well-formed structures in general.</p><p>Similarly, it is appealing to conceptualize the hidden layer of a Gradient Graph network as cleaning up a knowledge base triplet by subjecting it to semantic well-formedness conditions. The optimal triplet µ(x) is then the point to which the network converges in the limit of infinite computation time. However, our model differs from typical implementations of HG and OT in that the optimal structure µ(x) does not, in general, decompose into a unique combination of the input constituents (entity and relation embeddings). The resulting representations are in this sense gradient, rather than being the product of a combination of discrete objects. Furthermore, Gradient Graphs are, to our knowledge, the first application of these ideas to the automatic learning of an appropriate semantic optimization function from a large amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Comparison with translation-based approaches</head><p>Like a large class of Translation-based models <ref type="bibr" target="#b5">(Bordes et al., 2011;</ref><ref type="bibr">Yoon et al., 2016;</ref><ref type="bibr" target="#b10">Lin et al., 2015;</ref><ref type="bibr" target="#b7">Ji et al., 2016)</ref>, our inference procedure consists of the application of an affine transformation to an input x (Equation <ref type="formula">(3)</ref>), which is then scored using some regular operation. In our case, this scoring function is quadratic. A particular close cousin is the bilinear Semantic Matching Enery (SME) method of <ref type="bibr" target="#b3">Bordes et al. (2014)</ref>, which learns a global third-order tensor W that, when dotted along the third mode with a relation embedding r, yields a relation-specific matrix W r . Along with learned left and right bias vectors b snd b r , this weight matrix is fed into the bilinear scoring function <ref type="formula">(5)</ref>:</p><formula xml:id="formula_6">(5) score SME (e , r, e r ) = (W r e + b ) (W r e r + b r )</formula><p>Expanding out this expression, we get (6).</p><formula xml:id="formula_7">(6) e W r W r e r +b W r e r +b r W r e l +b b r</formula><p>The relation-specific bilinear form W r W r is, like our global W matrix, symmetric. The remaining terms, apart from the constant b b r , compute a pair of relation-specific bias vectors b W r and b r W r applied to the pair of entity embeddings. The resulting Energy function used to score triplets has more than a passing similarity to our Harmony function (1) when λ = ∞ and, thus, no optimization takes place. A distinctive characteristic of our approach in relation to these structurally similar models is that the transformation undergone by a Gradient Graph triplet is directly connected to the wellformedness criterion according to which triplets are evaluated in inference. As illustrated in the Discussion, our transformation of a compositional triplets using learned well-formedness criteria leads to two kinds of triplet embeddings: compositionally obtained type embeddings, and contextually optimized token embeddings. In qualitative and quantitative analyses of the learned representation, we see (1) that the space of compositionally obtained triplet embeddings has a reasonable structure, independently of the optimizing transformation, that is already sensitive to the context supplied by the relation, and (2) that semantic optimization improves these compositional representations in recognizable ways. Interestingly, im-proving triplets with respect to the Harmony function does not uniformly place them in regions that are high-Harmony in a global sense. In fact, we find that whereas positive triplets end up close to other positive triplets, plausible but negative triplets tend to be detained in clusters with other negative instances <ref type="table" target="#tab_1">(Tables 2 and 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Compositional and Gradient Models</head><p>Optimization with respect to H can be implemented wherever we can construct a triplet embedding x. In our experiments, we apply Harmonic optimization of triplet representations to two compositional embedding models drawn from the knowledge base completion literature: Dist-Mult and HolE. Both models specify a scoring function for triplet embeddings obtained via operations applied to embeddings of the three triplet components-two entity vectors and a relation vector-with no additional learned components apart from these representations of the triplet constituents. We take the terms occurring in these scoring functions to be components of the representation of the triplet, specifying what information about the triplet elements is important to evaluating the triplet's quality. Hence, we constructed Harmonic triplet embeddings according to the desideratum that every term occurring in the basic method's scoring function should also appear in the triplet representation x in the Harmonic model. For instance, the score of a DistMult triplet is a sum of three-way products of the corresponding elements of the embeddings e , r, and e r . Setting the products [e ] i [r] i [e r ] i to appear in our compositional triplet embeddings (as in Eqn <ref type="formula">(8)</ref>) satisfies this desideratum.</p><p>DistMult <ref type="bibr">(Yang et al., 2015</ref>) is a baseline model for scoring knowledge base triplets using the scoring function <ref type="formula">(7)</ref>:</p><formula xml:id="formula_8">(7)</formula><p>score DistMult (e , r, e r ) = e diag(r)e r where e , r, e r are d-dimensional embeddings and diag(r) is the d × d-dimensional matrix obtained by arranging the elements of r along the diagonal. <ref type="bibr" target="#b8">Kadlec et al. (2017)</ref> have recently shown that DistMult can outperform many more complicated scoring functions when hyperparameters are properly optimized, making it a strong baseline comparison for the method we propose. In addition, DistMult often occurs as a subcomponent in state-of-the-art KBC models-e.g. <ref type="bibr" target="#b16">(Schlichtkrull et al., 2017;</ref><ref type="bibr" target="#b19">Toutanova et al., 2015)</ref>. From this starting-point, we construct Harmonic Dist-Mult (HDistMult) by setting the triplet embedding x to the elementwise multiplication of the relation and the pair of entity vectors:</p><p>(8)</p><p>x HDM = e r e r where denotes elementwise multiplication.</p><p>Holographic Embeddings (HolE) were introduced by <ref type="bibr" target="#b13">Nickel et al. (2016)</ref> building on theoretical work by <ref type="bibr" target="#b15">(Plate, 1995)</ref>, as a means of constructing compressed tensor product representations of relational triplets. The method computes the score for a triplet (e , r, e r ) from the similarity between a relation vector and the circular correlation e e r of the entity vectors and a relation vector:</p><formula xml:id="formula_9">(9)</formula><p>score HolE (e , r, e r ) = r (e e r )</p><p>where the circular correlation of e and e r is computed as (10).</p><p>(10) e e r = F −1 F(e ) F(e r )</p><p>F and F −1 denote the Fourier Transform and its inverse, and F(e ) is the complex conjugate of F(e ). 1 Circular correlation is asymmetric (e e r = e r e )-allowing it to model asymmetric relations-and the result of the operation has the same dimensionality as the input vectors, while still carrying information about which pair of entities was bound together via correlation. We construct Harmonic HolE (HHolE) triplet embeddings via elementwise multiplication of relation vectors with the correlated pair of entity vectors:</p><p>(11)</p><p>x HHolE = r (e e r )</p><p>In both Harmonic models, the score for a candidate triplet (e , r, e r ) with embedding x is calculated by taking the Harmony of its optimal instantiation,ĥ = µ(x), i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(12)</head><p>score(x) = H(µ(x), x)</p><p>In the experiments, we train our networks using the log-softmax objective with negative sampling. For each positive training example (e , r, e r ) with embedding x, we construct N negative examples (ẽ n ,r n ,ẽ n r ) obtained by deleting either the left or right entity of the true triplet and replacing it with a randomly sampled entity vector. Letx n denote the embedding of the nth negatively sample triplet 1 The Fourier transform decomposes a function of time into its frequency components. In the context of holographic embeddings, its utility comes from the Convolution Theorem, which states that convolution in the time domain corresponds to elementwise multiplication in the frequency domain. This is useful in actual computations. The circular correlation-which consists of convolution with a time-reversed signal-can also be computed as a sum over off-diagonals of the tensor product of vectors, with time complexity O(d 2 ). In contrast, the Fast Fourier Transform (FFT) has time complexity O(n log n) <ref type="bibr" target="#b13">(Nickel et al., 2016)</ref>.</p><p>(ẽ n ,r n ,ẽ n r ). The training objective is then to minimize <ref type="formula">(13)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluated Gradient Graphs using the standard WN18 and FB15K datasets <ref type="bibr" target="#b4">(Bordes et al., 2013)</ref>-which are subsets of the WordNet <ref type="bibr" target="#b12">(Miller, 1995)</ref> and Freebase <ref type="bibr" target="#b2">(Bollacker et al., 2008)</ref> databases-on the Entity Reconstruction task. In Entity Reconstruction, the network ranks completions of triplets ( · , r, e r ) and (e , r, · ) with deleted left and right entities. The model is successful if it ranks the true triplet above other candidate completions. We report results in the filtered evaluation setting <ref type="bibr" target="#b4">(Bordes et al., 2013)</ref>, in which a test triplet is only ranked against triplets that do not occur in the database. The rank of a test triplet is thus the rank of the first correct answer to the query. For both DistMult and HolE, we report the originally reported results alongside results for our reimplementations, comparing these models with our Harmonic variants HDistMult and HHolE with and without optimization of hidden layer representations. The Harmonic models with λ = ∞ have the Harmony function H(x, x), i.e. where the hidden representation is just the compositional embedding itself and the Faithfulness penalty in (1) is 0.</p><p>Our models used 256-to 512-dimensional embeddings and manually tuned values of the hyperparameter λ. In all models, entity and relation embeddings were normalized to v = 1. We do not regularize parameters, but instead set an upper bound λ− ( a small constant) on the l 2 norm of the weight matrix W, which helps constrain the spectral norm (maximum eigenvalue) of W to remain lower than λ. This may be seen as adopting a uniform prior on weight matrices lying within the n-ball with squared radius λ − . Importantly, this procedure keeps the matrix V = W − λI negativedefinite-a necessary condition for the existence of a unique optimum for H(h, x).</p><p>Results from the experiments are reported in <ref type="table" target="#tab_0">Table 1</ref>. Overall, we found that models using our quadratic scoring function (1) to perform best across the board. This effect was particularly seen in more stringent evaluation criteria-Hits@1 and Hits@3-leading to, for instance-a 15% improvement in Hits@1 (accuracy) on Freebase between our DistMult reimplementation and quadratic HDistMult (λ = ∞). The bestDistMult models were those with high λ values; however, withinmodel comparison of HolE shows dramatic improvements from including the optimization component-a 32% increase in FB15K accuracy between the results of <ref type="bibr" target="#b13">(Nickel et al., 2016)</ref> and our HHolE with a permissive λ-criterion of 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In part, the appeal of our supracompositional representations stems from their ability to produce emeddings of tokens of semantic objects-that is, embeddings that take into account the context of a particular instance of a semantic type. Tokenized embeddings have proven useful in various settings. For instance, <ref type="bibr" target="#b6">Dasigi et al. (2017)</ref> construct token embeddings by superposing learned vectors for WordNet senses in ratios determined by a probability distribution computed from the context. The resulting representation is a contextweighted sum of discrete senses drawn from a handcrafted ontology. Closer to our approach, <ref type="bibr" target="#b1">Belanger and Kakade (2015)</ref> model text as a linear dynamical system that generates texts through transitions of a continuous-state, discrete-time dynamical system across time. Estimates of the system's most probable internal state can then be extracted as an embedding of the tokens, which prove useful in language modeling and other downstream tasks.</p><p>In our framework, types correspond to static entity and relation embeddings that are the input to f comp , and the triplet embeddings resulting from their combination. Token triplet embeddings are produced by optimization of the hidden layer of a GG. To understand the effect of optimizing the hidden layer of a GGraph both on its learned representations and on its performance in inference, we used the best-performing trained HHolE model to produce token embeddings of database triplets in order to inspect their semantic neighborhoods.</p><p>For a given compositional triplet embedding f comp (e , r, e r ) ≡ x, we first computed the optimized triplet representation µ(x) ≡ĥ using Equation (3). Treatingĥ as the contextually optimal (token) embedding of the triplet (e , r, e r ), we then examined the semantic neighborhood by computing the 5 closest optimized embeddings in the context of the same relation. <ref type="table" target="#tab_4">Table 4</ref> shows the semantic neighborhoods of compositional triplets x and optimized tripletsĥ for different possible completions of a number of queries. Rows 1 and 2 display completions of the query ( · , office position or title, US President), and Rows 2 and 4 consider the neighborhood of the McCain and Hilary Rodham Clinton have type embeddings that are close to actual presidents. This is sensible since, for instance, Hilary Rodham Clinton is married to Bill Clinton-one of her nearest neighbors. But both of the negative examples' token embeddings have neighborhoods that are mostly cleared of actual presidents-despite having type embedding neighborhoods that are relatively dense with presidents.</p><p>Turning to the second half of the table, we note that Bob Dylan's type embedding is already in a neighborhood dense with singer-songwriters.</p><p>It is appropriate, then, that this neighborhood undergoes no change apart from minor re-ranking when the triplet (Bob Dylan,has profession,singer-songwriter) is optimized. For more difficult cases, however, where Dylan is not a prototypical example, the semantic neighborhoods undergo dramatic reconfiguration.</p><p>For instance, optimizing the triplet (Bob Dylan,has profession,disc jockey) correctly places Dylan in the neighborhood of other DJs, despite the implausibility of this association in the neighborhood of his type embedding, which contains no DJs. This places him in the token neighborhood of Moby, who is otherwise quite unlike Bob Dylan except in respect of their common career as DJs.</p><p>Combined with our finding that optimization yields the most dramatic improvements in the more stringent evaluation criteria (Hits@1 and Hits@3), this suggests that our optimization procedure is particularly helpful in arbitrating between difficult cases. This qualitative observation about the neighborhoods of compositional and supracompositional triplets can be quantified. Using the triplet classification dataset introduced by <ref type="bibr" target="#b18">Socher et al. (2013)</ref>, which contains an equal number of positive and negative triplets, we find <ref type="table" target="#tab_1">(Table 2</ref>) that positive triplets, on average, end up in supracompositional neighborhoods that are more dense in positive examples than their compositional counterparts. On the other hands, negative triplets suffer a decrease in the number of positive triplets in the neighborhoods of their supracompositional embeddings.</p><p>To further quantify the role of semantic optimization in inference, we correlated the difference between the Harmony (score) of input triplets preand post-optimization with the change in its rank on the FB15K dataset. The change in Harmony is computed as ∆H = H(µ(x), x) − H(x, x), i.e. the difference between the Harmony of the token embedding and the Harmony of the type embedding. This comparison is model-internal-it does not compare models trained to do token inference with models trained for type inference. However, it serves as a useful index of the performance gains attributable to the optimization procedure. If optimizing a triplet representation indeed improves its relative position among all candidate triplets, we expect changes in Harmony to be nega-∆density t statistic p Pos 0.241 t = 99.7 p 10 −10</p><p>Neg -0.059 t = −62.4 p 10 −10 tively correlated with the change in rank of positive triplets. Consistent with this, we find that optimization leads to significant improvements in raw rank in our best trained HHolE model (Spearman's ρ = −0.0157, p &lt; 10 −6 , <ref type="figure">Figure 5</ref>). When considering the change in Mean Reciprocal Rank, a more standard evaluation metric, we find that ∆H is positively associated with improvements in MRR ρ = .1370, p 10 −10 , 2 particularly when triplets whose ranks do not change at all are omitted ρ = .3746, p 10 −10 . In other words, when semantic optimization makes a difference, it does so for the better.</p><p>For HDistMult, ∆H is significantly associated with increases in the rank of true triplets ρ = 0.1226, p 10 −10 , a result consistent with our finding that this class of models disprefers low settings of λ. This illustrates the importance of choices of representational format for embeddings of semantic data. Our optimization procedure can only operate over information that is contained in its compositional input. Hence, choices about how to combine the learned features of entities and relations-i.e. about the manner of composition-are central to our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Desiderata of a composition function</head><p>What factors affect the success of semantic optimization in combination with a particular composition scheme? We suspect that multiplicative interactions across embedding components-which are present in HHolE and absent in HDist-Mult-are essential for our optimization proce-  <ref type="formula">)</ref>). This is plotted against rank q (µ(x)) − rank q (x), the triplet's change in rank due to optimization for query q. A negative correlation indicates reductions in rank (improvements) associated with increasing optimization of triplet representations. dure to contribute helpfully to inference. Both DistMult and HolE are special cases of contracted Tensor Product Representations (TPRs), obtained by summing over (HolE) or discarding (DistMult) terms from the three-way tensor product e ⊗ r ⊗ e r . 3 In particular, DistMult retains only multiplicative interactions within components, omitting terms with non-matching indices. This fact appears to be crucial. In a followup experiment, we implemented a series of full TPR models trained on FB15K, using the composition operation <ref type="formula">(14)</ref>: <ref type="bibr">(14)</ref> x HTPR = e ⊗ r ⊗ e r Such models are necessarily small in size due to the rapid growth of dimensionality for TPRs as a function of the dimensionality of entity and relation embeddings. Consequently, their performance is also poor in comparison to our other implementations. However, the trend matched that which we observed within the HHolE class: models including the optimization procedure consistently outperformed those with λ = ∞ (see <ref type="table" target="#tab_3">Table  3</ref>). From this, we conclude that other embeddingbased KBC models incorporating cross-component multiplicative interactions are likely to see improvements from the addition of a semantic optimization step prior to scoring.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed Gradient Graphs, a general method for augmenting compositional representations of Knowledge Graphs with a postcomposition procedure that optimizes the wellformedness of triplet embeddings, highlighting the model's connection to Harmonic Grammar and Optimality Theory. The resulting model shows marked improvements over the compositional models it is implemented alongside, and also produces triplet token embeddings with properties that prove useful for inference about knowledge base entities. In future work, we intend to explore the utility of semantically-optimized token embeddings in other linguistic settings. We define the Harmony of hidden state h with respect to triplet embedding x as in <ref type="formula">(1)</ref>:</p><formula xml:id="formula_10">H(h, x) ≡ 1 2 h Wh + b h − λ(h − x) (h − x) = 1 2 h (W − λI) h + (b + 2λx) h − λx x ≡ 1 2 h V h + m(x) h − λx x</formula><p>Completing the square yields:</p><formula xml:id="formula_11">H(h, x) = 1 2 h − −1 2 V −1 m(x) V h − −1 2 V −1 m(x) + 1 2 −λx x − 1 4 m(x) V −1 m(x) ≡ 1 2 (h − µ(x)) V (h − µ(x)) + (x)</formula><p>which is valid because V = W − λI is symmetric. (x) does not depend on h, so it is sufficient to optimize 1 2 (h − µ(x)) V (h − µ(x)) . Setting ∂H(h,x) ∂h = 0 yields 2V (h − µ(x)) = 0; ∴ h = µ(x). Since V is negative-definite, this point is a maximum.</p><p>The truth of the claim may be more quickly perceived by observing that H(h, x) defines a Gaussian distribution over the hidden state variable h with mean µ(x) and precision matrix Σ −1 ≡ −V . The optimality of µ(x) then follows from the unimodality of Gaussians.</p><p>The training objective (13) may be justified by the following considerations. We take the compositional triplet data to be generated by hidden states of the gradient graph network, and maximize the log probability of the training data using the maximum a posteriori point estimate of the hidden state h. x ∈χq exp{ (x )} For given parameters, the denominator is constant. So, renormalizing over the discrete triplets χ q gives:</p><p>p(µ(x), x|q) = exp{H(µ(x), x)}</p><p>x ∈χq exp{H(µ(x), x)} Approximating the discrete distribution over all of χ q with a negative sample yields the objective (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Appendix B: Implementation details</head><p>In initial experiments, we searched through a number of candidate models. These included two Harmonic variants of the Rescal model <ref type="bibr" target="#b14">(Nickel et al., 2011)</ref>, as well as models that constructed x as a simple concatenation of entity and relation vectors, as well as three-way tensor products of these vectors. These initial experiments led us to focus on DistMult and HolE as the best-performing candidates. Our Harmonic models and reimplementations of the DistMult and HolE baselines were written in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and estimated using the Adam optimizer <ref type="bibr" target="#b9">(Kingma and Ba, 2015)</ref>. With the exception of the HolE reimplementation, we uniformly used the log-softmax loss (13), which performed best in initial experiments. In contrast, Yang et al. <ref type="formula">(2015)</ref> use a margin-based ranking loss that is linear in the margin between the scores of positive and negative examples up to a threshold, and <ref type="bibr" target="#b13">Nickel et al. (2016)</ref> use the pairwise linear margin loss applied to the scores squashed by the logistic function. For HolE, we used the linear margin loss, which provided by far the best performance in the experiments. For each model, we trained until performance on the validation set decrease, then chose the best-performing embedding size from among d ∈ {256, 512}. Batch size (512), negative sampling rate (500), and learning rate (0.001) were kept constant across models. We note in passing that regions of the hyperparameter space for DistMult explored by <ref type="bibr" target="#b8">Kadlec et al. (2017)</ref> were inaccessible to us for technical reasons.</p><p>For the Harmonic models, we manually tuned the λ hyperparameter.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>x),x)}+ N n=1 exp{H(µ(x n ),x n )} This has the effect of increasing the Harmony of positive examples relative to negative samples. The learning rule is thus Harmonymaximizing: the network parameters maximize the well-formedness of the positive examples relative to negative samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Effect of optimization on the rank of FB15K validation set triplets (N = 100, 000; 50, 000 triplets with two queries per triplet) from the best-performing HHolE model (d = 512, λ = 1.0). The horizontal axis is a triplet's change in Harmony pre-and post-optimiation (∆H ≡ H(µ(x), x) − H(x, x)) minus the mean change in Harmony for all triplets (µ(∆H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The "complete data" are thenD = { ĥ , x } = { µ(x), x }. For fixed x, H(h, x) models the conditional distribution p(h|x), with (16) p(h|x) = exp{H(h,x)} Z(x) where Z(x) = h exp {H(h , x)} dh = |2πV −1 | 1 2 exp{ (x)} is the partition functionconditioned on x. Let χ q = {x } be the set of candidate triplet embeddings consistent with a given query q. Choosing the discrete distribution p(x) = exp{ (x)}x ∈χq exp{ (x )} over triplet embeddings as the prior probability of the embedding x, 4 we have:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on FB15K and WN18. The results from the original DistMult and HolE models are drawn from(Yang et al., 2015)  and<ref type="bibr" target="#b13">(Nickel et al., 2016)</ref>. Our reimplementations * of DistMult and HolE differ in numerous details from those in the original papers (see Appendix B for technical details). Ensemble DistMult † refers to the hyperparameter-optimized Ensemble (product of experts) reimplementation of DistMult proposed by<ref type="bibr" target="#b8">Kadlec et al. (2017)</ref>. For each model, we report Mean Rank (MR) and Mean Reciprocal Rank (MRR), as well as Hits@N for N ∈ {1, 3, 10}. Hits@N denotes the fraction of test instances in which the true triplet completion had rank less than or equal to N . The best results within each category (DistMult and HolE) are marked in bold, and the best results overall are additionally underlined.entity embedding of Bob Dylan in the context of queries about his profession (e = Bob Dylan, r = has profession) while varying the profession e r . This illustrates how the representation of Bob Dylan varies across his different professional guises. The table illustrates the utility of token embeddings in inference. Token embeddings of George W. Bush and Barack Obama in the context of a query about their having held the office of U.S. president are in semantic neighborhoods with a greater density of true instances of U.S. Presidents than their type embeddings. The negative examples John</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">FB15K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WN18</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Rank</cell><cell></cell><cell>Hits@</cell><cell></cell><cell></cell><cell></cell><cell>Rank</cell><cell></cell><cell>Hits@</cell><cell></cell></row><row><cell>Model</cell><cell>λ</cell><cell>MR</cell><cell>MRR</cell><cell>1</cell><cell>3</cell><cell>10</cell><cell>λ</cell><cell>MR</cell><cell>MRR</cell><cell>1</cell><cell>3</cell><cell>10</cell></row><row><cell>DistMult</cell><cell>-</cell><cell>-</cell><cell>.350</cell><cell>-</cell><cell>-</cell><cell>.577</cell><cell>-</cell><cell>-</cell><cell>.830</cell><cell>-</cell><cell>-</cell><cell>.942</cell></row><row><cell>Ensemble DM  †</cell><cell>-</cell><cell>36</cell><cell>.837</cell><cell>.797</cell><cell>-</cell><cell>.904</cell><cell>-</cell><cell>457</cell><cell>.790</cell><cell>.784</cell><cell>-</cell><cell>.950</cell></row><row><cell>DistMult  *</cell><cell>-</cell><cell>28</cell><cell>.710</cell><cell>.605</cell><cell>.792</cell><cell>.876</cell><cell>-</cell><cell>220</cell><cell>.825</cell><cell>.714</cell><cell>.938</cell><cell>.950</cell></row><row><cell>HDistMult</cell><cell>∞</cell><cell>23</cell><cell>.806</cell><cell>.751</cell><cell>.845</cell><cell>.898</cell><cell>∞</cell><cell>164</cell><cell>.841</cell><cell>.740</cell><cell>.943</cell><cell>.955</cell></row><row><cell>HDistMult</cell><cell>50.0</cell><cell>23</cell><cell>.742</cell><cell>.661</cell><cell>.799</cell><cell>.881</cell><cell>3.0</cell><cell>184</cell><cell>.831</cell><cell>.732</cell><cell>.931</cell><cell>.945</cell></row><row><cell>HolE</cell><cell>-</cell><cell>-</cell><cell>.524</cell><cell>.402</cell><cell>.613</cell><cell>.739</cell><cell>-</cell><cell>-</cell><cell>.938</cell><cell>.930</cell><cell>.945</cell><cell>.949</cell></row><row><cell>HolE  *</cell><cell>-</cell><cell>39</cell><cell>.409</cell><cell>.289</cell><cell>.464</cell><cell>.647</cell><cell>-</cell><cell>205</cell><cell>.916</cell><cell>.893</cell><cell>.936</cell><cell>.946</cell></row><row><cell>HHolE</cell><cell>∞</cell><cell>32</cell><cell>.682</cell><cell>.575</cell><cell>.763</cell><cell>.850</cell><cell>∞</cell><cell>293</cell><cell>.919</cell><cell>.903</cell><cell>.934</cell><cell>.942</cell></row><row><cell>HHolE</cell><cell>1.0</cell><cell>21</cell><cell>.796</cell><cell>.727</cell><cell>.848</cell><cell>.901</cell><cell>2.0</cell><cell>183</cell><cell>.939</cell><cell>.931</cell><cell>.945</cell><cell>.951</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Change in neighborhood (top-5 clos-</cell></row><row><cell>est neighbors) density of true triplets (∆density)</cell></row><row><cell>for positive and negative triplets drawn from the</cell></row><row><cell>triplet classification dataset introduced by Socher</cell></row><row><cell>et al. (2013), which is derived from the FB15K test</cell></row><row><cell>set and consists of 59,071 positive triplets and the</cell></row><row><cell>same number of negative triplets. This resulted in</cell></row><row><cell>N = 118, 142 queries for both positive and nega-</cell></row><row><cell>tive examples (two for each triplet, querying both</cell></row><row><cell>the left and right entity). After computing each</cell></row><row><cell>triplet's neighborhood, we counted the number of</cell></row><row><cell>triplet neighbors that were in fact in the training,</cell></row><row><cell>validation, or test sets of FB15K, yielding a mea-</cell></row><row><cell>sure of the concentration of true and false examples</cell></row><row><cell>in the neighborhood of both type and token triplet</cell></row><row><cell>embeddings.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of HTPR models with and without optimization (controlled by λ).</figDesc><table><row><cell>For both</cell></row><row><cell>models, entities were 5-dimensional and relations</cell></row><row><cell>20-dimensional. This trend held across other hy-</cell></row><row><cell>perparameter settings.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Semantic neighborhoods of type (pre-) and token (post-optimization) triplets output by the bestperforming HHolE model (d = 512, λ = 1.0). US Presidents: Effect of optimization on the semantic neighborhoods of entity embeddings in the context of the query ( · , office title, US President). Guises of Bob Dylan: Effect of optimization on the semantic neighborhood of Bob Dylan in the context of four queries about his profession: Bob Dylan as singer-songwriter, screenwriter, disc jockey, and writer. Bob Dylan is a positive instance of each of these professions in FB15K. For each entity, we retrieved the 5 closest (Euclidian Distance) compositional triplet embeddings, as well as the five closest triplets, among all candidate triplets, when all these candidates are optimized. Triplet completions that in fact occur in FB15K are marked in bold. Human-readable entity names were retrieved from a mapping between Freebase machine IDs and names of Wikipedia articles built by<ref type="bibr" target="#b11">Ling and Weld (2012)</ref>. See main text for discussion of the results.Yang, B., Yih, W., He, X., Gao, J., and Deng,  L. (2015). Embedding entities and relations for learning and infer-ence in knowledge bases. In ICLR.Yoon, H.,Song, H., Park, S., and Park, S. (2016). A translation-based knowledge graph embedding preserving logical property of relations. In Proceedings of NAACL-HLT, pages 907-916.</figDesc><table><row><cell cols="2">7 Appendix A: Model details</cell></row><row><cell>(15)</cell><cell>Claim: µ(x) = −(W − λI) −1 ( 1 2 b + λx) is</cell></row><row><cell></cell><cell>the unique global optimum for H(h, x) for</cell></row><row><cell></cell><cell>any fixed x.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">In the Proceedings of the Society for Computation in Linguistics(2019)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">∆MRR is computed as MRR(µ(x)) − MRR(x), i.e. the difference between the Mean Reciprocal Rank of the supracompositional and the compositional triplets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">See<ref type="bibr" target="#b13">(Nickel et al., 2016)</ref> for discussion of holographic embeddings as compressed tensor products.In the Proceedings of the Society for Computation in Linguistics(2019)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that, when H(·, x) is evaluated at µ(x), the only nonzero term in H(µ(x), x) is (x). Hence, it is sufficient to perform gradient descent on the prior:(x)    In the Proceedings of the Society for Computation in Linguistics(2019)</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>US Presidents</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A linear dynamical system model for text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 32</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakhnenko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<editor>Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ontology-aware token embeddings for prepositional phrase attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL55</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2089" to="2098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledge graph completion with adaptive sparse transfer matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge base completion: Baselines strike back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleindienst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Workshop on Representation Learning for NLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 26</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28 th International Conference on Machine Learning</title>
		<meeting>the 28 th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Holographic reduced representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="623" to="641" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
		<title level="m">Modeling relational data with graph convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Legendre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Architecture</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL -Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
