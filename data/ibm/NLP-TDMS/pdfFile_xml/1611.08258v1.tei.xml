<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Cascaded Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Pazandeh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esat-Psi</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharif</forename><surname>Tech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umbc</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Cvl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zürich</surname></persName>
						</author>
						<title level="a" type="main">Weakly Supervised Cascaded Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection is a challenging task in visual understanding domain, and even more so if the supervision is to be weak. Recently, few efforts to handle the task without expensive human annotations is established by promising deep neural network. A new architecture of cascaded networks is proposed to learn a convolutional neural network (CNN) under such conditions. We introduce two such architectures, with either two cascade stages or three which are trained in an end-to-end pipeline. The first stage of both architectures extracts best candidate of class specific region proposals by training a fully convolutional network. In the case of the three stage architecture, the middle stage provides object segmentation, using the output of the activation maps of first stage. The final stage of both architectures is a part of a convolutional neural network that performs multiple instance learning on proposals extracted in the previous stage(s). Our experiments on the PASCAL VOC 2007, 2010, 2012 and large scale object datasets, ILSVRC 2013, 2014 datasets show improvements in the areas of weaklysupervised object detection, classification and localization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to train a system that detects objects in cluttered scenes by only naming the objects in the training images, without specifying their number or their bounding boxes, is understood to be of major importance. Then it becomes possible to annotate very large datasets or to automatically collect them from the web.</p><p>Most current methods to train object detection systems assume strong supervision <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19]</ref>. Providing both the bounding boxes and their labels as annotations for each object, still renders such methods more powerful than their weakly supervised counterparts. Although the availability of larger sets of training data is advantageous for the training of convolutional neural networks (CNNs), weak supervision as a means of producing those has only been embraced to a limited degree.</p><p>The proposed weak supervision methods have come in  <ref type="figure">Figure 1</ref>. Weakly Supervised Cascaded Deep CNN: Overview of the proposed cascaded weakly supervised object detection and classification method. Our cascaded networks take images and existing object labels to find the best location of objects samples in each of images. Trained networks based on these location is capable of detecting and classifying objects in images, under weakly supervision circumstances. some different flavours. One of the most common approaches <ref type="bibr" target="#b6">[7]</ref> consists of the following steps. The first step generates object proposals. The second stage extracts features from the proposals. And the final stage applies multiple instance learning (MIL) to the features and finds the box labels from the weak bag (image) labels. This approach can thus be improved by enhancing any of its setps. For instance, it would be advantageous if the first stage were to produce more reliable -and therefore fewer -object proposals.</p><p>It is the aforementioned approach that our weak supervision algorithm also follows. To improve the detection performance, object proposal generation, feature extraction, and MIL are trained in a cascaded manner, in an end-to-end way. We propose two architectures. The first is a two stage network. The first stage extracts class specific object proposals using a fully convolutional network followed by a global average (max) pooling layer. The second stage extracts features from the object proposals by a ROI pooling layer and performs MIL. Given the importance of getting better object proposals we added a middle stage to the pre-vious architecture in our three stage network. This middle stage performs a class specific segmentation using the input images and the extracted objectness of the first stage. This results in more reliable object proposals and a better detection.</p><p>The proposed architecture improves both initial object proposal extraction and final object detection. In the forward sense, less noisy proposals indeed lead to improved object detection, due to the non-convexity of the cost function. In the reverse, backward sense, due the weight sharing between the first layers of both stages, training the MIL on the extracted proposals will improve the performance of feature extraction in the first convolutional layers and as a result will produce more reliable proposals.</p><p>Next, we review related works in section 2 and discuss our proposed method in section 3. In section 4 we explain the details of our experiments, incl. the dataset and complete set of experiments and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Weakly supervised detection: In the last decade, several weakly supervised object detection methods have been studied using multiple instance learning algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. To do so they define images as the bag of regions, wherein they assume the image labeled positive contains at least one object instance of a certain category and an image labeled negative do not contain an object from the category of interest. The most common way of weakly supervised learning methods often work by selecting the candidate positive object instances in the positive bags, and then learning a model of the object appearance using appearance model. Due to the training phase of the MIL problem alternating between out of bag object extraction and training classifiers, the solutions are non-convex and as a result is sensitive to the initialization. In practice, a bad initialization is prone to getting the solution stuck in a local optima, instead of global optima. To alleviate this shortcoming, several methods try to improve the initialization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> as the solution strongly depends on the initialization, while some others focus on regularizing the optimization strategies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. Kumar et al. <ref type="bibr" target="#b16">[17]</ref> employ an iterative selflearning strategy to employ harder samples to a small set of initial samples at training stage. Joulin et al. <ref type="bibr" target="#b14">[15]</ref> use a convex relaxation of soft-max loss in order to minimize the prone to get stuck in the local minima. Deselaers et al. <ref type="bibr" target="#b8">[9]</ref> initialize the object locations via the objectness score. Cinbis et al. <ref type="bibr" target="#b6">[7]</ref> split the training date in a multi-fold manner for escaping from getting trapped into the local minima. In order to have more robustness from poor initialization, Song et al. <ref type="bibr" target="#b29">[30]</ref> apply Nesterov's smooting technique to latent SVM formulation <ref type="bibr" target="#b9">[10]</ref>. In <ref type="bibr" target="#b30">[31]</ref>, the same authors initialize the object locations based on sub-modular clustering method. Bilen et al. <ref type="bibr" target="#b3">[4]</ref> formulates the MIL to softly label the object instances by regularizing the latent object locations based on penalizing unlikely configurations. Further in <ref type="bibr" target="#b4">[5]</ref>, the authors extend their work <ref type="bibr" target="#b3">[4]</ref> by enforcing similarity between object windows via regularization technique. Wang et al. <ref type="bibr" target="#b34">[35]</ref> employ probabilistic latent semantic analysis on the windows of positive samples to select the most discriminative clusters that represents the object category. As a matter of fact, majority of the previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref> use a large collection of noisy object proposals to train their object detector. In contrast, our method only focuses on a very few clean collection of object proposals that are far more reliable, robust, computationally efficient, and gives better performance.</p><p>Object proposal generation: In <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>, Nguyen et al. and Pandey et al. extract dense regions of candidate proposals from an image using an initial bounding box. To handle the problem of not being able to generate enough candidate proposals because of fixed shape and size, object saliency <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> based approaches were proposed to extract region proposals. Following this, generic objectness measure <ref type="bibr" target="#b0">[1]</ref> was employed to extract region proposals. Selective search algorithm <ref type="bibr" target="#b32">[33]</ref>, a segmentation based object proposal generation was proposed, which is currently among the most promising techniques used for proposal generation. Recently, Ghodrati et al. <ref type="bibr" target="#b10">[11]</ref> proposed an inverse cascade method using various CNN feature maps to localize object proposals in a coarse to fine manner.</p><p>CNN based weakly supervised object detection: In view of the promising results of CNNs for visual recognition, some recent efforts in weakly supervised classification have been based on CNNs. Oquab et al. <ref type="bibr" target="#b20">[21]</ref> improved feature discrimination based on a pre-trained CNN. In <ref type="bibr" target="#b21">[22]</ref>, the same authors improved the performance further by incorporating both localization and classification on a new CNN architecture. Bilen et al. <ref type="bibr" target="#b3">[4]</ref> proposed a CNN-based convex optimization method to solve the problem to escape from getting stuck in local minima. Their soft similarity between possible regions and clusters was helpful in improving the optimization. Li et al. <ref type="bibr" target="#b17">[18]</ref> introduced a class-specific object proposal generation based on the mask out strategy of <ref type="bibr" target="#b1">[2]</ref>, in order to have a reliable initialization. They also proposed their two-stage algorithm, classification adaptation and detection adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>This section introduces our weak cascaded convolutional networks (WCCN) for object detection and classification with weak supervision. Our networks are designed to learn multiple different but related tasks all together jointly. The tasks are classification, localization, and multiple instance </p><formula xml:id="formula_0">C O N V C O N V 5 Shared Convs Image LocNet MilNet Loss1 Loss2 Figure 2. WCCN (2stage):</formula><p>The pipeline of end-to-end 2-stage cascaded CNN for weakly supervised object detection. Inputs to the network are images, labels and unsupervised object proposals. First stage learns to create a class activation map based on object categories to make some candidate boxes for each instance of objects. Second stage picks the best bounding box among the candidates to represent the specific category by multiple instance learning loss.</p><p>learning. We show that learning these tasks jointly in an end-to-end fashion results in better object detection and localization. The goal is to learn good appearance models from images with multiple objects where the only manual supervision signal is image-level labels. Our main contribution is improving multiple object detection with such weak annotation. To this end, we propose two different cascaded network architectures. The first one is a 2-stage cascade network that first localizes the objects and then learns to detect them in a multiple instance learning framework. Our second architecture is a 3-stage cascade network where the new stage performs semantic segmentation with pseudo ground truth in a weakly supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Two-stage Cascade</head><p>As mentioned earlier, there are only a few end-to-end frameworks with deep CNNs for weakly supervised object detection. In particular, there is not much prior art on object localization without localization information in the supervision. Suppose we have a dataset I with C classes in N training images. The set is given as I = {(I 1 , y 1 ), ..., (I N , y N )} where I are images and y = [y 1 , ..., y C ] ∈ {0, 1} C are vectors of labels indicating the presence or absence of each class in a given image.</p><p>In the proposed cascaded network, the initial fullyconvolutional stage learns to infer object location maps based on the object labels in the given images. This stage produces some candidate boxes of objects as input to the next stage. The second stage selects the best boxes through an end-to-end multiple instance learning in the network.</p><p>First stage (Location network): The first stage of our cascaded model is a fully-convolutional CNN with a global average pooling (GAP) or global maximum pooling (GMP) layer, inspired by <ref type="bibr" target="#b35">[36]</ref>. The training yields the object location or 'class activation' maps, that provide candidate bounding boxes. In order to learn multiple classes and to address the issue of multiple categories label for single image <ref type="bibr" target="#b21">[22]</ref>, we use an independent loss function for each class in this branch of the CNN architecture, so the loss function is the sum of C binary logistic regression loss functions.</p><p>Second stage (MIL network): The goal of the second stage is to select the best candidate boxes for each class from the outputs of the first stage, using MIL. To obtain an end-to-end framework, we incorporate an MIL loss function. For multiple instance learning, we consider x c = {x j |j = 1, 2, ..., n} as a bag for instances of image I and each of x is one of the candidate boxes and label sets of y x = {y i |y i ∈ {0, 1}, i = 1, ..., C} for the bag where C i=1 y i = 1 and the reason is that each positive bag should belong to one specific object category. Using bounding boxes of instances, we extract CNN representation for each box by ROI-pooling layer <ref type="bibr" target="#b11">[12]</ref>: f = {f ij } ∈ C×n . So we define and probabilities and loss as:</p><formula xml:id="formula_1">Score(I, f ) i = max(f i1 , ..., f in ) P (I, f i ) = exp(Score(I, f i ) i ) C k=1 exp(Score(I, f k ) k ) L M IL (P, y) = − C i=1 y i log(P (I, f i )) (1)</formula><p>The weights for conv1 till conv5 are shared between the two stages. For the second stage, we have additional two fully connected layers and a score layer for learning MIL task.</p><p>End-to-End Training: The whole cascade with two loss </p><formula xml:id="formula_2">C O N V C O N V 5</formula><p>Conv5 <ref type="figure">Figure 3</ref>. WCCN (3stage): The pipeline of end-to-end 3-stage cascaded CNN for weakly supervised object detection. For this cascaded network, we designed new architecture to have weakly supervised segmentation as second stage, so first and third stages are identical to the stages of the previous cascade. The new stage will improve the selecting candidate bounding boxes by providing more accurate object regions.</p><p>functions is learned jointly by end-to-end stochastic gradient descent optimization. The total loss function of the cascaded network is:</p><formula xml:id="formula_3">L T otal = L GAP (Labels(W ))+ L M IL (Labels(W )|candidateBoxes(W )).<label>(2)</label></formula><p>where W contains all network parameters. We set the hyperparameter balancing two loss functions to 1. We suspect cross-validation on this hyperparameter can improve the results in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Three-stage Cascade</head><p>In this section, we extend our 2-stage cascaded model by another stage that adds object segmentation as another task. We believe more information about the objects' boundary learned in a segmentation task can lead to acquisition of a better appearance model and then better object localization. For this purpose, our new stage uses another form of weak supervision to learn a segmentation model, embedded in the cascaded network and trained along with other stages. This extra stage will help the multi-loss CNN to have better initial locations for choosing candidate bounding boxes to pass to the next stage. So this new cascade has three stages: first stage, similar to previous cascade is a CNN with global pooling layer; second stage, fully convolutional network with segmentation loss; third stage, multiple instance learning with corresponding loss.</p><p>New stage (Segmentation Loss): Inspired by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, we propose to use a weakly supervised segmentation network which uses an object point of location and also label as supervisory signals. Incorporation of initial location of object from previous stage (location network) in the segmentation stage can obtain more meaningful object location map. The weak segmentation network uses the results of the first stage as supervision signal (i.e., pseudo ground truth) and learns jointly with the MIL stage to further improve the object localization results.</p><p>To calculate the loss for this stage, we define s ic for the CNN score for pixel i and class c in image I . Eq.3, shows the softmax for class c at pixel i .</p><formula xml:id="formula_4">S ic = exp(s ic )/ C k=1 exp(s ik )<label>(3)</label></formula><p>Considering y as the label set for image I , the loss function for the weakly supervised segmentation network is given by:</p><formula xml:id="formula_5">L Seg (S, G, y) = − C i=1 y i log(S tcc ) − i∈Is α i log(S tcGi ) By t c = argmax i∈I S ic<label>(4)</label></formula><p>where the first term is used for image-level label supervision and second term is for the set of labeled pixels in I s . G i is the supervision map for the segmentation which is obtained from first stage of cascade and not annotated by human. α i denotes the score of importance for each pixel at the map which is calculated in the last stage.</p><p>Output of this stage is a set of candidate bounding boxes of objects for pushing to next stage of the CNN cascade which uses multiple instance learning to choose the most accurate box as the representative of object category. In the experiments, we show that learning this extra task as another stage of cascade can improve performance of the whole network as a weakly supervised classifier.</p><p>End-to-End Training: Similar to the last cascade, the total loss in Eq.5 is calculated by simply adding all three loss terms. We learn all parameters of the network jointly in an end-to-end fashion.</p><formula xml:id="formula_6">L T otal = L GAP (Labels(W ))+ L Seg (M ap(W )|P oint(W ))+ L M IL (Labels(W )|candidateBoxes(W )).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Object Detection Training</head><p>Our cascaded network can be used in an object detector pipeline in two ways. The direct way is to use the network after training as the main part of detection. The network is capable of targeting the location and label of the existing object instances in the image. So we can push images and unsupervised object proposals to the cascade and operate all the stages for labeling, localizing and finding the best boxes for each of the object category or rejecting boxes as non-object.</p><p>Second way is to use best extracted location of objects in the training phase as new ground-truths (GT) and train an efficient supervised object detector pipeline like R-CNN or Fast-RCNN <ref type="bibr" target="#b11">[12]</ref>. So these obtained bounding boxes are acting as pseudo GT and replace the manual annotations. In both cases, at the testing time, we extract object proposals with EdgeBoxes <ref type="bibr" target="#b36">[37]</ref> and use the train networks in either case to detect objects among the pool of proposals. Nonmax-suppression is also used to clarify final decisions on the boxes and throwing away redundant cases. In the experiments, we show good results for both these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the following section, we discuss full details of our methods and experiments which we applied on object detection and classification in weakly supervised manner. We introduce datasets and also analyze performance of our approaches on them in many aspects of evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and metrics</head><p>The experiments for our proposed methods are extensively done on the PASCAL VOC 2007, 2010, 2012 datasets and also ILSVRC 2013, 2014 which are large scale datasets for objects. The PASCAL VOC is more common dataset to evaluate weakly supervised object detection approaches. The VOC datasets have 20 categories of objects, while ILSVRC dataset has 200 categories which we targeted also for weakly supervised object classification and localization. In all of the mentioned datasets, we incorporate the standard train, validation and test set.</p><p>Experimental metrics: To measure the object detection performance, average precision (AP) and correct localization (CorLoc) is used. Average precision is the standard metric from PASCAL VOC which takes a bounding box as a true detection where it has intersection-over-union (IoU) of more than 50% with ground-truth box. The Corloc is the fraction of positive images that the method obtained correct location for at least one object instance in the image. For the object classification, also we use PASCAL VOC standard average precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental and implementation details</head><p>We have evaluated both of our proposed cascaded CNN with two architectures: Alexnet <ref type="bibr" target="#b15">[16]</ref> and VGG-16 <ref type="bibr" target="#b26">[27]</ref>. In each case, the network has been pre-trained on ImageNet dataset <ref type="bibr" target="#b7">[8]</ref>. Since the multiple stages of cascades contain different CNN networks losses, in the following we explain details of each part separately to have better overview of the implementation. CNN architectures:</p><p>1. Loc Net: Inspired by <ref type="bibr" target="#b35">[36]</ref>, we removed fullyconnected layers from each of Alexnet or VGG-16 and replaced them by two convolutional layers and one global pooling layer. So for the Alexnet, the layers after conv5 layer have been removed and for VGG-16 after conv5-3. For global pooling layer, we have tested average and max pooling methods and we found that global average pooling performs better than maximum pooling. For the training loss criteria of this part of network, we use a simple sum of C (number of classes) binary logistic regression losses, similar to <ref type="bibr" target="#b21">[22]</ref>.</p><p>2. Seg Net: This part of network is second stage in the 3-stage cascaded network and is well-known fully convolutional network for segmentation task. The convolutional part is shared with the other stages which comes from the first stage and additional fully-connected layers and a deconvolutional layer is used to produce segmentation map. The loss function is explained in section 3. Since this loss is provided by weak supervision, part of the supervision is obtained from the last stage in form of best initial regions of object instances.</p><p>3. MIL Net: This last stage uses the shared convolutional feature maps as initial layers to train two fullyconnected layers with size of 4096 and a label prediction layer. Using the the selected candidate bounding boxes    from last stage (first stage in the 2stages cascade case and second stage in the 3stages cascade), it trains the multiple instance learning loss to select the best sample for each object presented in an image.</p><formula xml:id="formula_7">− − − − − − − − − − − − − − − − − − − − 89.</formula><p>Implementation details: We use MatConvNet <ref type="bibr" target="#b33">[34]</ref> as CNN toolkit and all the networks are trained on a Geforce Titan X GPU. During the training time, images have been re-sized to multiple scale of images ({480, 576, 688, 84, 1200}) with respect to the original aspect ratio. The learning rate for the CNN networks is 0.0001 for 20 epochs and batch size of 100. For each image, we use 2000 object proposals generated by EdgeBox or Selec-tiveSearch algorithms. At the last stage, we select 10 boxes for each object instance in each iteration for training multiple instance learning. To use Fast-RCNN detection with the ground-truths that are obtained by our methods, we set the number of iterations to 40K. For selecting the candidate boxes in our pipelines, we use a thresholding method like <ref type="bibr" target="#b35">[36]</ref> for weakly localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Detection performance</head><p>Comparison with the state-of-the-art: We evaluate the detection performance of our method in this section. To compare our approach, methods which use deep learning pipelines <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> or multiple instance learning algorithms <ref type="bibr" target="#b6">[7]</ref> or clustering based approaches <ref type="bibr" target="#b4">[5]</ref> are studied.</p><p>Tables 1, 4, 5 present results on PASCAL VOC 2007, 2010, 2012 for object detection on test sets with average precision measurement. It can be observed that by using the weakly supervision setup, we achieved the best performance among of all other recent methods. Our approaches does not incorporate any sophisticated clustering or optimized initialization step, and all the steps are trained together via an end-to-end learning of deep neural networks. There is a semantic relationship between improvements gain using different CNN architectures in our networks in comparison with using the same CNNs in other methods. We have almost the same improvement with two different architectures over other methods. The localization performance with CorLoc metric is also shown in <ref type="table" target="#tab_5">Table 3</ref> on PASCAL VOC 2007. Our best performance is 56.7% which is achieved by 3stage cascade network using VGG-16 architecture. However, our network with the Alexnet outperformed the other methods using similar network architectures with same number of layers and other non deep learning methods. Most of the other works use CNNs as some part of their pipeline, not in an end-to-end scheme or use it simply as a feature extractor. Differently, our cascaded deep networks will bring multiple concepts together in a single training method, learn better appearance model and feature representation for objects under weakly supervision circumstances. We also compared our object detector results on ILSVRC'13 only with <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref>, since no other weakly supervised object detector methods have been tried on this dataset. Results are shown in <ref type="table">Table 4</ref> and similar to previous tests, we achieved better number in performance. Since, some part of our work is inspired by GAP networks from <ref type="bibr" target="#b35">[36]</ref>, we compared our weakly supervised localization on the ILSVRC'14 dataset following their experimental setups and the results are in <ref type="table" target="#tab_8">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object detection training:</head><p>We compared our full detection pipeline with the state-of-the-art detection method, Fast RCNN implemented in Caffe <ref type="bibr" target="#b13">[14]</ref>.</p><p>Since the Fast RCNN <ref type="bibr" target="#b11">[12]</ref> is a supervised method, we use the pseudo ground-truth (GT) bounding boxes which are generated by our cascaded networks. By our experiments, In the <ref type="figure" target="#fig_2">Fig.5</ref>, it is shown that the Fast RCNN pipeline can also perform good results with our input bounding boxes. Fast RCNN trained by our generated GT performs slightly better than our detection full pipeline on the average precision of PAS-CAL VOC 2007 test set (0.3%). The main goal of this work is to find the most representative and discriminative samples that signify the existing categories in each image.</p><p>Object proposals: In our work, we evaluated the effect of different unsupervised object proposals generator. Edge-Box <ref type="bibr" target="#b36">[37]</ref> and SelectiveSearch <ref type="bibr" target="#b32">[33]</ref> are compared based on the detector trained by our networks. According to the results on the VOC 2007 detection test set, by training 2stage cascade using Alexnet with Edgebox, approximately 1.5% improvement can be obtained over SelectiveSearch. Similar to the other works like <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, EdgeBox performs better with CNN based object detectors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top-1 error Alexnet 65.17 VGG16 61.12 Alexnet-GAP <ref type="bibr" target="#b35">[36]</ref> 63.75 VGG16-GAP <ref type="bibr" target="#b35">[36]</ref> 57.20 WCCN 2stage Alexnet 62.2 WCCN 2stage VGG16 55.6  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Classification performance</head><p>Our proposed network design has dual purposes: object detection and classification in a weakly supervision manner. Obviously the structure of our cascade is helpful for training classification pipeline on images with multiple objects and minimum supervision of labels. We evaluated our method on PASCAL VOC 2007 and ILSVRC 2014. The performance is compared with other approaches which use novel methods in deep learning for classification on these datasets. <ref type="table" target="#tab_4">Table 2</ref> presents the comparison on VOC 2007 with different CNN architectures for all of the methods. Since first stage of our cascade is similar to <ref type="bibr" target="#b35">[36]</ref>, we show the result of classification on ILSVRC'14, the large scale dataset for classification, in <ref type="table" target="#tab_8">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Cascade Architecture Study</head><p>If an ablation study would be interesting over the stages of proposed cascades, it can be noticed that all of the re-sults show how each of the proposed cascades can affect the performance in detection or classification. Each stage in our multi-stage cascaded CNN can be analyzed by comparison with the CNN-based methods in same context. Training the stage with multiple instance loss can improve learning the best sample of each category over other works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b5">6]</ref>. It can be observed that adding the stage of segmentation to exploit better regions can outperform the two-stage cascade. Adding segmentation stage has impact on finding more accurate initial guess of object locations. For an instance of using the segmentation stage by Alexnet architecture, cascaded network improves almost 2.5% on detection and 2% on classification in PASCAL VOC 2007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Our idea of weak cascaded convolutional networks (WCCN) is about the approaches of cascaded CNNs for weakly supervised visual learning tasks like object detection, localization and classification. In this work, we proposed two multi-stage cascaded networks with different loss functions in each stage to conclude a better pipeline of deep convolutional neural network learning with weak supervision of object labels on images. Our insight was a paradigm of multi-task learning effectiveness using deep neural networks. We proved that our multi-task learning approaches that incorporate localization, multiple instance learning and weakly supervised segmentation of object regions achieve the state-of-the-art performance in weakly supervised object detection and classification. The extensive experiments for object detection and classification tasks on various datasets like PASCAL VOC 2007, 2010, 2012 and also large scale datasets, ILSVRC 2013, 2014 present the full capability of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Examples of our object detection results. Green bounding boxes are ground-truth annotations and red boxes are positive detection. Images are sampled from PASCAL VOC 2007 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>a t c h a i r c o w t a b l e d o g h o r s e m b i k e p e r s o n p l a n t s h e e p s o fFigure 5 .</head><label>5</label><figDesc>a e r o b i k e b i r d b o a t b o t t l e b u s c a r c Comparison between our detection full pipeline and training Fast RCNN using pseudo ground-truth bounding boxes extracted by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>30.8 41.0 20.1 30.2 Wang et al. [35] 48.8 41.0 23.6 12.1 11.1 42.7 40.9 35.5 11.1 36.6 18.4 35.3 34.8 12.1 42.3 39.7 31.0 Li et al., VGG16 [18] 54.5 47.4 41.3 20.8 17.7 51.9 63.5 46.1 21.8 57.1 22.1 34.4 50.5 Detection average precision (%) on the PASCAL VOC 2007 dataset test set.</figDesc><table><row><cell>Method</cell><cell cols="6">aero bike bird boat bottle bus</cell><cell>car</cell><cell cols="12">cat chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mAP</cell></row><row><cell>Bilen et al. [4]</cell><cell cols="4">42.2 43.9 23.1 9.2</cell><cell cols="4">12.5 44.9 45.1 24.9</cell><cell>8.3</cell><cell cols="4">24.0 13.9 18.6 31.6</cell><cell>43.6</cell><cell>7.6</cell><cell>20.9</cell><cell cols="5">26.6 20.6 35.9 29.6 26.4</cell></row><row><cell>Bilen et al. [5]</cell><cell cols="8">46.2 46.9 24.1 16.4 12.2 42.2 47.1 35.2</cell><cell>7.8</cell><cell cols="4">28.3 12.7 21.5 30.1</cell><cell>42.4</cell><cell>7.8</cell><cell>20.0</cell><cell cols="5">26.8 20.8 35.8 29.6 27.7</cell></row><row><cell>Cinbis et al. [7]</cell><cell cols="4">39.3 43.0 28.8 20.4</cell><cell>8.0</cell><cell cols="3">45.5 47.9 22.1</cell><cell>8.4</cell><cell cols="4">33.5 23.6 29.2 38.5</cell><cell cols="8">47.9 35.8 51.3 20.3 20.0 17.2 17.4 26.8 32.8 35.1 45.6 30.9</cell></row><row><cell>Li et al., Alexnet [18]</cell><cell cols="4">49.7 33.6 30.8 19.9</cell><cell>13</cell><cell cols="5">40.5 54.3 37.4 14.8 39.8</cell><cell>9.4</cell><cell cols="2">28.8 38.1</cell><cell cols="8">49.8 27.1 61.8 14.5 24.0 16.2 29.9 40.7 15.9 55.3 40.2 39.5</cell></row><row><cell>WSDDN [6]</cell><cell cols="8">46.4 58.3 35.5 25.9 14.0 66.7 53.0 39.2</cell><cell>8.9</cell><cell cols="4">41.8 26.6 38.6 44.7</cell><cell>59.0</cell><cell>10.8</cell><cell>17.3</cell><cell cols="5">40.7 49.6 56.9 50.8 39.3</cell></row><row><cell cols="9">WCCN 2stage Alexnet 43.5 56.8 34.1 19.2 13.4 63.1 51.5 33.1</cell><cell>5.8</cell><cell cols="4">39.3 19.6 32.9 46.2</cell><cell>56.1</cell><cell>11.2</cell><cell>17.5</cell><cell cols="5">38.5 45.7 52.6 43.3 36.2</cell></row><row><cell cols="14">WCCN 2stage VGG16 48.2 58.9 37.3 27.8 15.3 69.8 55.2 41.1 10.1 42.7 28.6 40.4 47.3</cell><cell>62.3</cell><cell>12.9</cell><cell>21.2</cell><cell cols="5">44.3 52.2 59.1 53.1 41.4</cell></row><row><cell cols="9">WCCN 3stage Alexnet 43.9 57.6 34.9 21.3 14.7 64.7 52.8 34.2</cell><cell>6.5</cell><cell cols="4">41.2 20.5 33.8 47.6</cell><cell>56.8</cell><cell>12.7</cell><cell>18.8</cell><cell cols="5">39.6 46.9 52.9 45.1 37.3</cell></row><row><cell cols="14">WCCN 3stage VGG16 49.5 60.6 38.6 29.2 16.2 70.8 56.9 42.5 10.9 44.1 29.9 42.2 47.9</cell><cell>64.1</cell><cell>13.8</cell><cell>23.5</cell><cell cols="5">45.9 54.1 60.8 54.5 42.8</cell></row><row><cell>Method</cell><cell cols="6">aero bike bird boat bottle bus</cell><cell>car</cell><cell cols="12">cat chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mAP</cell></row><row><cell>WSDDN [6]</cell><cell cols="13">95.0 92.6 91.2 90.4 79.0 89.2 92.8 92.4 78.5 90.5 80.4 95.1 91.6</cell><cell>92.5</cell><cell>94.7</cell><cell>82.2</cell><cell cols="5">89.9 80.3 93.1 89.1 89.0</cell></row><row><cell>Oquab et al. [21]</cell><cell cols="13">88.5 81.5 87.9 82.0 47.5 75.5 90.1 87.2 61.6 75.7 67.3 85.5 83.5</cell><cell>80.0</cell><cell>95.6</cell><cell>60.8</cell><cell cols="5">76.8 58.0 90.4 77.9 77.7</cell></row><row><cell>SPPnet [13]</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>82.4</cell></row><row><cell>Alexnet [6]</cell><cell cols="13">95.3 90.4 92.5 89.6 54.4 81.9 91.5 91.9 64.1 76.3 74.9 89.7 92.2</cell><cell>86.9</cell><cell>95.2</cell><cell>60.7</cell><cell cols="5">82.9 68.0 95.5 74.4 82.4</cell></row><row><cell>VGG16-net [27]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Classification average precision (%) on the PASCAL VOC 2007 test set.</figDesc><table><row><cell>Method</cell><cell cols="3">aero bike bird boat bottle bus</cell><cell>car</cell><cell cols="5">cat chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mAP</cell></row><row><cell>Bilen et al. [5]</cell><cell cols="7">66.4 59.3 42.7 20.4 21.3 63.4 74.3 59.6 21.1 58.2 14.0 38.5 49.5</cell><cell>60.0</cell><cell>19.8</cell><cell>39.2</cell><cell>41.7 30.1 50.2 44.1 43.7</cell></row><row><cell>Cinbis et al. [7]</cell><cell cols="7">65.3 55.0 52.4 48.3 18.2 66.4 77.8 35.6 26.5 67.0 46.9 48.4 70.5</cell><cell>69.1</cell><cell>35.2</cell><cell>35.2</cell><cell>69.6 43.4 64.6 43.7 52.0</cell></row><row><cell>Wang et al. [35]</cell><cell cols="7">80.1 63.9 51.5 14.9 21.0 55.7 74.2 43.5 26.2 53.4 16.3 56.7 58.3</cell><cell>69.5</cell><cell>14.1</cell><cell>38.3</cell><cell>58.8 47.2 49.1 60.9 48.5</cell></row><row><cell>Li et al., Alexnet [18]</cell><cell cols="7">77.3 62.6 53.3 41.4 28.7 58.6 76.2 61.1 24.5 59.6 18.0 49.9 56.8</cell><cell>71.4</cell><cell>20.9</cell><cell>44.5</cell><cell>59.4 22.3 60.9 48.8 49.8</cell></row><row><cell>Li et al., VGG16 [18]</cell><cell cols="7">78.2 67.1 61.8 38.1 36.1 61.8 78.8 55.2 28.5 68.8 18.5 49.2 64.1</cell><cell>73.5</cell><cell>21.4</cell><cell>47.4</cell><cell>64.6 22.3 60.9 52.3 52.4</cell></row><row><cell>WSDDN [6]</cell><cell cols="7">65.1 63.4 59.7 45.9 38.5 69.4 77.0 50.7 30.1 68.8 34.0 37.3 61.0</cell><cell>82.9</cell><cell>25.1</cell><cell>42.9</cell><cell>79.2 59.4 68.2 64.1 56.1</cell></row><row><cell cols="8">WCCN 2stage Alexnet 78.4 66.4 58.2 38.1 34.9 60.1 77.8 53.8 26.6 66.5 18.7 47.3 62.8</cell><cell>73.5</cell><cell>20.4</cell><cell>45.2</cell><cell>64</cell><cell>21.6 59.9 51.6 51.3</cell></row><row><cell cols="2">WCCN 2stage VGG16 81.2</cell><cell>70</cell><cell cols="3">62.5 41.7 38.2 63.4 81.1 57.7 30.4 70.3 21.7</cell><cell>51</cell><cell>65.9</cell><cell>75.7</cell><cell>23.9</cell><cell>47.9</cell><cell>67.5 25.6 62.4 53.9 54.6</cell></row><row><cell cols="8">WCCN 3stage Alexnet 79.7 68.1 60.4 38.9 36.8 61.1 78.6 56.7 27.8 67.7 20.3 48.1 63.9</cell><cell>75.1</cell><cell>21.5</cell><cell>46.9</cell><cell>64.8 23.4 60.2 52.4 52.6</cell></row><row><cell cols="8">WCCN 3stage VGG16 83.9 72.8 64.5 44.1 40.1 65.7 82.5 58.9 33.7 72.5 25.6 53.7 67.4</cell><cell>77.4</cell><cell>26.8</cell><cell>49.1</cell><cell>68.1 27.9 64.5 55.7 56.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Correct localization (%) on PASCAL VOC 2007 on positive (CorLoc) trainval set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">Detection top-1 error (%) on ILSVRC'14 validation set</cell></row><row><cell>Method</cell><cell>Top-1 error</cell></row><row><cell>Alexnet</cell><cell>42.6</cell></row><row><cell>VGG16</cell><cell>31.2</cell></row><row><cell>Alexnet-GAP [36]</cell><cell>44.9</cell></row><row><cell>VGG16-GAP [36]</cell><cell>33.4</cell></row><row><cell>WCCN 2stage Alexnet</cell><cell>41.2</cell></row><row><cell>WCCN 2stage VGG16</cell><cell>30.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Classification top-1 error (%) on ILSVRC'14 validation set</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by DBOF PhD scholarship, KU Leuven CAMETRON project. The authors would like to thank Nvidia for GPU donation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is an object?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-taught object localization with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">What&apos;s the Point: Semantic Segmentation with Point Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization with multi-fold multiple instance learning. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Localizing objects while learning their appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepproposal: Hunting objects by cascading deep convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A convex relaxation for weakly supervised classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly supervised discriminative localization and classification: a joint learning process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scene recognition and weakly supervised object localization with deformable part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">In defence of negative mining for annotating weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weaklysupervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM&apos;MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
