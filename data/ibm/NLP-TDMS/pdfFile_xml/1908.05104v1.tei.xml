<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">D-UNet: a dimension-fusion U shape network for chronic stroke lesion segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yongjin</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Shanshan</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">D-UNet: a dimension-fusion U shape network for chronic stroke lesion segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-MRI</term>
					<term>stroke segmentation</term>
					<term>deep learning</term>
					<term>dimensional fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Assessing the location and extent of lesions caused by chronic stroke is critical for medical diagnosis, surgical planning, and prognosis. In recent years, with the rapid development of 2D and 3D convolutional neural networks (CNN), the encoder-decoder structure has shown great potential in the field of medical image segmentation. However, the 2D CNN ignores the 3D information of medical images, while the 3D CNN suffers from high computational resource demands. This paper proposes a new architecture called dimension-fusion-UNet (D-UNet), which combines 2D and 3D convolution innovatively in the encoding stage. The proposed architecture achieves a better segmentation performance than 2D networks, while requiring significantly less computation time in comparison to 3D networks. Furthermore, to alleviate the data imbalance issue between positive and negative samples for the network training, we propose a new loss function called Enhance Mixing Loss (EML). This function adds a weighted focal coefficient and combines two traditional loss functions. The proposed method has been tested on the ATLAS dataset and compared to three state-of-the-art methods. The results demonstrate that the proposed method achieves the best quality performance in terms of DSC = 0.5349±0.2763 and precision = 0.6331±0.295).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S TROKE is the most common cerebrovascular disease and is one of the most common causes of death and disability worldwide <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. It is a group of diseases caused by a sudden cerebrovascular rupture or cerebrovascular infraction. The typical symptom of this disease is a focal neurological deficit, such as sudden seizures, language disorders, hemianopia, loss of feeling, etc. <ref type="bibr" target="#b2">[3]</ref>. These symptoms may develop into chronic diseases (such as dementia, hemiplegia, etc.), which can seriously affect the life quality of patients; these diseases consume a large part of social health care costs <ref type="bibr" target="#b3">[4]</ref>. At the subacute/chronic stages, effective rehabilitation can promote a long-term functional recovery. However, there have been few advances in largescale neuroimaging-based stroke predictions at the subacute and chronic stages. The most common research scan is a high-resolution T1-weighted structural MRI. Researches using these types of images at the subacture/chronic stages have revealed promising biomarkers. These could potentially provide additional information, beyond behavioral assessments, to predict an individuals likelihood of recovery for specific functions (e.g., motor, speech) and response to treatments <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Thus far, measures that include the size, location, and overlap of the lesion with existing brain regions or structures, such as the corticospinal tract, have been successfully used as predictors of long-term stroke recovery and rehabilitation <ref type="bibr" target="#b6">[7]</ref>. However, a key barrier to correctly analyzing these large-scale stroke neuroimaging datasets to predict outcomes is the accurate segmentation of lesions. As manually-based annotations may no longer be suitable for a wide range of data requirements, there is a need for automatic segmentation tools for their analyses. Strokes occur in different locations, with large differences in shape and unclear boundaries as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. A public dataset, Anatomical Tracings of Lesions-After-Stroke (ATLAS), is utilized to illustrate this variability <ref type="bibr" target="#b6">[7]</ref>. Firstly, the segmentation performance is reduced by motion artifacts in the MRI images. Secondly, the position and shape of the lesions are significantly different owing to the existence of multiple subtypes of strokes. The lesion volume can vary from hundreds to tens of thousands of cubic millimeters depending on the severity of the disease, and the lesion area can occur in the cerebrum, cerebellum, and other areas of the brain. Finally, the boundaries of some lesions are not clear, and different clinicians may inconsistently label different lesion areas. Therefore, the accurate automated segmentation is a challenging problem.</p><p>To tackle these difficulties, researchers have made many efforts, including intensity threshold processing, region growth, and deformable models. However, these methods rely on the hand-crafted feature extraction by experts; they have a limited feature representation and low generalization performance. In recent years, with the rapid development of deep learning, convolutional neural networks (CNN) have proven to have great potential in the field of medical image analysis <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b16">[17]</ref>. The study of CNN is mainly based on two- The first column is the raw data, the second column is the gold standard from the hand-marked lesions by the doctor, and the third column is the combination of the first two columns. Strokes occur in different locations, with large differences in shape and unclear boundaries. dimensional (2D) and three-dimensional (3D) approaches: (1) In the 2D CNN approaches, the MRI volume data are converted into several planar slices and independently predict the lesion area of each slice. These ignore the spatial characteristics of the MRI data such that the predictions are discontinuous. <ref type="bibr" target="#b1">(2)</ref> In the 3D CNN, approaches, spatial information is extracted for inference. However, due to their computational and storage requirements, the 3D CNN have been largely avoided.</p><p>In order to solve the problem of accurately automating the image segmentation, we propose a novel network called the Dimension-fusion-UNet (D-UNet). In this new model, the 3D spatial information in the MRI data is effectively utilized under the 2D framework of the subject and has low computing resource requirements. Our D-UNet has the following two technical achievements:</p><p>Dimension fusion network: First, in order to extract the information of consecutive slices from MRI data, we designed a novel downsampling block based on a UNet improvement. This improvement performs 3D and 2D feature extraction on a small number of consecutive slices in the early stage of the network. Then, in a novel way, their respective feature maps are fused to achieve a small number of parameters in the 2D network. Through the extraction of 3D features in the MRI data, D-UNet can achieve better performance than a pure 2D network.</p><p>Enhanced Mixing Loss: Second, in order to improve the convergence speed of the network, we propose a new loss function, called the Enhanced Mixing Loss, which not only enhances the gradient propagation of the traditional Dice Loss, but also combines the advantages of the Dice loss and Focal loss functions. This new method converges faster than using the two traditional loss functions, and exhibits a smoother convergence curve. In summary, this work has the following contributions: 1. We propose the D-UNet network to effectively segment the lesion area in the MRI data. The structure is based on the 2D UNet improvement. A part of the 3D convolution is added to the downsampling module to extract the spatial information in the MRI volume data; the extracted features are fused with the 2D structures in a new method. 2. We propose a novel loss function, which is expected to make the network converge in a faster and smoother fashion. It would not only enhance the gradient propagation in the traditional Dice loss, but also combine the merits of Dice loss and Focal loss functions. <ref type="bibr" target="#b2">3</ref>. The proposed method is tested on the ATLAS dataset and compared to three state-of-the-art, demonstrating the superior performance of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>We summarize some of the work related to stroke segmentation, including hand-crafted feature based methods and deep learning based methods. Among them, the deep learning methods include 2D-based CNN, 3D-based CNN, and the traditional segmentation loss function.</p><p>Hand-crafted feature based methods: Researchers have been working on the automatic segmentation and prediction of brain disease areas and have achieved good results <ref type="bibr" target="#b17">[18]</ref>. Kemmling et al. <ref type="bibr" target="#b19">[19]</ref> use a multivariate computed tomography perfusion (CTP)-based model to calculate the probability of voxelwise infarcts. <ref type="bibr">Kuo et al. [20]</ref> propose to use the SVM classifier to learn texture feature vectors for the segmentation of liver tumors. Chyzhyk et al. <ref type="bibr" target="#b21">[21]</ref>propose to construct an image data classifier from multimodal MRI data for voxel-based lesion segmentations. Sivakumar et al. <ref type="bibr" target="#b23">[22]</ref> use an adaptive neuro fuzzy inference system (ANFIS) classifier to detect and segment brain stroke areas automatically while using the heuristic histogram equalization technique (HHET) to enhance the internal regions of the brain image. These proposals in the literature are machine learning models based on multiple linear regressions, relying on the precise design of features by feature engineers. They achieve good performance on small sized data sets, but have limited generalization in larger data sets.</p><p>Deep learning based methods: Deep learning has emerged in recent years, which address a key limitation in traditional machine learning methods, which require engineers to artificially design features. Chen et al. <ref type="bibr" target="#b24">[23]</ref> propose a 2D network framework consisting of an ensemble of a DeconvNets (EDD)-Net and a multi-scale convolutional label evaluation net (MUSCLE Net); this ensemble achieves the best performance on a large clinical dataset. Cui et al. <ref type="bibr" target="#b26">[24]</ref> propose a network of cascaded structures for processing nasopharyngeal carcinoma cases in MRI images. The authors firstly segment the tumors and then classify the segmentation results to obtain four subregions of nasopharyngeal carcinoma. These deep learning methods convert the MRI data to 2D slices and apply 2D segmentation CNN for each slice. The 3D results are generated by connecting the 2D segmentation results. However, due to the limitations of the slices 2D characteristics, the important 3D context information in the volume data is neglected, thus the prediction may lose continuity.</p><p>3D CNN has proven to have great potential in the analysis of 3D MRI data. Kamnitsas et al. <ref type="bibr" target="#b27">[25]</ref> propose a two-path 3D CNN structure and uses a 3D fully connected conditional random field for post processing, which ranks first in the challenge of chronic stroke lesion segmentation (ISLES 2015). Zhang et al. <ref type="bibr" target="#b9">[10]</ref> propose 3D FC-DenseNet, which can make the network deeper by using the improved dense net tight connection structure to enhance the back propagation of image information and gradients. Feng et al. <ref type="bibr" target="#b28">[26]</ref> extract features from both the temporal and the spatial dimensions by using 3D convolution operations, which capture the dynamic information in multiple adjacent frames. However, they usually require more parameters and might sometimes over-fit on small training data sets <ref type="bibr" target="#b30">[27]</ref>, <ref type="bibr" target="#b31">[28]</ref>. In addition, 2D-based and 3D-based cascade methods have emerged. For example, Li et al. <ref type="bibr" target="#b32">[29]</ref> propose a hybrid densely connected UNet (H-DenseUNet), which first performs a 2D-based dense-UNet segmentation, and then uses a 3D-based CNN to correct the spatial continuity of the liver and the tumor.</p><p>The binary cross-entropy loss function <ref type="bibr" target="#b33">[30]</ref> is commonly used in deep learning based segmentation tasks. This function calculates the gradient by characterizing the difference in the probability distribution of each pixel in the predicted sample and the real sample. Tsung-Yi Lin et al. <ref type="bibr" target="#b35">[31]</ref> add a modulating factor to deal with the serious imbalance between the number of foreground and background pixels. Another common loss function is Dice's coefficient loss <ref type="bibr" target="#b36">[32]</ref>. This function directly calculates the gradient by the dice overlap coefficient of the predicted sample and the real label; it can also alleviate to some extent the segmentation problem resulting from the pixel imbalance between the foreground and the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>In this section, we introduce our approach including the proposed D-UNet framework, enhanced mixing loss, and the implementation details. In Section 3.1, we illustrate the proposed D-UNet framework, in Section 3.2, we introduce the enhanced mixing loss algorithm, and finally in Section 3.3, we present some implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">D-UNet for extracting three-dimensional information</head><p>The basic structure of the network consists of an improved UNet <ref type="bibr" target="#b38">[33]</ref>. This symmetrical encoder-decoder structure combines high-level semantics with low-level fine-grained surface information; it has achieved good effects on medical images. The encoding phase of the D-UNet consists of two dimensions. As shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>, both the 2D and 3D convolutions perform the downsampling operation in their respective dimensions; the results are combined through the dimension transform block which denote as a red cube. This fusion enables subsequent 2D networks to be integrated into the 3D information, refines the edges of the target area, and facilitates the ability of the network to identify small lesion areas. Meanwhile, since the 3D information is well extracted in the early stage of the network, and the trainable parameters of the network are extremely increased as the network deepens, the dimension transform block is only added in the early coding stage.</p><p>Specifically, consider <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, where H×W denotes the feature dimensions of height and width, D represents the depth in the volume feature, and C represents the channel of the feature map. The dimension transform block consists of 3D dimensionality reduction, channel excitation <ref type="bibr" target="#b39">[34]</ref>, and dimensional fusion. The squeeze-and-excite (SE) block has been proposed in recent years, where r denotes the reduction ratio, a hyperparameter which allows us to vary the capacity and computational cost of the SE block <ref type="bibr" target="#b39">[34]</ref>. This block activates the connection between different channels by weighting the feature channels. We apply this structure in the dimension fusion block in order to enhance the fusion effect of 3D features and 2D features.</p><p>In each dimension transform block, we first reduce the dimensions of the 3D branch feature map and then add with the 2D branch after SE weighted respectively. Specifically, let I 3 d and I 2 d denote the feature maps from 3D and 2D network respectively, which act as the input of dimension transform block, n denotes the batch size, h × w × d denotes the maps height, width, depth and last dimension c denotes the maps channel. We first convert I 3d ∈R n×h×w×d×c to I * 3d ∈R n×h×w×d×1 by using a 3D 1×1×1 convolution which filter number is set to 1, then we squeeze the dimensionality</p><formula xml:id="formula_0">of I * 3d from n × h × w × d × 1 to n × h × w × d.</formula><p>In order to keep the channel number consistent with the 2D branch for later integration, we also convert I * 3d ∈R n×h×w×d to I * 3d ∈R n×h×w×c by using a 2D 3×3 convolution that filter number is set to c. Let I 3d denote the I 3d after dimensionality reduction:</p><formula xml:id="formula_1">I 3d = f r (I 3d ), I 3d ∈ R n×h×w×c<label>(1)</label></formula><p>where f r indicates the dimensionality reduction operation, thus we convert the size of 3D feature map from I 3d ∈R n×h×w×d×c to I 3d ∈ R n×h×w×c . In order to enhance the feature expression ability of the two dimensions before fusion, we use an SE block to weight the 3D and 2D feature map channels, and add their channel weighted outputs:</p><formula xml:id="formula_2">T = f SE (I 3d ) + f SE (I 2d ), T ∈ R n×h×w×c<label>(2)</label></formula><p>The 3D and 2D features are fused in this step, where f SE denotes the SE weighted block proposed in <ref type="bibr" target="#b39">[34]</ref>. T denotes the feature map fusion which results in the dimension fusion block. More detailed parameter settings for the entire network are shown in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Enhanced Mixing Loss Function</head><p>In 3D medical data, especially MRI stroke images as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the volume occupied by the stroke is often very small throughout the scan interval. An extremely large number of background regions may dominate the loss function during training, which leads to the learning process easily falling into a local optimal solution. Therefore, we propose  Introduces the details of the dimension transform block, which has two branches for its input from 2D and 3D networks. First, the feature channel of the 3D network output (blue arrow) is compressed to 1 by using a 1x1x1 convolution. The compressed result is then squeezed in a spatial dimension and passed to a 2D 3x3 convolution. This makes the output consistent with the 2D network (gray arrow). Finally, each of the channels is weighted by the SE-block and then added together.</p><p>a new loss function, which refers to the method addressing the foreground-background voxel imbalance in <ref type="bibr" target="#b35">[31]</ref>, and combines two traditional loss functions in a concise manner <ref type="bibr" target="#b36">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Focal Loss</head><p>Focal loss (FL) is an improvement of the binary cross entropy loss (BCE), by adding a modulating factor. This reduces the loss contribution from easy samples and extends the range in low loss. We introduce the formula of focal loss from the binary cross entropy (BCE):</p><formula xml:id="formula_3">F L(p, g) = − N f i=1 α(1 − p) γ log(p), if g = 1 − N b i=1 (1 − α)p γ log(1 − p), otherwise<label>(3)</label></formula><p>where g ∈ 0, 1 represents the ground truth based on the pixel level; p ∈ [0, 1] represents the model prediction probability value, in which 0 denotes the background and 1 is the foreground; N f and N b represent the numbers of pixels of class 0 and class 1, respectively; α ∈ (0, 1] and γ ∈ [0, 5] are the modulation factors, which can be flexibly adjusted according to the situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Dice Coefficient Loss</head><p>The dice coefficient loss (DL) mitigates the imbalance problem of background and foreground pixels by modifying the segmentation evaluation index DSC between the prediction samples and the ground truth annotation, showing better performance in the segmentation task:</p><formula xml:id="formula_4">DL(p, g) = 1 − 2 N i=1 p i g i + δ N i=1 p 2 i + N i=1 g 2 i + δ<label>(4)</label></formula><p>where δ ∈ [0, 1] is a tunable parameter to prevent a divideby-zero error and let the negative samples also have a gradient propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Proposed Enhanced Mixing Loss</head><p>Based on the above two kinds of loss, we propose the enhanced mixing loss (EML) to increase the convergence speed. First, Log value was used in DL and we invert the value for keeping the value positive, thus enhancing the gradient obtained for each iteration. Then, in order to explore whether the two losses have mutually reinforcing relationships, we also add the focal loss. However, since the focal loss is based on the sum of all voxel probabilities, it is numerically much larger than the dice loss (DL(p, g) ∈ [0, 1]), which plays a leading role in gradient propagation. We hope that the newly added FL and log(DL) contribute equally to EML, so a balance factor of 1/N is added to FL to obtain a FL based voxel average. The formula for EML is as follows:</p><formula xml:id="formula_5">EM L(p, g) = 1 N F L(p, g) − log(DL(p, g))<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation details</head><p>In the data preprocessing, transverse section images have been selected for this experiment. Within each image, a square area is selected with the diagonal coordinates <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b46">40)</ref> and <ref type="bibr">(190,</ref><ref type="bibr">220)</ref>. This selected area eliminates irrelevant information and enlarges the proportion of stroke lesions in the entire image. Next, the cropped images are resized 192 × 192 using a bilinear interpolation. Finally, each slice of the processed image is integrated, with a spatial arrangement of two upper slices and one lower slice, forming a matrix of size 192 × 192 × 4. In the downsampling phase, modifications to reduce the total number of parameters from UNet are made. Specifically, the number of filters in the first convolution in the 2D-based and 3D-based streams is set to 32. After each pooling layer, the number of convolution filters is doubled, and finally the number of convolutions in the 2D stream is set to 512. The kernal initialization for each convolution is set using the Hes method <ref type="bibr" target="#b40">[35]</ref>. A batch normalization is conducted after each layer of convolution to improve the stability of the training. The parameter r in the dimension transform block is set to 16. α, γ, δ in the loss function is set to 1.1, 0.48, 1 respectly to fit our randomly selected dataset. With the SGD optimizer, the learning rate is set to 1e-6. Additional parameter settings are consistent with <ref type="bibr" target="#b42">[36]</ref>. We have also employed data augmentation methods to improve the robustness of the model, including setting the input mean to zero translation, scaling, and horizontal flipping. These methods are applied to all of our comparative experiments to ensure fairness. We have trained the models on three 1080TI GPUs. All of the models are trained using the first 150 epochs before validating, to optimize the performance of each architecture without any fine tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS AND DISCUS-SIONS</head><p>We compare our proposed method to the 2D and 3D convolutional UNet. In this section, we also show the superiority of the proposed loss and discuss the results of the proposed dimension-transform block at different stages. Datasets and quantitative indicators: We have used the Anatomical Tracings of Lesions-After-Stroke (ATLAS) dataset <ref type="bibr" target="#b6">[7]</ref> as our training and validation sets. The dataset contains 229 cases of chronic stroke with MRI T1 sequence scans, in which the size of each case is 233×197×189 while the physical size is 0.9×0.9×3.0mm 3 ; the scans delineate different lesion grade staging. We have randomly selected 183 cases (accounting for the overall 0.8 ratio) as the training set, and the remaining cases as validation sets. We report the model's performance in the Dice Similarity Coefficient (DSC), precision, and recall. DSC is an important indicator to assess the overall difference between our estimates and the ground truths. Recall usually reflects the extent of recall in the lesion area, which is an important reference in clinical practice. We perform threshold processing on all of the prediction results. When the probability that the pixel is predicted to be foreground is less than 0.5, we set it to zero, otherwise it is set to one. In addition, precision evaluates the quality of the segmentation, as the proportion of boundary pixels in the automatic segmentation that correspond to boundary pixels in the ground truth of the image. The quantitative indicator formulae are shown below:</p><formula xml:id="formula_6">DSC = 2T P 2T P + F P + F N (6) Recall = T P T P + F N<label>(7)</label></formula><p>P recision = T P T P + F P <ref type="bibr" target="#b7">(8)</ref> where, true positive (T P ) indicates that the model correctly predicted voxel. False positive (F P ) indicates the voxel that the model classify negative as positive. False negative (F N ) indicates that the positive voxel is mistakenly classified as negative by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance comparison with UNet and its 2D and 3D variants</head><p>We compare the proposed method with the baseline architecture, UNet, both qualitatively and quantitatively. We have trained two versions of UNet to better exploit its performance. The original version uses the parameter settings described in <ref type="bibr" target="#b38">[33]</ref>. In the transformed version, the number of convolution kernels is reduced and batch normalization is added after each convolution. Section 3.3 provides a more detailed description of the transform. Furthermore, we compare the proposed method with the 3D structure to prove that our method can extract the 3D information of the data with a small amount of structure. We show the results of different structure outputs, and choose case based DSC, recall, and precision as our quantitative indicators. Furthermore, we also conduct a global based DSC assessment as an auxiliary judgment. It is worth noting that the batch size of all networks is set to 36 during training, except for the 3D UNet. Since the 3D Unet has very high memory demands, the batch size is set to six.</p><p>In the quantitative comparison, as shown in <ref type="table" target="#tab_3">Table 2</ref>, the UNet (original), UNet (transform), and the proposed method are evaluated. The 2D transform is close to the original in both indicators. However, with respect to the kernel number, the transformation scheme is only half of the original network. It shows that a large number of convolution kernels are unnecessary for this small data set. We have also reduced the number of 3D Unet convolution kernels to match the number of 2D Unet convolution kernels. In addition, all the transform versions and our proposed method adds batch normalization after each convolution layer (except for the last layer of convolution) to ensure convergence stability. The 2D structure achieves a better index than the 3D structure, we consider the following factors: first, due to its huge network structure, 3D network requires a huge amount of time to train, making it difficult to adjust the hyperparameters to optimize performance. Then, Overfitting can easily occur in large network structures, especially if there are fewer training sets (our training set contains only 183 cases). Last but not least, 3D structure requires a large amount of computing resources, which limits the width and depth of the 3D structure by our implementation platform conditions, leading to a decrease in the performance. The results we reported is also consistent with that of <ref type="bibr" target="#b32">[29]</ref>, which also shows the phenomenon that the segmentation performance of the 3d structure is lower than the 2d structure. In addition, the time required to train a 3D UNet (140 hours) is about six times that of 2D UNet (23.8 hours). By comparing the segmentation performance and the number of parameters, the metrics for the proposed method are improved by 3.83% over a single 2D structure, and the number of parameters is only increased by 2%. These results show that with the addition of 3D structure, the overall performance of the network is improved. It proves that compared with the simple 2D network structure, the proposed dimension transform block can effectively utilize 3D information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with other state-of-the-art methods</head><p>In this section, we compare the proposed model with other existing well-known frameworks and methods mentioned in <ref type="bibr" target="#b43">[37]</ref>. SegNet <ref type="bibr" target="#b44">[38]</ref> performs nonlinear upsampling; this is accomplished using unpooling for the maxpooling index defined in the downsampling phase. Pyramid Scene Parsing Network <ref type="bibr" target="#b45">[39]</ref> introduces more context information by using atrous convolution and pyramid pooling. DeepLab v3 plus <ref type="bibr" target="#b46">[40]</ref> is the latest version of the Google DeepLab series, which combines the advantages of Atrous Spatial Pyramid Pooling and an encoder-decoder structure. The above methods have achieved good results in the deep learning segmentation task. We compare with these methods to demonstrate the superior performance of the proposed D-UNet. All of the network parameters are configured according to the original articles, except for the minor changes mentioned in Section 4.1; the networks are implemented in our platform. The training set and validation set are strictly consistent (including data preprocessing). During training, the batch size is set to 36. Additional details can be viewed the open source code.</p><p>The prediction results obtained with the different frameworks are illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. Seven prediction maps are randomly selected and sorted by the size of the lesion area in ascending order. As shown in the figure, it can be observed that the lesion with a very small foreground area is quite fuzzy, indicating this case is very difficult for the network. In spite of this, our method correctly detects the lesion area. It proves that our approach possesses the ability to identify difficult samples. From the third and fourth rows of <ref type="figure" target="#fig_3">Fig.  3</ref>, we have found that all models correctly predicted the location of the lesion area in the case of big differences in foreground and background. Furthermore, our method is closer to the ground truth in terms of lesion boundaries.</p><p>Observing the last two rows, we have found that the proposed method still maintains a good, feature expression ability when the boundary is very blurry. This is because we combine the 3D features and the spatial dimensions to more effectively express the characteristics of the edge blurred lesions. In order to show the distribution of the results and prove the stability of the proposed method, we draw a box plot based on the DSC score for each case. These results are shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. The lower edge of all methods is zero because the ATLAS data set has a very large number of small lesion areas (e.g., the first to third rows of <ref type="figure" target="#fig_3">Fig. 3)</ref>, which easily cause the model to fail to recognize the lesions. The upper edge and median line for the proposed method are the best in comparison with other highly recognized methods, which means it not only achieves the highest segmentation performance on a single case, but also yields better median scores for all cases.</p><p>To quantitatively illustrate the superiority of our method, we compare the results of these algorithms and summarize them in <ref type="table" target="#tab_4">Table 3</ref>, where DSC, recall, and precision are based on the mean ± standard deviation of each case, and DSC (global) represents the metric based on the voxel calculation. Hu et al. use the ATLAS dataset, and select some specific cases as training/validation sets to summarize some traditional segmentation methods. We compare several deep learning segmentation frameworks with the methods mentioned in <ref type="bibr" target="#b43">[37]</ref>. The top half of the table shows the implementation in <ref type="bibr" target="#b43">[37]</ref>, and the bottom half is implemented on our platform. It can be seen from the table that the deep learning based methods achieve better performance than the traditional algorithms. With respect to the DSC scores, the proposed method ranked first with a score of 0.7231. This is 0.30 higher than the Clusterize method in DSC (the lowest), and 0.0383 higher than the UNet method (second best). Our method is superior in terms of the segmentation performance for each case. DSC (global) can reflect a voxel-based overall DSC score more intuitively. With respect to recall and precision, the performance of the traditional algorithm Clusterize is the highest in recall, but its precision score is the lowest, which indicates that this algorithm identifies many non-lesional areas as lesions, thus causing a high recall. The proposed method ranked third in the recall score of 0.5243, and the highest in the precision score of 0.6631, which indicates that the identified regions are basically the correct lesion areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss validity</head><p>In order to compare the effectiveness of the proposed loss, we trained on the proposed model and compared several common losses in the segmentation task. As illustrated in  <ref type="figure" target="#fig_2">Fig. 5</ref>, the dice coefficient loss converges faster than the focal loss, but is subsequently exceeded by the focal loss. In each period, our method converges faster than other methods.</p><p>We also performed a quantitative analysis of the three losses, as shown in <ref type="table" target="#tab_5">Table 4</ref>. It is worth noting that our goal is to make the proposed model converge faster. This  statistical score is only used as a secondary reference. Since DL directly uses 1 -DSC as punished, it has an advantage to achieve the highest DSC score. Therefore, despite the proposed EML is slightly lower than DL in DSC (0.0115 lower) and Recall (0.057±0.002 lower), we consider that it is within an acceptable range. On the other hand, EML achieves the highest precision whereas DL presents the lowest, which indicates that EML can effectively reduce the false positive results (This phenomenon is also consistent with FL). Therefore, EML is competitive in segmentation performance compared to DL and FL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison of dimension fusion blocks between different layers</head><p>We also compare the results of the dimension transform block used in different ways (Add, SE) and in different downsampling layers. The DSC is used as the main evaluation index of model performance, and the number of their parameters is enumerated. The 3D framework used in the experiment, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, has only two pooling layers for the dimension transform block within three layers. The 'add' in <ref type="table" target="#tab_6">Table 5</ref> represents the use of the final fusion operation of the dimension fusion block, (i.e., the Add block in <ref type="figure" target="#fig_1">Fig. 2</ref>), and the 'SE' indicates the use of the SE block the figure <ref type="bibr" target="#b39">[34]</ref>. The last column of the name in the architecture indicates which layer is used in the conversion structure. For example, 'Add-23' means the corresponding 3D structure fusion before the second and third maxpooling in the 2D structure.</p><p>In order to prove the validity of dimensional transformation on 2D networks, we compare the performance of UNet (division) and different structures using the dimension fusion block in different layers. All of the results with using the dimensional fusion are better than UNet without using the dimension fusion block. We suspect this is because the network fuses the 3D features in downsampling. By comparing Add-1, Add-12, and Add-23, we find that the deeper the 3D structure is, the better the fusion performance results. This can be interpreted as a deeper 3D structure provides a better feature extraction ability. However, we found an interesting phenomenon: the DSC with more layers in the case Add-123 has decreased. This may be because the gradient of the 3D and 2D structure is tighter with the fusion of more layers; this change in gradient may result in a decrease in the efficiency of the feature extraction. We demonstrate the validity of the proposed fusion structure and demonstrate that our method obtains 3D information in an efficient manner.</p><p>To prove that the SE block can achieve better fusion by weighting the output of the two dimensions, we show a comparison of the addition of SE (the last three rows) and direct fusion in <ref type="table" target="#tab_6">Table 5</ref>. The structure of the SE block is consistent with the direct fusion for the number of layers, but all of the structures using the SE block are higher than the DSC of the direct fusion structure. It is shown that the nonlinear weighting before feature fusion can enhance the fusion effect of features.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Automated stroke segmentation plays an important role in clinical diagnosis and prognosis. It is of great value to quickly and accurately identify areas of the lesions and help physicians make surgical plans without high computing resource demands. In this paper, we propose an end-to-end training method for the automatic stroke segmentation, in which 3D context information can be effectively utilized, with low hardware requirements. Meanwhile, we propose a new loss function for faster and smoother convergence. The proposed method has been compared with three stateof-the-art methods; it achieves the best performance on two quality metrics (DSC = 0.5349±0.2763, Precision = 0.6331±0.2958). In future work, we hope to increase the punishment for extremely difficult samples inspired from <ref type="bibr" target="#b48">[41]</ref>, which may further enhance the performance of the proposed EML. We plan to validate our method on a larger clinical dataset to verify the generalization of the method in the current 3D structure, further study the possibility of dimension fusion block combinations, and extend our model to different applications, for example, calculating overlap of the lesion with existing brain regions or structures, for used as predictors of long-term stroke recovery and rehabilitation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The MRI T1 sequence stroke image from the ATLAS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The entire D-UNet architecture is shown in (a). This network improves 2D UNet, which combines 3D convolution in the downsampling phase and uses a dimension transform block to combine them. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 ,</head><label>5</label><figDesc>the results show several DSC rising graphs on the training sets as the number of training iterations increases. Comparing the scores of the losses in the training set, in the early stage of training (about 30 epoch) shown on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Comparisons of our method, Baseline, DenseUnet, DeepLabv3+, PSPNet, and FCN-8s on four different patients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Box plots of DSC score results for different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The DSC score curve of the training set during the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This work was supported by funding from the National Natural Science Foundation of China (61601450, 61871371, and 81830056), Science and Technology Planning Project of Guangdong Province (2017B020227012) (Corresponding authors: Shanshan Wang) • Y. Z and W. H are with the School of Shenzhen University, Shenzhen 518060, China. E-mail: yjzhou@szu.edu.cn, 2170249218@email.szu.edu.cn • P. D is with School of Information Technologies, University of Sydney, NSW 2006, Australia.</figDesc><table /><note>•• Y. X is with School of Computer Science, Northwestern Polytechnical University, Xian 710072, China.• S. Wang is with the Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China. E-mail: ss.wang@siat.ac.cn, sophi- asswang@hotmail.com.• Code will be available at: https://github.com/SZUHvern/D- UNet/tree/master.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Architecture of the proposed D-UNet. Feature size denotes the size of the manipulated feature map while the last dimension indicates the channel number. Up-sampling [*] indicates that the corresponding layer number is concatenated before up sampling and N * indicates the number of features of the corresponding layer number, for example, Up-sampling block 1 being connected to Convolution block 4, N is set to 256, up sampling block 2 being connected to dimension fusion block 3, N is set to 128, and so on.</figDesc><table><row><cell></cell><cell>Feature size</cell><cell>Two-dimensional operation</cell><cell>Feature size</cell><cell>Three-dimensional operation</cell></row><row><cell>Input</cell><cell>192×192×4</cell><cell>-</cell><cell>192×192×4×1</cell><cell>-</cell></row><row><cell>Convolution block 1</cell><cell>192×192×32</cell><cell>2×(3×3 Conv+ Bn)</cell><cell>192×192×4×3</cell><cell>2×(3×3×3 Conv+ Bn)</cell></row><row><cell>Pooling</cell><cell>96×96×32</cell><cell>2×2 max pooling</cell><cell>96×96×2×32</cell><cell>2×2×2 max pooling</cell></row><row><cell>Convolution block 2</cell><cell>96×96×64</cell><cell>2×(3×3 Conv+ Bn)</cell><cell>96×96×2×64</cell><cell>2×(3×3×3 Conv+ Bn)</cell></row><row><cell>Dimension fusion block 2</cell><cell>96×96×64</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pooling</cell><cell>48×48×64</cell><cell>2×2 max pooling</cell><cell>48×48×1×64</cell><cell>2×2×2 max pooling</cell></row><row><cell>Convolution block 3</cell><cell>48×48×128</cell><cell>2×(3×3 Conv+ Bn)</cell><cell>48×48×1×128</cell><cell>2×(3×3×3 Conv+ Bn)</cell></row><row><cell>Dimension fusion block 3</cell><cell>48×48×128</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pooling</cell><cell>24×24×128</cell><cell>2×2 max pooling</cell><cell>-</cell><cell>-</cell></row><row><cell>Convolution block 4</cell><cell>24×24×256</cell><cell>2×(3×3 Conv+ Bn)</cell><cell>-</cell><cell>-</cell></row><row><cell>Dropout</cell><cell>24×24×256</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pooling</cell><cell>12×12×256</cell><cell>2×2 max pooling</cell><cell>-</cell><cell>-</cell></row><row><cell>Convolution block 5</cell><cell>12×12×512</cell><cell>2×(3×3 Conv+ Bn)</cell><cell>-</cell><cell>-</cell></row><row><cell>Dropout</cell><cell>12×12×512</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Up-sampling block 1-4</cell><cell>192×192×32</cell><cell>2×2 Up-sampling[*]</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>2×(3×3 Conv+ Bn)</cell><cell></cell><cell></cell></row><row><cell>Convolution</cell><cell>192×192×1</cell><cell>1×1 Conv</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Comparison results of the proposed method with baseline approaches.</figDesc><table><row><cell>Method</cell><cell>DSC</cell><cell>DSC(global)</cell><cell>Recall</cell><cell>Precision</cell><cell>Total parameters</cell></row><row><cell>2D UNet(original)</cell><cell>0.4874±0.2858</cell><cell>0.7117</cell><cell>0.4838±0.2983</cell><cell>0.5612±0.3229</cell><cell>31,030,593</cell></row><row><cell>2D UNet(transform)</cell><cell>0.4966±0.2906</cell><cell>0.7146</cell><cell>0.5038±0.3044</cell><cell>0.5511±0.3298</cell><cell>7,771,297</cell></row><row><cell>3D UNet(transform)</cell><cell>0.4710±0.2877</cell><cell>0.7098</cell><cell>0.4736±0.3099</cell><cell>0.5531±0.3247</cell><cell>22,597,826</cell></row><row><cell>Ours</cell><cell>0.5349±0.2763</cell><cell>0.7231</cell><cell>0.5243±0.2910</cell><cell>0.6331±0.2958</cell><cell>8,640,163</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>The quantitative comparison of different methods. Among them, DSC, Recall, and Precision are based on the mean ± standard deviation calculated in each case, and DSC (global) represents the DSC based on the voxel calculation.</figDesc><table><row><cell>Method</cell><cell>DSC</cell><cell>DSC(global)</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>Clusterize</cell><cell>0.23±0.19</cell><cell>-</cell><cell>0.79±0.23</cell><cell>0.16±0.15</cell></row><row><cell>ALI</cell><cell>0.36±0.25</cell><cell>-</cell><cell>0.55±0.31</cell><cell>0.31±0.25</cell></row><row><cell>Lesion gnb</cell><cell>0.36±0.23</cell><cell>-</cell><cell>0.69±0.29</cell><cell>0.30±0.20</cell></row><row><cell>LINDA</cell><cell>0.45±0.31</cell><cell>-</cell><cell>0.52±0.34</cell><cell>0.50±0.34</cell></row><row><cell>SegNet</cell><cell>0.3292±0.2514</cell><cell>0.5993</cell><cell>03318±0.2654</cell><cell>0.3846±0.2883</cell></row><row><cell>PSP</cell><cell>0.4462±0.2633</cell><cell>0.6729</cell><cell>0.4704±0.2780</cell><cell>0.4998±0.2913</cell></row><row><cell>Deeplab v3 plus</cell><cell>0.4529±0.2921</cell><cell>0.7104</cell><cell>0.4456±0.3032</cell><cell>0.5627±0.3249</cell></row><row><cell>UNet</cell><cell>0.4966±0.2906</cell><cell>0.7146</cell><cell>0.5038±0.3044</cell><cell>0.5511±0.3298</cell></row><row><cell>Ours</cell><cell>0.5349±0.2763</cell><cell>0.7231</cell><cell>0.5243±0.2910</cell><cell>0.6331±0.2958</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Quantitative analysis of the three loss functions.</figDesc><table><row><cell>Method</cell><cell>DSC(global)</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>FL</cell><cell>0.6805</cell><cell cols="2">0.4339±0.2625 0.6225±0.3667</cell></row><row><cell>DL</cell><cell>0.7346</cell><cell>0.53±0.2908</cell><cell>0.6143±0.3324</cell></row><row><cell>EML</cell><cell>0.7231</cell><cell>0.5243±0.2910</cell><cell>0.6331±0.2958</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Comparison results of the proposed method with baseline approaches.</figDesc><table><row><cell>Architecture</cell><cell>DSC</cell><cell>Total parameters</cell></row><row><cell>2D UNet(transform)</cell><cell>0.4966±0.2906</cell><cell>7,771,297</cell></row><row><cell>Add-1</cell><cell>0.5102±0.2932</cell><cell>7,802,210</cell></row><row><cell>Add-12</cell><cell>0.5216±0.2776</cell><cell>7,970,019</cell></row><row><cell>Add-23</cell><cell>0.5248±0.2770</cell><cell>8,635,043</cell></row><row><cell>Add-123</cell><cell>0.5110±0.2762</cell><cell>8,636,260</cell></row><row><cell>SEAdd-12</cell><cell>0.5235±0.2851</cell><cell>7,971,299</cell></row><row><cell>SEAdd-23</cell><cell>0.5349±0.2763</cell><cell>8,640,163</cell></row><row><cell>SEAdd-123</cell><cell>0.5186±0.2865</cell><cell>8,647,012</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>shanshan wang Shanshan Wang Shanshan Wang received her dual PhD degree in information technologies and biomedical engineering from the University of Sydney and Shanghai Jiao Tong University. She is an associate professor in Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences. Her research interests include machine learning, fast medical imaging and radiomics. She has published over 40 journal and conference papers in these areas. She is a member of the IEEE.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimates of worldwide burden of cancer in 2008: GLOBOCAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H R</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of cancer</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2893" to="2917" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Liver tumor volume estimation by semi-automatic segmentation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marziliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Thng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Engineering in Medicine and Biology 27th Annual Conference. IEEE</title>
		<imprint>
			<biblScope unit="page" from="3296" to="3299" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Acute ischemic stroke</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H B</forename><surname>Van Der Worp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="572" to="579" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cost of stroke in the United Kingdom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Age and ageing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="32" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interrater agreement for final infarct MRI lesion delineation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A B</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K Y</forename><surname>Jonsdottir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mouridsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stroke</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3768" to="3771" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measurement of infarct volume in stroke patients using adaptive segmentation of diffusion weighted MR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A L</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S J</forename><surname>Allder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G S</forename><surname>Delay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A large, open source dataset of stroke anatomical brain images and manual lesion segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S L</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J M</forename><surname>Anglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N W</forename><surname>Banks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J]. Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180011</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Ishemic Stroke Lesion Segmentation by Analyzing MRI Images Using Dilated and Transposed Convolutions in Convolutional Neural Networks[C]</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Fourth International Conference on Computing Communication Control and Automation (ICCUBEA)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards clinical diagnosis: Automated stroke lesion segmentation on multi-spectral MR image using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="57006" to="57016" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic segmentation of acute ischemic stroke from DWI using 3-D fully convolutional DenseNets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2149" to="2160" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stroke lesion detection using convolutional neural networks[C]</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D R</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reboucas</forename><surname>Filho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P P</forename><surname>De Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimizing U-Net to Segment Left Ventricle from Magnetic Resonance Imaging[C]</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Charmchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Punithakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boulanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="327" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Classification of Atrial Fibrillation with Pre-Trained Convolutional Neural Network Models[C]</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Qayyum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meriaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G C Y</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Engineering and Sciences (IECBES). IEEE</title>
		<imprint>
			<biblScope unit="page" from="594" to="599" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Drinet for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2453" to="2462" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic real-time CNNbased neonatal brain ventricles segmentation[C]</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N G</forename><surname>Cuccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tyagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="716" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D fully convolutional networks for co-segmentation of tumors on PET-CT images[C]</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image segmentation of liver CT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Symposium on Computational Intelligence and Design (ISCID)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="210" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Querying Representative and Informative Super-pixels for Filament Segmentation in Bioimages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M X</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on computational biology and bioinformatics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multivariate dynamic prediction of ischemic infarction and tissue salvage as a function of time and degree of recanalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kemmling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Flottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N D</forename><surname>Forkert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cerebral Blood Flow &amp; Metabolism</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1397" to="1405" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Texture-based treatment prediction by automatic liver tumor segmentation on computed tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer, Information and Telecommunication Systems (CITS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An active learning approach for stroke lesion segmentation on multimodal MRI data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chyzhyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dacosta-Aguayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="26" to="36" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An efficient automated methodology for detecting and segmenting the ischemic stroke in brain MRI images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ganeshkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Imaging Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fully automatic acute ischemic lesion segmentation in DWI using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">NeuroImage: Clinical</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="633" to="643" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic semantic segmentation of brain gliomas from MRI images using a deep cascaded neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J]. Journal of healthcare engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient multiscale 3D CNN with fully connected CRF for accurate brain lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V F J</forename><surname>Newcombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical image analysis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A Deep Learning Approach for Targeted Contrast-Enhanced Ultrasound Based Prostate Cancer Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on computational biology and bioinformatics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">H-DenseUNet: hybrid densely connected UNet for liver and tumor segmentation from CT volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2663" to="2674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="37" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>V-Net</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<title level="m">Fourth International Conference on 3D Vision (3DV). IEEE</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A comparison of automated lesion segmentation approaches for chronic stroke T1-weighted MRI data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K L</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Liew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J]. bioRxiv</title>
		<imprint>
			<biblScope unit="page">441451</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Beyond the pixelwise loss for topology-aware delineation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koziski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3136" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
