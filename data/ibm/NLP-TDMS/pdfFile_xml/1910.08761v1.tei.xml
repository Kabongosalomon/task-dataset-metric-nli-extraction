<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Component Attention Guided Face Super-Resolution Network: CAGFace</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratheesh</forename><surname>Kalarot</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Auckland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
							<email>taoli@purdue.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
							<email>fatih.porikli@anu.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Component Attention Guided Face Super-Resolution Network: CAGFace</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To make the best use of the underlying structure of faces, the collective information through face datasets and the intermediate estimates during the upsampling process, here we introduce a fully convolutional multi-stage neural network for 4× super-resolution for face images. We implicitly impose facial component-wise attention maps using a segmentation network to allow our network to focus on faceinherent patterns. Each stage of our network is composed of a stem layer, a residual backbone, and spatial upsampling layers. We recurrently apply stages to reconstruct an intermediate image, and then reuse its space-to-depth converted versions to bootstrap and enhance image quality progressively. Our experiments show that our face super-resolution method achieves quantitatively superior and perceptually pleasing results in comparison to state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Our brains are wonderfully attuned to perceiving faces. In addition to the visual cortex in the occipital lobe, the entire region of the brain called the fusiform gyrus is dedicated to interpreting and forming a mental representation of faces <ref type="bibr" target="#b35">[36]</ref>. From early childhood, even very shortly after birth, human brains possess facial inference capacities and display more interest in face images than any other pattern <ref type="bibr" target="#b37">[38]</ref>. As a species, we almost obsessively monitor and pay close attention to subtle details in paces that can allow gleaning into the origin, emotional state, internal thought process, level of engagement, and health qualities of others around us. Most of us pay more attention to faces than we do to anything other object categories. Supporting this, many gaze tracking studies show that the profile picture or avatar is the first place the eye is drawn to on social media profiles <ref type="bibr" target="#b49">[50]</ref>. Pictures with human faces are with a large margin more likely to receive likes than the ones with no faces. It is not surprising that almost one-third of social media images are selfies and more than half are tagged with a label relates to face.</p><p>The resolution of the faces is an essential factor, and cer-  tain features appear to be found more attractive in higher resolution across individuals and cultures <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref>. Especially the eye and mouth regions are critical for face perception, as well as for neural responses to faces <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b8">9]</ref>. Moreover, the interpretation of facial features is determined by the relative arrangement of parts within a face context <ref type="bibr" target="#b36">[37]</ref>. Attention selection and guidance, thus, are important elements of high-resolution stimuli in the modeling of the processes in visual processing. High-resolution face images provide crucial clues not only for human observation but also for computer analysis <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b72">73]</ref>. The performance of common facial analy-sis techniques, such as face alignment <ref type="bibr" target="#b2">[3]</ref> and identification <ref type="bibr" target="#b48">[49]</ref>, degrade when the resolution of a face is low. To provide a viable way to recover a high-resolution (HR) face image from its low-resolution (LR) counterpart, many face super-resolution methods <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref> that rely on deep learning networks are proposed in recent years. Some of these methods explore direct image intensity correspondences between LR and HR faces, albeit being limited to low-resolution, e.g., 16×16, input images where the whole face to be included in the image. They can neither handle large input faces due to computational and memory requirements in training and inference times nor can they resolve fine-grained face-specific patterns. Besides, their dependency on near-frontal faces, which is prevalent in popular datasets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b19">20]</ref>, restricts their usage for large pose variations causing distorted facial details. A naive idea to remedy this problem is to augment the training data with large pose variations during the training stage. However, this strategy leads to suboptimal results due to the increased variance of face data to be modeled and also potentially erroneous localization of facial landmarks, which is a difficult task in small LR images under large pose variations.</p><p>In this paper, in contrast to previous attempts that often demand and apply the whole face image through their neural layers, we adapt a patch-based face super-resolution method that can operate efficiently on large input faces. Our intuition is that, although it is challenging to detect facial landmarks of the face accurately, it is possible to estimate patch-based attention maps of facial components approximately and steer the super-resolution process with these attention maps to facilitate more natural and accurate resolution enhancement.</p><p>Our model consists of an off-line trained component network and two super-resolution stages. We first segment facial components using a neural network trained off-line. These components can be hair, skin, eyes, mouth, eyebrows, nose, ears, neck, and similar facial regions. In particular, we use three components; hair, skin, and other parts (eyes, mouth, nose, eyebrows, ears) for simplicity. We apply Gaussian smoothing to decrease the sensitivity of component segmentation errors. We multiply the input image pixel-wise with each component heatmaps to obtain heatmap-weighted components, which allows us to impose components as implicit attention priors. We stack the original image and the attention maps into a block. In the training phase, we randomly sample patches from this face-wise block where each patch includes the cropped original image and the corresponding attention maps. The random sampling generates identically sized patches and their augmented (flipped) versions. In testing, we process the LR image patch-wise and aggregate their HR estimations.</p><p>Each super-resolution stage has three main components, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>; a stem layer that blends the input patch channels, a residual backbone that applies fully convolutional blocks on low-resolution feature maps, and a spatial upsampling layer that reconstructs the high-resolution image. The residual backbone is made up of fully convolutional residual units. After a series of residual units, we embed a direct skip connection from the first feature layer to the last one to maintain the influence of the original reference image on the feature map of the last layer. Thus, our backbone is conditioned on reconstructing the residual info, which includes the missing high-resolution patterns in visual data. The residual blocks and direct skip connection also allow us to deepen the backbone, which boosts the overall representation capacity of the network and to increase the areas of the receptive fields for the higher level convolutional layers, which enables better contextual feedback. The residual backbone utilizes the low-resolution image and space-to-depth shuffled estimated high-resolution output of the previous stage, which permits transferring the initial model into progressively more complex networks in the following stages. Note that, each state is an independent network. Following the residual backbone, we apply spatial upsampling layers to reconstruct a higher-resolution image from its feature map. These layers use pixel shuffling with learned weights; therefore, we do not require deconvolutions. The residual backbone prepares the best possible feature maps, which have a large number of channels, and the spatial upsampling layers rearrange these feature maps into the high-resolution images using the learned weights of the filters of these layers.</p><p>To summarize, the contributions of this paper are:</p><p>• We introduce a patch-based, fully convolutional network for single image face super-resolution that processes patches in their original low-resolution throughout its backbone and layers then reconstructs the highresolution output from rearranged feature maps.</p><p>• We recurrently apply the super-resolution stages to leverage on the reconstructed high-resolution outputs from the previous stage to enhance estimated highresolution details progressively.</p><p>• As our experiments demonstrate, our method outperforms existing face super-resolution methods by a large margin without inducing perceptual artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image super-resolution aims at restoring an HR counterpart of a given an LR image as input. This task has been one of the most fundamental challenges in computer vision, and many approaches have been proposed within the last two decades including kernel interpolations <ref type="bibr" target="#b30">[31]</ref>, edge statistics <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b45">46]</ref>, patch-based schemes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>, Bayesian methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43]</ref>, and supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b69">70</ref>]. An in-depth discussion of the available solutions can be found in recent surveys <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>With the compelling advance of deep learning models, in particular, the generative adversarial networks (GAN) <ref type="bibr" target="#b15">[16]</ref>, a new wave of convolutional neural network (CNN) based image super-resolution methods have also been proposed. Most notably, SRCNN <ref type="bibr" target="#b9">[10]</ref> and SRGAN <ref type="bibr" target="#b29">[30]</ref> apply a CNN and a GAN, respectively, to hallucinate HR image pixels. The work in <ref type="bibr" target="#b27">[28]</ref> progressively estimates the residual of high-frequency details using a Laplacian pyramid superresolution network, <ref type="bibr" target="#b53">[54]</ref> introduces the SFT-GAN for classconditioned image super-resolution, and <ref type="bibr" target="#b54">[55]</ref> proposes ES-RGAN that leveraged a relativistic GAN <ref type="bibr" target="#b23">[24]</ref> to estimate the distance between two images. Unlike most super-resolution models that are trained using synthetic LR images, <ref type="bibr" target="#b71">[72]</ref> obtains LR-HR image pairs by zooming-in and -out camera lens to characterize the imaging system degradation functions. We refer readers to <ref type="bibr" target="#b63">[64]</ref> for a comprehensive overview of deep learning-based and to <ref type="bibr" target="#b59">[60]</ref> for canonical super-resolution approaches.</p><p>Many super-resolution methods employ facial priors to achieve higher-resolution faces. Earlier methods assume faces are in a controlled environment, and the variations are minor. For instance, the work proposed in <ref type="bibr" target="#b1">[2]</ref> uses a spatial gradient distribution as a prior for the frontal faces. In <ref type="bibr" target="#b52">[53]</ref>, a mapping between LR and HR faces is modeled by an eigen-transform. <ref type="bibr" target="#b26">[27]</ref> learns a nonlinear Lagrangian model for HR face images by finding the model parameters that best fit the LR image. The work in <ref type="bibr" target="#b58">[59]</ref> incorporates face priors by mapping specific facial components (similar to our method), yet the correspondence between the components is explicitly based on landmark detection, which is diffi-cult to obtain when the upsampling factor is large. The cascaded framework proposed by <ref type="bibr" target="#b74">[75]</ref> super-resolves tiny faces by alternatively optimizing for face hallucination and dense correspondence field estimation. The method presented in <ref type="bibr" target="#b44">[45]</ref> generates facial parts by CNNs and explicitly synthesizes fine-grained facial structures through part enhancement. FSRNet <ref type="bibr" target="#b5">[6]</ref> computes facial landmark heatmaps and alignment parsing maps for end-to-end training.</p><p>The imposed image quality measure and the terms of the loss function between the reconstructed and original HR images play a critical role in super-resolution. Peak Signal-to-Noise Ratio (PSNR) is the most common metric to measure the quality <ref type="bibr" target="#b18">[19]</ref>; however, a higher PSNR value does not necessarily imply a more visually appealing result <ref type="bibr" target="#b56">[57]</ref>. To better simulate the human visual perception, Structural Similarity Index Measure (SSIM) <ref type="bibr" target="#b55">[56]</ref> separates the task of similarity measurement into three components: luminance, contrast, and structure. Multi-Scale Structural Similarity Index Measure (MS-SSIM) <ref type="bibr" target="#b57">[58]</ref> adapts to the variations of viewing conditions, and Feature Similarity Index (FSIM) <ref type="bibr" target="#b70">[71]</ref> extends SSIM to feature space. Inception Score (IS) <ref type="bibr" target="#b41">[42]</ref> measures the quality of generated images and their diversity, and Fréchet Inception Distance (FID) <ref type="bibr" target="#b17">[18]</ref> extracts features from an intermediate layer of an Inception Network <ref type="bibr" target="#b47">[48]</ref>. Accordingly, many loss functions have been proposed to train deep neural networks for super-resolution, such as pixel-wise mean squared error (MSE) <ref type="bibr" target="#b50">[51]</ref>. While the MSE results in higher PSNR values, it often causes blur and suppresses sharp textures <ref type="bibr" target="#b56">[57]</ref>. To overcome this, perceptual loss <ref type="bibr" target="#b22">[23]</ref> imposes feature similarity between the superresolved and LR images. Perceptual loss is computed over the layer right before the FC layers of VGG19 in <ref type="bibr" target="#b29">[30]</ref>, or the B1, B2, and B3 blocks of ResNet50 in <ref type="bibr" target="#b3">[4]</ref>. A heatmap loss is proposed to preserve structural consistency between LR and HR images further <ref type="bibr" target="#b3">[4]</ref>. Leveraging an adversarial loss from a discriminator has been shown to generate convincing results <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b38">39]</ref> as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method: CAGFace</head><p>Our face super-resolution solution is composed of multiple stages. Here, we use two consecutive stages that achieve 4× super-resolution, yet our methodology can be applied recurrently for higher upsampling goals. As aforementioned, we bootstrap the super-resolution process by spaceto-depth rearranging the estimated high-resolution image into multiple low-resolution channels, imposing the feature maps of the first stem layer (explained below) via a stage-wise skip connections for additional regularization, and applying a second stage network. Our patch-recurrent approach progressively bootstraps on the estimated highresolution results, thus provides additional performance improvements.</p><p>First, we use a network that segments the facial components. We apply a layer that imposes spatial attention by multiplying the LR input image by the component heatmaps. After a random sampling of patches, we apply two stages of super-resolution networks.</p><p>We achieve 4× super-resolution with two consecutive stages of 2× resolution enhancing networks. Notice that, unlike existing methods, our method does not employ a 2× super-resolution, followed by a second 2× super-resolution on the output of the first stage. The spatial size of the input feature map to the second stage is identical to the size of the original LR image. We learn the most useful features for 4× super-resolution after the first stage that also reconstructs a 2× image. This mid-stage reconstruction enables us to provide an additional regularization for our loss function.</p><p>Each stage contains a separate stem layer, a collection of multiple residual blocks, and an upsampling layer followed by a final mixing layer, as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. Please see <ref type="table" target="#tab_1">Table 1</ref> for the network parameters. In addition to these, the second stage has a depth-to-image conversion layer. These stages have similar kernels, yet their hyperparameters are different. Our network has conventional residual block skip connections and also a stage-wise skip connection that propagates the original features after the first stem layer to just before the final 4× upsampling layers, as well as a skip-connection over the component network that imposes attention priors on the input image. In testing, we process the LR image patch-wise and aggregate their HR estimations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Component Network</head><p>For segmentation of the facial components, we followed a similar model that to BiSeNet <ref type="bibr" target="#b64">[65]</ref> that is developed initially for generic purpose pixel labeling such as segmentation of Cityscapes images. BiSeNet has spatial and context paths that are devised to handle the loss of spatial information and shrinkage of the receptive fields, respectively. The spatial path has three convolution layers to obtain a smaller feature map. Context path appends a global average pooling layer on the tail of the Xception network <ref type="bibr" target="#b6">[7]</ref>. We fine-tuned this model on the CelebAMask-HQ dataset, which has 30,000 high-resolution (1024×1024) face images selected from the CelebA-HQ. Each image has a 512×512, manually-annotated, binary segmentation mask and 19 facial attributes such as skin, nose, eyes, eyebrows, ears, mouth, lip, hair, hat, eyeglass, earring, necklace, neck, and cloth.</p><p>We apply a spatial pooling layer that employs fixed Gaussian spatial kernels to suppress the segmentation errors by providing smoothing. This layer also allows higher values in attention maps to be assigned to more confidently segmented pixels. Finally, we multiply the input image with three spatially pooled components to obtain three gray-level attention maps. Sample attention maps are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We stack the original LR image and the attention maps into a block, which is our approach to administer attention to the input image. We steer the remaining of our super-resolution network using these maps as attention priors. In the training phase, we randomly sample patches from this block. Each patch, as a result, has the cropped original image and the corresponding attention maps. The random sampling generates identically sized patches and their augmented (flipped in 6 ways) versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stem Layer</head><p>The layer takes a patch block as the input tensor and applies convolutional filters on it. Each depth-wise channel is a color channel of the LR image and corresponding heatmap-weighted components, which are normalized to [-1,1] for efficient backpropagation. The stem layer in the first stage arranges the patch block in a 6-channel tensor. It then applies 256 filters, 3×3×6 each. For the following stage, we have additional channels. After the first stage, we estimated a 2× super-resolved HR image; thus, we first rearrange (space-to-depth) the pixels of an estimated HR image into 4 LR images. We then combine these LR images into a 12-channel tensor. Notice that we do not impose the heatmaps explicitly again.</p><p>Our network uses the original LR resolution frames in all its layers and stages. Since we use the same image size for all layers (except the upsampling layers), the learning becomes more efficient. Multiple references provide spatially vibrant local patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Residual Backbone</head><p>The residual backbone applies fully convolutional blocks on low-resolution feature maps generated by the stem layers. It is made up of 16 fully convolutional residual units. Each residual unit has a front convolutional layer followed by a ReLU and a second convolutional layer with a skip connection from the first one. Similarly, the residual backbone has also a direct skip connection from the input to the last residual block. This skip connection allows our network to learn the missing high-resolution details by reconstructing the residual info. The structure of the residual backbone of each stage is identical. The residual blocks and the direct skip connection also permits deepening the residual backbone for each stage. This boosts the overall capacity and increases the receptive field sizes. Thus, residual backbone feature maps have better access to contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Spatial Upsampling</head><p>We apply spatial upsampling layers to reconstruct a higher-resolution image from the feature map of the residual backbone. Since we shuffle pixels and we apply a set of convolutional filters, our upsampling does not require deconvolution operations. We rearrange the comparably large number of feature map channels per pixel into a highresolution image using the learned weights of the filters of the upsampling layers. We set the number of layers for the first stage and the second stage to 4 and 5 as the second stage feature map has to generate pixels. Each stage provides 2× super-resolution, yet it is possible to set the upsampling factor to larger ratios since there the feature maps are sufficiently deep.</p><p>For the goal of higher PSNR results, MSE would be the ideal loss function. However, MSE heavily penalizes the outliers. Recently, the work in <ref type="bibr" target="#b32">[33]</ref> empirically demonstrated that the mean absolute error (MAE) works better than the MSE. In our experiments, we also made a similar observation. In particular, at the initial stages of the training, using the MSE based loss functions caused insta-  bility. However, MAE-based loss at the later epochs converges slowly. Therefore, we opted to impose the Huber loss function, which is differentiable and combines the benefits of the MAE and MSE. It is defined as</p><formula xml:id="formula_0">L δ (d) = 1 2 d 2 for |d| ≤ δ, δ|d| − δ 2 2 otherwise (1) where d = I HR (x, y) −Î HR (x, y)<label>(2)</label></formula><p>is the pixel-wise difference between the target (groundtruth) HR image I HR and the estimated HR imageÎ HR . Above, we set δ = 1, which is the point where the Huber loss function changes from quadratic to linear. We trained the first stage and then the second stage by using the learned first stage parameters for initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We use 1024×1024 face images from the Flickr-Faces-HQ Dataset (FFHQ) <ref type="bibr" target="#b24">[25]</ref>, which consists of 70,000 highquality PNG images with considerable variation in terms of facial attributes such as age and ethnicity as well as image background. It also provides sufficient coverage of accessories such as eyeglasses, sunglasses, and hats. The images were crawled from Flickr. We then randomly split the FFHQ dataset into non-overlapping training, testing, and validation subsets of ratio 80%, 15%, and 5%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>To quantitatively measure the performance and provide comprehensive comparisons with state-of-the-art methods,  <ref type="figure">Figure 4</ref>: Comparison with state-of-the-art methods for the patch-based version (output HR image is 1024×1024). As visible, our method can super-resolve without artifacts and noise-like patterns. Reconstructed images are visually pleasing and resemble the ground-truth better than the existing methods (for a better view, see in color on digital display).</p><p>we used four quality assessment metrics including PSNR, SSIM, FID, and MS-SSIM. FID <ref type="bibr" target="#b17">[18]</ref> is defined as:</p><formula xml:id="formula_1">FID = ||µ r − µ g || 2 + Tr(Σ r + Σ g − 2(Σ r Σ g ) 1/2 ),<label>(3)</label></formula><p>where X r ∼ N (µ r , Σ r ) and X g ∼ N (µ g , Σ g ) are activations of Inception-v3 pool3 layer for real and generated samples, respectively. Lower FIDs mean the generated results are closer to the original image, measured by the Fréchet distance between two distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons</head><p>We quantitatively compare our method with seven stateof-the-art super-resolution approaches as well as with the bicubic upsampling.</p><p>In the inference (test) time, we can process the given image either by taking it as a whole (whole-face) or by patch-by-patch (patch-based). The memory limitations of GPUs set an upper bound on the input image size, in particular for the training phase. For example, memory limitations of the single GPU we used prohibited training with 1024×1024 input images. Thus, to train with and infer from such relatively large images (e.g., 1024×1024), we employ the patch-based version. For these two alternative versions, we trained separate models:</p><p>• The whole-face version uses the entire face image as an input. One can argue that using the face as a whole would provide better semantics. In our experiments,  <ref type="figure">Figure 5</ref>: Comparisons with the state-of-the-art for the whole-face version, i.e. training with 64×64 face images as input to generate 4× HR outputs of size 256×256. As visible, our method generates superior results for whole-face training as well.</p><p>we set the input size to 256×256 face images.</p><p>• The patch-based version uses the same network as above and identical size overlapping patches. We set the patch size to 256×256 for 1024×1024 HR outputs. We also tested 128×128 patch size. The patch-based version allows generating very large output faces without being restricted by the GPU memory.</p><p>The only difference between the above versions is the training data. images (patch-based) and 256×256 images (whole-face), respectively. We used the best models available for the state-of-the-art methods provided by their authors. Our results showcase the superior quality of the proposed method. In particular, our patch-based version achieves the most pleasing HR reconstructions without any artifacts. In comparison, the GAN-based methods introduce perceptually unignorable fragmentations, remnant noise-like patterns, and broken textures. We also provide quantitative results in <ref type="table" target="#tab_6">Tables 2 and 3</ref>, where our model outperforms the compared state-of-the-art methods with a remarkable margin under various metrics, including PSNR, SSIM, and FID. Notice that the compared methods either do not take advantage of facial semantic information or impose incorrect semantic bias. They do not use facial components to guide the super-resolution process. Most use lower resolution images in their training, which may be further limiting their representation capacity. These explain why bicubic upsampling, a deterministic approach without any semantic bias, performs better than its data-driven counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We evaluated variants of our patch-wise model with different hyper-parameterizations, i.e., F , the number of fea-    gets better. Most of the progress towards convergence were achieved in the initial epochs when we trained the final model using an NVIDIA DGX-1. As shown in <ref type="figure">Figure 6</ref>, the network trained with different hyperparameters converges to a similar level after a few epochs, indicating that the proposed network is generally applicable regardless of the settings of F and L, thus can adapt in accordance with real world scenarios such as GPU memory limit.</p><p>We also analyzed the performance of using a single stage 4× super-resolution instead of two-stage network. This version, even though attained better scores than the compared state-of-the-art methods, could not reach our twostage PSNR performance: 33.71 dB (single stage) vs. 34.10 dB (two stage) for the patch-based version, and 26.46 dB (single stage) vs. 27.42 dB (two stage) for the whole-face version. <ref type="figure">Figure 7</ref> compares the inference speed of our model achieved for both 1024×1024 (patch-based) and 256×256 (whole-face) image resolutions on a GTX 2080Ti GPU at the 4× super-resolution setting. We can attain 270 fps on the whole 256×256 image size and 15 fps of 1024×1024 in typical settings. The whole-face processing sets an upper bound on the overall latency of the model for the parallel-processing platforms while significantly exceeding real-time speed. Our patch-based solution can reach 15 fps and substantially improves the PSNR scores. It is possible to attain real-time performance for the patch-based version by sampling the input patches from the input image with lesser degrees of overlaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We show that imposing attention maps implicitly and regularizing the super-resolution process using loss functions from intermediate and final upscaling stages significantly improves the performance as demonstrated in our superior results. Our patch-based method has the advantage of processing any input size image. As future work, we plan to train the entire network, including the component segmentation part in an end-to-end fashion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Input LR image.(b) Our SR results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Our method can 4× super-resolve face images of any size. Top row: 64×64 LR input and our result. Middle row: enlarged areas from the above images. Bottom row: enlarged areas when the input LR image is 256×256 (see supplementary for the whole image and its super-resolved counterpart). Please zoom in for the best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>CAGFace architecture. First, facial components are segmented, and component-wise attention maps are generated. For training, random patches are sampled. The super-resolution network has two stages; the first stage estimates a 2× intermediate HR image. The second stage builds on the space-to-depth converted intermediate HR image and uses the original features of the first stem layer through a stage-wise skip-connection while implicitly imposing the component-wise attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Sample attention maps from component network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Effect of different network parameters on accuracy on the test dataset at initial epochs during the training phase. F is the number of features in each layer and L is the number of layers. PSNR is in dB. As visible, most versions converge to the higher PSNR scores quickly in a few epochs. This shows that our network is robust to the different hyper-parameterizations.tures per layer, and L, the number of Resblock layers. Figure 6 demonstrates the training performance in terms of the attained PSNR scores after the initial training epochs of different configurations. As expected, with the increasing number of features and Resblock layers the performance Effect of different network parameters on speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>CAGFace network parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Comparison results for 1024×1024 outputs. Our method is trained with patches.</figDesc><table><row><cell></cell><cell cols="2">PSNR SSIM MS-SSIM</cell><cell>FID</cell></row><row><cell>Bicubic</cell><cell>25.57 0.766</cell><cell>0.935</cell><cell>135.51</cell></row><row><cell>SRCNN [10]</cell><cell>23.12 0.688</cell><cell>0.900</cell><cell>147.21</cell></row><row><cell>FSRCNN [11]</cell><cell>22.45 0.709</cell><cell>0.930</cell><cell>139.78</cell></row><row><cell>EDSR [33]</cell><cell>22.47 0.706</cell><cell>0.901</cell><cell>129.14</cell></row><row><cell>SRGAN [30]</cell><cell>17.57 0.415</cell><cell>0.757</cell><cell>156.07</cell></row><row><cell>ESRGAN [55]</cell><cell>15.43 0.267</cell><cell>0.747</cell><cell>166.36</cell></row><row><cell cols="2">EnhanceNet [41] 23.64 0.701</cell><cell>0.897</cell><cell>116.38</cell></row><row><cell>SRFBN [32]</cell><cell>21.96 0.693</cell><cell>0.895</cell><cell>132.59</cell></row><row><cell>Ours</cell><cell>27.42 0.816</cell><cell>0.958</cell><cell>74.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparison results for 256×256 outputs. Our method is trained with whole-faces.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Materials</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image up-sampling using total-variation regularization with a new observation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1647" to="1659" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<title level="m">Hallucinating faces. fg</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-fan: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-aware face hallucination via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="690" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fsrnet: Endto-end learning face super-resolution with facial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedigns of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>eedigns of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedigns of IEEE Conference on Computer Vision and Pattern Recognitio (CVPR</title>
		<meeting>eedigns of IEEE Conference on Computer Vision and Pattern Recognitio (CVPR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pixel recursive super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5439" to="5448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perception and processing of faces in the human brain is tuned to typical feature locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dehaas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Henriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Journal of Neuroscience</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="9289" to="9302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
	<note>In European conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic facial expression analysis: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Examplebased super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and Applications</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimedia super-resolution via deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="198" to="217" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image quality metrics: Psnr vs. ssim</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G S B M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00734</idno>
		<title level="m">The relativistic discriminator: a key element missing from standard gan</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transport-based single frame super resolution of very low resolution face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4876" to="4884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Maxims or myths of beauty? a metaanalytic and theoretical review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langlois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kalakanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hallamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychological Bulletin</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="390" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Survey: Interpolation methods in medical image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gonner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Spitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1049" to="1075" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Facial attractiveness: Evolutionary based research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debruine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1638" to="1659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The fascinating science behind why we see faces in objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Martinez-Conde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mentalfloss</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The many faces of configural processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mondloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in Cognitive Science</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="255" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conspec and conlern: A twoprocess theory of infant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychological Review</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="164" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on video deblurring: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image superresolution using support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1596" to="1610" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast image/video upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2008" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inverse mapping the neuronal substrates of face categorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schyns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Cerebral Cortex</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to hallucinate face images via component generation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4537" to="4543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image hallucination with primal sketch priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">729</biblScope>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Misleading first impressions: Different for different facial images of the same person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Psychological Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1404" to="1417" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image super-resolution survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Ouwerkerk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1039" to="1052" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Patch based blind image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="709" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hallucinating face by eigentransformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A universal image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="81" to="84" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Mean squared error: Love it or leave it? a new look at signal fidelity measures. IEEE signal processing magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="98" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals, Systems &amp; Computers</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Structured face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Single-image superresolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fast image super-resolution based on in-place example regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1059" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep learning for single image super-resolution: A brief review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep learning for single image super-resolution: A brief review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedigns of European Conference on Computer Vision (ECCV)</title>
		<meeting>eedigns of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Hallucinating very low-resolution unaligned and noisy face images by transformative discriminative autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ultra-resolving face images by discriminative generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="318" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hallucinating very low-resolution unaligned and noisy face images by transformative discriminative autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3760" to="3768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Image super-resolution: The techniques, applications, and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="389" to="408" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Non-local kernel regression for image and video restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Fsim: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Zoom to learn, learn to zoom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3762" to="3770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning Face Hallucination in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3871" to="3877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep cascaded binetwork for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
