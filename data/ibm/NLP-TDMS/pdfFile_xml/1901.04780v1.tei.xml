<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Martín-Martín</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose. Our code and video are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>6D object pose estimation is the crux to many important real-world applications, such as robotic grasping and manipulation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref>, autonomous navigation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">41]</ref>, and augmented reality <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Ideally, a solution should deal with objects of varying shape and texture, show robustness towards heavy occlusion, sensor noise, and changing lighting conditions, while achieving the speed requirement of real-time tasks. The advent of cheap RGB-D sensors has enabled methods that infer poses of low-textured objects even in poorly-lighted environments more accurately than RGB-only methods. Nonetheless, it is difficult for existing methods to satisfy the requirements of accurate pose estimation and fast inference simultaneously.</p><p>Classical approaches first extract features from RGB-D RGB-D DenseFusion <ref type="figure" target="#fig_4">Figure 1</ref>. We develop an end-to-end deep network model for 6D pose estimation from RGB-D data, which performs fast and accurate predictions for real-time applications such as robot grasping and manipulation.</p><p>data and perform correspondence grouping and hypothesis verification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. However, the reliance on handcrafted features and fixed matching procedures have limited their empirical performances in presence of heavy occlusion and lighting variation. Recent success in visual recognition has inspired a family of data-driven methods that use deep networks for pose estimation from RGB-D inputs, such as PoseCNN <ref type="bibr" target="#b39">[40]</ref> and MCN <ref type="bibr" target="#b15">[16]</ref>.</p><p>However, these methods require elaborate post-hoc refinement steps to fully utilize the 3D information, such as a highly customized Iterative Closest Point (ICP) <ref type="bibr" target="#b1">[2]</ref> procedure in PoseCNN and a multi-view hypothesis verification scheme in MCN. These refinement steps cannot be optimized jointly with the final objective and are prohibitively slow for real-time applications. In the context of autonomous driving, a third family of solutions has been proposed to better exploit the complementary nature of color and depth information from RGB-D data with endto-end deep models, such as Frustrum PointNet <ref type="bibr" target="#b21">[22]</ref> and PointFusion <ref type="bibr" target="#b40">[41]</ref>. These models have achieved good performances in driving scenes and the capacity of real-time inference. However, as we demonstrate empirically, these methods fall short under heavy occlusion, which is common in manipulation domains.</p><p>In this work, we propose an end-to-end deep learning approach for estimating 6-DoF poses of known objects from RGB-D inputs. The core of our approach is to embed and fuse RGB values and point clouds at per-pixel level, as opposed to prior work which uses image crops to compute global features <ref type="bibr" target="#b40">[41]</ref> or 2D bounding boxes <ref type="bibr" target="#b21">[22]</ref>. This perpixel fusion scheme enables our model to explicitly reason about the local appearance and geometry information, which is essential to handle heavy occlusion. Furthermore, we propose an iterative method which performs pose refinement within the end-to-end learning framework. This greatly enhances model performance while keeping the inference speed real-time.</p><p>We evaluate our method in two popular benchmarks for 6D pose estimation, YCB-Video <ref type="bibr" target="#b39">[40]</ref> and LineMOD <ref type="bibr" target="#b11">[12]</ref>. We show that our method outperforms the state-of-the-art PoseCNN after ICP refinement <ref type="bibr" target="#b39">[40]</ref> by 3.5% in pose accuracy while being 200x faster in inference time. In particular, we demonstrate its robustness in highly cluttered scenes thanks to our novel dense fusion method. Last, we also showcase its utility in a real robot task, where the robot estimates the poses of objects and grasp them to clear up a table.</p><p>In summary, the contributions of this work are two-fold: First, we present a principled way to combine color and depth information from the RGB-D input. We augment the information of each 3D point with 2D information from an embedding space learned for the task and use this new color-depth space to estimate the 6D pose. Second, we integrate an iterative refinement procedure within the neural network architecture, removing the dependency of previous methods of a post-processing ICP step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Pose from RGB images. Classical methods rely on detecting and matching keypoints with known object models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43]</ref>. Newer methods address the challenge by learning to predict the 2D keypoints <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> and solve the poses by PnP <ref type="bibr" target="#b9">[10]</ref>. Though prevail in speed-demanding tasks, these methods become unreliable given low-texture or low-resolution inputs. Other methods propose to directly estimate objects pose from images using CNN-based architectures <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref>. Many such methods focus on orientation estimation: Xiang et al. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> learns a viewpoint-aware pose estimator by clustering 3D features from object models. Mousavian et al. <ref type="bibr" target="#b19">[20]</ref> predicts 3D object parameters and recovers poses by single-view geometry constraints. Sundermeyer et al. <ref type="bibr" target="#b29">[30]</ref> implicitly encode orientation in a latent space and in test time find the best match in a codebook as the orientation prediction. However, pose estimation in 3D remains a challenge for the lack of depth information. Our method leverages both image and 3D data to estimate object poses in 3D in an end-to-end architecture.</p><p>Pose from depth / point cloud. Recent studies have proposed to directly tackle the 3D object detection problem in discretized 3D voxel spaces. For example, Song et al. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> generate 3D bounding box proposals and estimate the poses by featuring the voxelized input with 3D ConvNets. Although the voxel representation effectively encodes geometric information, these methods are often prohibitively expensive: <ref type="bibr" target="#b28">[29]</ref> takes nearly 20 seconds for each frame.</p><p>More recent 3D deep learning architectures have enabled methods that directly performs 6D pose estimation on 3D point cloud data. As an example, both Frustrum PointNets <ref type="bibr" target="#b21">[22]</ref> and VoxelNet <ref type="bibr" target="#b41">[42]</ref> use a PointNet-like <ref type="bibr" target="#b22">[23]</ref> structure and achieved state-of-the-art performances on the KITTI benchmark <ref type="bibr" target="#b10">[11]</ref>. Our method also makes use of similar architecture. However, unlike urban driving applications for which point cloud alone provides enough information, generic object pose estimation tasks such as the YCB-Video dataset <ref type="bibr" target="#b39">[40]</ref> demands reasoning over both geometric and appearance information. We address such a challenge by proposing a novel 2D-3D sensor fusion architecture.</p><p>Pose from RGB-D data. Classical approaches extract 3D features from the input RGB-D data and perform correspondence grouping and hypothesis verification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. However, these features are either hardcoded <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25]</ref> or learned by optimizing surrogate objectives <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref> such as reconstruction <ref type="bibr" target="#b14">[15]</ref> instead of the true objective of 6D pose estimation. Newer methods such as PoseCNN <ref type="bibr" target="#b39">[40]</ref> directly estimates 6D poses from image data. Li et al. <ref type="bibr" target="#b15">[16]</ref> further fuses the depth input as an additional channel to a CNN-based architecture. However, these approaches rely on expensive post-processing steps to make full use of 3D input. In comparison, our method fuses 3D data to 2D appearance feature while retaining the geometric structure of the input space, and we show that it outperforms <ref type="bibr" target="#b39">[40]</ref> on the YCB-Video dataset <ref type="bibr" target="#b39">[40]</ref> without the post-processing step.</p><p>Our method is most related to PointFusion <ref type="bibr" target="#b40">[41]</ref>, in which geometric and appearance information are fused in a heterogeneous architecture. We show that our novel local feature fusion scheme significantly outperforms PointFusion's naive fusion-by-concatenation method. In addition, we use a novel iterative refinement method to further improve the pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>Our goal is to estimate the 6D pose of a set of known objects present in an RGB-D image of a cluttered scene. Without loss of generality, we represent 6D poses as homogeneous transformation matrix, p ∈ SE(3). In other  words, a 6D pose is composed by a rotation R ∈ SO(3) and a translation t ∈ R 3 , p = [R|t]. Since we estimate the 6D pose of the objects from camera images, the poses are defined with respect to the camera coordinate frame.</p><p>Estimating the pose of a known object in adversarial conditions (e.g. heavy occlusion, poor lighting, . . . ) is only possible by combining the information contained in the color and depth image channels. However, the two data sources reside in different spaces. Extracting features from heterogeneous data sources and fusing them appropriately is the key technical challenge in this domain.</p><p>We address this challenge with (1) a heterogeneous architecture that processes color and depth information differently, retaining the native structure of each data source (Sec. 3.3), and (2) a dense pixel-wise fusion network that performs color-depth fusion by exploiting the intrinsic mapping between the data sources (Sec. 3.4). Finally, the pose estimation is further refined with a differentiable iterative refinement module (Sec. 3.6). In contrast to the expensive post-hoc refinement steps used in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>, our refinement module can be trained jointly with the main architecture and only takes a small fraction of the total inference time. The architecture contains two main stages. The first stage takes color image as input and performs semantic segmentation for each known object category. Then, for each segmented object, we feed the masked depth pixels (converted to 3D point cloud) as well as an image patch cropped by the bounding box of the mask to the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture Overview</head><p>The second stage processes the results of the segmentation and estimates the object's 6D pose. It comprises four components: a) a fully convolutional network that processes the color information and maps each pixel in the image crop to a color feature embedding, b) a PointNet-based <ref type="bibr" target="#b22">[23]</ref> network that processes each point in the masked 3D point cloud to a geometric feature embedding, c) a pixel-wise fusion network that combines both embeddings and outputs the estimation of the 6D pose of the object based on an unsupervised confidence scoring, and d) an iterative self-refinement methodology to train the network in a curriculum learning manner and refine the estimation result iteratively. <ref type="figure" target="#fig_1">Fig. 2</ref> depicts a), b) and c) and <ref type="figure">Fig. 3</ref> illustrates d). The details our architecture are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semantic Segmentation</head><p>The first step is to segment the objects of interest in the image. Our semantic segmentation network is an encoderdecoder architecture that takes an image as input and generates an N +1-channelled semantic segmentation map. Each channel is a binary mask where active pixels depict objects of each of the N possible known classes. The focus of this work is to develop a pose estimation algorithm. Thus we use an existing segmentation architecture proposed by <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dense Feature Extraction</head><p>The key technical challenge in this domain is the correct extraction of information from the color and depth channels and their synergistic fusion. Even though color and depth present a similar format in the RGB-D frame, their information resides in different spaces. Therefore, we process them separately to generate color and geometric features from embedding spaces that retain the intrinsic structure of the data sources. Dense 3D point cloud feature embedding: Previous approaches have used CNN to process the depth image as an additional image channel <ref type="bibr" target="#b15">[16]</ref>. However, such method neglects the intrinsic 3D structure of the depth channel. Instead, we first convert the segmented depth pixels into a 3D point cloud using the known camera intrinsics, and then use a PointNet-like architecture to extract geometric features.</p><p>PointNet by Qi et al. <ref type="bibr" target="#b22">[23]</ref> pioneered the use of a symmetric function (max-pooling) to achieve permutation invariance in processing unordered point sets. The original architecture takes as input a raw point cloud and learns to encode the information about the vicinity of each point and of the point cloud as a whole. The features are shown to be effective in shape classification and segmentation <ref type="bibr" target="#b22">[23]</ref> and pose estimation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41]</ref>. We propose a geometric embedding network that generates a dense per-point feature by mapping each of the P segmented points to a d geo -dimensional feature space. We implement a variant of PointNet architecture that uses average-pooling as opposed to the commonly used max-pooling as the symmetric reduction function. Dense color image feature embedding: The goal of the color embedding network is to extract per-pixel features such that we can form dense correspondences between 3D point features and image features. The reason for forming these dense correspondences will be clear in the next section. The image embedding network is a CNN-based encoder-decoder architecture that maps an image of size H × W × 3 into a H × W × d rgb embedding space. Each pixel of the embedding is a d rgb -dimensional vector representing the appearance information of the input image at the corresponding location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pixel-wise Dense Fusion</head><p>So far we have obtained dense features from both the image and the 3D point cloud inputs; now we need to fuse the information. A naive approach would be to generate a global feature from the dense color and depth features from the segmented area. However, due to heavy occlusion and segmentation errors, the set of features from previous step may contain features of points/pixels on other objects or parts of the background. Therefore, blindly fusing color and geometric features globally would degrade the performance of the estimation. In the following we describe a novel pixel-wise 1 dense fusion network that effectively combines the extracted features, especially for pose estimation under heavy occlusion and imperfect segmentation. Pixel-wise dense fusion: The key idea of our dense fusion network is to perform local per-pixel fusion instead of global fusion so that we can make predictions based on each fused feature. In this way, we can potentially select the predictions based on the visible part of the object and minimize the effects of occlusion and segmentation noise. Concretely, our dense fusion procedure first associates the geometric feature of each point to its corresponding image feature pixel based on a projection onto the image plane using the known camera intrinsic parameters. The obtain pairs of features are then concatenated and fed to another network to generate a fixed-size global feature vector using a symmetric reduction function. While we refrained from using a single global feature for the estimation, here we enrich each dense pixel-feature with the global densely-fused feature to provide a global context.</p><p>We feed each of the resulting per-pixel features into a final network that predicts the object's 6D pose. In other words, we will train this network to predict one pose from each densely-fused feature. The result is a set of P predicted poses, one per feature. This defines our first learning objective, as we will see in Sec. 3.5. We will now explain our approach to learn to choose the best prediction in a selfsupervised manner, inspired by the work by Xu et al. <ref type="bibr" target="#b40">[41]</ref>. Per-pixel self-supervised confidence: We would like to train our pose estimation network to decide which pose estimation is likely to be the best hypothesis based on the specific context. To do so, we modify the network to output a confidence score c i for each prediction in addition to the pose estimation predictions. We will have to reflect this second learning objective in the overall learning objective, as we will see at the end of the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">6D Object Pose Estimation</head><p>Having defined the overall network structure, we now take a closer look at the learning objective. We define the pose estimation loss as the distance between the points sampled on the objects model in ground truth pose and corresponding points on the same model transformed by the predicted pose. Specifically, the loss to minimize for the prediction per dense-pixel is defined as</p><formula xml:id="formula_0">L p i = 1 M j ||(Rx j + t) − (R i x j +t i )||<label>(1)</label></formula><p>where x j denotes the j th point of the M randomly selected 3D points from the object's 3D model, p = [R|t] is the ground truth pose, andp i = [R i |t i ] is the predicted pose generated from the fused embedding of the i th dense-pixel. The above loss function is only well-defined for asymmetric objects, where the object shape and/or texture determines a unique canonical frame. Symmetric objects have more than one and possibly an infinite number of canonical frames, which leads to ambiguous learning objectives. Therefore, for symmetric objects, we instead minimize the  <ref type="figure">Figure 3</ref>. Iterative Pose Refinement. We introduce an network module that refines the pose estimation in an iterative procedure. distance between each point on the estimated model orientation and the closest point on the ground truth model. The loss function becomes:</p><formula xml:id="formula_1">L p i = 1 M j min 0&lt;k&lt;M ||(Rx j + t) − (R i x k +t i )|| (2)</formula><p>Optimizing over all predicted per dense-pixel poses would be to minimize the sum of the per dense-pixels losses: L = 1 N i L p i . However, as explained before, we would like our network to learn to balance the confidence among the per dense-pixel predictions. To do that we weight the per dense-pixel loss with the dense-pixel confidence, and add a second confidence regularization term:</p><formula xml:id="formula_2">L = 1 N i (L p i c i − w log(c i )),<label>(3)</label></formula><p>where N is the number of randomly sampled dense-pixel features from the P elements of the segment and w is a balancing hyperparameter. Intuitively, low confidence will result in low pose estimation loss but would incur high penalty from the second term, and vice versa. We use the pose estimation that has the highest confidence as the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Iterative Refinement</head><p>The iterative closest point algorithm (ICP) <ref type="bibr" target="#b1">[2]</ref> is a powerful refinement approach used by many 6D pose estimation methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref>. However, the best-performing ICP implementations are often not efficient enough for realtime applications. Here we propose a neural network-based iterative refinement module that can improve the final pose estimation result in a fast and robust manner.</p><p>The goal is to enable the network to correct its own pose estimation error in an iterative manner. The challenge here is training the network to refine the previous prediction as opposed to making new predictions. To do so, we must include the prediction made in a previous iteration as part of the input to the next iteration. Our key idea is to consider the previously predicted pose as an estimate of canonical frame of the target object and transform the input point cloud into this estimated canonical frame. This way, the transformed point cloud implicitly encodes the estimated pose. We then feed the transformed point cloud back into the network and predict a residual pose based on the previously estimated pose. This procedure can be applied iteratively and generate potentially finer pose estimation each iteration.</p><p>The procedure is illustrated in <ref type="figure">Fig. 3</ref>. Concretely, we train a dedicated pose residual estimator network to perform the refinement given the initial pose estimation from the main network. At each iteration, we reuse the image feature embedding from the main network and perform dense fusion with the geometric features computed for the new transformed point cloud. The pose residual estimator uses as input a global feature from the set of fused pixel features. After K iterations, we obtain the final pose estimation as the concatenation of the per-iteration estimations:</p><formula xml:id="formula_3">p = [R K |t K ] · [R K−1 |t K−1 ] · · · · · [R 0 |t 0 ]<label>(4)</label></formula><p>The pose residual estimator can be trained jointly with the main network. However, the pose estimation at the beginning of the training is too noisy for it to learn anything meaningful. Thus in practice, the joint training starts after the main network has converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the experimental section, we would like to answer the following questions: (1) How does the dense fusion network compare to naive global fusion-by-concatenation? (2) Is the dense fusion and prediction scheme robust to heavy occlusion and segmentation errors? (3) Does the iterative refinement module improve the final pose estimation? (4) Is our method robust and efficient enough for downstream tasks such as robotic grasping?</p><p>To answer the first three questions, we evaluate our method on two challenging 6D object pose estimation datasets: YCB-Video Dataset <ref type="bibr" target="#b39">[40]</ref> and LineMOD <ref type="bibr" target="#b11">[12]</ref>. The YCB-Video Dataset features objects of varying shapes and texture levels under different occlusion conditions. Hence it's an ideal testbed for our occlusion-resilient multi-modal fusion method. The LineMOD dataset is a widely-used dataset that allows us to compare with a broader range of existing methods. We compare our method with state-ofthe-art methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref> as well as model variants. To answer the last question, we deploy our model to a real robot platform and evaluate the performance of a robot grasping task that uses the predictions from our model.  and segmentation masks. We follow prior work <ref type="bibr" target="#b39">[40]</ref> and split the dataset into 80 videos for training and 2,949 key frames chosen from the rest 12 videos for testing and include the same 80,000 synthetic images released by <ref type="bibr" target="#b39">[40]</ref> in our training set. In our experiments, we compare with the result of <ref type="bibr" target="#b39">[40]</ref> after depth refinement(ICP) and learningbased depth method <ref type="bibr" target="#b40">[41]</ref>. LineMOD Dataset. The LineMOD dataset Hinterstoisser et al. <ref type="bibr" target="#b11">[12]</ref> consists of 13 low-textured objects in 13 videos. It is widely adopted by both classical methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36]</ref> and recent learning-based approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>. We use the same training and testing set as prior learning-based works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref> without additional synthetic data and compare with the best ICP-refined results of the state-ofthe-art algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>We use two metrics to report on the YCB-Video Dataset. The average closest point distance (ADD-S) <ref type="bibr" target="#b39">[40]</ref> is an ambiguity-invariant pose error metric which takes care of both symmetric and non-symmetric objects into an overall evaluation. Given the estimated pose [R|t] and ground truth pose [R|t], ADD-S calculates the mean distance from each 3D model point transformed by [R|t] to its closest neighbour on the target model transformed by <ref type="bibr">[R|t]</ref>. We report the area under the ADD-S curve (AUC) following PoseCNN <ref type="bibr" target="#b39">[40]</ref>. We follow prior work and set the maximum threshold of AUC to be 0.1m. We also report the percentage of ADD-S smaller than 2cm (&lt;2cm), which measures the predictions under the minimum tolerance for robot manipulation (2cm for most of the robot grippers).</p><p>For the LineMOD dataset, we use the Average Distance of Model Points (ADD) <ref type="bibr" target="#b12">[13]</ref> for non-symmetric objects and ADD-S for the two symmetric objects (eggbox and glue) following prior works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>The image embedding network consists of a Resnet-18 encoder followed by 4 up-sampling layers as the decoder. The PointNet architecture is an MLP followed by an average-pooling reduction function. Both color and geometric dense feature embedding are of dimension 128. We choose w = 0.01 for Eq. 3 by empirical evaluation. The iterative pose refinement module consists of a 4 fully connected layers that directly output the pose residual from the global dense feature. We use the 2 refinement iterations for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Architectures</head><p>We compare four model variants that showcase the effectiveness of our design choices.</p><p>• PointFusion <ref type="bibr" target="#b40">[41]</ref> uses a CNN to extract a fixed-size feature vector and fuse by directly concatenating the image feature with the geometry feature. The rest of the network is similar to our architecture. The comparison to this baseline demonstrates the effectiveness of our dense fusion network.</p><p>• Ours (single) uses our dense fusion network, but instead <ref type="figure">Figure 4</ref>. Qualitative results on the YCB-Video Dataset. All three methods shown here are tested with the same segmentation masks as in PoseCNN. Each object point cloud in different color are transformed with the predicted pose and then projected to the 2D image frame. The first two rows are former RGB-D methods and the last row is our approach with dense fusion and iterative refinement (2 iterations). of performing per-point prediction, it only outputs a single prediction using the global feature vector.</p><p>• Ours (per-pixel) performs per-pixel prediction based on each densely fused feature.</p><p>• Ours (iterative) is our complete model that uses the iterative refinement (Sec. 3.6) on top of Ours (per-pixel). <ref type="table" target="#tab_0">Table 1</ref> shows the evaluation results for all the 21 objects in the YCB-Video Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation on YCB-Video Dataset</head><p>We report the ADD-S AUC(&lt;0.1m) and the ADD-S&lt;2cm metrics on PoseCNN <ref type="bibr" target="#b39">[40]</ref> and our four model variants. To ensure a fair comparison, all methods use the same segmentation masks as in PoseCNN <ref type="bibr" target="#b39">[40]</ref>. Among our model variants, Ours (Iterative) achieves the best performance. Our method is able to outperform PoseCNN + ICP <ref type="bibr" target="#b39">[40]</ref> even without iterative refinement. In particular, Ours (Iterative) outperforms PoseCNN + ICP by 3.5% on the ADD-S&lt;2cm metric. Effect of dense fusion Both of our dense fusion baselines (Ours (single) and Ours (per-pixel)) outperform PointFusion by a large margin, which shows that dense fusion has a clear advantage over the global fusion-by-concatenation method used in PointFusion. Effect of iterative refinement <ref type="table" target="#tab_0">Table 1</ref> shows that our iterative refinement improves the overall pose estimation performance. In particular, it significantly improves the performances for texture-less symmetric object, e.g., bowl (29%), banana (6%), and extra large clamp (6%) which suffer from orientation ambiguity. Robustness towards occlusion The main advantage of our dense fusion method is its robustness towards occlusions. To quantify the effect of occlusion on final performance, we calculate the visible surface ratio of each object instance (further detail available in supplementary material). Then we calculate how the accuracy (ADD-S&lt;2cm percentage) changes with extent of occlusion. As shown in <ref type="figure" target="#fig_5">Fig. 5</ref>, the performances of PointFusion and PoseCNN+ICP degrade significantly as the occlusion increases. In contrast, none of our methods experiences notable performance drop. In particular, the performance of both Ours (per-pixel) and Ours (iterative) only decrease by 2% overall. Time efficiency We compare the time efficiency of our model with PoseCNN+ICP in <ref type="table" target="#tab_2">Table 3</ref>. We can see that our method is two order of magnitude faster than PoseCNN+ICP. In particular, PoseCNN+ICP spends most of time on the post processing ICP. In contrast, all of our computation component, namely segmentation (Seg), pose estimation (PE), and iterative refinement (Refine), are equally efficient, and the overall runtime is fast enough for real-time application (16 FPS, about 5 objects in each frame). Qualitative evaluation <ref type="figure">Fig. 4</ref> visualizes some sample predictions made by PoseCNN+ICP, PointFusion, and our iterative refinement model. As we can see, PoseCNN+ICP and PointFusion fail to estimate the correct pose of the bowl in the leftmost column and the cracker box in the middle col-    <ref type="figure">Figure 6</ref>. Iterative refinement performance on LineMOD dataset We visualize how our iterative refinement procedure corrects initially sub-optimal pose estimation. <ref type="table" target="#tab_1">Table 2</ref> compares our method with previous RGB methods with depth refinement(ICP) (results from <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>) on the ADD metric <ref type="bibr" target="#b12">[13]</ref>. Even without the iterative refinement step, our method can outperform 7% over the state-of-theart depth refinement method. After processing the iterative refinement approach, the final result has another 8% improvement, which proves that our learning-based depth method is superior to the sophisticated application of ICP in both accuracy and efficiency. We visualize the estimated 6D pose after each refinement iteration in <ref type="figure">Fig.6</ref>, where our pose estimation improves by an average of 0.8 cm (ADD) after 2 refinement iterations. The results of some other color-only methods are also listed in <ref type="table" target="#tab_1">Table 2</ref> for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Evaluation on LineMOD Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Robotic Grasping Experiment</head><p>In our last experiment, we evaluate whether the poses estimated by our approach are accurate enough to enable robot grasping and manipulation. As shown in <ref type="figure" target="#fig_4">Fig. 1</ref>, we place 5 YCB objects on a table and command the robot to grasp them using the estimated pose. We follow a similar procedure to Tremblay et al. <ref type="bibr" target="#b33">[34]</ref>: we place the five objects in four different random locations on the table, at three random orientations, including configurations with partial occlusions. Since the order of picking the objects is not optimized, we do not allow configurations where objects lay on top of each other. The robot attempts 12 grasps on each object, 60 attempts in total. The robot uses the estimated object orientation to compute an alignment of the gripper's fingers to the object narrower dimension.</p><p>The robot succeeds on 73% of the grasps using our proposed approach to estimate the pose of the objects. The most difficult object to grasp is the banana (7 out of 12 successful attempts). One possible reason is that our banana model is not exactly the same as in the dataset -ours is plain yellow. This characteristic hinders the estimation, especially of the orientation, and leads to some failed grasp attempts along the longer axis of the object. In spite of this less accurate case, our results indicate that our approach is robust enough to be deployed in real-world robotic tasks without explicit domain adaptation, even with a different RGB-D sensor and in a different background than the ones in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a novel approach to estimating 6D poses of known objects from RGB-D images. Our approach fuses a dense representation of features that include color and depth information based on the confidence of their predictions. With this dense fusion approach, our method outperforms previous approaches in several datasets, and is significantly more robust against occlusions. Additionally, we demonstrated that a robot can use our proposed approach to grasp and manipulate objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Invisible surface percentage calculation</head><p>The invisible surface percentage is a measurement that quantifies how occluded an object is given the camera viewpoint. The measurement is used in Sec.4.5 of the main manuscript. Following are the details of how to compute the invisible surface percentage.</p><p>First, we transform the ground truth model of an object to its target pose. Then, the 3D points on the surface of the model are sampled and projected back to a 2D image plane as depth pixels according to the camera intrinsic parameters. The projected depth pixels should be close to the depth measured by a depth sensor if there is no occlusion. In other words, if the distance between the measured depth of a pixel and the model-projected depth is larger than a margin, we consider the pixel as being occluded and thus invisible. Concretely, suppose a projected depth pixel p has depth value d(p), and the measured depth of p isd(p). p is considered invisible if |d(p) −d(p)| &gt; h. The margin h is set to be 20mm in the experiment. The invisible surface percentage is thus the percentage of the points that are invisible out of all sampled points on the object model surface. Since around half of the points on an object model are always invisible due to self-occlusion, <ref type="figure" target="#fig_5">Fig.5</ref> in the main manuscript shows results starting from 60 invisible surface percentage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Details of the robotic grasping experiment</head><p>The robot used in the experiment is a Toyota HSR (Human Support Robot). The robot is equipped with an Asus Xtion RGB-D sensor, a holonomic mobile base, and a two-finger gripper. We deployed our pose estimation model trained on YCB-Video dataset without finetuning. Note that our camera (Asus Xtion) is different from the one used to capture the YCB-Video dataset (Kinect-v2). Our experiment shows that our model is able to tolerate the difference in camera and perform accurate pose estimation. The evaluation includes five YCB objects: 005 tomato soup can, 006 mustard bottle, 007 tuna fish can, 011 banana, and 021 bleach cleanser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Additional iterative refinement examples</head><p>See <ref type="figure">Fig. 7</ref>. <ref type="figure">Figure 7</ref>. Iterative refinement performance on LineMOD dataset The initial estimation is outputted by Ours (per-pixel). We first transform the object model with the estimated pose and ground truth pose into the 3D space. The ADD distance is the average distance between each corresponding point pair on the two transformed model point clouds. Here we show our iterative refinement performance in more situations includes blurring and low light conditions, where we can see clear improvement on accuracy by using our neural network based iterative refinement method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our 6D pose estimation model. Our model generates object segmentation masks and bounding boxes from RGB images. The RGB colors and point cloud from the depth map are encoded into embeddings and fused at each corresponding pixel. The pose predictor produces a pose estimate for each pixel and the predictions are voted to generate the final 6D pose prediction of the object. (The iterative procedure of our approach is not depicted here for simplicity)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>illustrates the overall proposed architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 1 .</head><label>1</label><figDesc>Datasets YCB-Video Dataset. The YCB-Video Dataset Xiang et al. [40] features 21 YCB objects Calli et al. [5] of varying shape and texture. The dataset contains 92 RGB-D videos, where each video shows a subset of the 21 objects in different indoor scenes. The videos are annotated with 6D poses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Model performance under increasing levels of occlusion. Here the levels of occlusion is estimated by calculating the invisible surface percentage of each object in the image frame. Our methods work more robustly under heavy occlusion compared to baseline methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation of 6D pose (ADD-S[40]) on YCB-Video Dataset. Objects with bold name are symmetric.</figDesc><table><row><cell></cell><cell cols="6">PointFusion [41] PoseCNN+ICP [40] Ours (single)</cell><cell cols="4">Ours (per-pixel) Ours (iterative)</cell></row><row><cell></cell><cell>AUC</cell><cell>&lt;2cm</cell><cell>AUC</cell><cell>&lt;2cm</cell><cell cols="3">AUC &lt;2cm AUC</cell><cell>&lt;2cm</cell><cell cols="2">AUC &lt;2cm</cell></row><row><cell>002 master chef can</cell><cell>90.9</cell><cell>99.8</cell><cell>95.8</cell><cell>100.0</cell><cell>93.9</cell><cell>100.0</cell><cell>95.2</cell><cell>100.0</cell><cell>96.4</cell><cell>100.0</cell></row><row><cell>003 cracker box</cell><cell>80.5</cell><cell>62.6</cell><cell>92.7</cell><cell>91.6</cell><cell>90.8</cell><cell>98.4</cell><cell>92.5</cell><cell>99.3</cell><cell>95.5</cell><cell>99.5</cell></row><row><cell>004 sugar box</cell><cell>90.4</cell><cell>95.4</cell><cell>98.2</cell><cell>100.0</cell><cell>94.4</cell><cell>99.2</cell><cell>95.1</cell><cell>100.0</cell><cell>97.5</cell><cell>100.0</cell></row><row><cell>005 tomato soup can</cell><cell>91.9</cell><cell>96.9</cell><cell>94.5</cell><cell>96.9</cell><cell>92.9</cell><cell>96.7</cell><cell>93.7</cell><cell>96.9</cell><cell>94.6</cell><cell>96.9</cell></row><row><cell>006 mustard bottle</cell><cell>88.5</cell><cell>84.0</cell><cell>98.6</cell><cell>100.0</cell><cell>91.2</cell><cell>97.8</cell><cell>95.9</cell><cell>100.0</cell><cell>97.2</cell><cell>100.0</cell></row><row><cell>007 tuna fish can</cell><cell>93.8</cell><cell>99.8</cell><cell>97.1</cell><cell>100.0</cell><cell>94.9</cell><cell>100.0</cell><cell>94.9</cell><cell>100.0</cell><cell>96.6</cell><cell>100.0</cell></row><row><cell>008 pudding box</cell><cell>87.5</cell><cell>96.7</cell><cell>97.9</cell><cell>100.0</cell><cell>88.3</cell><cell>97.2</cell><cell>94.7</cell><cell>100.0</cell><cell>96.5</cell><cell>100.0</cell></row><row><cell>009 gelatin box</cell><cell>95.0</cell><cell>100.0</cell><cell>98.8</cell><cell>100.0</cell><cell>95.4</cell><cell>100.0</cell><cell>95.8</cell><cell>100.0</cell><cell>98.1</cell><cell>100.0</cell></row><row><cell>010 potted meat can</cell><cell>86.4</cell><cell>88.5</cell><cell>92.7</cell><cell>93.6</cell><cell>87.3</cell><cell>91.4</cell><cell>90.1</cell><cell>93.1</cell><cell>91.3</cell><cell>93.1</cell></row><row><cell>011 banana</cell><cell>84.7</cell><cell>70.5</cell><cell>97.1</cell><cell>99.7</cell><cell>84.6</cell><cell>62.0</cell><cell>91.5</cell><cell>93.9</cell><cell>96.6</cell><cell>100.0</cell></row><row><cell>019 pitcher base</cell><cell>85.5</cell><cell>79.8</cell><cell>97.8</cell><cell>100.0</cell><cell>86.9</cell><cell>80.9</cell><cell>94.6</cell><cell>100.0</cell><cell>97.1</cell><cell>100.0</cell></row><row><cell>021 bleach cleanser</cell><cell>81.0</cell><cell>65.0</cell><cell>96.9</cell><cell>99.4</cell><cell>91.6</cell><cell>98.2</cell><cell>94.3</cell><cell>99.8</cell><cell>95.8</cell><cell>100.0</cell></row><row><cell>024 bowl</cell><cell>75.7</cell><cell>24.1</cell><cell>81.0</cell><cell>54.9</cell><cell>83.4</cell><cell>55.4</cell><cell>86.6</cell><cell>69.5</cell><cell>88.2</cell><cell>98.8</cell></row><row><cell>025 mug</cell><cell>94.2</cell><cell>99.8</cell><cell>95.0</cell><cell>99.8</cell><cell>90.3</cell><cell>94.7</cell><cell>95.5</cell><cell>100.0</cell><cell>97.1</cell><cell>100.0</cell></row><row><cell>035 power drill</cell><cell>71.5</cell><cell>22.8</cell><cell>98.2</cell><cell>99.6</cell><cell>83.1</cell><cell>64.2</cell><cell>92.4</cell><cell>97.1</cell><cell>96.0</cell><cell>98.7</cell></row><row><cell>036 wood block</cell><cell>68.1</cell><cell>18.2</cell><cell>87.6</cell><cell>80.2</cell><cell>81.7</cell><cell>76.0</cell><cell>85.5</cell><cell>93.4</cell><cell>89.7</cell><cell>94.6</cell></row><row><cell>037 scissors</cell><cell>76.7</cell><cell>35.9</cell><cell>91.7</cell><cell>95.6</cell><cell>83.6</cell><cell>75.1</cell><cell>96.4</cell><cell>100.0</cell><cell>95.2</cell><cell>100.0</cell></row><row><cell>040 large marker</cell><cell>87.9</cell><cell>80.4</cell><cell>97.2</cell><cell>99.7</cell><cell>91.2</cell><cell>88.6</cell><cell>94.7</cell><cell>99.2</cell><cell>97.5</cell><cell>100.0</cell></row><row><cell>051 large clamp</cell><cell>65.9</cell><cell>50.0</cell><cell>75.2</cell><cell>74.9</cell><cell>70.5</cell><cell>77.1</cell><cell>71.6</cell><cell>78.5</cell><cell>72.9</cell><cell>79.2</cell></row><row><cell cols="2">052 extra large clamp 60.4</cell><cell>20.1</cell><cell>64.4</cell><cell>48.8</cell><cell>66.4</cell><cell>50.2</cell><cell>69.0</cell><cell>69.5</cell><cell>69.8</cell><cell>76.3</cell></row><row><cell>061 foam brick</cell><cell>91.8</cell><cell>100.0</cell><cell>97.2</cell><cell>100.0</cell><cell>92.1</cell><cell>100.0</cell><cell>92.4</cell><cell>100.0</cell><cell>92.5</cell><cell>100.0</cell></row><row><cell>MEAN</cell><cell>83.9</cell><cell>74.1</cell><cell>93.0</cell><cell>93.2</cell><cell>88.2</cell><cell>87.9</cell><cell>91.2</cell><cell>95.3</cell><cell>93.1</cell><cell>96.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative evaluation of 6D pose (ADD[13]) on the LineMOD dataset. Objects with bold name are symmetric.</figDesc><table><row><cell></cell><cell cols="2">RGB</cell><cell></cell><cell></cell><cell>RGB-D</cell><cell></cell><cell></cell></row><row><cell></cell><cell>BB8 [24] w ref.</cell><cell>PoseCNN +DeepIM [17, 40]</cell><cell>Implicit [30]+ICP</cell><cell>SSD-6D [14]+ICP</cell><cell>PointFusion [41]</cell><cell>Ours (per-pixel)</cell><cell>Ours (iterative)</cell></row><row><cell>ape</cell><cell>40.4</cell><cell>77.0</cell><cell>20.6</cell><cell>65</cell><cell>70.4</cell><cell>79.5</cell><cell>92.3</cell></row><row><cell>bench vi.</cell><cell>91.8</cell><cell>97.5</cell><cell>64.3</cell><cell>80</cell><cell>80.7</cell><cell>84.2</cell><cell>93.2</cell></row><row><cell>camera</cell><cell>55.7</cell><cell>93.5</cell><cell>63.2</cell><cell>78</cell><cell>60.8</cell><cell>76.5</cell><cell>94.4</cell></row><row><cell>can</cell><cell>64.1</cell><cell>96.5</cell><cell>76.1</cell><cell>86</cell><cell>61.1</cell><cell>86.6</cell><cell>93.1</cell></row><row><cell>cat</cell><cell>62.6</cell><cell>82.1</cell><cell>72.0</cell><cell>70</cell><cell>79.1</cell><cell>88.8</cell><cell>96.5</cell></row><row><cell>driller</cell><cell>74.4</cell><cell>95.0</cell><cell>41.6</cell><cell>73</cell><cell>47.3</cell><cell>77.7</cell><cell>87.0</cell></row><row><cell>duck</cell><cell>44.3</cell><cell>77.7</cell><cell>32.4</cell><cell>66</cell><cell>63.0</cell><cell>76.3</cell><cell>92.3</cell></row><row><cell>eggbox</cell><cell>57.8</cell><cell>97.1</cell><cell>98.6</cell><cell>100</cell><cell>99.9</cell><cell>99.9</cell><cell>99.8</cell></row><row><cell>glue</cell><cell>41.2</cell><cell>99.4</cell><cell>96.4</cell><cell>100</cell><cell>99.3</cell><cell>99.4</cell><cell>100.0</cell></row><row><cell>hole p.</cell><cell>67.2</cell><cell>52.8</cell><cell>49.9</cell><cell>49</cell><cell>71.8</cell><cell>79.0</cell><cell>92.1</cell></row><row><cell>iron</cell><cell>84.7</cell><cell>98.3</cell><cell>63.1</cell><cell>78</cell><cell>83.2</cell><cell>92.1</cell><cell>97.0</cell></row><row><cell>lamp</cell><cell>76.5</cell><cell>97.5</cell><cell>91.7</cell><cell>73</cell><cell>62.3</cell><cell>92.3</cell><cell>95.3</cell></row><row><cell>phone</cell><cell>54.0</cell><cell>87.7</cell><cell>71.0</cell><cell>79</cell><cell>78.8</cell><cell>88.0</cell><cell>92.8</cell></row><row><cell>MEAN</cell><cell>62.7</cell><cell>88.6</cell><cell>64.7</cell><cell>79</cell><cell>73.7</cell><cell>86.2</cell><cell>94.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Another challenging case is the clamp in the middle row due to poor segmentation (not shown in the figure). Our approach localizes the clamp from only the visible part of the object and effectively reduces the dependency on accurate segmentation result.</figDesc><table><row><cell>input crop</cell><cell>initial</cell><cell>iteration 1</cell><cell>iteration 2</cell><cell>iteration 3</cell></row><row><cell>Runtime breakdown (second per frame on YCB-Video Dataset). Our method is approximately 200x faster than PoseCNN+ICP. Seg means Segmentation, and PE means Pose Es-timation. PoseCNN+ICP [40] Ours Seg PE ICP ALL Seg PE Refine ALL 0.03 0.17 10.4 10.6 0.03 0.02 0.01 0.06 umn due to heavy occlusion, whereas our method remains robust. ADD (m): ADD (m):</cell><cell>0.029 0.015</cell><cell>0.022 0.010</cell><cell>0.018 0.008</cell><cell>0.018 0.007</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Since the mapping between pixels and 3D points is unique, we will use interchangeably pixel-fusion and point-fusion.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work has been partially supported by JD.com American Technologies Corporation ("JD") under the SAIL-JD AI Research Initiative and by an ONR MURI award (1186514-1-TBCJE). This article solely reflects the opinions and conclusions of its authors and not JD or any entity associated with JD.com.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeing 3d chairs: Exemplar part-based 2d-3d alignment using a large dataset of cad models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3762" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rotational subgroup voting and pose clustering for robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kiforenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kraft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4137" to="4145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Advanced Robotics (ICAR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The moped framework: Object recognition and pose estimation for manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1284" to="1306" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="998" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simultaneous object recognition and segmentation from single or multiple model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="188" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="858" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="548" to="562" />
		</imprint>
	</monogr>
	<note>in Asian conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A unified framework for multi-view multi-class object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08103</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00175</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pose estimation for augmented reality: A hands-on survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Spindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2633" to="2651" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Project tango</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marder-Eppstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2016 Real-Time Live!, ser. SIGGRAPH &apos;16</title>
		<meeting><address><addrLine>Anaheim, California</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04670</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08488</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminatively trained templates for 3d object detection: A real time scalable approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rios-Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2048" to="2055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rothganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="259" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition and pose estimation based on pre-trained convolutional neural network features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1329" to="1335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="634" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="712" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Discovery of latent 3d keypoints via end-to-end geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Latent-class hough forests for 3d object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-Time Seamless Single Shot 6D Object Pose Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10790</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">6d pose estimation using an improved method based on point pair features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 4th International Conference on Control, Automation and Robotics (ICCAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="405" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3109" to="3118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1903" to="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Subcategory-aware convolutional neural networks for object proposals and detection</title>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="924" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10871</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06396</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Single image 3d object detection and pose estimation for grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3936" to="3943" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
