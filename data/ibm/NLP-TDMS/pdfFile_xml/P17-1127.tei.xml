<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T22:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 30 -August 4, 2017. July 30 -August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangguo</forename><surname>Wang</surname></persName>
							<email>chenwangliangguo@bit.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<email>jingjiang@smu.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Leong</forename><surname>Chieu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Hui</forename><surname>Ong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Liao</surname></persName>
							<email>liaolj@bit.edu.cn†</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">School of Information Systems</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">DSO National Laboratories</orgName>
								<orgName type="institution">Management University</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1385" to="1393"/>
							<date type="published">July 30 -August 4, 2017. July 30 -August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/P17-1127</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we study how to improve the domain adaptability of a deletion-based Long Short-Term Memory (LSTM) neu-ral network model for sentence compression. We hypothesize that syntactic information helps in making such models more robust across domains. We propose two major changes to the model: using explicit syntactic features and introducing syntactic constraints through Integer Linear Programming (ILP). Our evaluation shows that the proposed model works better than the original model as well as a traditional non-neural-network-based model in a cross-domain setting.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence compression is the task of compressing long, verbose sentences into short, concise ones. It can be used as a component of a text summarization system. <ref type="figure">Figure 1</ref> shows two example input sentences and the compressed sentences written by human. The task has been studied for almost two decades. Early work on this task mostly relies on syntactic information such as constituency-based parse trees to help decide what to prune from a sentence or how to re-write a sentence <ref type="bibr" target="#b10">(Jing, 2000;</ref><ref type="bibr" target="#b12">Knight and Marcu, 2000)</ref>. Recently, there has been much interest in applying neural network models to solve the problem, where little or no linguistic analysis is performed except for tokenization ( <ref type="bibr" target="#b5">Filippova et al., 2015;</ref><ref type="bibr" target="#b16">Rush et al., 2015;</ref><ref type="bibr" target="#b1">Chopra et al., 2016)</ref>.</p><p>Although neural network-based models have achieved good performance on this task recently, they tend to suffer from two problems: (1) They require a large amount of data for training. For example, <ref type="bibr" target="#b5">Filippova et al. (2015)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>used close to two</head><p>In-domain Input: The southern Chinese city of Guangzhou has set up a special zone allowing foreign consulates to build permanent offices and residences and avoid prohibitive local rents, the china daily reported Tuesday. Compressed (by human): Guangzhou opens new consulate area. Compressed (by machine): Guangzhou sets up special zone for foreign consulates.</p><p>Out-of-domain Input: Wherever she was, she helped other loyal and flexible wives cope. Compressed (by human): she helped other wives cope. Compressed (by machine): wives and flexible wives <ref type="figure">Figure 1</ref>: Examples of in-domain and out-ofdomain results by a standard abstractive sequenceto-sequence model trained on the Gigaword corpus. The first input sentence comes from the Gigaword corpus while the second input sentence comes from the written news corpus used by <ref type="bibr" target="#b2">Clarke and Lapata (2008)</ref>. million sentence pairs to train an LSTM-based sentence compression model. <ref type="bibr" target="#b16">Rush et al. (2015)</ref> used about four million title-article pairs from the Gigaword corpus ( <ref type="bibr" target="#b14">Napoles et al., 2012</ref>) as training data. Although it may be easy to automatically obtain such training data in some domains (e.g., the news domain), for many other domains, it is not possible to obtain such a large amount of training data. (2) These neural network models trained on data from one domain may not work well on out-of-domain data. For example, when we trained a standard neural sequence-to-sequence model 1 on 3.8 million title-article pairs from the Gigaword corpus and applied it to both in-domain data and out-of-domain data, we found that the performance on in-domain data was good but the performance on out-of-domain data could be very poor. Two example compressed sentences by this trained model are shown in <ref type="figure">Figure 1</ref> to illustrate the comparison between in-domain and out-ofdomain performance.</p><p>The two limitations above imply that these neural network-based models may not be good at learning generalizable patterns, or in other words, they tend to overfit the training data. This is not surprising because these models do not explicitly use much syntactic information, which is more general than lexical information.</p><p>In this paper, we aim to study how syntactic information can be incorporated into neural network models for sentence compression to improve their domain adaptability. We hope to train a model that performs well on both in-domain and out-ofdomain data. To this end, we extend the deletionbased LSTM model for sentence compression by <ref type="bibr" target="#b5">Filippova et al. (2015)</ref>. Although deletion-based sentence compression is not as flexible as abstractive sentence compression, we chose to work on deletion-based sentence compression for the following reason. Abstractive sentence compression allows new words to be used in a compressed sentence, i.e., words that do not occur in the input sentence. Oftentimes these new words serve as paraphrases of some words or phrases in the source sentence. But to generate such paraphrases, the model needs to have seen them in the training data. Because we are interested in a cross-domain setting, the paraphrases learned in one domain may not work well in another domain if the two domains have very different vocabularies. On the other hand, a deletion-based method does not face such a problem in a cross-domain setting.</p><p>Specifically, we propose two major changes to the model by <ref type="bibr" target="#b5">Filippova et al. (2015)</ref>: (1) We explicitly introduce POS embeddings and dependency relation embeddings into the neural network model. (2) Inspired by a previous method <ref type="bibr" target="#b2">(Clarke and Lapata, 2008)</ref>, we formulate the final predictions as an Integer Linear Programming problem to incorporate constraints based on syntactic relations between words and expected lengths of the compressed sentences. In addition to the two major changes above, we also use bi-directional LSTM to include contextual information from both directions into the model. We evaluate our method using around 10,000 sentence pairs released by <ref type="bibr" target="#b5">Filippova et al. (2015)</ref> and two other data sets representing out-ofdomain data. We test both in-domain and outof-domain performance. The experimental results showed that our proposed method can achieve competitive performance compared with the original method in the single-domain setting but with much less training data (around 8,000 sentence pairs for training instead of close to two million sentence pairs). In the cross-domain setting, our proposed method can clearly outperform the original method. We also compare our method with a traditional ILP-based method using syntactic structures of sentences but not based on neural networks <ref type="bibr" target="#b2">(Clarke and Lapata, 2008)</ref>. We find that our method can outperform this baseline for both in-domain and out-of-domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we present our sentence compression method that is aimed at working in a crossdomain setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>Recall that we focus on deletion-based sentence compression. Our problem setup is the same as that by <ref type="bibr" target="#b5">Filippova et al. (2015)</ref>. Let us use s = (w 1 , w 2 , . . . , w n ) to denote an input sentence, which consists of a sequence of words. Here w i ∈ V, where V is the vocabulary. We would like to delete some of the words in s to obtain a compressed sentence that still contains the most important information in s. To represent such a compressed sentence, we can use a sequence of binary labels y = (y 1 , y 2 , . . . , y n ), where y i ∈ {0, 1}. Here y i = 0 indicates that w i is deleted, and y i = 1 indicates that w i is retained.</p><p>We assume that we have a set of training sentences and their corresponding deletion/retention labels, denoted as D = {(s j , y j )} N j=1 . Our goal is to learn a sequence labeling model from D so that for any unseen sentence s we can predict its label sequence y and thus compress the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Our Base Model</head><p>We first introduce our base model, which uses LSTM to perform sequence labeling. This base model is largely based on the model by <ref type="bibr" target="#b5">Filippova et al. (2015)</ref> with some differences, which will be explained below.</p><p>We assume that each word in the vocabulary has a d-dimensional embedding vector. For input sentence s, let us use (w 1 , w 2 , . . . , w n ) to denote the sequence of the word embedding vectors, where w i ∈ R d . We use a standard bi-directional LSTM model to process these embedding vectors sequentially from both directions to obtain a sequence of hidden vectors (h 1 , h 2 , . . . , h n ), where h i ∈ R h . We omit the details of the bi-LSTM and refer the interested readers to the work by <ref type="bibr" target="#b9">Graves et al. (2013)</ref> for further explanation. Following <ref type="bibr">Filip- pova et al. (2015)</ref>, our bi-LSTM has three layers, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>We then use the hidden vectors to predict the label sequence. Specifically, label y i is predicted from h i as follows:</p><formula xml:id="formula_0">p(y i | h i ) = softmax(Wh i + b), (1)</formula><p>where W ∈ R 2×h and b ∈ R 2 are a weight matrix and a weight vector to be learned.</p><p>There are some differences between our base model and the LSTM model by <ref type="bibr" target="#b5">Filippova et al. (2015)</ref>. <ref type="formula">(1)</ref>   <ref type="formula" target="#formula_2">(2015)</ref> combined the predicted y i−1 with w i to help predict y i . This adds some dependency between consecutive labels. We do not do this because later we will introduce an ILP layer to introduce dependencies among labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Incorporation of Syntactic Features</head><p>Note that in the base model that we presented above, there is no explicit use of any syntactic information such as the POS tags of the words or the parse tree structures of the sentences. Because we believe that syntactic information is important for learning a generalizable model for sentence compression, we would like to introduce syntactic features into our model. First, we perform part-of-speech tagging on the input sentences. For sentence s, let us use (t 1 , t 2 , . . . , t n ) to denote the POS tags of the words inside, where t i ∈ T and T is a POS tag set. We further assume that each t ∈ T has an embedding vector (to be learned). Let us use (t 1 , t 2 , . . . , t n ) (t i ∈ R p , p &lt; |T |) to denote the sequence of POS embedding vectors of this sentence. We can then simply concatenate w i with t i as a new vector to be processed by the bi-LSTM model.</p><p>Next, we perform dependency parsing on the input sentences. For each word w i in sentence s, let r i ∈ R denote the dependency relation between w i and its parent word in the sentence, where R is the set of all dependency relation types. We then assume that each r ∈ R has an embedding vector (to be learned). Let (r 1 , r 2 , . . . , r n ) (r ∈ R q , q &lt; |R|) denote corresponding dependency embedding vectors of this sentence. We can also concatenate w i with r i and feed the new vector to the bi-LSTM model.</p><p>In our model, we combine the word embedding, POS embedding and dependency embedding into a single vector to be processed by the bi-LSTM model:</p><formula xml:id="formula_1">x i = w i ⊕ t i ⊕ r i , − → h i = LSTM− → Θ ( − → h i−1 , x i ), ← − h i = LSTM← − Θ ( ← − h i+1 , x i ), h i = − → h i ⊕ ← − h i ,</formula><p>where ⊕ represents concatenation of vectors, and − → Θ and ← − Θ are parameters of the bi-LSTM model. The complete model is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Global Inference through ILP</head><p>Although the method above has explicitly incorporated some syntactic information into the bi-LSTM model, the syntactic information is used in a soft manner through the learned model weights. We hypothesize that there are also hard constraints we can impose on the compressed sentences. For example, the method above as well as the original method by <ref type="bibr" target="#b5">Filippova et al. (2015)</ref> cannot impose any length constraint on the compressed sentences. This is because the labels y 1 , y 2 , . . . , y n are not jointly predicted.</p><p>We propose to use Integer Linear Programming (ILP) to find an optimal combination of the labels y 1 , y 2 , . . . , y n for a sentence, subject to some constraints. Specifically, the ILP problem consists of two parts: the objective function, and the constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Objective Function</head><p>Recall that the trained bi-LSTM model above produces a probability distribution for each label y i , as defined in Eqn. (1). Let us use α i to denote the probability of y i = 1 as estimated by the bi-LSTM model. Intuitively, we would like to set y i to 1 if α i is large.</p><p>Besides the probability estimated by the bi-LSTM model, here we also consider the depth of the word w i in the dependency parse tree of the sentence. Intuitively, a word closer to the root of the tree is more likely to be retained. In order to incorporate this observation, we define dep(w i ) to be the depth of the word w i in the dependency parse tree of the sentence. The root node of the tree has a depth of 0, an immediate child of the root has a depth of 1, and so on. For example, the dependency parse tree of an example sentence together with the depth of each word is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. We can see that some of the words that are deleted according to the ground truth have a relatively larger depth, such as the first "she" (with a depth of 4) and the word "flexible" (with a depth of 5).</p><p>Based on these considerations, we define the objective function to be the following:</p><formula xml:id="formula_2">max n i=1 y i (α i − λ · dep(w i )),<label>(2)</label></formula><p>where λ is a positive parameter to be manually set, and y i is the same as defined before, which is either 0 or 1 to indicate whether w i is deleted or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constraints</head><p>We further introduce some constraints to capture tow considerations. The first consideration is related to the syntactic structure of a sentence, and the second consideration is related to the length of the compressed sentence. Some of the constraints are inspired by <ref type="bibr" target="#b2">Clarke and Lapata (2008)</ref>.</p><p>Our constraints are listed below: (1) No missing parent: Generally, we believe that if a word is retained in the compressed sentence, its parent in the dependency parse tree should also be retained.</p><p>(2) No missing child: For some dependency relations such as nsubj, if the parent word is retained, it makes sense to also keep the child word; otherwise the sentence may become ungrammatical. (3) Max length: Since we are trying to compress a sentence, we may need to impose a minimum compression rate. This could be achieved by setting a maximum value of the sum of y i . (4) Min length: We observe that the original model sometimes produces very short compressed sentences. We therefore believe that it is also important to maintain a mininum length of the compressed sentence. This can be achieved by setting a minimum value of the sum of y i .</p><p>Formally, the constraints are listed as follows:</p><formula xml:id="formula_3">n i=1 y i &lt;= βn, n i=1 y i &gt;= γn, ∀y i : y i ≤ y p i , ∀r i ∈ T : y i ≥ y p i ,</formula><p>where w p i is the parent word of w i in the dependency parse tree, r i is the dependency relation type between w i and w p i , and T is a set of dependency relations for which the child word is often retained when the parent word is retained in the compressed sentence.</p><p>The set T is derived as follows. For each dependency relation type, based on the training data, we compute the conditional probability of the child word being retained given that the parent word is retained. If this probability is higher than 90%, we include this dependency relation type in T .  For Google News and BNC News, we have the ground truth compressed sentences, which are deletion-based compressions, i.e., subsequences of the original sentences. For Research Papers, we use it only for manual evaluation in terms of readability and informativeness, as we will explain below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Experiment Settings</head><p>We evaluate three settings of our method: BiLSTM: In this setting, we use only the base bi-LSTM model without incorporating any syntactic feature. BiLSTM+SynFeat: In this setting, we combine word embeddings with POS embeddings and de-pendency embeddings as input to the bi-LSTM model and use the predictions of y from the bi-LSTM model. BiLSTM+SynFeat+ILP: In this setting, on top of BiLSTM+SynFeat, we solve the ILP problem as described in Section 2.4 to predict the final label sequence y.</p><p>In the experiments, our model was trained using the Adam ( <ref type="bibr" target="#b11">Kingma and Ba, 2015</ref>) algorithm with a learning rate initialized at 0.001. The dimension of the hidden layers of bi-LSTM is 100. Word embeddings are initialized from GloVe 100-dimensional pre-trained embeddings ( <ref type="bibr" target="#b15">Pennington et al., 2014</ref>). POS and dependency embeddings are randomly initialized with 40-dimensional vectors. The embeddings are all updated during training. Dropping probability for dropout layers between stacked LSTM layers is 0.5. The batch size is set as 30. For the ILP part, λ is set to 0.5, β and γ are turned by the validation data and finally they are set to 0.7 and 0.2, respectively. We utilize an open source ILP solver 4 in our method.</p><p>We compare our methods with a few baselines: LSTM: This is the basic LSTM-based deletion method proposed by <ref type="bibr" target="#b5">(Filippova et al., 2015</ref>). We report both the performance they achieved using close to two million training sentence pairs and the performance of our re-implementation of their model trained on the 8,000 sentence pairs. LSTM+: This is advanced version of the model proposed by <ref type="bibr" target="#b5">Filippova et al. (2015)</ref>, where the authors incorporated some dependency parse tree information into the LSTM model and used the prediction on the previous word to help the prediction on the current word.</p><p>Traditional ILP: This is the ILP-based method proposed by <ref type="bibr" target="#b2">Clarke and Lapata (2008)</ref>. This method does not use neural network models and is an unsupervised method that relies heavily on the syntactic structures of the input sentences <ref type="bibr">5</ref> . Abstractive seq2seq: This is an abstractive sequence-to-sequence model trained on 3.8 million Gigaword title-article pairs as described in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automatic Evaluation</head><p>With the two datasets Google News and BNC News that have the ground truth compressed sentences, we can perform automatic evaluation. We first split the Google News dataset into a training set, a validation set and a test set. We took the first 1,000 sentence pairs from Google News as the test set, following the same practice as <ref type="bibr" target="#b5">Filippova et al. (2015)</ref>. We then use 8,000 of the remaining sentence pairs for training and the other 1,000 sentence pairs for validation. For the NBC News dataset, we use it only as a test set, applying the sentence compression models trained from the 8,000 sentence pairs from Google News.</p><p>We use the ground truth compressed sentences to compute accuracy and F1 scores. Accuracy is defined as the percentage of tokens for which the predicted label y i is correct. F1 scores are derived from precision and recall values, where precision is defined as the percentage of retained words that overlap with the ground truth, and recall is defined as the percentage of words in the ground truth compressed sentences that overlap with the generated compressed sentences.</p><p>We report both in-domain performance and cross-domain performance in <ref type="table">Table 1</ref>. From the table, we have the following observations: (1) For the abstractive sequence-to-sequence model, it was trained on the Gigaword data, so for both Google News and NBC News, the performance shown is cross-domain performance. We can see that indeed this abstractive method performed poorly in cross-domain settings. <ref type="formula" target="#formula_2">(2)</ref> In the in-domain setting, with the same amount of training data (8,000), our BiLSTM method with syntactic features (BiLSTM+SynFeat and BiLSTM+SynFeat+ILP) performs similarly to or better than the LSTM+ method proposed by <ref type="bibr">Filip- pova et al. (2015)</ref>, in terms of both F1 and accuracy. This shows that our method is comparable to the LSTM+ method in the in-domain setting. <ref type="formula">(3)</ref> In the in-domain setting, even compared with the <ref type="bibr">5</ref> We use an open source implementation: https:// github.com/cnap/sentence-compression.  performance of LSTM+ trained on 2 million sentence pairs, our method trained on 8,000 sentence pairs does not perform substantially worse. <ref type="formula">(4)</ref> In the out-of-domain setting, our BiLSTM+SynFeat and BiLSTM+SynFeat+ILP methods clearly outperform the LSTM and LSTM+ methods. This shows that by incorporating more syntactic features, our methods learn a sentence compression model that is less domain-dependent. <ref type="formula">(5)</ref> The Traditional ILP method also works better than the LSTM and LSTM+ methods in the out-of-domain setting. This is probably because the Traditional ILP method relies heavily on syntax, which is less domain-dependent compared with lexical patterns. But the Traditional ILP method performs worse in the in-domain setting than both the LSTM and LSTM+ methods and our methods.</p><p>Overall, <ref type="table">Table 1</ref> shows that our proposed method combines both the strength of neural network models in the in-domain setting and the strength of the syntax-based methods in the crossdomain setting. Therefore, our method works reasonably well for both in-domain and out-ofdomain data.</p><p>We also notice that on Google News, adding the ILP layer decreased the sentence compression performance. After some analysis, we think the reason is that some of the constraints used in the ILP layer have led to less deletion but the ground truth compressed sentences in the Google News data tend to be shorter compared with those in the NBC News data.</p><p>We also conduct additional experiments to see the effect of the training data size on our meth- <ref type="table" target="#tab_0">Acc  CR  F1  Acc  CR   LSTM (Filippova et al., 2015)  2 million  0.80  - 0.39  - - - LSTM+ (Filippova et al., 2015)</ref> 2 million 0.82 -0.  <ref type="table">Table 1</ref>: Automatic evaluation of our sentence compression methods. CR standards for compression rate and is defined as the average percentage of words that are retained after compression.</p><note type="other">size of Google News NBC News training data F1</note><p>ods and the LSTM+ method. <ref type="figure" target="#fig_5">Figure 4</ref> shows the F1 scores on the in-domain Google News data and the out-of-domain NBC News data when we train the models using different amounts of sentence pairs. We can see that in the in-domain setting, our method does not have any advantage over the LSTM+ method. But in the cross-domain setting, our method that uses ILP to impose syntax-based constraints clearly performs better than LSTM+ when the amount of training data is relatively small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Manual Evaluation</head><p>The evaluation above does not look at the readability of the compressed sentences. In order to evaluate whether sentences generated by our method are readable, we adopt the manual evaluation procedure by <ref type="bibr" target="#b5">Filippova et al. (2015)</ref> to compare our method with LSTM+ and Traditional ILP in terms of readability and informativeness. We asked two raters to score a randomly selected set of 100 sentences from the Research Papers dataset. The compressed sentences were randomly ordered and presented to the human raters to avoid any bias. The raters were asked to score the sentences on a five-point scale in terms of both readability and informativeness. We show the average scores of the three methods we compare in <ref type="table" target="#tab_1">Table 3</ref>. We can see that our BiLSTM+SynFeat+ILP method clearly outperforms the two baseline methods in the manual evaluation.</p><p>We also show a small sample of input sentences from the Research Papers dataset and the automatically compressed sentences by different methods in <ref type="table" target="#tab_0">Table 2</ref>. As we can see from the table, a general weakness of the LSTM+ method is that the compressed sentences may not be grammatical. In comparison, our method does better in terms of preserving grammaticality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Sentence compression can be seen as sentencelevel summarization. Similar to document summarization, sentence compression methods can be divided into extractive compression and abstractive compression methods, based on whether words in the compressed sentence all come from the source sentence. In this paper, we focus on deletion-based sentence compression, which is a spacial case of extractive sentence compression.</p><p>An early work on sentence compression was done by <ref type="bibr" target="#b10">Jing (2000)</ref>, who proposed to use several resources to decide whether a phrase in a sentence should be removed or not. <ref type="bibr" target="#b12">Knight and Marcu (2000)</ref> proposed to apply a noisy-channel model from machine translation to the sentence compression task, but their model encountered the problem that many SCFG rules have unreliable probability estimates with inadequate data. <ref type="bibr">Galley and McKe- own (2007)</ref> tried to solve this problem by utilizing parent annotation, Markovization and lexicalization, which have all been shown to improve the quality of the rule probability estimates. <ref type="bibr" target="#b3">Cohn and Lapata (2007)</ref> formulated sentence compression as a tree-to-tree rewrite problem. They utilized a synchronous tree substitution grammar (STSG) to license the space of all possible rewrites. Each rule has a weight learned from the training data. For prediction, an algorithm was used to search for the best scoring compression using the gram-Although dynamic oracles are widely used in dependency parsing and available for most standard transition systems , no dynamic oracle parsing model has yet been proposed for phrase structure grammars T: Although are used for transition systems model has been proposed for structure grammars . S: Although dynamic oracles are . B: Although oracles are used no model has been proposed for structure grammars .</p><p>As described above , we used Bayesian Optimization to find optimal hyperparameter configurations in fewer steps than in regular grid search . T: As described we used Optimization to find configurations in steps in search . S: As described above Optimization to find optimal hyperparameter configurations steps than in grid search . B: As described , we used Bayesian Optimization to find optimal hyperparameter configurations in steps.</p><p>Following the phrase structure of a source sentence , we encode the sentence recursively in a bottom-up fashion to produce a vector representation of the sentence and decode it while aligning the input phrases and words with the output . T: Following structure of a sentence we encode sentence recursively to produce a representation of the sentence and decode it while aligning phrases and words with output . S: Following the structure of a source sentence encode the sentence recursively in a bottom-up fashion . B: Following the structure , we encode the sentence recursively in a bottom-up fashion to produce a vector representation and decode it .   <ref type="bibr" target="#b4">Cohn and Lapata (2008)</ref> extended this model to abstractive sentence compression, which includes substitution, reordering and insertion. <ref type="bibr" target="#b13">McDonald (2006)</ref> proposed a graphbased sentence compression method. The general idea is that each word pair in the original sentence has a score. The task then becomes how to find a compressed sentence with a length limit according word pair scores. Their method is similar to graphbased dependency parsing. <ref type="bibr" target="#b2">Clarke and Lapata (2008)</ref> first used an ILP framework for sentence compression. In the paper, the author put forward three models. The first model is a language model reformulated by ILP. As the first model treats all the words equally, the second model uses a corpus to learn an importance score for each word and then incorporates it in the ILP model. The Last model, which is based on <ref type="bibr" target="#b13">(McDonald, 2006</ref>), replaces the decoder with an ILP model and adds many linguistic constraints such as dependency parsing compared with the previous two ILP models. <ref type="bibr" target="#b7">Filippova and Strube (2008)</ref> represented sentences with dependency parse trees and an ILPbased method was used to decide whether the dependencies were preserved or not. Different from most previous work that treats sentence extraction and sentence compression separately, <ref type="bibr" target="#b0">Berg- Kirkpatrick et al. (2011)</ref> jointly model the two processes in one ILP problem. Bigrams and subtrees are represented by some features, and feature are learned on some training data. The ILP problem maximizes the coverage of weighted bigrams and deleted subtrees of the summary. In recent years, neural network models, especially sequence-to-sequence models, have been applied to sentence compression. Our work is based on the deletion-based LSTM model for sentence compression by <ref type="bibr" target="#b5">Filippova et al. (2015)</ref>. There has also been much interest in applying sequence-to-sequence models for abstractive sentence compression <ref type="bibr" target="#b16">(Rush et al., 2015;</ref><ref type="bibr" target="#b1">Chopra et al., 2016)</ref>. As we pointed out in Section 1, in a cross-domain setting, abstractive sentence compression may not be suitable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we studied how to modify an LSTM model for deletion-based sentence compression so that the model works well in a cross-domain setting. We hypothesized that incorporation of syntactic information into the training of the LSTM model would help. We thus proposed two ways to incorporate syntactic information, one through directly adding POS tag embeddings and dependency type embeddings, and the other through the objective function and constraints of an Integer Linear Programming (ILP) model. The experiments showed that our proposed bi-LSTM model with syntactic features and an ILP layer works well in both in-domain and cross-domain settings. In comparison, the original LSTM model does not work well in the cross-domain setting, and a traditional ILP method does not work well in the in-domain setting. Therefore, our proposed method is relatively more robust than these baselines. We also manually evaluated the compressed sentences generated by our method and found that the method works better than the baselines in terms of both readability and informativeness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our three-layered bi-LSTM model. Word embeddings, POS tag embeddings and dependency type embeddings are concatenated in the input layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Filippova et al. (2015) first encoded the input sentence in its reverse order using the same LSTM before processing the sentence for sequence labeling. (2) Filippova et al. (2015) used only a single-directional LSTM while we use bi-LSTM to capture contextual information from both directions. (3) Although Filippova et al. (2015) did not use any syntactic information in their basic model, they introduced some features based on dependency parse trees in their advanced models. Here we follow their basic model be- cause later we will introduce more explicit syntax- based features. (4) Filippova et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Dependency parse tree of an example sentence. The numbers below the words indicate the depths of the words in the tree. Words in gray are supposed to be deleted based on the ground truth.</figDesc><graphic url="image-1.png" coords="5,75.54,62.81,446.45,100.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Because we are mostly interested in a cross- domain setting where the model is trained on one domain and test on a different domain, we need data from different domains for our evaluation. Here we use three datasets. Google News: The first dataset contains 10,000 sentence pairs collected and released by Filippova et al. (2015) 2 . The sentences were automatically acquired from the web through Google News us- ing a method introduced by Filippova and Altun (2013). The news articles were from 2013 and 2014. BNC News: The second dataset contains around 1,500 sentence pairs collected by Clarke and Lap- ata (2008) 3 . The sentences were from British Na- tional Corpus (BNC) and the American News Text corpus before 2008. Research Papers: The last dataset contains 100 sentences taken from 10 randomly selected papers published at the ACL conference in 2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: F1 scores with different sizes of training data for in-domain and cross-domain settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Some input sentences from the Research Papers dataset and the automatically compressed 
sentences using different methods. T: Traditional ILP method. S: LSTM+. B: BiLSTM+SynFeat+ILP. 

readability informativeness 

Traditional ILP 
3.94 
3.33 
LSTM+ 
3.69 
3.07 
BiLSTM+SynFeat+ILP 4.29 
3.46 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Manual evaluation. 

mar rules. Besides, </table></figure>

			<note place="foot" n="1"> http://opennmt.net/</note>

			<note place="foot" n="2"> Available at http://storage.googleapis. com/sentencecomp/compression-data.json. 3 Available at http://jamesclarke.net/ research/resources/.</note>

			<note place="foot" n="4"> gnu.org/software/glpk</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is supported by DSO grant DSOCL15223.</p><p>The work was conducted during the first author's visit to the Singapore Management University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the 2016 Conference of the North American Chapter</title>
		<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression: An integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large margin synchronous generation and its application to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Meeting of Conference on Empirical Methods in Natural Language and Conference on Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentence compression beyond word deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sentence compression by deletion with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dependency tree based sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Natural Language Generation Conference</title>
		<meeting>the Fifth International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lexicalized markov grammars for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentence reduction for automatic text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth conference on Applied natural language processing</title>
		<meeting>the sixth conference on Applied natural language processing</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statisticsbased summarization step one: Sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th National Conference on Artificial Intelligence</title>
		<meeting>the 17th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative sentence compression with soft syntactic evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan T Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Chapter</title>
		<meeting>European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics Valencia</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Annotated Gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
