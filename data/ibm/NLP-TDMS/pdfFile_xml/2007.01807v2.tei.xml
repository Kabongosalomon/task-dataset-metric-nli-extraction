<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continuously Indexed Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Katabi</surname></persName>
						</author>
						<title level="a" type="main">Continuously Indexed Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing domain adaptation focuses on transferring knowledge between domains with categorical indices (e.g., between datasets A and B). However, many tasks involve continuously indexed domains. For example, in medical applications, one often needs to transfer disease analysis and prediction across patients of different ages, where age acts as a continuous domain index. Such tasks are challenging for prior domain adaptation methods since they ignore the underlying relation among domains. In this paper, we propose the first method for continuously indexed domain adaptation. Our approach combines traditional adversarial adaptation with a novel discriminator that models the encoding-conditioned domain index distribution. Our theoretical analysis demonstrates the value of leveraging the domain index to generate invariant features across a continuous range of domains. Our empirical results show that our approach outperforms the stateof-the-art domain adaption methods on both synthetic and real-world medical datasets 1 . example, in medial applications the index can be a vector of age, blood pressure, activity level, etc.</p><p>Our contributions are as follows:</p><p>• We identify the problem of adaptation across continuously indexed domains and propose continuously indexed domain adaptation (CIDA) as the first general DA method arXiv:2007.01807v2 [cs.</p><p>LG] 30 Aug 2020</p><p>Continuously Indexed Domain Adaptation for addressing this problem. Further, we analyze our method and provide theoretical guarantees that CIDA aligns continuously indexed domains at equilibrium.</p><p>• We derive two advanced versions, probabilistic CIDA and multi-dimensional CIDA, to further improve performance and handle multi-dimensional domain indices, with minimal overhead.</p><p>• We provide empirical results using both synthetic and real-world medical datasets which show that CIDA and its probabilistic and multi-dimensional variants significantly improve performance over the state-of-the-art DA methods for continuously indexed domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning often assumes that training and test data come from the same distribution, so that the trained model generalizes well to the test scenario. This assumption breaks however when the model is trained and tested in distinct domains, i.e., different source and target domains. Domain adaption (DA) leverages labeled data from the source domains and unlabeled data (or a limited amount of labeled data) from the target domains to significantly improve performance <ref type="bibr" target="#b1">(Ben-David et al., 2010;</ref><ref type="bibr" target="#b6">Ganin et al., 2016;</ref><ref type="bibr" target="#b20">Tzeng et al., 2017;</ref><ref type="bibr" target="#b24">Zhang et al., 2019)</ref>.</p><p>Existing DA methods however focus on adaption among * Equal contribution 1 MIT Computer Science and Artificial Intelligence Laboratory, Massachusetts, USA. Correspondence to: Hao Wang &lt;hoguewang@gmail.com&gt;.</p><p>Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s). <ref type="bibr">1</ref> Code will soon be available at https://github.com/ hehaodele/CIDA categorical domains where the domain index is just a label. A common example would be to adapt a model from one image dataset to another, e.g., adapting from MNIST to SVHN. However, many real-world tasks require adaptation among continuously index domains. For example, in medical applications, one needs to adapt disease diagnosis and prognosis across patients of different ages, where age is a continuous domain index. Treating the age of the source and target domains as domain labels is unlikely to yield the best results because it does not take advantage of the relationship between the disease manifestation and the person's age. Similar issues appear in robotics. For example, underwater robots have to operate at different water depths and viscosity, and one expects that adaptation across datasets from different depths or viscosity (e.g., lake vs. sea) should take into account the relationship between the robot operation and the physical properties of the liquid in which it operates. These examples highlight the limitations of current DA methods when applied to continuously indexed domains.</p><p>So, how should we perform domain adaption across continuously indexed domains? We note that in the above examples the domain index plays the role of a distance metric -i.e., it captures a similarity distance between the domains with respect to the task. Thus, one approach for addressing the problem is to modify traditional adversarial adaptation to make the discriminator regress the domain index using a distance-based loss, like the L 2 or L 1 loss. Although this is better than categorical DA, we show analytically that such treatment can lead to equilibriums with relatively poor domain alignments. A better solution is to develop a probabilistic discriminator that models the domain index distribution. We show that such a discriminator not only successfully captures the underlying relation among domains, but also enjoys better theoretical guarantees in terms of domain alignment. We also note that our method can be naturally generalized to handle multi-dimensional domain indices, achieving further performance gain. For</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Adversarial Domain Adaptation. Much prior work has focused on the problem of domain adaptation <ref type="bibr" target="#b27">(Zhao et al., 2017;</ref><ref type="bibr" target="#b12">Long et al., 2018;</ref><ref type="bibr" target="#b16">Saito et al., 2018;</ref><ref type="bibr" target="#b17">Sankaranarayanan et al., 2018;</ref><ref type="bibr" target="#b24">Zhang et al., 2019)</ref>. The key idea is to match the distributions of the source and target domains. This is achieved by matching their distributions' statistics either directly <ref type="bibr" target="#b13">(Pan et al., 2010;</ref><ref type="bibr" target="#b19">Tzeng et al., 2014;</ref><ref type="bibr" target="#b18">Sun &amp; Saenko, 2016)</ref> or with the help of an adversarial loss <ref type="bibr" target="#b6">(Ganin et al., 2016;</ref><ref type="bibr" target="#b27">Zhao et al., 2017;</ref><ref type="bibr" target="#b20">Tzeng et al., 2017;</ref><ref type="bibr" target="#b24">Zhang et al., 2019;</ref><ref type="bibr" target="#b11">Kuroki et al., 2019)</ref>. Adversarial domain adaptation is particularly popular due to its relatively strong theoretical insights <ref type="bibr" target="#b7">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b25">Zhao et al., 2018;</ref><ref type="bibr" target="#b24">Zhang et al., 2019;</ref><ref type="bibr" target="#b26">Zhao et al., 2019)</ref> and its compatibility with neural networks. It aligns the distributions of the source and target domains by generating an encoding indistinguishable from a perspective of discriminator that is trained to classify the domain of the data. In this paper, we build on adversarial domain adaptation and extend it to address continuously indexed domains.</p><p>Incremental Domain Adaptation. Closest to our work are incremental DA approaches. Essentially they assume the domain shifts smoothly over time and try to incrementally adapt the source domain to multiple target domains. Different methods are used to perform categorical DA for each domain pair, such as optimal transport <ref type="bibr" target="#b9">(Jimenez et al., 2019)</ref>, adversarial loss <ref type="bibr" target="#b3">(Bitarafan et al., 2016)</ref>, generative adversarial networks <ref type="bibr" target="#b22">(Wulfmeier et al., 2018)</ref>, and linear transform . <ref type="bibr" target="#b4">Bobu et al. (2018)</ref> notices such incremental adaptation procedure is prone to catastrophic forgetting, a tendency to forget the knowledge of previous domains while specializing to a new domain, and therefore proposes a replay technique to tackle the issue. Here we note several key differences between CIDA and the methods above. (1) These approaches incrementally perform pair-wise categorical DA. Hence failure in adapting one domain pair can lead to catastrophic failures for all following pairs. (2) They only work on DA tasks with one single domain shifting dimension (usually 'time'), while our method naturally generalizes to multi-dimensional settings. Such differences are empirically verified in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we formalize the problem of adaptation among continuously indexed domains, and describe our methods for addressing the problem. We then provide theoretical guarantees for the proposed methods in Sec. 4.</p><p>Problem. We consider the unsupervised domain adaptation setting and assume a set of continuous domain indices U = U s ∪ U t , where U s and U t are the domain index sets for the source and the target domains, and U is part of a metric space (i.e., a metric like the Euclidian distance is defined over the set). The input and labels are denoted as x and y, respectively. With access to the labeled data</p><formula xml:id="formula_0">{(x s i , y s i , u s i )} n i=1 from source domains (u s i ∈ U s ) and unla- beled data {(x t i , u t i )} m i=1 from target domains (u t i ∈ U t )</formula><p>, the goal is to accurately predict the labels {(y t i )} m i=1 for data in the target domains.</p><p>Multi-Dimensional Domain Indices. For clarity, we introduce our methods and theory in the context of unidimensional domain indices. However, they can directly apply to multi-dimensional domain indices. Later in Sec. 5, we show that the ability of handling multi-dimensional domain indices brings further performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Continuously Indexed Domain Adaptation (CIDA)</head><p>To perform adaptation across a continuous range of domains, we leverage the idea of learning domain-invariant encodings with adversarial training. We propose to learn an encoder 2 E and a predictor F such that the distribution of the encodings z = E(x) ∈ Z (or z = E(x, u)) from all domains U are aligned so that all labels can be accurately predicted by the shared predictor F . Formally, domain-invariant encodings require that p(z|u 1 ) = p(z|u 2 ), ∀u 1 , u 2 ∈ U. It implies that z and u are independent (u ⊥ ⊥ z), i.e., p(u|z) = p(u) or equivalently p(z|u) = p(z). This is achieved with the help of a discriminator D. In continuously indexed domains however, small changes in u should lead to small changes in the encoding. Thus, instead of classifying the encoding into categorical domains, the discriminator D in CIDA regresses the domain index.</p><p>Formally, CIDA performs a minimax optimization with the value function V (E, F, D) as:</p><formula xml:id="formula_1">min E,F max D V p (E, F ) − λ d V d (D, E),<label>(1)</label></formula><p>where we have</p><formula xml:id="formula_2">V p (E, F ) E s [L p (F (E(x, u)), y)] V d (D, E) E[L d (D(E(x, u)), u)]</formula><p>where E and E s denote the expectations taken over the entire data distribution p(x, y, u) and the source data distribution p s (x, y, u). Note that the label y is only accessible in the source domains. L p is the prediction loss (e.g., crossentropy loss for classification tasks), and L d is the domain index loss. λ d is a hyperparameter balancing both losses. The main difference between CIDA and traditional adversarial domain adaptation is that the discriminator loss L d is a monotonic function of the metric defined over U.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Variants of CIDA</head><p>Note that there can be various designs for both D and L d . For example, D can either directly predict the domain index or predict its mean and variance, and L d can be either the L 2 or L 1 loss. Different designs come with different theoretical guarantees.</p><p>Vanilla CIDA. In the vanilla CIDA, D directly predicts the domain index, and correspondingly L d is the L 2 loss between the predicted and ground-truth domain index,</p><formula xml:id="formula_3">L d (D(z), u) = (D(z) − u) 2 ,<label>(2)</label></formula><p>Vanilla CIDA above only guarantees matching the mean of the distribution p(u|z) (see theoretical results in Sec. 4).</p><p>Therefore in the following, we introduce an advanced version, dubbed probabilistic CIDA (PCIDA), which enjoys better theoretical guarantees to match both the mean and variance of the distribution p(u|z). We note that PCIDA can be extended to match higher-order moments.</p><p>Probabilistic CIDA. The major improvement from CIDA to PCIDA is that in PCIDA, the discriminator predicts the distribution of p(u|z) instead of providing point estimation.</p><p>We start with the simplest probabilistic model, Gaussian distributions, where the discriminator D outputs the mean and variance of p(u|z) as D µ (z) and D σ 2 (z), respectively. To train such a discriminator, we use the negative log-likelihood as the loss function:</p><formula xml:id="formula_4">L d (D(z), u) = (D µ (z) − u) 2 2D σ 2 (z) + 1 2 log D σ 2 (z),<label>(3)</label></formula><p>where D(z) = (D µ (z), D σ 2 (z)).</p><p>Extension to Gaussian Mixture Models. PCIDA can be naturally extended from a single Gaussian to a Gaussian mixture model (GMM) by using a mixture density network as the discriminator D <ref type="bibr" target="#b2">(Bishop, 1994)</ref> and the corresponding negative log-likelihood as L d (·, ·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Results</head><p>In this section, we provide theoretical guarantees for CIDA and PCIDA. As standard in adversarial domain adaption, we analyze a game in which the encoder aims to fool the discriminator and prevent it from inferring the domain index. We first analyze a simplified game between the encoder and the discriminator (without the predictor) to gain insight of the aligned encodings. We then discuss the full three-player game and show our framework preserves the prediction power while aligning the encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis for the Simplified Game</head><p>We consider a simplified game which does not involve the predictor F , defined by the V d (E, D) term in Eqn. 1:</p><formula xml:id="formula_5">max E min D V d (E, D) = E[L d (D(E(x, u)), u)]. (4)</formula><p>We first analyze the equilibrium of the simplified game for CIDA. Recall that, in CIDA, the discriminator D predicts the domain index u given the encoding z and the domain index loss L d is the L 2 loss. We show that in the equilibrium of CIDA, the encoder will align the mean of the conditional domain distribution p(u|z) to the mean of the marginal domain distribution p(u).</p><p>Lemma 4.1 below analyzes the discriminator D with the encoder E fixed and states that the optimal discriminator D outputs the mean domain index of all data with the same encoding z.</p><p>Lemma 4.1 (Optimal Discriminator for CIDA). For E fixed, the optimal D is</p><formula xml:id="formula_6">D * E (E(x, u)) = E u∼p(u|z) [u], where z = E(x, u). Proof. With E fixed, the optimal D D * E = argmin D E (x,u)∼p(x,u) [L d (D(E(x, u)), u)] = argmin D E (z,u)∼p(z,u) [ D(z) − u 2 2 ] = argmin D E z∼p(z) E u∼p(u|z) [ D(z) − u 2 2 ]</formula><p>Notice that</p><formula xml:id="formula_7">E u∼p(u|z) [(D(z) − u) 2 ] =E u∼p(u|z) [u 2 ] − 2D(z)E u∼p(u|z) [u] + D(z) 2 ,</formula><p>is a quadratic form of D(z) which achieves the minimum at D(z) = E u∼p(u|z) <ref type="bibr">[u]</ref>.</p><p>Assuming that D always achieves its optimum w.r.t E during the training, the minimax game in Eqn. 4 can be reformulated as maximizing</p><formula xml:id="formula_8">C d (E) where C d (E) min D V d (E, D) = V d (E, D * E ) = E (x,u)∼p(x,u) (E u∼p(u|z) [u] − u) 2 = E z∼p(z) E u∼p(u|z) (E u∼p(u|z) [u] − u) 2 = E z∼p(z) V u∼p(u|z) [u] = E z V[u|z],</formula><p>where V denotes variance.</p><p>Next we analyze the virtual training criterion C d (E) for the encoder and derive the global optimum.  </p><formula xml:id="formula_9">(u), i.e., E[u|z] = E[u], ∀z. Proof. We first show C d (E) ≤ V[u]</formula><p>and then show the equality is achieved when</p><formula xml:id="formula_10">E[u|z] = E[u], ∀z. C d (E) − V[u] = EzV[u|z] − V[u] = Ez[E[u 2 |z] − E[u|z] 2 ] − (E[u 2 ] − E[u] 2 ) = E[u] 2 − Ez[E[u|z] 2 ].</formula><p>By the convexity of x 2 and Jensen's inequality, we have</p><formula xml:id="formula_11">E[u] 2 = (E z [E[u|z]]) 2 ≤ E z [E[u|z] 2 ] and the equality is achieved when E[u|z] is constant w.r.t. z. By Lemma 4.2 we have E[u|z] = E[u], ∀z.</formula><p>As Theorem 4.1 states, the vanilla CIDA using the L 2 loss guarantees that the mean of the distribution p(u|z) matches the mean of the marginal distribution p(u). It means that there is a risk the encoder E only aligns the mean of the distributions without exactly matching the entire distributions. However, surprisingly, we find that CIDA often achieves good empirical performance (see Sec. 5 for more details). Next, we analyze PCIDA and show that PCIDA enjoys better theoretical guarantees and matches both the mean and variance of the distribution p(u|z).</p><p>Recall that in PCIDA, the discriminator D outputs the mean and variance of p(u|z) as D µ (z) and D σ 2 (z). We use the negative log-likelihood (Eqn. 3) as the domain loss L d . We start from analyzing the discriminator D when the encoder E is fixed. Lemma 4.3 states that the optimal discriminator D, given the encoding z, will output the mean and variance of the domain index distribution p(u|z) .</p><formula xml:id="formula_12">Lemma 4.3 (Optimal Discriminator for PCIDA). With E fixed, the optimal D is D * µ,E (z) = E u∼p(u|z) [u], D * σ 2 ,E (z) = V u∼p(u|z) [u], where z = E(x, u), and D = (D µ , D σ 2 ).</formula><p>Proof of Lemma 4.3 is similar to that of Lemma 4.1 (see the Supplement for details).</p><p>Assuming discriminator D always reaches optimum, the virtual training criterion C d (E) for the encoder becomes:</p><formula xml:id="formula_13">C d (E) = min D V d (E, D) = V d (E, D * E ) = E z,u (E[u|z] − u) 2 2V[u|z] + 1 2 log(V[u|z]) .</formula><p>Now we analyze C d (E) and provide PCIDA's global optimum.</p><p>Lemma 4.4 (Uniqueness of Constant Expectation and Variance). If there exist constants µ c and σ 2 c such that Proof. Given that</p><formula xml:id="formula_14">E u∼p(u|z) [u] = µ c and V u∼p(u|z) [u] = σ 2 c for any z, we have µ c = E u∼p(u) and σ 2 c = V u∼p(u) [u].</formula><formula xml:id="formula_15">C d (E) = E z,u (E[u|z] − u) 2 2V[u|z] C1 + E z,u 1 2 log(V[u|z]) C2 ,</formula><p>we analyze the upper bounds of the two terms separately. For the first term,</p><formula xml:id="formula_16">C1 = EzE u|z (E[u|z] − u) 2 2V[u|z] = Ez E u|z (E[u|z] − u) 2 2V[u|z] = Ez V[u|z] 2V[u|z] = Ez 1 2 = 1 2 .</formula><p>For the second term, by the concavity of log(x) and Jensen's inequality, we have that . Can we match higher-order moments? We believe our methodology can generalize to match higher-order moments by using PCIDA with more complex parametric probabilistic models. For example, one can use skew-normal distributions <ref type="bibr" target="#b0">(Azzalini, 2013)</ref> to match the third moment (skewness).</p><formula xml:id="formula_17">2C 2 ≤ log(E z [V[u|z]])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of the Three-player Game</head><p>We analyze the equilibrium state of the three-player game of E, F and D as defined in Eqn. 1. We divide the situation into two cases based on whether the domain index u is independent of the label y.</p><formula xml:id="formula_18">4.2.1. u ⊥ ⊥ y</formula><p>The domain index u is independent of the label y when it captures nuisance variations that are irrelevant to the task of predicting the label y. In this case, we prove the following theorem showing that the optimal encoding captures all the information in the input x that is relevant to the predictive tasks while aligning the domain index distributions.</p><p>Lemma 4.5 (Optimal Predictor). Given the encoder E,</p><formula xml:id="formula_19">the prediction loss V p (F, E) L p (F (E(x, u)), y) ≥ H(y|E(x, u)) where H(·)</formula><p>is the entropy. The optimal predictor F * that minimizes the prediction loss is F * (E(x, u)) = P y (·|E(x, u)).</p><p>Assuming the predictor F and the discriminator D are trained to achieve their optimal losses, by Lemma 4.5, the three-player game (Eqn. 1) can be rewritten as following training procedure of the encoder E,</p><formula xml:id="formula_20">min E C(E) H(y|E(x, u)) − λ d C d (E).<label>(5)</label></formula><p>Theorem 4.3. If the encoder E, the predictor F and the discriminator D have enough capacity and are trained to reach optimum, any global optimal encoder E * has the following properties:</p><formula xml:id="formula_21">H(y|E * (x, u)) = H(y|x, u) (6a) C d (E * ) = max E C d (E )<label>(6b)</label></formula><p>Proof. Since E(x, u) is a function of x, u, by the data processing inequality, we have H(y|E(x, u)) ≥ H(y|x, u).</p><formula xml:id="formula_22">Hence, C(E) = H(y|E(x, u)) − λ d C d (E) ≥ H(y|x, u) − λ d max E C d (E ).</formula><p>The equality holds if and only if H(y|x, u) = H(y|E(x, u)) and C d (E) = max E C d (E ). Therefore, we only need to prove that the optimal value of C(E) is equal to H(y|x, u) − λ d max E C d (E ) in order to prove that any global encoder E * satisfies both Eqn. 6a and Eqn. 6b.</p><p>We show that C(E) can achieve H(y|x, u) − λ d max E C d (E ) by considering the following encoder E 0 : E 0 (x, u) = P y (·|x, u). It can be examined that H(y|E 0 (x, u)) = H(y|x, u) and E 0 (x, u) ⊥ ⊥ u which leads to C(E 0 ) = max E C(E ) using Corollary 4.1.</p><p>Theorem 4.3 shows that, at the equilibrium, the optimal encoder preserves all the information about label y contained in the data x and the domain index u while aligning the encoding cross domains.</p><p>Note that in general the encoder E is a probabilistic encoder that generates z stochastically. For example, one can use a probabilistic encoder parameterized by a natural-parameter network <ref type="bibr" target="#b21">(Wang et al., 2016)</ref> and generate z using the reparameterization trick <ref type="bibr" target="#b10">(Kingma &amp; Welling, 2013)</ref>. Empirically, we find that directly using a deterministic encoder also works favorably and therefore keep the encoder deterministic in Sec. 5 for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">u ⊥ ⊥ y</head><p>The domain index u is dependent of the label y when it contains information relevant to predicting y. In this case, discretization of the inherently continuous domain index u is necessary to perform categorical domain adaptation. However, this discretization inevitably loses information in u and could hurt the predictive task since u ⊥ ⊥ y. In contrast, our methods CIDA/PCIDA performs domain adaption with the continuous domain index u, thus, can fully retain information in u that is relevant to the label y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate CIDA and its variants on two toy datasets, one image dataset (Rotating MNIST), and three real-world medical datasets. These empirical studies verify our theoretical findings in Sec. 3 and show that:</p><p>• Using categorical domain adaption to align continuously indexed domains leads to poor alignment with marginal (or no) performance gain compared to no adaptation. • CIDA aligns domains with continuous indices and achieves significant performance boost compared to categorical domain adaption methods. • PCIDA's ability to predict a distribution rather than a single value is helpful in avoiding bad equilibriums and improving prediction performance. • The performance gains of CIDA and PCIDA increase with multi-dimensional domain indices.  <ref type="figure" target="#fig_6">Figure 1</ref>. Results on the Circle dataset with 30 domains. <ref type="figure" target="#fig_6">Fig. 1(a)</ref> shows domain index by color. The first 6 domains are source domains, marked by green boxes. Red dots and blue crosses are positive and negative data samples. Black lines show the decision boundaries generated according to model predictions.  <ref type="figure" target="#fig_0">Figure 2</ref>. Results on the Sine dataset with 12 domains. The first 5 domains are source domains marked by green boxes. Red dots and blue crosses are positive and negative data samples. Black lines show the decision boundaries generated according to model predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baselines and Implementations</head><p>We compare variants of CIDA with state-of-the-art domain adaptation methods including Domain Adversarial Neural Network (DANN) <ref type="bibr" target="#b6">(Ganin et al., 2016)</ref>, Conditional Domain Adversarial Neural Network (CDANN) <ref type="bibr" target="#b27">(Zhao et al., 2017)</ref>, Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b20">(Tzeng et al., 2017)</ref>, Margin Disparity Discrepancy (MDD) , and Continuous Unsupervised Adaptation (CUA) <ref type="bibr" target="#b4">(Bobu et al., 2018)</ref>. ADDA and MDD merge data with different domain indices into one source and one target domains; DANN, CDANN, and CUA divide the continuous domain spectrum into several separate domains and perform adaptation between multiple source and target domains. CUA adapts from the source domains to each target domain one-by-one from the closest target to the farthest one. For a fair comparison with CIDA, all baselines use both x and the domain index u as inputs to the encoder.</p><p>All methods are implemented using PyTorch <ref type="bibr" target="#b14">(Paszke et al., 2019)</ref> with the same neural network architecture. λ d is chosen from {0.2, 0.5, 1.0, 2.0, 5.0} and kept the same for all tasks associated with the same dataset (see the Supplement for more details about training).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Toy Datasets</head><p>To gain insight into the differences between CIDA and the baselines, we start with two toy datasets: Circle and Sine.</p><p>Circle Dataset includes 30 domains indexed from 1 to 30.  <ref type="figure" target="#fig_6">Fig. 1(a)</ref> shows the 30 domains in different colors. We also use arrows to indicates domain 1 and domain 15. Each domain contains data on a circle. The task is binary classification. <ref type="figure" target="#fig_6">Fig. 1(b)</ref> shows positive samples as red dots and negative samples as blue crosses. As shown in the figure, the ground-truth decision boundary continuously evolves with the domain index. We use domains 1 to 6 as source domains and the rest as target domains. <ref type="figure" target="#fig_6">Fig. 1</ref> compares the results of CIDA with the baselines. The figure shows that overall categorical DA methods perform poorly when asked to align domains with continuous indices. CUA is the best performing baseline since it incrementally adapts 24 pairs of domains. Still, CUA's performance is inferior to CIDA which produces a more accurate decision boundary.</p><p>Sine Dataset includes 12 domains as shown in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>. Each domain covers 1 6 the period of the sinosoid. We consider the first 5 domains as source domains and the rest as target domains. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the results. The figure shows that it is very challenging for the baselines to capture the ground-truth decision boundary. The baselines either produce incorrect decision boundaries (ADDA and MDD) or only capture the correct trend with very rugged boundaries (DANN, CDANN and CUA). In contrast, CIDA can successfully recover the ground-truth boundary. We also note that while CUA performed better than the other baselines on the Circle dataset, it performed worse than DANN and CDANN on the Sine dataset. This is because CUA performs incremental pairwise adaptation; it fails on the pair <ref type="bibr">(5,</ref><ref type="bibr">9)</ref>, and this failure propagates to the following domains.</p><p>Overall, both the results from the Circle and Sine datasets demonstrate that CIDA captures the underlying relationship between the domain index and the classification task and leverages it to improve performance. In contrast, the baselines cannot accurately capture this relationship, and hence yield worse results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Rotating MNIST</head><p>We further evaluate our methods on the Rotating MNIST dataset. The goal is to adapt from regular MNIST digits with mild rotation to significantly Rotating MNIST digits. We designate images that are Rotating by 0 • to 45 • as the labeled source domain, and assign images Rotating by 45 • to 360 • to the target domains. Naturally, the domain index is the rotation angle of the image. Since the target domain has a much larger range of rotation angles, we split the target domain into seven target domains for categorical domain adaptation baselines, DANN and CUA. These seven target domains contain images Rotating by <ref type="bibr">[45,</ref><ref type="bibr">90)</ref>, <ref type="bibr">[90,</ref><ref type="bibr">135)</ref>, · · · , [315, 360) degrees, respectively. <ref type="table" target="#tab_1">Table 1</ref> compares the accuracy our proposed CIDA/PCIDA with different baselines. We can see ADDA and DANN hardly improve and sometimes even decrease the accuracy compared to not performing adaptation at all. This is because without capturing the underlying structure, adversarial encoding alignment may harm the transferability of the data. CUA's performs fairly well in target domains near source domains but poorly in distant domains. 3 On the other hand, CIDA and PCIDA can learn such domain structure and successfully adapt the knowledge from source domains to target domains (see the Supplement for more details such as model architectures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Healthcare Datasets</head><p>Dataset Description. We use three medical datasets, Sleep Heart Health Study (SHHS) <ref type="bibr" target="#b15">(Quan et al., 1997)</ref>, Multi-Ethnic Study of Atherosclerosis (MESA)  and Study of Osteoporotic Fractures (SOF) <ref type="bibr" target="#b5">(Cummings et al., 1990)</ref>. Each dataset contains full-night breathing signals of subjects and the corresponding sleep stage labels ('Awake', 'Light Sleep', 'Deep Sleep', and 'Rapid Eye Movement (REM)'). Breathing signals are split into 30-second segments with one label for each segment. We consider the task of sleep stage prediction, i.e., to predict the sleep stage label y given a breathing segment x. This is a natural task in sleep studies and can be performed in the patient home by having them wearing a breathing belt. The breathing signal can then serve to predict sleep stages and also detect apnea (temporary cessation of breathing).</p><p>The datasets also contain subjects' information such as age, which is a natural domain index u. SHHS, MESA, and SOF include 2,651, 2,055, and 453 subjects, respectively. On av- Intra-Dataset Adaptation. We first evaluate our methods' performance on adaptation across continuously indexed domains within the same dataset using 'age' as the domain index. We cover two cases:</p><p>• Domain Extrapolation. For example, the source domain has data with a domain index (age) from the range <ref type="bibr">[44,</ref><ref type="bibr">52]</ref>, while the target domain contains data with a domain index range of (52,90]. • Domain Interpolation. For example, in the source domain, the domain index range is <ref type="bibr">[44,52]∪(75,90]</ref>, while in the target domain, the domain index range is <ref type="bibr">(52,</ref><ref type="bibr">75]</ref>.</p><p>The first three rows of <ref type="table" target="#tab_2">Table 2</ref> show the results for domain extrapolation. One observation is that directly using categorical domain adaptation only achieves minimal performance boost compared to models trained only on the source domains (Source-Only). Some methods such as DANN and CUA achieve no or even negative performance improvement. On the other hand, CIDA variants can successfully transfer across subjects with different ages and significantly improve upon all baselines. Similarly, the last three rows in <ref type="table" target="#tab_2">Table 2</ref> show the results for domain interpolation. Note that Source-Only can already achieve satisfactory accuracy in domain interpolation, since the model naturally learns the average of data from both sides (e.g., <ref type="bibr">[44,52]∪(75,90]</ref>) and performs prediction for the data in the middle (e.g., <ref type="bibr">(52,</ref><ref type="bibr">75]</ref>). For example, in the task 'SHHS@Outside → SHHS@(52,75]', Source-Only already has a high accuracy of 82.4%, leaving little room for improvement. Interestingly, PCIDA can still further improve the accuracy by a tangible margin. This also shows PCIDA's ability to avoid bad equilibriums by using a discriminator that predicts distributions rather than values.</p><p>Cross-Dataset Adaptation. Most clinical trials collect data from a population with a specific medical condition, and exclude people who have other conditions. However in practice many patients have multiple medical conditions and hence doctors need to apply the results of a particular study outside the population for which the data is collected. Thus, in this section we consider cross-dataset adaption. Specifically, we evaluate how different methods perform when transferring among the datasets SHHS, MESA, and SOF. <ref type="table">Table 3</ref> shows the accuracy of all methods in these cross-dataset settings. We observe that categorical domain adaptation barely improves upon models trained with only source domains, while CIDA and PCIDA can naturally transfer across continuously indexed domains even in the crossdataset setting with significant performance improvement. Interestingly, when the task is hard such as transferring from SOF, a very old people dataset, to datasets with much more diverse age range, PCIDA becomes a clear winner. But when the task is relatively easier, such as transferring from datasets with diverse age range to a very old dataset, CIDA is marginally better than PCIDA; but, this latter difference in performance is minor, and PCIDA performs well across all scenarios.</p><p>We also note that SHHS and MESA are both diverse datasets with similar age distribution, which is why the Source-Only model already achieves high accuracy. Interestingly, even in such cases PCIDA can still achieve stable performance gain compared to all baselines.</p><p>Multi-Dimensional Indices. As mentioned in Sec. 3, both CIDA and PCIDA naturally generalize to multi-dimensional domain indices. To demonstrate this feature, we leverage that the SHHS dataset includes multiple variables per patient in addition to their age, such as their heart rate, their physical and emotional health scores, etc. We combine such variables with the person's age to create a multi-dimensional domain index. (see more details on different domain indices in the Supplement). <ref type="table" target="#tab_3">Table 4</ref> shows the accuracy for multi-dimensional CIDA/PCIDA. For reference, we report corresponding accuracy for Source-Only. Note that Source-Only takes both the breathing signals (x) and the domain index (u) as input, as is done in all previous experiments. As expected we can observe substantial improvement in accuracy with multi-dimensional domain indices.</p><p>Note that in the case of multi-dimensional indices, domain extrapolation means that the target domain indices are outside the convex hull of the source domain indices. Similarly, domain interpolation means that the target domain indices are inside the convex hull of the source domain indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We identify the problem of adaptation across continuously indexed domains, propose a series of methods for addressing it, and provide supporting theoretical analysis and empirical results using both synthetic and real-world medical data. Our work demonstrates the viability of efficient adaptation across continuously indexed domains and its potential impact on important real-world applications. Future work could investigate the possibility of matching higher or even infinite order moments, and the application of the proposed methods to other datasets in robotics or the medical field. </p><formula xml:id="formula_23">V u∼p(u) [u] = E u∼p(u) [(u − E[u]) 2 ] = E z∼p(z) E u∼p(u|z) [(u − E[u|z]) 2 ] = E z∼p(z) V u∼p(u|z) [u] = E z∼p(z) σ 2 = σ 2 ,</formula><p>concluding the proof. Proof. The optimal D:</p><formula xml:id="formula_24">D * E (x, u) = argmin D E (z,u)∼p(z,u) [L d (D(z), u)],</formula><p>where the objective function expands to</p><formula xml:id="formula_25">E (z,u)∼p(z,u) [L d ((D µ (z), D σ 2 (z)), u)] =E (z,u)∼p(z,u) [ (D µ (z) − u) 2 2D σ 2 (z) + 1 2 log D σ 2 (z)] =E z∼p(z) E u∼p(u|z) [ (D µ (z) − u) 2 2D σ 2 (z) + 1 2 log D σ 2 (z)].</formula><p>Notice that</p><formula xml:id="formula_26">E u∼p(u|z) [ (Dµ(z) − u) 2 2D σ 2 (z) + 1 2 log D σ 2 (z)] = E[u 2 |z] 2D σ 2 (z) − Dµ(z) D σ 2 (z) E[u|z] + Dµ(z) 2 2D σ 2 (z) + 1 2 log D σ 2 (z).</formula><p>Taking the derivative w.r.t. D(z) and setting it to 0, we get the optimal D * µ,E (z) = E[u|z] and D * σ 2 ,E (z) = V[u|z], completing the proof.   angle randomly sampled the corresponding domain index interval. Therefore, this dataset contains images with rotation angles evenly spread in the range [0, 360 • ). We note that this is different from the Rotating MNIST dataset in <ref type="bibr">[?]</ref>, where the images are Rotating by 8 fixed angles. Another difference is that in our Rotating MNIST, the amount of data in target domains is 7 times as many as that in source domain while in <ref type="bibr">[?]</ref>, the target domain has the same amount of data as the source domain. Implementations. We use the same neural network architecture in all methods for fair comparison. Mainly, we use a four-layer convolutional neural networks to encode the image and a three-layer MLP to make the prediction, while the discriminator is a four-layer MLP. In addition, we make two augmentations to provide the model with a stronger inductive bias. First, we add a Spacial Transfer Network <ref type="figure">(STN)</ref> [?] to the image encoder. Basically, the STN will take the image and the domain index as input and output a set of rotation parameters which are then applied to rotate the given image. Second, we add the dropout layers to the STN and the ConvNet backbone. As mentioned in <ref type="bibr">[?]</ref>, dropout can be viewed as a way of performing Bayesian inference. Here, we use this dropout switch to make image encoder either deterministic or probabilistic. For more details, please refer to our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experiments on the Toy Datasets</head><p>Visualization of the decision boundary (approximately). Unlike shallow models such as logistic regression, plotting deep neural networks' exact decision boundaries is not straightforward. To generate a virtual decision boundary for visualization, we fit an SVM with the RBF kernel by neural networks' prediction and draw the decision boundary of the SVM. To be fair, when fitting the SVM, we ensure that the fitting accuracy is the same for all deep learning models. Note that since the generated boundaries are not exact, we can observe some data points on the wrong side of the boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Categorical Domains versus Continuously Indexed Domains</head><p>Continuous Indices. As mentioned in the main paper, the hypothesis of 'continuous indices' is that input x and labels y are drawn from p(x, y|u) given a specific domain index u ∈ U, and that p(x, y|u) (and p(y|x, u)) is continuous w.r.t. u. Therefore, CIDA tries to produce correct predictions in a continuous range of target domains by effectively capturing the underlying relation (functional) between p(y|x, u) and u. Distance Metrics. Such a hypothesis comes with a distance metric for domain indices, which are captured by the regression loss (e.g., euclidean distance for L 2 loss) in the discriminator. This is a key difference between CIDA and categorical domain adaptation, where any pair of domains effectively has the same distance. This is also true for categorical domain adaptation methods such as <ref type="bibr">[?]</ref>. Note that [?] uses a least-square loss as a surrogate for cross-entropy to perform domain classification in the discriminator, therefore still treating different domains as equal. This is substantially different from CIDA where the L 2 loss and the Gaussian (or Gaussian Mixture Model) loss are use to regress the domain indices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Lemma 4. 2 (</head><label>2</label><figDesc>Uniqueness of Constant Expectation). If there exists a constant µ c such that E u∼p(u|z) [u] = µ c for any z, we have µ c = E u∼p(u) [u].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Theorem 4.1 (Global Optimum for CIDA). The global maximum of C d (E) is achieved if and only if the encoder E satisfies that the expectations of the domain index u over the conditional distribution p(u|z) for any given z are identical to the expectation over the marginal distribution p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Theorem 4.2 (Global Optimum for PCIDA). In PCIDA (with the Gaussian model), the global optimum is achieved if and only if the mean and variance of the distribution p(u|z) given any z are identical to those of the marginal distribution p(u).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Lemma 1.3 (Optimal Discriminator for PCIDA). With E fixed, the optimal D is D * µ,E (z) = E u∼p(u|z) [u], D * σ 2 ,E (z) = V u∼p(u|z) [u], where z = E(x, u).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :</head><label>1</label><figDesc>Age histograms for three medical datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the equality holds when V[u|z] is constant w.r.t. z. Further, in the proof of Theorem 4.1, we show that E z [V[u|z]] ≤ V[u] and the maximum is achieved when E[u|z] is constant w.r.t. z. Together with Lemma 4.4, we then have that C d (E) reaches the global optimal 0.5 + 0.5 log(V[u]) if and only Remark 4.1 (Matching Higher-Order Moments). By Theorem 4.1 and Theorem 4.2, we show that CIDA using the L 2 loss matches the mean of p(u|z) while the PCIDA with the Gaussian model matches both the mean and variance of p(u|z)</figDesc><table /><note>if E[u|z] = E[u] and V[u|z] = V[u] for all z. Corollary 4.1. For both CIDA and PCIDA, the global op- timum of C d (E) is achieved if the encoding of all domains (continuously indexed by u) are totally aligned, i.e., z ⊥ ⊥ u.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Rotating MNIST accuracy (%) for various adaptation methods. We report the accuracy at the source domain and each target domain. X • denotes the domain whose images are Rotating by X • to X + 45 • . The last column shows the average accuracy across target domains. We use bold face to mark the best results.Method# Target Domains 0 • (Source) 45 • 90 • 135 • 180 • 225 • 270 • 315 • Average</figDesc><table><row><cell>Source-Only ADDA DANN CUA CIDA (Ours) PCIDA (Ours)</cell><cell>-1 1 7 ∞ ∞</cell><cell>98.4 95.0 98.1 91.4 99.1 98.6</cell><cell>81.4 29.8 33.6 70.2 25.5 44.0 80.1 44.8 42.2 73.9 60.1 55.0 87.2 56.7 79.6 90.1 82.2 90.5</cell><cell>41.4 59.2 43.6 52.7 91.2 91.9</cell><cell>39.0 46.2 46.8 45.1 91.5 87.1</cell><cell>30.4 23.7 57.3 55.2 96.2 80.0</cell><cell>81.1 61.4 79.3 88.4 97.5 88.2</cell><cell>48.1 47.2 56.3 61.5 85.7 87.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Accuracy (%) for intra-dataset adaptation. 'SHHS@Outside → SHHS@(52,75]' means transferring from age range outside (52,75] (i.e.,[44,52]∪(75,90]) to(52,75]  within SHHS. 'SO' is short for 'Source-Only'. We use bold face mark the best results.Table 3. Accuracy (%) for cross-dataset adaptation. We use bold face to mark the best results. SHHS is [44, 90], while the age ranges for MESA and SOF are [54, 95] and [72, 90], respectively. Apparently SOF subjects are much older. SHHS subjects and MESA subjects have similar age ranges but the distributions are actually different (see the histogram plot in the Supplement).</figDesc><table><row><cell>Domain Extrapolation Domain Interpolation</cell><cell cols="2">Task SHHS@[44,52] → SHHS@(52,90] MESA@[54,58] → MESA@(58,95] SOF@[75,82] → SOF@(82,90] SHHS@Outside → SHHS@(52,75] MESA@Outside → MESA@(58,75] 83.5 SO 77.4 80.1 74.7 82.4 SOF@Outside → SOF@(79,86] 71.8</cell><cell>ADDA DANN CDANN MDD CUA CIDA PCIDA 78.0 77.1 77.5 77.7 77.4 79.8 80.6 80.7 79.9 80.4 80.3 80.1 82.7 82.5 74.8 74.2 74.4 74.6 74.5 76.7 76.7 81.7 82.5 82.3 82.5 82.4 82.2 83.7 83.5 83.2 83.3 83.8 83.4 83.5 84.7 71.5 71.4 70.9 71.8 71.5 71.8 73.6</cell></row><row><cell></cell><cell>Task SOF → SHHS SOF → MESA SHHS → MESA MESA → SHHS SHHS → SOF MESA → SOF</cell><cell cols="2">Source-Only ADDA DANN CDANN MDD CUA CIDA PCIDA 75.6 76.0 75.2 75.6 75.8 75.3 75.9 80.1 74.0 75.1 74.6 75.2 74.9 73.6 74.8 80.0 82.8 83.0 82.6 82.1 83.0 82.1 83.2 85.3 80.7 81.8 80.9 80.9 81.2 81.0 80.8 83.4 78.7 79.5 79.0 79.2 79.7 79.1 81.1 80.9 75.9 76.6 77.0 76.9 76.9 76.0 79.3 79.0</cell></row><row><cell cols="3">erage, there are 1,000 segments (i.e., 8.33 hours of breathing signals) for each subject. Different datasets have different domain index distributions. For example, subjects' age range in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Accuracy</figDesc><table><row><cell>(%) for the multi-dimensional domain in-dex setting. The task is SHHS@[44,52] → SHHS@(52,90]. # Dimensions Source-Only CIDA PCIDA 1 77.4 79.8 80.6 2 77.6 81.0 81.1 4 77.7 81.2 81.3 11 77.7 82.6 82.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>(Uniqueness of Constant Expectation). z and u are random variables. If E u∼p(u|z) [u] is constant w.r.t z , then E u∼p(u|z) [u] = E u∼p(u) [u], ∀z.</figDesc><table><row><cell>1 Proof</cell></row><row><cell>Lemma 1.1</cell></row></table><note>Proof. Let E u∼p(u|z) [u] = µ, ∀ z. We then have E u∼p(u) [u] = E z∼p(z) E u∼p(u|z) [u] = E z∼p(z) µ = µ. Lemma 1.2 (Uniqueness of Constant Expectation and Variance). z and u are random variables. If E u∼p(u|z) [u] and V u∼p(u|z) [u] are constants w.r.t z , then E u∼p(u|z) [u] = E u∼p(u) [u] and V u∼p(u|z) [u] = V u∼p(u) [u] for any z. Proof. Let E u∼p(u|z) [u] = µ and V u∼p(u|z) [u] = σ 2 for any z. By the previous lemma, we have E u∼p(u) [u] = µ. For the variance, we have:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>11 domain indices in the SHHS dataset. u 1 Age u 2 Resting heart rate u 3 Gender u 4 Physical functioning u 5 Role limitation due to physical health u 6 General health u 7 Role limitation due to emotional problems u 8 Energy/fatigue u 9 Emotional well being u 10 Social functioning u 11 Pain Level</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Network structure for the encoder.</figDesc><table><row><cell>Kernel</cell><cell>Stride</cell><cell cols="2">Channel In Channel Middle Channel Out</cell><cell>Type</cell><cell>Number</cell></row><row><cell>13</cell><cell>2</cell><cell>1</cell><cell>-</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In general the encoder E(x, u) can be probabilistic. For example, z can be generated from a Gaussian distribution whose mean and variance are given by E(x, u).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We note that CUA's performance on our Rotating MNIST data is worse than in the original papers, possibly because our Rotating MNIST has images rotated by all angles as opposed to only 8 fixed angles. Also we are using different model architectures. Please refer to the Supplement for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sleepdata.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Guang-He Lee and Mingmin Zhao for the insightful and tremendously helpful discussions. We are also grateful to Xingjian Shi, Xiaomeng Li, Hongzi Mao as well as other members at NETMIT and CSAIL for their comments to improve this paper. We would also like to thank Daniel R. Mobley and NSRR for their help with the datasets.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments</head><p>In this section we provide more details for our experiments. The code is available at https://github.com/ hehaodele/CIDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experiment on the Healthcare Datasets</head><p>Dataset details. The three real-world medical datasets [?, ?, ?] with detailed information are publicly available 1 . They can all be freely accessed upon request and submission of relevant IRB documents. In <ref type="figure">Fig. 1</ref> we plot the histograms subjects' age in the three medical datasets. All the three datasets contains many health retaled variables of the subjects.</p><p>In <ref type="table">Table 1</ref>, we list all the variables we considered as the domain indices. Implementations. We use the same neural network architecture in all methods for fair comparison. <ref type="table">Table 2</ref> shows the neural network architecture for the encoders taking breathing signals x as input. 'Number' in the tables indicates the number of corresponding blocks stacked in the network. The predictor includes 3 fully connected layers, each with batch normalization and ReLU. Similarly, the discriminator includes 5 fully connected layers. For the baseline models, we explore different λ d (the hyperparameter for the discriminator term) in the range {0.2, 0.5, 1.0, 2.0, 5.0} and find that λ d = 2.0 produce stable and the best results in the toy datasets. We follow recommendations from the original papers for other hyperparameters. We set λ d = 2.0 for all methods including CIDA/PCIDA. We train all models using the Adam optimizer [?] with a learning rate of 10 −4 . We run all experiments on a server with four NVIDIA Titan Xp GPUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experiment on the</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The skew-normal and related families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Azzalini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental evolving domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bitarafan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Baghshah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gheisari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2128" to="2141" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adapting to continuously shifting domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Appendicular bone density and age predict hip fracture in women</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Nevitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Browner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Cauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Genant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mascioli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Seeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Steiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">263</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="665" to="668" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Continuous manifold based adaptation for evolving visual domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="867" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">O</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gheche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Maretic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cdot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11448</idno>
		<title level="m">Continuous domain adaptation using optimal transport</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation based on source-guided discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kuroki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Charoenphakdee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4122" to="4129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1647" to="1657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNN</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The sleep heart health study: design, rationale, and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Iber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Kiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Rapoport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Redline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Samet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sleep</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1077" to="1085" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8503" to="8512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep CORAL: correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Natural-parameter networks: A class of probabilistic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incremental adversarial domain adaptation for continually changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The national sleep research resource: towards a sleep data commons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rueschman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mobley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Redline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1351" to="1358" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bridging theory and algorithm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05801</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8568" to="8579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On learning invariant representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Des Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7523" to="7532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning sleep stages from radio signals: A conditional adversarial architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4100" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Matching p(u|z) versus Matching p(z|u)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matching the mean and variance of p(u|z) requires matching the mean and variance of 4 100 univariate distributions, i.e., 2 × 4 100 parameters in total. On the other hand, Matching the mean and variance of p(z|u) only requires matching the mean and variance of 4 100-dimensional distributions</title>
	</analytic>
	<monogr>
		<title level="m">general, matching the entire p(u|z) for any z is equivalent to matching the entire p(z|u) for any u. This is because p(u|z) = p(u) ⇐⇒ p(z|u) = p(z) ⇐⇒ u ⊥ ⊥ z</title>
		<imprint/>
	</monogr>
	<note>However, matching the mean and variance of p(u|z) for any z is different from matching the mean and variance of p(z|u) for any u. Considering the dimension of z is much higher than that of u, the former is actually stronger alignment. i.e., 400 parameters in total. Therefore the former implies stronger alignment</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
