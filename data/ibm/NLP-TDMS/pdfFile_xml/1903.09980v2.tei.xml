<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cluster Alignment with a Teacher for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep2">Institute for AI</orgName>
								<orgName type="laboratory">BNRist Lab, THBI Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucen</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep2">Institute for AI</orgName>
								<orgName type="laboratory">BNRist Lab, THBI Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep2">Institute for AI</orgName>
								<orgName type="laboratory">BNRist Lab, THBI Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cluster Alignment with a Teacher for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning methods have shown promise in unsupervised domain adaptation, which aims to leverage a labeled source domain to learn a classifier for the unlabeled target domain with a different distribution. However, such methods typically learn a domain-invariant representation space to match the marginal distributions of the source and target domains, while ignoring their fine-level structures. In this paper, we propose Cluster Alignment with a Teacher (CAT) for unsupervised domain adaptation, which can effectively incorporate the discriminative clustering structures in both domains for better adaptation. Technically, CAT leverages an implicit ensembling teacher model to reliably discover the class-conditional structure in the feature space for the unlabeled target domain. Then CAT forces the features of both the source and the target domains to form discriminative class-conditional clusters and aligns the corresponding clusters across domains. Empirical results demonstrate that CAT achieves state-of-the-art results in several unsupervised domain adaptation scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has achieved remarkable performance in a wide variety of computer vision tasks, such as image recognition <ref type="bibr" target="#b16">[15]</ref> and object detection <ref type="bibr" target="#b34">[33]</ref>. However, classifiers trained on specific datasets cannot always generalize effectively to new datasets owing to the well-known domain shift problem <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b45">43]</ref>. Enabling models to generalize from a source domain to a target domain is usually referred to as domain adaptation (DA) <ref type="bibr">[1]</ref>. In many cases, it is expensive or difficult to collect annotations on the target domain. Learning algorithms attempting to tackle the transferring problem from a fully labeled source domain to an unlabeled target domain is called unsupervised domain adaptation (UDA) <ref type="bibr" target="#b11">[10]</ref>. UDA is particularly challenging because the target domain cannot provide explicit information to facilitate the adaptation of classifiers. * Corresponding author. Existing methods aligning the marginal distributions while ignoring the class-conditional structures cannot perform well in these cases. However, CAT incorporates the discriminative clustering structures in both domains for better adaptation, thus delivers a more reasonable domain-invariant cluster-structure feature space with enhanced discriminative power. See <ref type="figure" target="#fig_2">Fig. 3</ref> and Appendix. A for the learned feature space of real data.</p><p>Recently, deep models have been developed with promise in unsupervised domain adaptation to learn expressive features <ref type="bibr" target="#b47">[45,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b38">36]</ref>. These deep UDA methods mainly focus on matching the source and target domains via adversarial training <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b14">13]</ref> or kernelized training <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b25">24]</ref>. The main hypothesis behind them is that the marginal distributions of the two domains can be aligned in some feature space learned by optimizing a deep network, and thus the classifier trained with source data tends to perform well on the target domain. Theoretical analysis [1] also shows that minimizing the divergence between the marginal distributions in the learned feature space is beneficial to reduce the classifier's error.</p><p>However, these methods are not problemless. In classification, as the classes correspond to different semantics and different characteristics, the marginal distribution of the data naturally has a class-conditional multi-modal structure. Moreover, the modes corresponding to the same class in different domains are not always geometrically similar. Thus, it is not sufficient for existing deep UDA methods to only minimize the discrepancy between the marginal distributions while neglecting their structures, and such methods tend to fail in challenging cases, such as those in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Properly incorporating this fine-grained class-conditional structure has been shown beneficial in various tasks. For example, Shi and Sha <ref type="bibr" target="#b42">[40]</ref> make the discriminative clustering assumption which helps to adapt the decision boundaries for the source domain to the target domain discriminatively. 1 However, one limitation of <ref type="bibr" target="#b42">[40]</ref> is that it adopts a simple linear transformation to learn the feature space, which cannot effectively extract high-order features from raw data (e.g., images) as the deep UDA methods. Another limitation is that <ref type="bibr" target="#b42">[40]</ref> builds a nearest neighbor based prediction model, which outputs the prediction of one sample based on all the source data. Then, the training is not compatible with the stochastic training of deep network and has a high complexity.</p><p>In this paper, we present Cluster Alignment with a Teacher (CAT), a new deep UDA model that incorporates the class-conditional structures for more effective adaptation. CAT conjoins the complimentary advantages of deep learning methods and discriminative clustering methods for UDA. Technically, there are three learning objectives in CAT. At first, CAT minimizes the supervised classification loss on the labeled source data and builds a teacher classifier, i.e. an implicit ensemble of the source classifier, to provide pseudo labels for unlabeled target data. The underlying notion is that the golden classifier trained on source domain can perform well on a majority of target samples because of the similarity between the two domains and the teacher-student paradigm is not sensitive to the false pseudo labels <ref type="bibr" target="#b17">[16]</ref>. To exploit the fine-grained class-conditional structures in the feature space and address the aforementioned issues suffered by existing deep UDA methods, CAT also includes two objectives which depend on the pseudo labels provided by the teacher classifier. On one hand, for discriminative learning in both domains, CAT deploys a class-conditional clustering loss to force the features from the same class to concentrate together and the features from different classes to be separated. On the other hand, for the class-conditional alignment between the two domains, CAT aligns the clusters which correspond to the same class but come from different domains via a conditional feature matching loss. The prediction models used in CAT are a student deep network and its implicit ensemble (i.e., the teacher classifier), thus CAT can address the training issues of <ref type="bibr" target="#b42">[40]</ref> and also enjoy the more flexible ability of feature learning. Furthermore, it is obvious that CAT is compatible to the marginal distribution alignment methods on the tasks where the source data is similarly distributed as the target data. The former can provide a fine-grained classconditional alignment of domains and the latter can provide a global alignment of them.</p><p>We evaluate the proposed CAT through extensive experiments on both synthetic and real-world datasets. Empirical results show that CAT presents striking performance across various tasks. In addition, we further combine CAT with the existing deep UDA methods, and CAT can bias them successfully to achieve the discriminative alignment between domains, establishing new state-of-the-art baselines on popular benchmarks. In the combined methods, we also propose a confidence-thresholding technique to filter out lowconfidence target samples (which are likely to be mapped into incorrect clusters by the marginal distribution alignment methods) to enhance the stability of the training.</p><p>To summarize, our contributions are three-folds:</p><p>• We exploit the discriminative class-conditional structures of distributions in deep UDA and propose CAT to achieve better alignment between the source domain and the target domain.</p><p>• CAT is compatible and applicable to the existing UDA methods which rely on marginal distribution alignment.</p><p>• Empirically, CAT is not sensitive to hyper-parameters and can boost the marginal distribution alignment approaches significantly, achieving new state-of-the-art across various settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Unsupervised domain adaptation has drawn increasing interests, and has been developed mainly in two directions: Maximum Mean Discrepancy (MMD) based approaches and adversarial training based approaches. Tzeng et al. <ref type="bibr" target="#b47">[45]</ref> and Long et al. <ref type="bibr" target="#b23">[22]</ref> minimize MMD to match the two domains while <ref type="bibr" target="#b24">[23]</ref> proposes to align the joint distributions of them using Joint MMD criterion. Since the development of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b4">4]</ref>, adversarial training has been applied into domain adaptation and fruitful works emerge. Ganin et al. <ref type="bibr" target="#b7">[7]</ref> develop the framework of domain adversarial training and plenty of works are proposed to improve it by aligning source domain and target domain better in the feature space <ref type="bibr" target="#b46">[44,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b15">14]</ref> or image space <ref type="bibr" target="#b21">[20]</ref>. Zhang et al. <ref type="bibr" target="#b52">[50]</ref> successfully extend RevGrad <ref type="bibr" target="#b7">[7]</ref> to consider each domain's characteristics using collaborative games. Image to image translation approaches <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b41">39]</ref> also play an important role in the advancement of domain adaptation and demonstrate impressive performance, especially on semantic segmentation tasks. In addition, Saito et al. <ref type="bibr" target="#b39">[37]</ref> propose to align the two domains using decision boundaries of task-specific classifier. Associative DA <ref type="bibr" target="#b12">[11]</ref> proposes an associative loss to reduce discrepancy between domains and SimNet <ref type="bibr" target="#b32">[31]</ref> proposes to use a similarity-based classifier in UDA. Though the existing methods match the two domains in different ways, most of them ignore the class-conditional information in the alignment procedure, thus hard to attain the objective of discriminative learning. Conversely, CAT explicitly discovers classes in the feature space via a teacher model and hence constructs a more reasonable matching procedure. Concurrently, several works <ref type="bibr" target="#b53">[51,</ref><ref type="bibr" target="#b50">48]</ref> analyze the conditional distribution shift and label distribution shift issues in UDA theoretically, but only propose limited solutions. In contrast, we provide a more practical and more powerful way to solve them. Using a teacher model for labeling data is inspired by the impressive consistency-based methods in semi-supervised learning (SSL) <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b44">42]</ref>. Recent attempts to apply SSL techniques in UDA include <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b48">46]</ref>. CAT differs from these previous works in that CAT exploits the discriminative class-conditional structures in both the alignment and classification procedures while they focus on improving the classifier for the target domain by implementing the cluster assumption <ref type="bibr" target="#b3">[3]</ref>. CAT imposes a much stronger regularization and assists in a better alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first introduce the setting and framework of deep UDA and then present the Cluster Alignment with a Teacher (CAT). Finally, we discuss about CAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep unsupervised domain adaptation</head><p>In an UDA task, we are given a set of source sam-</p><formula xml:id="formula_0">ples X s = {x i s } N i=1 with labels Y s = {y i s } N i=1 , y i s ∈ {1, 2, ..., K} and a set of unlabeled target samples X t = {x i t } M i=1 .</formula><p>Notably, the two sets of samples are drawn from different distributions which lead to the domain shift challenge. Therefore, the UDA algorithms should learn to adapt the classifier trained on the source domain to the unlabeled target domain. Deep learning techniques have been introduced into UDA <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b25">24]</ref> and they demonstrate remarkable performance across tasks. Generally, in these methods, the classifier h (parameterized by θ) is constructed as h = g • f where f maps samples into features in the space F and g outputs the predictions based on the extracted features. The learning includes simultaneously optimizing the classifier h w.r.t. the labeled source data and minimizing the distance between the marginal distributions of the two domains in the feature space F, resulting in a domain-invariant feature space. Technically, in the source domain, we minimize the supervised loss as:</p><formula xml:id="formula_1">min θ L y (X s , Y s ) = 1 N N i=1 (h(x i s ; θ), y i s ),<label>(1)</label></formula><p>where is a pre-defined loss, e.g., cross-entropy loss. Meanwhile, we minimize the discrepancy loss as:</p><formula xml:id="formula_2">min θ L d (X s , X t ) = D(f (X s , θ), f (X t , θ)),<label>(2)</label></formula><p>where D is a distance and usually correlated with the H∆H distance in the error bound theory of DA [1]. The theory reveals that the expected error on target samples of any classifier h drawn from a hypothesis set H has the following bound <ref type="bibr">[1,</ref><ref type="bibr" target="#b51">49]</ref>:</p><formula xml:id="formula_3">t (h) ≤ s (h) + 1 2 d H∆H (s, t) + min h∈H ( s (ĥ, l s ) + t (ĥ, l t )) ≤ s (h) + 1 2 d H∆H (s, t) + t (l s , l t ) + min h∈H ( s (ĥ, l s ) + t (ĥ, l s )),<label>(3)</label></formula><p>where s (h) denotes the expected error on source samples of h, and l s and l t represent the labelling functions [1] for the source and target domains, respectively. t (l s , l t ) denotes the disagreement between the labelling functions in the target domain. Notably, minĥ ∈H ( s (ĥ, l s ) + t (ĥ, l s )) can be small enough by optimizingĥ w.r.t. the labeled source data. The supervised loss and discrepancy loss focus on minimizing s (h) and d H∆H (s, t) respectively to obtain small target domain classification error. However, the methods working in the above framework are not problemless. Theoretically, they ignore minimizing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cluster Alignment with a Teacher</head><p>To overcome these issues, we expect to exploit the finelevel structures in the feature space for discriminative learning and match the class-conditional distributions of source and target domains to reduce of the mismatching between l s and l t . Therefore, in the deep UDA scenario, we propose Cluster Alignment with a Teacher (CAT), a new deep UDA model for more effective adaptation. Specifically, for the objectives of discriminative learning and class-conditional alignment between domains, we propose a discriminative clustering loss L c to force the features of both the source and the target domains to form discriminative clusters, and a cluster-based alignment loss L a to align the clusters corresponding to the same class in different domains. Given them, we propose to train CAT by solving the following optimization problem: where the hyper-parameter α sets a relative trade-off. The whole framework is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We build a teacher classifier, i.e. an implicit ensemble of the classifier to be optimized, to provide pseudo labels for the unlabeled target data. These pseudo labels will be used in L c and L a . We use stochastically sampled mini-batches in the two objectives and the two classifiers make predictions in a forwardpropagation way, thus CAT can be trained more efficiently than <ref type="bibr" target="#b42">[40]</ref>. Furthermore, L c and L a optimize the feature space directly and will be more effective than the nearest neighbor based clustering loss in <ref type="bibr" target="#b42">[40]</ref>. We elaborate L c and L a in the following sections.</p><formula xml:id="formula_4">min θ L y + α(L c + L a ),<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Discriminative clustering with a teacher</head><p>For better classification and alignment, we propose to discover the class-conditional structures in the feature space in both the source and the target domains, and then shape them to be discriminative clusters. In the source domain, the class-conditional structure is obvious because the data is fully labeled. Nevertheless, in the target domain, we cannot obtain the class-conditional structure easily due to the lack of labels. The semantic similarity between the two domains implies that the classifier h trained on the source domain can predict most of target samples correctly. Consequently, using pseudo labels <ref type="bibr" target="#b19">[18]</ref> as the annotations for target data and conducting self-training is a direct approach, but it suffers from the error amplification issue which can be detrimental to the learning procedure. To discover the class-conditional structure of the target features in a reliable way, we introduce a teacher classifierh defined as an implicit ensemble of the previous student classifier h <ref type="bibr" target="#b17">[16]</ref> to provide pseudo labels for target data.</p><p>Based on the pseudo labels given by the teacher, we can explicitly force the target class-conditional structure to be more discriminative using a clustering loss. For the source domain, a similar one can be applied. Formally, resembling the effective SNTG loss in <ref type="bibr" target="#b26">[25]</ref>, we employ the following discriminative clustering loss (we omit the dependence of f on θ for clarity, unless stated otherwise):</p><formula xml:id="formula_5">L c (X s , X t ) = L c (X s ) + L c (X t ),<label>(5)</label></formula><formula xml:id="formula_6">L c (X ) = 1 |X | 2 |X | i=1 |X | j=1 δ ij d f (x i ), f (x j ) + (1 − δ ij ) max 0, m − d f (x i ), f (x j ) ,<label>(6)</label></formula><p>where d is the distance (e.g., squared Euclidean distance) between two features, m is a pre-defined margin, and δ ij is an indicator function which outputs 1 only if x i and x j have the same ground truth label (source domain) or teacherannotated label (target domain). L c encourages the features from the same class to concentrate together and pushes the features from different classes far away from each other with a distance m at least. This loss modifies the structures in the representation space gradually, and consequently, it demonstrates a class-conditional cluster structure (as shown in Sec. <ref type="bibr">4.4)</ref>. Note that minimizing L c is consistent with the cluster assumption <ref type="bibr" target="#b26">[25]</ref> of classifier and benefits the performance of classification.</p><p>It's a common doubt whether the incorrect predictions of the teacher classifier would destroy the training dynamics. However, previous works on semi-supervised learning <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b44">42]</ref> have validated that this kind of training always leads to good convergence and demonstrates robustness against incorrect labels. Intuitively, the teacher instructs the training of one instance through a bundle of others' predictions which alleviates the negative influence of incorrect predictions notably and aids the student to give better predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cluster alignment via conditional feature matching</head><p>Once the feature space presents discriminative cluster structure, the classifier is expected to make more accurate predictions. However, the label predictor g trained on source domain features may fail due to the geometrical mismatching between the clusters which correspond to the same class in different domains. This kind of mismatching is brought by the individual characteristics of each domain. As a result, it is necessary to impose a class-conditional alignment of two domains to learn better domain-invariant features and adjust the target feature space to be more suitable for classification. Naturally, we expect to minimize the divergence between the corresponding clusters in source domain and the teacher-annotated target domain:</p><formula xml:id="formula_7">min θ D(F s,k ||F t,k ),<label>(7)</label></formula><p>where F s,k (F t,k ) denotes the set consisting of all the features belonging to class k of domain s (domain t). Extensive previous works <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b20">19]</ref> have been proposed to minimize the distance between two sets of samples but we expect to achieve this in a more simple and efficient way by exploiting the separable and tight clusters in the feature space. Drawing inspiration from feature matching GANs <ref type="bibr" target="#b40">[38]</ref> which optimizes the distance between the first-order statistics of distributions and demonstrates striking results on SSL tasks, we choose to extend it to work in a conditional way. Formally, we introduce the following cluster alignment loss:</p><formula xml:id="formula_8">L a (X s , X t ) = 1 K K k=1 λ s,k − λ t,k 2 2 ,<label>(8)</label></formula><p>where λ s,k and λ t,k are calculated by</p><formula xml:id="formula_9">λ s,k = 1 |X s,k | x i s ∈X s,k f (x i s ), λ t,k = 1 |X t,k | x i t ∈X t,k f (x i t )<label>(9)</label></formula><p>where X s,k is the subset of X s containing all the source samples whose ground-truth labels are k and X t,k is the subset of X t including all the target samples annotated as class k by the teacher classifierh. This loss is slightly different from the original feature matching loss: it matches the statistics of the representation space F which totally determine the predictions instead of those produced by an extra critic network. Arguably, the objective has a local optima where class-conditional distributions are matched thoroughly. The cluster alignment loss and the discriminative clustering loss work together to align the class-conditional structures of the two domains in a discriminative way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Improved marginal distribution alignment</head><p>In fact, the source domain and target domain in the existing popular UDA tasks (e.g., digits adaptation and Office-31) have analogous marginal distributions. Therefore, in these experiments, we combine CAT with the marginal distribution alignment methods, and CAT contributes to bias them to match the cluster-based marginal distributions. The negative effects of these methods of ignoring the discriminability may hurt the stability of training and the capability of converged models. For example, several target circle samples in <ref type="figure" target="#fig_0">Fig. 1</ref>-left will be misclassified by them.</p><p>We are dedicated to delivering a technique to improve these models given the observation that in the early stages of training, a portion of target samples lie around the decision boundaries of the adapted classifier, i.e., they have low classification confidence (the largest output probability) and are likely to be misclassified. Therefore, these samples are possible to be mapped into the incorrect clusters in the marginal alignment process and the training falls into local optima. To solve this, we propose to use confidence-thresholding method to hold out uncertain data points with confidence less than p, while aligning the confident instances which are more geometrically close to the source domain with the source data. Formally, we instantiate this technique in the typical and brief RevGrad <ref type="bibr" target="#b7">[7]</ref> and propose robust RevGrad (rRevGrad) which optimizes the following loss:</p><formula xml:id="formula_10">min θ max φ L d (X s , X t ) = 1 N N i=1 log c f (x i s ; θ); φ + 1 MM i=1 log 1 − c f (x i t ; θ); φ γ i ,<label>(10)</label></formula><p>where c is the critic model parameterized by φ and γ i is an indicator function which outputs 1 only if teacher's confidence of x i t is greater than p. With the divergence between the two domains decreasing, more and more target samples are selected into the domain adversarial training. Gradually, almost all the target samples are included in the training which avoids the lost of target information. We empirically observed that rRevGrad improves the classification performance on the target domain and enhances the stability of training (see Sec.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion</head><p>Comparison with SSL based deep UDA methods <ref type="bibr" target="#b43">[41,</ref><ref type="bibr" target="#b38">36,</ref><ref type="bibr" target="#b6">6]</ref>. CAT not only implements of cluster assumption for better classification but also imposes a class-conditional alignment between domains which is more principal in UDA. However, <ref type="bibr" target="#b43">[41,</ref><ref type="bibr" target="#b38">36,</ref><ref type="bibr" target="#b6">6]</ref> focus on improving classifier to make it more consistent and robust for the target domain based on cluster assumption. Thus CAT is compatible to these methods (see <ref type="bibr">Sec. 4)</ref>.</p><p>Comparison with MSTN <ref type="bibr" target="#b51">[49]</ref>. The cluster alignment loss using conditional feature matching technique is similar to the semantic loss in MSTN <ref type="bibr" target="#b51">[49]</ref>. However, in MSTN, minimizing distance between centers is necessary but not sufficient to achieve semantic alignment. In CAT, we regularize the features to form separable and tight clusters, so the feature matching based loss can match the clusters naturally. They are also different in implementation.</p><p>Mini-batch stochastic training of CAT. We implement the two objectives in CAT using stochastically sampled mini-batches as X s and X t . Specifically, L c is an instancewise loss and can work well. The class-conditional expectation λ s,k or λ t,k in L c could be none when these is no points belonging to class k. At this time, we remove the term corresponding to class k in Eq. 8 and calculate the mean of the other terms. We empirically find CAT needs only 0.05× more training time on a GTX 1080Ti when combining with existing methods.</p><p>Teacher-student paradigm. First, using teacher as labeling function on target domain avoids the error amplification issue. Furthermore, once the classifier becomes more accurate on the target domain, the teacher classifier per-forms better as well. Then the pseudo labels used in L c and L a are more likely to be correct which in turn enhances the classifier. Consequently, a boosting cycle between them is formed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To demonstrate the effectiveness of CAT, we evaluate it through various experiments on synthetic imbalanced dataset and three challenging UDA tasks: SVHN-MNIST-USPS, Office-31 <ref type="bibr" target="#b36">[35]</ref> and ImageCLEF-DA 2 . We show that CAT considers and exploits the fine-level class-conditional structures of the source and target domains, and makes the learned feature space discriminative and aligned, thus yielding improved performance on the target domain.</p><p>Datasets and configurations. SVHN-MNIST-USPS is a challenging adaptation task of digits between three datasets SVHN <ref type="bibr" target="#b30">[29]</ref>, MNIST <ref type="bibr" target="#b18">[17]</ref> and USPS. We conduct experiments in three directions: SVHN→MNIST, MNIST→USPS and USPS→MNIST. We follow the protocol in <ref type="bibr" target="#b46">[44]</ref>: we use the whole training sets for the adaptation from SVHN to MNIST and randomly sample 2000 images from MNIST and 1800 images from USPS for the adaptation between the two datasets. Following MSTN <ref type="bibr" target="#b51">[49]</ref>, the images are cast to 28 × 28 × 1 when using LeNet <ref type="bibr" target="#b18">[17]</ref> as classifier. When combining with MCD <ref type="bibr" target="#b39">[37]</ref> and VADA <ref type="bibr" target="#b43">[41]</ref>, We take the identical settings as the original methods.</p><p>Imbalanced SVHN-MNIST-USPS. We randomly sample 1000 instances from class 0 and 100 instances from class 1 from the original source domain and construct a new one. Then we sample 100 instances from class 0 and 1000 instances from class 1 from the target domain to form a new target. Therefore, the synthetic adaptation dataset contains several imbalanced two-class adaptation tasks. The experiment settings are the same as those of SVHN-MNIST-USPS.</p><p>Office-31 and ImageCLEF-DA are two real-world datasets which are widely used in domain adaptation research. Office-31 is composed of three domains: Amazon (A), DSLR (D) and Webcam (W), containing 2817, 498 and 795 images from 31 categories, respectively. ImageCLEF-DA includes three domains: Caltech-256 (C), ImageNet ILSVRC 2012 (I) and Pascal VOC 2012 (P), containing 600 images from 12 classes, respectively. We use data augmentation such as random flipping and cropping in training for fair comparison with the baselines.</p><p>Implementation. In synthetic and digits experiments using LeNet, we set m = 30 according to the performance of CAT on the synthetic dataset and we forwardpropagate a target sample twice under different perturbations(i.e., dropout) and use the latter as the prediction of the teacher for its simplicity (similar with the Π model of <ref type="bibr" target="#b17">[16]</ref>). In all the other experiments, we fix m = 3 and deploy a tem- <ref type="bibr" target="#b2">2</ref> Source code is at https://github.com/thudzj/CAT. poral ensemble <ref type="bibr" target="#b17">[16]</ref> of previous predictions of h as teacher (the accumulation decay constant is set to 0.6). We design a ramp-up function similar with that of <ref type="bibr" target="#b17">[16]</ref> to update α in experiments using LeNet and set α = 2 1+exp(−10t) − 1 suggested by RevGrad <ref type="bibr" target="#b7">[7]</ref> in which t increases linearly from 0 to 1 in the others. We set p = 0.9 in all the experiments without tuning. Refer to Appendix. E for more details of the used architectures and optimization settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on imbalanced SVHN-MNIST-USPS</head><p>We first test CAT on the imbalanced SVHN-MNIST-USPS dataset, a challenging task where the source domains have 10 : 1 ratio of class imbalance while the target domains have 1 : 10. We implement CAT and RevGrad <ref type="bibr" target="#b7">[7]</ref> based on the official codes of MSTN <ref type="bibr" target="#b51">[49]</ref> using LeNet <ref type="bibr" target="#b18">[17]</ref>. The results are shown in <ref type="table">Table 1</ref>. We repeat each task 3 times and report the averaged test accuracy and standard deviation.</p><p>It is notable that RevGrad <ref type="bibr" target="#b7">[7]</ref> and MSTN <ref type="bibr" target="#b51">[49]</ref> fail thoroughly owing to their obsession of matching the marginal distributions. In contrast, CAT gives almost completely correct predictions for the target domains. This experiment verifies that existing methods through aligning marginal distributions are restrictive and require the modes corresponding to the same class but different domains to be geometrically similar. They are sensitive and fragile in practical tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SVHN-MNIST-USPS digits datasets</head><p>We apply CAT to the popular digits adaptation task SVHN-MNIST-USPS and compare to the state-of-the-art approaches in <ref type="table" target="#tab_3">Table 3</ref> (all baseline results are taken from related literature). CAT, RevGrad+CAT and rRevGrad+CAT follow the settings of MSTN <ref type="bibr" target="#b51">[49]</ref> using the LeNet <ref type="bibr" target="#b18">[17]</ref>. We implement MCD+CAT and VADA+CAT based on the official codes of MCD <ref type="bibr" target="#b39">[37]</ref> and VADA <ref type="bibr" target="#b43">[41]</ref> using their architectures instead of LeNet for fair comparison. We only integrate CAT with the first stage algorithm VADA in DIRT-T <ref type="bibr" target="#b43">[41]</ref> while discarding the fine-tuning stage for simpleness.</p><p>There are several conclusions we can make. First, CAT reveals strikingly improved test accuracy on SVHN to MNIST task without tuning the hyper-parameters, and CAT even outperforms MCD <ref type="bibr" target="#b39">[37]</ref> and VADA <ref type="bibr" target="#b43">[41]</ref> which use much wider and deeper neural networks thanks to the classconditional discriminative alignment between the source and target domains. This task is the most challenging one among the three because of the complex samples and the   internal class imbalance of SVHN. Second, CAT does not perform well enough on the other two tasks but when combining with rRevGrad and MCD <ref type="bibr" target="#b39">[37]</ref>, CAT outperforms the strong baselines MSTN <ref type="bibr" target="#b51">[49]</ref> and MCD <ref type="bibr" target="#b39">[37]</ref> with obvious margins. Third, applying CAT into RevGrad <ref type="bibr" target="#b7">[7]</ref>, MCD <ref type="bibr" target="#b39">[37]</ref> and VADA <ref type="bibr" target="#b43">[41]</ref> can enhance the base methods significantly, especially on the typical and simple RevGrad <ref type="bibr" target="#b7">[7]</ref>. Finally, rRevGrad+CAT displays higher test accuracy and lower variance than those of RevGrad+CAT and the advantage is particularly obvious when the two domains have different class-conditional structures (e.g., SVHN to MNIST), so we utilize rRevGrad+CAT on more challenging tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on Office-31 and ImageCLEF-DA</head><p>We evaluate CAT using two sets of extensive experiments on the widely used Office-31 and ImageCLEF-DA. They contain more realistic and high-dimensional images, providing a good complement to the digits adaptation task. The results are provided in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table" target="#tab_4">Table 4</ref>, respectively. We integrate CAT with rRevGrad and JAN <ref type="bibr" target="#b24">[23]</ref> (using ResNet-50 <ref type="bibr" target="#b13">[12]</ref> and AlexNet <ref type="bibr" target="#b16">[15]</ref> as the classifiers), which is sufficient to testify the effectiveness of discriminative cluster-based alignment and teacher-student paradigm.</p><p>We observe that CAT can boost rRevGrad and JAN [23] significantly, especially on the difficult A to W, A to D and D to A tasks in Office-31, and the combined models surpass the strong baselines RevGrad <ref type="bibr" target="#b7">[7]</ref> and JAN <ref type="bibr" target="#b24">[23]</ref> by obvious margins. CAT based methods also outperform MSTN <ref type="bibr" target="#b51">[49]</ref> on various tasks substantially which proves the class-conditional discriminative alignment is superior to the semantic alignment used by MSTN <ref type="bibr" target="#b51">[49]</ref>. The improvement of test accuracy on most tasks of ImageCLEF-DA shows that CAT can still work well when the domains are small containing only 600 images. We further confirm that CAT can deliver a discriminative and aligned feature space by visualizing the learned features in the Appendix. A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis</head><p>Visualization of feature space. We visualize the features of the two domains learned by the powerful Method I to P P to I I to C C to I C to P P to C Avg   rRevGrad+CAT and RevGrad <ref type="bibr" target="#b7">[7]</ref> on the SVHN to MNIST task using t-SNE <ref type="bibr" target="#b28">[27]</ref>. The results are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. As expected, using CAT <ref type="figure" target="#fig_2">(Fig. 3b</ref>), the features are concentrated and form tight clusters and those from different classes are separated. In contrast, the features learned by RevGrad <ref type="bibr" target="#b7">[7]</ref> ( <ref type="figure" target="#fig_2">Fig. 3a)</ref> are more overlapping and less discriminative. Clustering in the feature space. We further examine the feature space shaped by CAT and other baselines by conducting K-means <ref type="bibr" target="#b22">[21]</ref> clustering using the aligned features. We utilize the trained models to infer the hidden features of all the images from two domains. Then the features are clustered into k components by K-means in scikitlearn <ref type="bibr" target="#b49">[47]</ref>. We set k as the number of categories. We greedily set the label of a cluster as the most frequent label in it to calculate clustering accuracy as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. As expected, the feature spaces learned by rRevGrad+CAT demonstrate more discriminative cluster structure and this is consistent with the classification results. Appendix. B, C and D provide more analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we address the challenges of making better alignment between domains and advocate to exploit the discriminative class-conditional structures for effective adaptation in deep UDA. We propose Cluster Alignment with a Teacher (CAT) to achieve the objectives of discriminative learning and class-conditional alignment via a discriminative clustering loss and a cluster-based alignment loss. CAT produces a domain-invariant feature space with improved discriminative power and enhances the performance significantly. CAT establishes new state-of-the-art baselines on benchmarks and additional analyses testify its effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(Best viewed in color.) Left: The two domains have diverse modes. Right: The two domains have different class imbalance ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The framework of CAT (The source supervised loss L y is omitted for clarity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(Best viewed in color.) (a) Feature space learned by RevGrad. (b) Feature space learned by rRevGrad+CAT. The features are projected to 2-D using t-SNE. Blue violet denotes the source domain and the other colors denotes classes of target domain. See Appendix. A for more results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Summary of clustering accuracy(%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>SVHN to MNIST MNIST to USPS USPS to MNIST RevGrad [7] 27.4 ± 6.3 26.7 ± 2.0 17.9 ± 1.4 MSTN [49] 25.8 ± 3.6 30.3 ± 1.0 29.4 ± 0.5 CAT 100.0 ± 0.05 100.0 ± 0.0 99.9 ± 0.2</figDesc><table><row><cell>Table 1: Summary of domain adaptation results on the im-</cell></row><row><cell>balanced digits datasets in terms of test accuracy (%).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy on the Office-31 datasets in terms of test accuracy (%) (ResNet-50 and AlexNet). RevGrad+CAT 98.0 ± 0.8 93.7 ± 1.1 95.7 ± 1.3 rRevGrad+CAT 98.8 ± 0.02 94.0 ± 0.7 96.0 ± 0.9</figDesc><table><row><cell>Method</cell><cell cols="3">SVHN to MNIST MNIST to USPS USPS to MNIST</cell></row><row><cell>Source Only</cell><cell cols="3">60.1 ± 1.1 75.2 ± 1.6 57.1 ± 1.7</cell></row><row><cell>DDC [45]</cell><cell cols="3">68.1 ± 0.3 79.1 ± 0.5 66.5 ± 3.3</cell></row><row><cell>CoGAN [20]</cell><cell>-</cell><cell cols="2">91.2 ± 0.8 89.1 ± 0.8</cell></row><row><cell>DRCN [8]</cell><cell cols="3">82.0 ± 0.1 91.8 ± 0.09 73.7 ± 0.04</cell></row><row><cell>ADDA [44]</cell><cell cols="3">76.0 ± 1.8 89.4 ± 0.2 90.1 ± 0.8</cell></row><row><cell>LEL [26]</cell><cell>81.0 ± 0.3</cell><cell>-</cell><cell>-</cell></row><row><cell>AssocDA [11]</cell><cell>97.6</cell><cell>-</cell><cell>-</cell></row><row><cell>MSTN [49]</cell><cell cols="2">91.7 ± 1.5 92.9 ± 1.1</cell><cell>-</cell></row><row><cell>CAT</cell><cell cols="3">98.1 ± 1.3 90.6 ± 2.3 80.9 ± 3.1</cell></row><row><cell>RevGrad [7]</cell><cell>73.9</cell><cell cols="2">77.1 ± 1.8 73.0 ± 2.0</cell></row><row><cell>MCD [37]</cell><cell cols="3">96.2 ± 0.4 94.2 ± 0.7 94.1 ± 0.3</cell></row><row><cell>MCD+CAT</cell><cell cols="3">97.1 ± 0.2 96.3 ± 0.5 95.2 ± 0.4</cell></row><row><cell>VADA [41]</cell><cell>94.5</cell><cell>-</cell><cell>-</cell></row><row><cell>VADA+CAT</cell><cell>95.2</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Summary of domain adaptation results on the digits datasets in terms of test accuracy (%).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy on the ImageCLEF-DA datasets in terms of test accuracy (%) (ResNet-50 and AlexNet).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Besides UDA tasks, previous work<ref type="bibr" target="#b31">[30]</ref> has also shown an interesting exploration of the class-conditional structures for learning deep models that are robust against adversarial attacks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t (l s , l t ) which may lead to a large upper bound of t (h) and result in unsatisfying target domain performance<ref type="bibr" target="#b51">[49]</ref>. Empirically, the data in classification naturally has a classconditional multi-modal structure, thus aligning marginal distributions while ignoring the fine-level discriminative structures of domains may hurt the target classification performance (Fig. 1-left). Moreover, these methods may fail in more practical and challenging problems, e.g. the source and target domains have obviously different class imbalance ratios (Fig. 1-right).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the NSFC Projects (Nos. 61621136008, 61620106010), Beijing NSF Project (No. L172037), Tiangong Institute for Intelligent Computing, Beijing Academy of Artificial Intelligence (BAAI), the NVIDIA NVAIL Program with GPU/DGX Acceleration and the JP Morgan Faculty Research Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexnet</forename></persName>
		</author>
		<idno>15] 66.2 ± 0.2 70.0 ± 0.2 84.3 ± 0.2 71.3 ± 0.4 59.3 ± 0.5 84.5 ± 0.3 73.9 RTN [24] 67.4 ± 0.3 81.3 ± 0.3 89.5 ± 0.4 78.0 ± 0.2 62.0 ± 0.2 89.1 ± 0.1 77.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A theory of learning from different domains. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>] Shai Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaughan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised learning (chapelle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<editor>o. et al.</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structured generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luona</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3899" to="3909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reconstructionclassification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page">597</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Associative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Frerix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2765" to="2773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixiang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on information theory</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<title level="m">Learning transferable features with deep adaptation networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06636</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Smooth neighbors on teacher graphs for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Label efficient learning of transferable representations acrosss domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li F Fei-Fei</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="165" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00479</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Max-mahalanobis linear discriminant analysis networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8004" to="8013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08824</idno>
		<title level="m">From source to target and back: symmetric bi-directional adaptive gan</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Information-theoretical learning of discriminative clusters for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6438</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adversarial feature augmentation for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08561</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Data Mining: Practical machine learning tools and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Ian H Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Domain adaptation with asymmetrically-relaxed distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ezra</forename><surname>Winston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6872" to="6881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5419" to="5428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3801" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On learning invariant representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Tachet Des</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7523" to="7532" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
