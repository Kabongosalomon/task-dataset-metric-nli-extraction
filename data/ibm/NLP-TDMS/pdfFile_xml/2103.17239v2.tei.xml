<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Going deeper with Image Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Going deeper with Image Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for instance we obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. We share our code and models 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Residual architectures are prominent in computer vision since the advent of ResNet <ref type="bibr" target="#b27">[27]</ref>. They are defined as a sequence of functions of the form</p><formula xml:id="formula_0">x l+1 = g l (x l ) + R l (x l ),<label>(1)</label></formula><p>where the function g l and R l define how the network updates the input x l at layer l. The function g l is typically identity, while R l is the main building block of the network: many variants in the literature essentially differ on how one defines this residual branch R l is constructed or parametrized <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b72">72]</ref>. Residual architectures highlight the strong interplay between optimization and architecture design. As pointed out by He et al. <ref type="bibr" target="#b27">[27]</ref>, residual networks do not offer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He et al. discussed <ref type="bibr" target="#b28">[28]</ref> the importance of having a clear path both forward and backward, and advocate setting g l to the identity function. The vision transformers <ref type="bibr" target="#b18">[19]</ref> instantiate a particular form of residual architecture: after casting the input image into a set x 0 of vectors, the network alternates self-attention layers (SA) with feed-forward networks (FFN), as</p><p>x l = x l + SA(η(x l ))</p><formula xml:id="formula_1">x l+1 = x l + FFN(η(x l ))<label>(2)</label></formula><p>where η is the LayerNorm operator <ref type="bibr" target="#b0">[1]</ref>. This definition follows the original architecture of Vaswani et al. <ref type="bibr" target="#b66">[66]</ref>, except the LayerNorm is applied before the block (pre-norm) in the residual branch, as advocated by He et al. <ref type="bibr" target="#b28">[28]</ref>. Child et al. <ref type="bibr" target="#b12">[13]</ref> adopt this choice with LayerNorm for training deeper transformers for various media, including for image generation where they train transformers with 48 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b75">75]</ref> and for transformers applied to NLP or speech tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b75">75]</ref>. In Section 2, we revisit this topic for transformer architectures solving image classification problems. Examples of approaches closely related to ours include Fixup <ref type="bibr" target="#b75">[75]</ref>, T-Fixup <ref type="bibr" target="#b34">[34]</ref>, ReZero <ref type="bibr" target="#b1">[2]</ref> and SkipInit <ref type="bibr" target="#b15">[16]</ref>.</p><p>Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to improve the training of deeper architecture compared to current methods for image transformers. Formally, we add a learnable diagonal matrix on output of each residual block, initialized close to (but not at) 0. Adding this simple layer after each residual block improves the training dynamic, allowing us to train deeper high-capacity image transformers that benefit from depth. We refer to this approach as LayerScale.</p><p>Section 3 introduces our second contribution, namely class-attention layers, that we present in <ref type="figure" target="#fig_3">Figure 2</ref>. It is akin to an encoder/decoder architecture, in which we explicitly separate the transformer layers involving self-attention between patches, from class-attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear classifier. This explicit separation avoids the contradictory objective of guiding the attention process while processing the class embedding. We refer to this new architecture as CaiT (Class-Attention in Image Transformers).</p><p>In the experimental Section 4, we empirically show the effectiveness and complementary of our approaches:</p><p>• LayerScale significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands of parameters to the network at training time (negligible w.r.t. the total number of weights).</p><p>• Our architecture with specific class-attention offers a more effective processing of the class embedding.</p><p>• Our best CaiT models establish the new state of the art on Imagenet-Real <ref type="bibr" target="#b5">[6]</ref> and Imagenet V2 matched frequency <ref type="bibr" target="#b52">[52]</ref> with no additional training data. On ImageNet1k-val <ref type="bibr" target="#b54">[54]</ref>, our model is on par with the state of the art (86.5%) while requiring less FLOPs (329B vs 377B) and having less parameters than the best competing model (356M vs 438M).</p><p>• We achieve competitive results on Transfer Learning.</p><p>We provide visualizations of the attention mechanisms in Section 5. We discuss related works along this paper and in the dedicated Section 6, before we conclude in Section 7. The appendices contain some variations we have tried during our exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deeper image transformers with LayerScale</head><p>Our goal is to increase the stability of the optimization when training transformers for image classification derived from the original architecture by Vaswani et al.. <ref type="bibr" target="#b66">[66]</ref>, and especially when we increase their depth. We consider more specifically the vision transformer (ViT) architecture proposed by Dosovitskiy et al. <ref type="bibr" target="#b18">[19]</ref> as the reference architecture and adopt the data-efficient image transformer (DeiT) optimization procedure of Touvron et al. <ref type="bibr" target="#b63">[63]</ref>. In both works, there is no evidence that depth can bring any benefit when training on Imagenet only: the deeper ViT architectures have a low performance, while DeiT only considers transformers with 12 blocks of layers. The experimental section 4 will confirm that DeiT does not train deeper models effectively. <ref type="figure" target="#fig_1">Figure 1</ref> depicts the main variants that we compare for helping the optimization. They cover recent choices from the literature: as discussed in the introduction, the architecture (a) of ViT and DeiT is a pre-norm architecture <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b63">63]</ref>, in which the layer-normalisation η occurs at the beginning of the residual branch. Note that the original architecture of Vaswani et al. <ref type="bibr" target="#b66">[66]</ref> applies the normalization after the block, but in our experiments the DeiT training does not converge with post-normalization.</p><p>Fixup <ref type="bibr" target="#b75">[75]</ref>, ReZero <ref type="bibr" target="#b1">[2]</ref> and SkipInit <ref type="bibr" target="#b15">[16]</ref> introduce learnable scalar weighting α l on the output of residual blocks, while removing the pre-normalization and the warmup, see <ref type="figure" target="#fig_1">Figure 1</ref>(b). This amounts to modifying Eqn. 2 as</p><formula xml:id="formula_2">x l = x l + α l SA(x l ) x l+1 = x l + α l FFN(x l ).<label>(3)</label></formula><p>ReZero simply initializes this parameter to α = 0. Fixup initializes this parameter to α = 1 and makes other modifications: it adopts different policies for the initialization of the block weights, and adds several weights to the parametrization. In our experiments, these approaches do not converge even with some adjustment of the hyper-parameters.</p><p>Our empirical observation is that removing the warmup and the layernormalization is what makes training unstable in Fixup and T-Fixup. Therefore we re-introduce these two ingredients so that Fixup and T-Fixup converge   <ref type="bibr" target="#b12">[13]</ref>. (b) ReZero/Skipinit and Fixup remove the η normalization and the warmup (i.e., a reduced learning rate in the early training stage) and add a learnable scalar initialized to α = 0 and α = 1, respectively. Fixup additionally introduces biases and modifies the initialization of the linear layers. Since these methods do not converge with deep vision transformers, (c) we adapt them by reintroducing the pre-norm η and the warmup. Our main proposal (d) introduces a perchannel weighting (i.e, multiplication with a diagonal matrix diag(λ1, . . . , λ d ), where we initialize each weight with a small value as λi = ε.</p><p>with DeiT models, see <ref type="figure" target="#fig_1">Figure 1</ref>(c). As we see in the experimental section, these amended variants of Fixup and T-Fixup are effective, mainly due to the learnable parameter α l . When initialized at a small value, this choice does help the convergence when we increase the depth.</p><p>Our proposal LayerScale is a per-channel multiplication of the vector produced by each residual block, as opposed to a single scalar, see <ref type="figure" target="#fig_1">Figure 1</ref>(d).</p><p>Our objective is to group the updates of the weights associated with the same output channel. Formally, LayerScale is a multiplication by a diagonal matrix on output of each residual block. In other terms, we modify Eqn. 2 as</p><formula xml:id="formula_3">x l = x l + diag(λ l,1 , . . . , λ l,d ) × SA(η(x l )) x l+1 = x l + diag(λ l,1 , . . . , λ l,d ) × FFN(η(x l )),<label>(4)</label></formula><p>where the parameters λ l,i and λ l,i are learnable weights. The diagonal values are all initialized to a fixed small value ε: we set it to ε = 0.1 until depth 18, ε = 10 −5 for depth 24 and ε = 10 −6 for deeper networks. This formula is akin to other normalization strategies ActNorm <ref type="bibr" target="#b37">[37]</ref> or LayerNorm but executed on output of the residual block. Yet we seek a different effect: ActNorm is a datadependent initialization that calibrates activations so that they have zero-mean and unit variance, like batchnorm <ref type="bibr" target="#b35">[35]</ref>. In contrast, we initialize the diagonal with small values so that the initial contribution of the residual branches to the function implemented by the transformer is small. In that respect our motivation is therefore closer to that of ReZero <ref type="bibr" target="#b1">[2]</ref>, SkipInit <ref type="bibr" target="#b15">[16]</ref>, Fixup <ref type="bibr" target="#b75">[75]</ref> and T-Fixup <ref type="bibr" target="#b34">[34]</ref>: to train closer to the identity function and let the network inte-grate the additional parameters progressively during the training. LayerScale offers more diversity in the optimization than just adjusting the whole layer by a single learnable scalar as in ReZero/SkipInit, Fixup and T-Fixup. As we will show empirically, offering the degrees of freedom to do so per channel is a decisive advantage of LayerScale over existing approaches.In Appendix A, we present other variants or intermediate choices that support our proposal, and a control experiment that aims at disentangling the specific weighting of the branches of LayerScale from its impact on optimization. Formally, adding these weights does not change the expressive power of the architecture since, as they can be integrated into the previous matrix of the SA and FFN layers without changing the function implemented by the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Specializing layers for class attention</head><p>In this section, we introduce the CaiT architecture, depicted in <ref type="figure" target="#fig_3">Figure 2</ref> (right). This design aims at circumventing one of the problems of the ViT architecture: the learned weights are asked to optimize two contradictory objectives: (1) guiding the self-attention between patches while (2) summarizing the information useful to the linear classifier. Our proposal is to explicitly separate the two stages, in the spirit of an encoder-decoder architecture, see Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Later class token.</head><p>As an intermediate step towards our proposal, we insert the so-called class token, denoted by CLS, later in the transformer. This choice eliminates the discrepancy on the first layers of the transformer, which are therefore fully employed for performing self-attention between patches only. As a baseline that does not suffer from the contradictory objectives, we also consider average pooling of all the patches on output of the transformers, as typically employed in convolutional architectures.</p><p>Architecture. Our CaiT network consists of two distinct processing stages visible in <ref type="figure" target="#fig_3">Figure 2</ref>:</p><p>1. The self-attention stage is identical to the ViT transformer, but with no class embedding (CLS).</p><p>2. The class-attention stage is a set of layers that compiles the set of patch embeddings into a class embedding CLS that is subsequently fed to a linear classifier.</p><p>This class-attention alternates in turn a layer that we refer to as a multi-head class-attention (CA), and a FFN layer. In this stage, only the class embedding is updated. Similar to the one fed in ViT and DeiT on input of the transformer, it is a learnable vector. The main difference is that, in our architecture, we do no copy information from the class embedding to the patch embeddings during the forward pass. Only the class embedding is updated by residual in the CA and FFN processing of the class-attention stage.  In the ViT transformer (left), the class embedding (CLS) is inserted along with the patch embeddings. This choice is detrimental, as the same weights are used for two different purposes: helping the attention process, and preparing the vector to be fed to the classifier. We put this problem in evidence by showing that inserting CLS later improves performance (middle). In the CaiT architecture (right), we further propose to freeze the patch embeddings when inserting CLS to save compute, so that the last part of the network (typically 2 layers) is fully devoted to summarizing the information to be fed to the linear classifier.</p><p>Multi-heads class attention. The role of the CA layer is to extract the information from the set of processed patches. It is identical to a SA layer, except that it relies on the attention between (i) the class embedding x class (initialized at CLS in the first CA) and (ii) itself plus the set of frozen patch embeddings x patches . We discuss why we include x class in the keys in Appendix B.</p><p>Considering a network with h heads and p patches, and denoting by d the embedding size, we parametrize the multi-head class-attention with several projection matrices,</p><formula xml:id="formula_4">W q , W k , W v , W o ∈ R d×d , and the corresponding biases b q , b k , b v , b o ∈ R d .</formula><p>With this notation, the computation of the CA residual block proceeds as follows. We first augment the patch embeddings (in matrix form) as z = [x class , x patches ] (see Appendix B for results when z = x patches ). We then perform the projections:</p><formula xml:id="formula_5">Q = W q x class + b q ,<label>(5)</label></formula><formula xml:id="formula_6">K = W k z + b k ,<label>(6)</label></formula><formula xml:id="formula_7">V = W v z + b v .<label>(7)</label></formula><p>The class-attention weights are given by</p><formula xml:id="formula_8">A = Softmax(Q.K T / d/h)<label>(8)</label></formula><p>where Q.K T ∈ R h×1×p . This attention is involved in the weighted sum A × V to produce the residual output vector</p><formula xml:id="formula_9">out CA = W o A V + b o ,<label>(9)</label></formula><p>which is in turn added to x class for subsequent processing. The CA layers extract the useful information from the patches embedding to the class embedding. In preliminary experiments, we empirically observed that the first CA and FFN give the main boost, and a set of 2 blocks of layers (2 CA and 2 FFN) is sufficient to cap the performance. In the experimental section, we denote by 12+2 a transformer when it consists of 12 blocks of SA+FFN layers and 2 blocks of CA+FFN layers.</p><p>Complexity. The layers contain the same number of parameters in the classattention and self-attention stages: CA is identical to SA in that respect, and we use the same parametrization for the FFNs. However the processing of these layers is much faster: the FFN only processes matrix-vector multiplications.</p><p>The CA function is also less expensive than SA in term of memory and computation because it computes the attention between the class vector and the set of patch embeddings: Q ∈ R d means that Q.K T ∈ R h×1×p . In contrast, in the "regular self-attention" layers SA, we have Q ∈ R p×d and therefore Q.K T ∈ R h×p×p . In other words, the initially quadratic complexity in the number of patches becomes linear in our extra CaiT layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we report our experimental results related to LayerScale and CaiT. We first study strategies to train at deeper scale in Section 4.1, including our LayerScale method. Section 4.2 shows the interest of our class-attention design. We present our models in Subsection 4.3. Section 4.4 details our results on Imagenet and Transfer learning. We provide an ablation of hyper-parameter and ingredients in Section 4.5. Note, in Appendix A and B, we provide variations on our methods and corresponding results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setting.</head><p>Our implementation is based on the Timm library <ref type="bibr" target="#b68">[68]</ref>. Unless specified otherwise, for this analysis we make minimal changes to hyperparameters compared to the DeiT training scheme <ref type="bibr" target="#b63">[63]</ref>. In order to speed up training and optimize memory consumption we have used a sharded training provided by the Fairscale library 2 with fp16 precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminary analysis with deeper architectures</head><p>In our early experiments, we observe that Vision Transformers become increasingly more difficult to train when we scale architectures. Depth is one of the main source of instability. For instance the DeiT procedure <ref type="bibr" target="#b63">[63]</ref> fails to properly converge above 18 layers without adjusting hyper-parameters. Large ViT <ref type="bibr" target="#b18">[19]</ref> models with 24 and 32 layers were trained with large training datasets, but when trained on Imagenet only the larger models are not competitive.</p><p>In the following, we analyse various ways to stabilize the training with different architectures. At this stage we consider a Deit-Small model 3 during 300 epochs to allow a direct comparison with the results reports by Touvron et al. <ref type="bibr" target="#b63">[63]</ref>. We measure the performance on the Imagenet1k <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b54">54]</ref> classification dataset as a function of the depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Adjusting the drop-rate of stochastic depth.</head><p>The first step to improve convergence is to adapt the hyper-parameters that interact the most with depth, in particular Stochastic depth <ref type="bibr" target="#b33">[33]</ref>. This method is already popular in NLP <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref> to train deeper architectures. For ViT, it was first proposed by Wightman et al. <ref type="bibr" target="#b68">[68]</ref> in the Timm implementation, and subsequently adopted in DeiT <ref type="bibr" target="#b63">[63]</ref>. The per-layer drop-rate depends linearly on the layer depth, but in our experiments this choice does not provide an advantage compared to the simpler choice of a uniform drop-rate d r . In <ref type="table" target="#tab_11">Table 1</ref> we show that the default stochastic depth of DeiT allows us to train up to 18 blocks of SA+FFN. After that the training becomes unstable. By increasing the drop-rate hyper-parameter d r , the performance increases until 24 layers. It saturates at 36 layers (we measured that it drops to 80.7% at 48 layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison of normalization strategies</head><p>We carry out an empirical study of the normalization methods discussed in Section 2. As previously indicated, Rezero, Fixup and T-Fixup do not converge when training DeiT off-the-shelf. However, if we re-introduce Layer-Norm 4 and warmup, Fixup and T-Fixup achieve congervence and even improve training compared to the baseline DeiT. We report the results for these "adaptations" of Fixup and T-Fixup in <ref type="table" target="#tab_11">Table 1</ref>.</p><p>The modified methods are able to converge with more layers without saturating too early. ReZero converges, we show (column α = ε) that it is better to initialize α to a small value instead of 0, as in LayerScale. All the methods have a beneficial effect on convergence and they tend to reduce the need for stochastic depth, therefore we adjust these drop rate accordingly per method. <ref type="figure" target="#fig_4">Figure 3</ref> provides the performance as the function of the drop rate d r for Layer-Scale. We empirically use the following formula to set up the drop-rate for the <ref type="table" target="#tab_11">Table 1</ref>: Improving convergence at depth on ImageNet-1k. The baseline is DeiT-S with uniform drop rate of d = 0.05 (same expected drop rate and performance as progressive stochastic depth of 0.1). Several methods include a fix scalar learnable weight α per layer as in <ref type="figure" target="#fig_1">Figure 1</ref>(c). We have adapted Rezero, Fixup, T-Fixup, since the original methods do not converge: we have re-introduced the Layer-normalization η and warmup. We have adapted the drop rate dr for all the methods, including the baseline. The column α = ε reports the performance when initializing the scalar with the same value as for LayerScale. †: failed before the end of the training.  CaiT-S models derived from on Deit-S: d r = min(0.1 × depth 12 − 1, 0).This formulaic choice avoids cross-validating this parameter and overfitting, yet it does not generalize to models with different d: We further increase (resp. decrease) it by a constant for larger (resp. smaller) working dimensionality d.</p><p>Fixup and T-Fixup are competitive with LayerScale in the regime of a relatively low number of blocks <ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref><ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref>. However, they are more complex than LayerScale: they employ different initialization rules depending of the type of layers, and they require more changes to the transformer architecture. Therefore we only use LayerScale in subsequent experiments. It is much simpler and parametrized by a single hyper-parameter ε, and it offers a better performance for the deepest models that we consider, which are also the more accurate.  The ratio between the norm of the residual and the norm of the main branch is shown for each layer of the transformer and for various epochs (darker shades correspond to the last epochs). For the model trained with layerscale, the norm of the residual branch is on average 20% of the norm of the main branch. We observe that the contribution of the residual blocks fluctuates more for the model trained without layerscale and in particular is lower for some of the deeper layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Analysis of Layerscale</head><p>Statistics of branch weighting. We evaluate the impact of Layerscale for a 36-blocks transformer by measuring the ratio between the norm of the residual activations and the norm of the activations of the main branch g l (x) 2 / x 2 . The results are shown in <ref type="figure" target="#fig_6">Figure 4</ref>. We can see that training a model with Layerscale makes this ratio more uniform across layers, and seems to prevent some layers from having a disproportionate impact on the activations. Similar to prior works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b75">75]</ref> we hypothetize that the benefit is mostly the impact on optimization. This hypothesis is supported by the control experiment that we detail in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Class-attention layers</head><p>In <ref type="table" target="#tab_1">Table 2</ref> we study the impact on performance of the design choices related to class embedding. We depict some of them in <ref type="figure" target="#fig_3">Figure 2</ref>. As a baseline, average pooling of patches embeddings with a vanilla DeiT-Small achieves a better performance than using a class token. This choice, which does not employ any class embedding, is typical in convolutional networks, but possibly weaker with transformers when transferring to other tasks <ref type="bibr" target="#b20">[20]</ref>.</p><p>Late insertion. The performance increases when we insert the class embedding later in the transformer. It is maximized two layers before the output. Our interpretation is that the attention process is less perturbed in the 10 first layers, yet it is best to keep 2 layers for compiling the patches embedding into the class embedding via class-attention, otherwise the processing gets closer to a weighted average.</p><p>Our class-attention layers are designed on the assumption that there is no benefit in copying information from the class embedding back to the patch embeddings in the forward pass. <ref type="table" target="#tab_1">Table 2</ref> supports that hypothesis: if we compare the performance for a total number of layers fixed to 12, the performance of CaiT with 10 SA and 2 CA layers is identical to average pooling and better  than the DeiT-Small baseline with a lower number of FLOPs. If we set 12 layers in the self-attention stage, which dominates the complexity, we increase the performance significantly by adding two blocks of CA+FFN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Our CaiT models</head><p>Our CaiT models are built upon ViT: the only difference is that we incorporate LayerScale in each residual block (see Section 2) and the two-stages architecture with class-attention layers described in Section 3. <ref type="table" target="#tab_2">Table 3</ref> describes our different models. The design parameters governing the capacity are the depth and the working dimensionality d. In our case d is related to the number of heads h as d = 48 × h, since we fix the number of components per head to 48. This choice is a bit smaller than the value used in DeiT. We also adopt the crop-ratio of 1.0 optimized for DeiT by Wightman <ref type="bibr" target="#b68">[68]</ref>. <ref type="table" target="#tab_8">Table 9</ref> and 10 in the ablation section 4.5 support these choices. We incorporate talking-heads attention <ref type="bibr" target="#b55">[55]</ref> into our model. It increases the performance on Imagenet of DeiT-Small from 79.9% to 80.3%.</p><p>The hyper-parameters are identical to those provided in DeiT <ref type="bibr" target="#b63">[63]</ref>, except mentioned otherwise. We use a batch size of 1024 samples and train during 400 epochs with repeated augmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">29]</ref>. The learning rate of the AdamW optimizer <ref type="bibr" target="#b44">[44]</ref> is set to 0.001 and associated with a cosine training schedule, 5 epochs of warmup and a weight decay of 0.05. We report in <ref type="table" target="#tab_3">Table 4</ref> the two hyper-parameters that we modify depending on the model complexity, namely the drop rate d r associated with uniform stochastic depth, and the initialization value ε of LayerScale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning at higher resolution (↑) and distillation (Υ).</head><p>We train all our models at resolution 224, and optionally fine-tune them at a higher resolution to trade performance against accuracy <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b64">64]</ref>: we denote the model by ↑384 models fine-tuned at resolution 384×384. We also train models with distillation (Υ) as suggested by Touvron et al.. <ref type="bibr" target="#b63">[63]</ref>. We use a RegNet-16GF <ref type="bibr" target="#b50">[50]</ref> as teacher and adopt the "hard distillation" <ref type="bibr" target="#b63">[63]</ref> for its simplicity. <ref type="table" target="#tab_2">Table 3</ref> provides different complexity measures for our models. As a general observation, we observe a subtle interplay between the width and the depth, both contribute to the performance as reported by Dosovitskiy et al.. <ref type="bibr" target="#b18">[19]</ref> with longer training schedules. But if one parameter is too small the gain brought by increasing the other is not worth the additional complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Performance/complexity of CaiT models</head><p>Fine-tuning to size 384 (↑) systematically offers a large boost in performance without changing the number of parameters. It also comes with a higher computational cost. In contrast, leveraging a pre-trained convnet teacher with hard distillation as suggested by Touvron et al. <ref type="bibr" target="#b63">[63]</ref> provides a boost in accuracy without affecting the number of parameters nor the speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Comparison with the state of the art on Imagenet</head><p>Our main classification experiments are carried out on ImageNet <ref type="bibr" target="#b54">[54]</ref>, and also evaluated on two variations of this dataset: ImageNet-Real <ref type="bibr" target="#b5">[6]</ref> that corrects and give a more detailed annotation, and ImageNet-V2 <ref type="bibr" target="#b52">[52]</ref> (matched frequency) that provides a separate test set. In <ref type="table" target="#tab_11">Table 5</ref> we compare some of our models with the state of the art on Imagenet classification when training without external data. We focus on the models CaiT-S36 and CaiT-M36, at different resolutions and with or without distillation.</p><p>On Imagenet1k-val, CaiT-M48↑448Υ achieves 86.5% of top-1 accuracy, which is a significant improvement over DeiT (85.2%). It is the state of the art, on par with a recent concurrent work <ref type="bibr" target="#b7">[8]</ref> that has a significantly higher number of FLOPs. Our approach outperforms the state of the art on Imagenet with reassessed labels, and on Imagenet-V2, which has a distinct validation set which makes it harder to overfit. <ref type="table" target="#tab_11">Table 5</ref>: Complexity vs accuracy on Imagenet <ref type="bibr" target="#b54">[54]</ref>, Imagenet Real <ref type="bibr" target="#b5">[6]</ref> and Imagenet V2 matched frequency <ref type="bibr" target="#b52">[52]</ref> for models trained without external data. We compare CaiT with DeiT <ref type="bibr" target="#b63">[63]</ref>, Vit-B <ref type="bibr" target="#b18">[19]</ref>, TNT <ref type="bibr" target="#b26">[26]</ref>, T2T <ref type="bibr" target="#b74">[74]</ref> and to several state-of-the-art convnets: Regnet <ref type="bibr" target="#b50">[50]</ref> improved by Touvron et al. <ref type="bibr" target="#b63">[63]</ref>, EfficientNet <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b71">71]</ref>, Fix-EfficientNet <ref type="bibr" target="#b65">[65]</ref> and NFNets <ref type="bibr" target="#b7">[8]</ref>. Most reported results are from corresponding papers, and therefore the training procedure differs for the different models. For Imagenet V2 matched frequency and Imagenet Real we report the results provided by the authors. When not available (like NFNet), we report the results measured by Wigthman <ref type="bibr" target="#b68">[68]</ref> with converted models, which may be suboptimal. The RegNetY-16GF is the teacher model that we trained for distillation. We report the best result in bold and the second best result(s) underlined.    <ref type="bibr" target="#b30">[30]</ref> 437,513 24,426 8,142 iNaturalist 2019 <ref type="bibr" target="#b31">[31]</ref> 265,240 3,003 1,010 Flowers-102 <ref type="bibr" target="#b46">[46]</ref> 2,040 6,149 102 Stanford Cars <ref type="bibr" target="#b38">[38]</ref> 8,144 8,041 196 CIFAR-100 <ref type="bibr" target="#b39">[39]</ref> 50,000 10,000 100 CIFAR-10 <ref type="bibr" target="#b39">[39]</ref> 50,000 10,000 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Transfer learning</head><p>We evaluated our method on transfer learning tasks by fine-tuning on the datasets in <ref type="table" target="#tab_5">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning procedure.</head><p>For fine-tuning we use the same hyperparameters as for training. We only decrease the learning rates by a factor 10 (for CARS, Flowers, iNaturalist), 100 (for CIFAR-100, CIFAR-10) and adapt the number of epochs (1000 for CIFAR-100, CIFAR-10, Flowers-102 and Cars-196, 360 for iNaturalist 2018 and 2019). We have not used distillation for this finetuning.</p><p>Results. <ref type="table" target="#tab_11">Table 7</ref> compares CaiT transfer learning results to those of Efficient-Net <ref type="bibr" target="#b62">[62]</ref>, ViT <ref type="bibr" target="#b18">[19]</ref> and DeiT <ref type="bibr" target="#b63">[63]</ref>. These results show the excellent generalization of the transformers-based models in general. Our CaiT models achieve excellent results, as shown by the overall better performance than EfficientNet-B7 across datasets. <ref type="table" target="#tab_11">Table 7</ref>: Results in transfer learning. All models are trained and evaluated at resolution 224 and with a crop-ratio of 0.875 in this comparison (see <ref type="table" target="#tab_9">Table 10</ref> for the comparison of crop-ratio on Imagenet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>ImageNet </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation</head><p>In this section we provide different sets of ablation, in the form of a transition from DeiT to CaiT. Then we provide experiments that have guided our hyperparameter optimization. As mentioned in the main paper, we use the same hyperparameters as in DeiT <ref type="bibr" target="#b63">[63]</ref> everywhere except stated otherwise. We have only changed the number of attention for a given working dimension (see Section 4.5.2), and changed the crop-ratio (see Section 4.5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Step by step from DeiT-Small to CaiT-S36</head><p>In <ref type="table" target="#tab_11">Table 8</ref> we present how to gradually transform the Deit-S [63] architecture into CaiT-36, and measure at each step the performance/complexity changes. One can see that CaiT is complementary with LayerScale and offers an improvement without significantly increasing the FLOPs. As already reported in the literature, the resolution is another important step for improving the performance and fine-tuning instead of training the model from scratch saves a lot of computation at training time. Last but not least, our models benefit from longer training schedules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Optimization of the number of heads</head><p>In <ref type="table" target="#tab_8">Table 9</ref> we study the impact of the number of heads for a fixed working dimensionality. This architectural parameter has an impact on both the accuracy, and the efficiency: while the number of FLOPs remain roughly the same, the compute is more fragmented when increasing this number of heads and on typical hardware this leads to a lower effective throughput. Choosing 8 heads <ref type="table" target="#tab_11">Table 8</ref>: Ablation: we present the ablation path from DeiT-S to our CaiT models. We highlight the complementarity of our approaches and optimized hyper-parameters. Note, Fine-tuning at higher resolution supersedes the inference at higher resolution. See <ref type="table" target="#tab_11">Table 1</ref>   in the self-attention offers a good compromise between accuracy and speed. In Deit-Small, this parameter was set to 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Adaptation of the crop-ratio</head><p>In the typical ("center-crop") evaluation setting, most convolutional neural networks crop a subimage with a given ratio, typically extracting a 224×224 center crop from a 256×256 resized image, leading to the typical ratio of 0.875. Wightman et al. <ref type="bibr" target="#b68">[68]</ref> notice that setting this crop ratio to 1.0 for transformer models has a positive impact: the distilled DeiT-B↑ 384 reaches a top1-accuracy on Imagenet1k-val of 85.42% in this setting, which is a gain of +0.2% compared to the accuracy of 85.2% reported by Touvron et al. <ref type="bibr" target="#b63">[63]</ref>. Our measurements concur with this observation: We observe a gain for almost all our models and most of the evaluation benchmarks. For instance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Longer training schedules</head><p>As shown in <ref type="table" target="#tab_11">Table 8</ref> , increasing the number of training epochs from 300 to 400 improves the performance of CaiT-S-36. However, increasing the number of training epochs from 400 to 500 does not change performance significantly (83.44 with 400 epochs 83.42 with 500 epochs). This is consistent with the observation of the DeiT <ref type="bibr" target="#b63">[63]</ref> paper, which notes a saturation of performance from 400 epochs for the models trained without distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Visualizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Attention map</head><p>In <ref type="figure">Figure 6</ref> we show the attention maps associated with the individual 4 heads of a XXS CaiT model, and for the two layers of class-attention. In CaiT and in contrast to ViT, the class-attention stage is the only one where there is some interaction between the class token and the patches, therefore it conveniently concentrates all the spatial-class relationship. We make two observations:</p><p>• The first class-attention layer clearly focuses on the object of interest, corresponding to the main part of the image on which the classification decision is performed (either correct or incorrect). In this layer, the different heads focus either on the same or on complementary parts of the objects. This is especially visible for the waterfall image;   • The second class-attention layer seems to focus more on the context, or at least the image more globally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Illustration of saliency in class-attention</head><p>In <ref type="figure" target="#fig_9">figure 7</ref> we provide more vizualisations for a XXS model. They are just illustration of the saliency that one may extract from the first class-attention layer. As discussed previously this layer is the one that, empirically, is the most related to the object of interest. To produce these visual representations we simply average the attention maps from the different heads (depicted in <ref type="figure">Figure 6</ref>), and upsample the resulting map to the image size. We then modulate the gray-level image with the strength of the attention after normalizing it with a simple rule of the form (x − x min )/(x max − x min ). We display the resulting image with cividis colormap. For each image we show this saliency map and provides all the class for which the model assigns a probability higher than 10%. These visualizations illustrate how the model can focus on two distinct regions (like racket and tennis ball on the top row/center). We can also observe some failure cases, like the top of the church classified as a flagpole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Since AlexNet <ref type="bibr" target="#b40">[40]</ref>, convolutional neural networks (CNN) are the standard in image classification <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b64">64]</ref>, and more generally in computer vision. While a deep CNN can theoretically model long range interaction between pixels across many layers, there has been research in increasing the range of interactions within a single layer. Some approaches adapt the receptive field of convolutions dynamically <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">42]</ref>. At another end of the spectrum, attention can be viewed as a general form of non-local means, which was used in filtering (e.g. denoising <ref type="bibr" target="#b9">[10]</ref>), and more recently in conjunction with convolutions <ref type="bibr" target="#b67">[67]</ref>. Various other attention mechanism have been used successfully to give a global view in conjunction with (local) convolutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b78">78]</ref>, most mimic squeeze-and-excitate <ref type="bibr" target="#b32">[32]</ref> for leveraging global features. Lastly, Lamb-daNetworks <ref type="bibr" target="#b2">[3]</ref> decomposes attention into an approximated content attention and a batch-amortized positional attention component.</p><p>Hybrid architectures combining CNNs and transformers blocks have also been used on ImageNet <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b69">69]</ref> and on COCO <ref type="bibr" target="#b10">[11]</ref>. Originally, transformers without convolutions were applied on pixels directly <ref type="bibr" target="#b47">[47]</ref>, even scaling to hundred of layers <ref type="bibr" target="#b12">[13]</ref>, but did not perform at CNNs levels. More recently, a transformer architecture working directly on small patches has obtained state of the art results on ImageNet <ref type="bibr" target="#b18">[19]</ref>. Nevertheless, the state of the art has since returned to CNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">48]</ref>. While some small improvements have been applied on the transformer architecture with encouraging results <ref type="bibr" target="#b74">[74]</ref>, their performance is below the one of DeiT <ref type="bibr" target="#b63">[63]</ref>, which uses a vanilla ViT architecture.</p><p>Encoder/decoder architectures. Transformers were originally introduced for machine translation <ref type="bibr" target="#b66">[66]</ref> with encoder-decoder models, and gained popularity as masked language model encoders (BERT) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">43]</ref>. They yielded impressive results as scaled up language models, e.g. GPT-2 and 3 <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b8">9]</ref>. They became a staple in speech recognition too <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b36">36]</ref>, being it in encoder and sequence criterion or encoder-decoder seq2seq <ref type="bibr" target="#b60">[60]</ref> conformations, and hold the state of the art to this day <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b77">77]</ref> with models 36 blocks deep. Note, transforming only the class token with frozen trunk embeddings in CaiT is reminiscent of nonautoregressive encoder-decoders <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b41">41]</ref>, where a whole sequence (we have only one prediction) is produced at once by iterative refinements.</p><p>Deeper architectures usually lead to better performance <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b61">61]</ref>, however this complicates their training process <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b59">59]</ref>. One must adapt the architecture and the optimization procedure to train them correctly. Some approaches focus on the initialization schemes <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b70">70]</ref>, others on multiple stages training <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b56">56]</ref>, multiple loss at different depth <ref type="bibr" target="#b61">[61]</ref>, adding components in the architecture <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b75">75]</ref> or regularization <ref type="bibr" target="#b33">[33]</ref>. As pointed in our paper, in that respect our LayerScale approach is more related to Rezero <ref type="bibr" target="#b1">[2]</ref> and Skipinit <ref type="bibr" target="#b15">[16]</ref>, Fixup <ref type="bibr" target="#b75">[75]</ref>, and T-Fixup <ref type="bibr" target="#b34">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have shown how train deeper transformer-based image classification neural networks when training on Imagenet only. We have also introduced the simple yet effective CaiT architecture designed in the spirit of encoder/decoder architectures. Our work further demonstrates that transformer models offer a competitive alternative to the best convolutional neural networks when considering trade-offs between accuracy and complexity.</p><p>We can see that the control training with fixed weights also converges, but it is only slightly better than the baseline with adjusted stochastic depth droprate d r . Nevertheless, the results are lower than those obtained with the learnable weighting factors. This suggests that the evolution of the parameters during training has a beneficial effect on the deepest models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Design of the class-attention stage</head><p>In this subsection we report some results obtained when considering alternative choices for the class-attention stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Not including class embedding in keys of class-attention.</head><p>In our approach we chose to insert the class embedding in the class-attention: By defining z = [x class , x patches ], (B.1)</p><p>we include x class in the keys and therefore the class-attention includes attention on the class embedding itself in Eqn. 6 and Eqn. 7. This is not a requirement as we could simply use a pure cross-attention between the class embedding and the set of frozen patches. If we do not include the class token in the keys of the class-attention layers, i.e., if we define z = x patches , we reach 83.31% (top-1 acc. on ImageNet1k-val) with CaiT-S-36, versus 83.44% for the choice adopted in our main paper. This difference of +0.13% is likely not significant, therefore either choice is reasonable. In order to be more consistent with the self-attention layer SA, in the sense that each query has its key counterpart, we have kept the class embedding in the keys of the CA layers as stated in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II</head><p>Remove LayerScale in Class-Attention. If we remove LayerScale in the Class-Attention blocks in the CaiT-S-36 model, we obtain a top-1 accuracy of 83.36% on ImageNet1k-val, versus 83.44% with LayerScale. The difference of +0.08% is not significant enough to conclude on a clear advantage. For the sake of consistency we have used LayerScale after all residual blocks of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distillation with class-attention</head><p>In the main paper we report results with the hard distillation proposed by Touvron et al. <ref type="bibr" target="#b63">[63]</ref>, which in essence replaces the label by the average of the label and the prediction of the teacher output. This is the choice we adopted in our main paper, since it provides better performance than traditional distillation. The DeiT authors also show the advantage of considering an additional "distillation token". In their case, employed with the ViT/DeiT architecture, this choice improves the performance compared to hard distillation. Noticeably it accelerates convergence.</p><p>In <ref type="table" target="#tab_11">Table B</ref>.1 we report the results obtained when inserting a distillation token at the same layer as the class token, i.e., on input of the class-attention stage. In our case we do not observe an advantage of this choice over hard distillation when using class-attention layers. Therefore in our paper we have only considered the hard distillation also employed by Touvron et al. <ref type="bibr" target="#b63">[63]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Normalization strategies for transformer blocks. (a) The ViT image classifier adopts pre-normalization like Child et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: In the ViT transformer (left), the class embedding (CLS) is inserted along with the patch embeddings. This choice is detrimental, as the same weights are used for two different purposes: helping the attention process, and preparing the vector to be fed to the classifier. We put this problem in evidence by showing that inserting CLS later improves performance (middle). In the CaiT architecture (right), we further propose to freeze the patch embeddings when inserting CLS to save compute, so that the last part of the network (typically 2 layers) is fully devoted to summarizing the information to be fed to the linear classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>We measure the impact of stochastic depth on ImageNet with a DeiT-S with LayerScale for different depths. The drop rate of stochastic depth needs to be adapted to the network depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Analysis of the contribution of the residual branches (Top: Self-attention ; Bottom: FFN) for a network comprising 36 layers, without (red) or with (blue) Layerscale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>We represent FLOPs and parameters for our best CaiT ↑ 384 and ↑ 448 Υ models trained with distillation. They are competitive on ImageNet-1k-val with the sota in the high accuracy regime, from XS-24 to M-48. Convolution-based neural networks like NFNets and EfficientNet are better in low-FLOPS and low-parameters regimes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Head 1 ↓ 2 ↓ 3 ↓ 4 ↓Figure 6 :</head><label>12346</label><figDesc>Head Head Head Visualization of the attention maps in the class-attention stage, obtained with a XXS model. For each image we present two rows: the top row correspond to the four heads of the attention maps associated with the first CA layer. The bottom row correspond to the four heads of the second CA layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Illustration of the regions of focus of a CaiT-XXS model, according to the response of the first class-attention layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Variations on CLS with Deit-Small (no LayerScale): we change the layer at which the class embedding is inserted. In ViT and DeiT, it is inserted at layer 0 jointly with the projected patches. We evaluate a late insertion of the class embedding, as well as our design choice to introduce specific class-attention layers. depth: SA+CA insertion layer top-1 acc. #params FLOPs</figDesc><table><row><cell></cell><cell cols="3">Baselines: DeiT-S and average pooling</cell><cell></cell></row><row><cell>12: 12 + 0</cell><cell>0</cell><cell>79.9</cell><cell>22M</cell><cell>4.6B</cell></row><row><cell>12: 12 + 0</cell><cell>n/a</cell><cell>80.3</cell><cell>22M</cell><cell>4.6B</cell></row><row><cell></cell><cell cols="3">Late insertion of class embedding</cell><cell></cell></row><row><cell>12: 12 + 0</cell><cell>2</cell><cell>80.0</cell><cell>22M</cell><cell>4.6B</cell></row><row><cell>12: 12 + 0</cell><cell>4</cell><cell>80.0</cell><cell>22M</cell><cell>4.6B</cell></row><row><cell>12: 12 + 0</cell><cell>8</cell><cell>80.0</cell><cell>22M</cell><cell>4.6B</cell></row><row><cell>12: 12 + 0</cell><cell>10</cell><cell>80.5</cell><cell>22M</cell><cell>4.6B</cell></row><row><cell>12: 12 + 0</cell><cell>11</cell><cell>80.3</cell><cell>22M</cell><cell>4.6B</cell></row><row><cell cols="4">DeiT-S with class-attention stage (SA+FFN)</cell><cell></cell></row><row><cell>12: 9 + 3</cell><cell>9</cell><cell>79.6</cell><cell>22M</cell><cell>3.6B</cell></row><row><cell>12: 10 + 2</cell><cell>10</cell><cell>80.3</cell><cell>22M</cell><cell>4.0B</cell></row><row><cell>12: 11 + 1</cell><cell>11</cell><cell>80.6</cell><cell>22M</cell><cell>4.3B</cell></row><row><cell>13: 12 + 1</cell><cell>12</cell><cell>80.8</cell><cell>24M</cell><cell>4.7B</cell></row><row><cell>14: 12 + 2</cell><cell>12</cell><cell>80.8</cell><cell>26M</cell><cell>4.7B</cell></row><row><cell>15: 12 + 3</cell><cell>12</cell><cell>80.6</cell><cell>27M</cell><cell>4.8B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>CaiT models: The design parameters are depth and d. The mem columns correspond to the memory usage. All models are initially trained at resolution 224 during 400 epochs. We also fine-tune these models at resolution 384 (identified by ↑384) or train them with distillation (Υ). The FLOPs are reported for each resolution.</figDesc><table><row><cell>CAIT</cell><cell>depth</cell><cell cols="8">d #params FLOPs (×10 9 ) Top-1 acc. (%): Imagenet1k-val</cell></row><row><cell>model</cell><cell>(SA+CA)</cell><cell></cell><cell>(×10 6 )</cell><cell cols="6">@224 @384 @224 ↑384 @224Υ ↑384Υ</cell></row><row><cell>XXS-24</cell><cell cols="2">24 + 2 192</cell><cell>12.0</cell><cell>2.5</cell><cell>9.5</cell><cell>77.6</cell><cell>80.4</cell><cell>78.4</cell><cell>80.9</cell></row><row><cell>XXS-36</cell><cell cols="2">36 + 2 192</cell><cell>17.3</cell><cell>3.8</cell><cell>14.2</cell><cell>79.1</cell><cell>81.8</cell><cell>79.7</cell><cell>82.2</cell></row><row><cell>XS-24</cell><cell cols="2">24 + 2 288</cell><cell>26.6</cell><cell>5.4</cell><cell>19.3</cell><cell>81.8</cell><cell>83.8</cell><cell>82.0</cell><cell>84.1</cell></row><row><cell>XS-36</cell><cell cols="2">36 + 2 288</cell><cell>38.6</cell><cell>8.1</cell><cell>28.8</cell><cell>82.6</cell><cell>84.3</cell><cell>82.9</cell><cell>84.8</cell></row><row><cell>S-24</cell><cell cols="2">24 + 2 384</cell><cell>46.9</cell><cell>9.4</cell><cell>32.2</cell><cell>82.7</cell><cell>84.3</cell><cell>83.5</cell><cell>85.1</cell></row><row><cell>S-36</cell><cell cols="2">36 + 2 384</cell><cell>68.2</cell><cell>13.9</cell><cell>48.0</cell><cell>83.3</cell><cell>85.0</cell><cell>84.0</cell><cell>85.4</cell></row><row><cell>S-48</cell><cell cols="2">48 + 2 384</cell><cell>89.5</cell><cell>18.6</cell><cell>63.8</cell><cell>83.5</cell><cell>85.1</cell><cell>83.9</cell><cell>85.3</cell></row><row><cell>M-24</cell><cell cols="3">24 + 2 768 185.9</cell><cell cols="2">36.0 116.1</cell><cell>83.4</cell><cell>84.5</cell><cell>84.7</cell><cell>85.8</cell></row><row><cell>M-36</cell><cell cols="3">36 + 2 768 270.9</cell><cell cols="2">53.7 173.3</cell><cell>83.8</cell><cell>84.9</cell><cell>85.1</cell><cell>86.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>M-48</cell></row></table><note>Hyper-parameters for training CaiT models: The only parameters that we ad- just per model are the drop rate dr of stochastic depth and the LayerScale initialization ε.CAIT model XXS-24 XXS-36 XS-24 XS-36 S-24 S-36 S-48 M-24 M-36−6 10 −5 10 −6 10 −6 10 −5 10 −6 10 −6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Datasets used for our different tasks.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Train size Test size #classes</cell></row><row><cell>ImageNet [54]</cell><cell>1,281,167</cell><cell>50,000</cell><cell>1000</cell></row><row><cell>iNaturalist 2018</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>for adapting stochastic depth before adding LayerScale. †: training failed.</figDesc><table><row><cell>Improvement</cell><cell cols="3">top-1 acc. #params FLOPs</cell></row><row><cell>DeiT-S [d=384,300 epochs]</cell><cell>79.9</cell><cell>22M</cell><cell>4.6B</cell></row><row><cell>+ More heads [8]</cell><cell>80.0</cell><cell>22M</cell><cell>4.6B</cell></row><row><cell>+ Talking-heads</cell><cell>80.5</cell><cell>22M</cell><cell>4.6B</cell></row><row><cell>+ Depth [36 blocks]</cell><cell>69.9 †</cell><cell>64M</cell><cell>13.8B</cell></row><row><cell>+ Layer-scale [init ε = 10 −6 ]</cell><cell>80.5</cell><cell>64M</cell><cell>13.8B</cell></row><row><cell>+ Stch depth. adaptation [dr=0.2]</cell><cell>83.0</cell><cell>64M</cell><cell>13.8B</cell></row><row><cell>+ CaiT architecture [specialized class-attention layers]</cell><cell>83.2</cell><cell>68M</cell><cell>13.9B</cell></row><row><cell>+ Longer training [400 epochs]</cell><cell>83.4</cell><cell>68M</cell><cell>13.9B</cell></row><row><cell>+ Inference at higher resolution [256]</cell><cell>83.8</cell><cell>68M</cell><cell>18.6B</cell></row><row><cell>+ Fine-tuning at higher resolution [384]</cell><cell>84.8</cell><cell>68M</cell><cell>48.0B</cell></row><row><cell>+ Hard distillation [teacher: RegNetY-16GF]</cell><cell>85.2</cell><cell>68M</cell><cell>48.0B</cell></row><row><cell>+ Adjust crop ratio [0.875 → 1.0]</cell><cell>85.4</cell><cell>68M</cell><cell>48.0B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Deit-Small: for a fixed 384 working dimensionality and number of parameters, impact of the number of heads on the accuracy and throughput (images processed per second at inference time on a singe V100 GPU).</figDesc><table><row><cell cols="5"># heads dim/head throughput (im/s) GFLOPs top-1 acc.</cell></row><row><cell>1</cell><cell>384</cell><cell>1079</cell><cell>4.6</cell><cell>76.80</cell></row><row><cell>2</cell><cell>192</cell><cell>1056</cell><cell>4.6</cell><cell>78.06</cell></row><row><cell>3</cell><cell>128</cell><cell>1043</cell><cell>4.6</cell><cell>79.35</cell></row><row><cell>6</cell><cell>64</cell><cell>989</cell><cell>4.6</cell><cell>79.90</cell></row><row><cell>8</cell><cell>48</cell><cell>971</cell><cell>4.6</cell><cell>80.02</cell></row><row><cell>12</cell><cell>32</cell><cell>927</cell><cell>4.6</cell><cell>80.08</cell></row><row><cell>16</cell><cell>24</cell><cell>860</cell><cell>4.6</cell><cell>80.04</cell></row><row><cell>24</cell><cell>16</cell><cell>763</cell><cell>4.6</cell><cell>79.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>We compare performance with the defaut crop-ratio of 0.875 usually used with convnets, and the crop-ratio of 1.0<ref type="bibr" target="#b68">[68]</ref> that we adopt for CaiT.</figDesc><table><row><cell>Network</cell><cell cols="3">Crop Ratio ImNet Real</cell><cell>V2</cell></row><row><cell></cell><cell>0.875 1.0</cell><cell>top-1</cell><cell cols="2">top-1 top-1</cell></row><row><cell>S36</cell><cell></cell><cell>83.4 83.3</cell><cell>88.1 88.0</cell><cell>73.0 72.5</cell></row><row><cell>S36↑384</cell><cell></cell><cell>84.8 85.0</cell><cell>88.9 89.2</cell><cell>74.7 75.0</cell></row><row><cell>S36Υ</cell><cell></cell><cell>83.7 84.0</cell><cell>88.9 88.9</cell><cell>74.1 74.1</cell></row><row><cell>M36Υ</cell><cell></cell><cell>84.8 84.9</cell><cell>89.2 89.2</cell><cell>74.9 75.0</cell></row><row><cell>S36↑384Υ</cell><cell></cell><cell>85.2 85.4</cell><cell>89.7 89.8</cell><cell>75.7 76.2</cell></row><row><cell>M36↑384Υ</cell><cell></cell><cell>85.9 86.1</cell><cell>89.9 90.0</cell><cell>76.1 76.3</cell></row><row><cell cols="5">our model M36↑384Υ increases to 86.1% top-1 accuracy on Imagenet-val1k.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table B .</head><label>B</label><figDesc>1: CaiT models with and without distillation token. All these models are trained with the same setting during 400 epochs.</figDesc><table><row><cell></cell><cell cols="2">Distillation token</cell></row><row><cell>Model</cell><cell></cell><cell></cell></row><row><cell cols="2">XXS-24Υ 78.4</cell><cell>78.5</cell></row><row><cell>M-24Υ</cell><cell>84.8</cell><cell>84.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://pypi.org/project/fairscale/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebookresearch/deit 4 Bachlechner et al. report that batchnorm is complementary to ReZero, while removing Layer-Norm in the case of transformers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>Thanks to Jakob Verbeek for his detailled feedback on an earlier version of this paper, to Alaa El-Nouby for fruitful discussions, to Mathilde Caron for suggestions regarding the vizualizations, and to Ross Wightman for the Timm library and the insights that he shares with the community.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this supplemental material, we provide variations on the architecture presented in our main paper. These experiments have guided some of our choices for the design of class-attention layers and LayerScale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Variations on LayerScale init</head><p>For the sake of simplicity and to avoid overfitting per model, we have chosen to do a constant initialization with small values depending on the model depth. In order to give additional insight on the importance of this initialization we compare in <ref type="table">Table A</ref>.1 other possible choices.</p><p>LayerScale with 0 init. We initialize all coefficients of LayerScale to 0. This resembles Rezero, but in this case we have distinct learnable parameters for each channel. We make two observations. First, this choice, which also starts with residual branches that output 0 the beginning of the training, gives a clear boost compared to the block-wise scaling done by our adapted ReZero. This confirms the advantage of introducing a learnable parameter per channel and not only per residual layer. Second, LayerScale is better: it is best to initialize to a small ε different from zero.</p><p>Random init. We have tested a version in which we try a different initial weight per channel, but with the same average contribution of each residual block as in LayerScale. For this purpose we initialize the channel-scaling values with the Uniform law (U[0, 2ε]). This simple choice choice ensures that the expectation of the scaling factor is equal to the value of the classical initialization of LayerScale. This choice is overall comparable to the initialization to 0 of the diagonal, and inferior to LayerScale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-training.</head><p>LayerScale makes it possible to get increased performance by training deeper models. At the end of training we obtain a specific set of scaling factors for each layer. Inspired by the lottery ticket hypothesis <ref type="bibr" target="#b23">[23]</ref>, one question that arises is whether what matters is to have the right scaling factors, or to include these learnable weights in the optimization procedure. In other terms, what happens if we re-train the network with the scaling factors obtained by a previous training?</p><p>In this experiment below, we try to empirically answer that question. We compare the performance (top-1 validation accuracy, %) on ImageNet-1k with DeiT-S architectures of differents depths. Everything being identical otherwise, I <ref type="table">Table A</ref>.1: Performance when increasing the depth. We compare different strategies and report the top-1 accuracy (%) on ImageNet-1k for the DeiT training (Baseline) with and without adapting the stochastic depth rate dr (uniform drop-rate), and a modified version of Rezero with LayerNorm and warmup. We compare different initialisation of the diagonal matrix for LayerScale. We also report results with 0 initialization, Uniform initialisation and small constant initialisation. Except for the baseline dr = 0.1, we have adapted the stochastic depth rate dr. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodhisattwa</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Prasad Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04887</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rezero is all you need: Fast convergence at large depth</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lambdanetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multigrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Characterizing signal propagation to close the performance gap in unnormalized resnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel L</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08692</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno>2020. 21</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno>2020. 21</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Practical automated data augmentation with a reduced search space</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<title level="m">Batch normalization biases residual blocks towards the identity function in deep networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<imprint>
			<pubPlace>Georg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Training vision transformers for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05644</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11556</idno>
		<idno>ICLR 2020. 8</idno>
		<title level="m">Reducing transformer depth on demand with structured dropout</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Training with quantization noise for extreme model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07320</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02281</idno>
		<title level="m">Nonautoregressive neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112,2021.14</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06-01" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The inaturalist challenge 2018 dataset</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The inaturalist challenge 2019 dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="1921" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving transformer optimization through better initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Shi Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note>PMLR, 2020. 2, 4</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A comparative study on transformer vs rnn in speech applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06317</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>2013. 15</idno>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">15</biblScope>
		</imprint>
		<respStmt>
			<orgName>CIFAR</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deterministic nonautoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06901</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Selective kernel networks. Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Fixing weight decay regularization in adam</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Rwth asr systems for librispeech: Hybrid vs attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10580,2020.21</idno>
		<title level="m">Meta pseudo labels</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurips</title>
		<imprint>
			<date type="published" when="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge. International journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02436</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Talkingheads attention. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605,2021.21</idno>
		<title level="m">Jonathon Shlens, P. Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">End-to-end asr: from supervised to semi-supervised learning with modern architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08460</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>III</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Fixing the train-test resolution discrepancy. Neurips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08237,2020.14</idno>
		<title level="m">Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution discrepancy: Fixefficientnet</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677,2020.21</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Dynamical isometry and a mean field theory of cnns: How to train 10, 000-layer vanilla convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05393</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Adversarial examples improve image recognition. Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Self-training and pre-training are complementary for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paden</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11430</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09321</idno>
		<title level="m">Fixup initialization: Residual learning without normalization</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yu E Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955,2020.21</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Pushing the limits of semi-supervised learning for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10504</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno>2020. 21</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
