<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Mention-Ranking Model for Abstract Anaphora Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Marasović</surname></persName>
							<email>marasovic@cl.uni-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="laboratory">Research Training Group AIPHES</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Born</surname></persName>
							<email>born@cl.uni-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="laboratory">Research Training Group AIPHES</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Opitz</surname></persName>
							<email>opitz@cl.uni-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="laboratory">Research Training Group AIPHES</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Frank</surname></persName>
							<email>frank@cl.uni-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="laboratory">Research Training Group AIPHES</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Mention-Ranking Model for Abstract Anaphora Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Resolving abstract anaphora is an important, but difficult task for text understanding. Yet, with recent advances in representation learning this task becomes a more tangible aim. A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent. We propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese Net. We overcome the lack of training data by generating artificial anaphoric sentenceantecedent pairs. Our model outperforms state-of-the-art results on shell noun resolution. We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus. This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders. We found model variants that outperform the baselines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors. Our model selects syntactically plausible candidates and -if disregarding syntax -discriminates candidates using deeper features.</p><p>Abstract anaphora has been extensively studied in linguistics and shown to exhibit specific properties in terms of semantic antecedent types, their degrees of abstractness, and general dis-2 Abadi et al. (2015)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real † Leo Born, Juri Opitz and Anette Frank contributed equally to this work. world, which is arguably the most frequently occurring type. Distinct from these are diverse types of abstract anaphora (AA) <ref type="bibr" target="#b2">(Asher, 1993)</ref> where reference is made to propositions, facts, events or properties. An example is given in (1) below. <ref type="bibr">1</ref> While recent approaches address the resolution of selected abstract shell nouns <ref type="bibr" target="#b23">(Kolhatkar and Hirst, 2014)</ref>, we aim to resolve a wide range of abstract anaphors, such as the NP this trend in (1), as well as pronominal anaphors (this, that, or it).</p><p>Henceforth, we refer to a sentence that contains an abstract anaphor as the anaphoric sentence (AnaphS), and to a constituent that the anaphor refers to as the antecedent (Antec) (cf. <ref type="formula" target="#formula_0">(1)</ref>).</p><p>(1) Ever-more powerful desktop computers, designed with one or more microprocessors as their "brains", are expected to increasingly take on functions carried out by more expensive minicomputers and mainframes. "[Antec The guys that make traditional hardware are really being obsoleted by microprocessor-based machines]", said Mr. Benton. [ AnaphS As a result of this trendAA, longtime powerhouses HP, IBM and Digital Equipment Corp. are scrambling to counterattack with microprocessor-based systems of their own.] A major obstacle for solving this task is the lack of sufficient amounts of annotated training data. We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types. This enables us to use neural methods which have shown great success in related tasks: coreference resolution <ref type="bibr" target="#b8">(Clark and Manning, 2016a)</ref>, textual entailment <ref type="bibr" target="#b4">(Bowman et al., 2016)</ref>, learning textual similarity <ref type="bibr" target="#b29">(Mueller and Thyagarajan, 2016)</ref>, and discourse relation sense classification <ref type="bibr" target="#b36">(Rutherford et al., 2017)</ref>.</p><p>Our model is inspired by the mention-ranking model for coreference resolution <ref type="bibr" target="#b43">(Wiseman et al., 2015;</ref><ref type="bibr">Manning, 2015, 2016a,b)</ref> and combines it with a Siamese Net <ref type="bibr" target="#b29">(Mueller and Thyagarajan, 2016)</ref>, <ref type="bibr" target="#b31">(Neculoiu et al., 2016)</ref> for learning similarity between sentences. Given an anaphoric sentence (AntecS in (1)) and a candidate antecedent (any constituent in a given context, e.g. being obsoleted by microprocessor-based machines in (1)), the LSTM-Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space. These representations are combined into a joint representation used to calculate a score that characterizes the relation between them. The learned score is used to select the highest-scoring antecedent candidate for the given anaphoric sentence and hence its anaphor. We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphorsimilar to the encoding proposed by <ref type="bibr" target="#b44">Zhou and Xu (2015)</ref> for individuating multiply occurring predicates in SRL. With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent. <ref type="figure" target="#fig_0">Fig. 1</ref> displays our architecture.</p><p>In contrast to other work, our method for generating training data is not confined to specific types of anaphora such as shell nouns <ref type="bibr" target="#b23">(Kolhatkar and Hirst, 2014)</ref> or anaphoric connectives <ref type="bibr" target="#b38">(Stede and Grishina, 2016)</ref>. It produces large amounts of instances and is easily adaptable to other languages. This enables us to build a robust, knowledge-lean model for abstract anaphora resolution that easily extends to multiple languages. We evaluate our model on the shell noun resolution dataset of <ref type="bibr" target="#b25">Kolhatkar et al. (2013b)</ref> and show that it outperforms their state-of-the-art results. Moreover, we report results of the model (trained on our newly constructed dataset) on unrestricted abstract anaphora instances from the ARRAU corpus <ref type="bibr" target="#b34">(Poesio and Artstein, 2008;</ref><ref type="bibr" target="#b41">Uryupina et al., 2016)</ref>. To our knowledge this provides the first state-of-the-art benchmark on this data subset.</p><p>Our TensorFlow 2 implementation of the model and scripts for data extraction are available at: https://github.com/amarasovic/ neural-abstract-anaphora.</p><p>course properties <ref type="bibr" target="#b2">(Asher, 1993;</ref><ref type="bibr" target="#b42">Webber, 1991)</ref>. In contrast to nominal anaphora, abstract anaphora is difficult to resolve, given that agreement and lexical match features are not applicable. Annotation of abstract anaphora is also difficult for humans <ref type="bibr" target="#b12">(Dipper and Zinsmeister, 2012)</ref>, and thus, only few smaller-scale corpora have been constructed. We evaluate our models on a subset of the AR-RAU corpus <ref type="bibr" target="#b41">(Uryupina et al., 2016)</ref> that contains abstract anaphors and the shell noun corpus used in <ref type="bibr" target="#b25">Kolhatkar et al. (2013b)</ref>. <ref type="bibr">3</ref> We are not aware of other freely available abstract anaphora datasets.</p><p>Little work exists for the automatic resolution of abstract anaphora. Early work <ref type="bibr" target="#b13">(Eckert and Strube, 2000;</ref><ref type="bibr" target="#b39">Strube and Müller, 2003;</ref><ref type="bibr" target="#b5">Byron, 2004;</ref><ref type="bibr" target="#b30">Müller, 2008)</ref> has focused on spoken language, which exhibits specific properties. Recently, event coreference has been addressed using feature-based classifiers <ref type="bibr" target="#b18">(Jauhar et al., 2015;</ref><ref type="bibr" target="#b26">Lu and Ng, 2016)</ref>. Event coreference is restricted to a subclass of events, and usually focuses on coreference between verb (phrase) and noun (phrase) mentions of similar abstractness levels (e.g. purchase -acquire) with no special focus on (pro)nominal anaphora. Abstract anaphora typically involves a full-fledged clausal antecedent that is referred to by a highly abstract (pro)nominal anaphor, as in (1). <ref type="bibr" target="#b35">Rajagopal et al. (2016)</ref> proposed a model for resolution of events in biomedical text that refer to a single or multiple clauses. However, instead of selecting the correct antecedent clause(s) (our task) for a given event, their model is restricted to classifying the event into six abstract categories: this these changes, responses, analysis, context, finding, observation, based on its surrounding context. While related, their task is not comparable to the full-fledged abstract anaphora resolution task, since the events to be classified are known to be coreferent and chosen from a set of restricted abstract types.</p><p>More related to our work is <ref type="bibr" target="#b1">Anand and Hardt (2016)</ref> who present an antecedent ranking account for sluicing using classical machine learning based on a small training dataset. They employ features modeling distance, containment, discourse structure, and -less effectively -content and lexical correlates. <ref type="bibr">4</ref> Closest to our work is <ref type="bibr" target="#b25">Kolhatkar et al. (2013b)</ref> (KZH13) and <ref type="bibr" target="#b23">Kolhatkar and Hirst (2014)</ref> (KH14) on shell noun resolution, using classical machine learning techniques. Shell nouns are abstract nouns, such as fact, possibility, or issue, which can only be interpreted jointly with their shell content (their embedded clause as in (2) or antecedent as in <ref type="formula" target="#formula_0">(3)</ref>). KZH13 refer to shell nouns whose antecedent occurs in the prior discourse as anaphoric shell nouns (ASNs) (cf. (3)), and cataphoric shell nouns (CSNs) otherwise (cf. (2)). 5</p><p>(2) Congress has focused almost solely on the fact that [special education is expensive -and that it takes away money from regular education.]</p><p>(3 KZH13 presented an approach for resolving six typical shell nouns following the observation that CSNs are easy to resolve based on their syntactic structure alone, and the assumption that ASNs share linguistic properties with their embedded (CSN) counterparts. They manually developed rules to identify the embedded clause (i.e. cataphoric antecedent) of CSNs and trained SVM rank <ref type="bibr" target="#b19">(Joachims, 2002)</ref> on such instances. The trained SVM rank model is then used to resolve ASNs. KH14 generalized their method to be able to create training data for any given shell noun, however, their method heavily exploits the specific properties of shell nouns and does not apply to other types of abstract anaphora. <ref type="bibr" target="#b38">Stede and Grishina (2016)</ref> study a related phenomenon for German. They examine inherently anaphoric connectives (such as demzufolge -according to which) that could be used to access their abstract antecedent in the immediate context. Yet, such connectives are restricted in type, and the study shows that such connectives are often ambiguous with nominal anaphors and require sense disambiguation. We conclude that they cannot be easily used to acquire antecedents automatically.</p><p>In our work, we explore a different direction: we construct artificial training data using a general pattern that identifies embedded sentence constituents, which allows us to extract relatively secure training data for abstract anaphora that captures a wide range of anaphora-antecedent rela-tions, and apply this data to train a model for the resolution of unconstrained abstract anaphora.</p><p>Recent work in entity coreference resolution has proposed powerful neural network-based models that we will adapt to the task of abstract anaphora resolution. Most relevant for our task is the mention-ranking neural coreference model proposed in <ref type="bibr" target="#b7">Clark and Manning (2015)</ref>, and their improved model in <ref type="bibr" target="#b8">Clark and Manning (2016a)</ref>, which integrates a loss function <ref type="bibr" target="#b43">(Wiseman et al., 2015)</ref> which learns distinct feature representations for anaphoricity detection and antecedent ranking.</p><p>Siamese Nets distinguish between similar and dissimilar pairs of samples by optimizing a loss over the metric induced by the representations. It is widely used in vision <ref type="bibr" target="#b6">(Chopra et al., 2005)</ref>, and in NLP for semantic similarity, entailment, query normalization and QA <ref type="bibr" target="#b29">(Mueller and Thyagarajan, 2016;</ref><ref type="bibr" target="#b31">Neculoiu et al., 2016;</ref><ref type="bibr" target="#b11">Das et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mention-Ranking Model</head><p>Given an anaphoric sentence s with a marked anaphor (mention) and a candidate antecedent c, the mention-ranking (MR) model assigns the pair (c, s) a score, using representations produced by an LSTM-Siamese Net. The highest-scoring candidate is assigned to the marked anaphor in the anaphoric sentence. <ref type="figure" target="#fig_0">Fig. 1</ref> displays the model.</p><p>We learn representations of an anaphoric sentence s and a candidate antecedent c using a bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; <ref type="bibr" target="#b14">Graves and Schmidhuber, 2005)</ref>. One bi-LSTM is applied to the anaphoric sentence s and a candidate antecedent c, hence the term siamese. Each word is represented with a vector w i constructed by concatenating embeddings of the word, of the context of the anaphor (average of embeddings of the anaphoric phrase, the previous and the next word), of the head of the anaphoric phrase 6 , and, finally, an embedding of the constituent tag of the candidate, or the S constituent tag if the word is in the anaphoric sentence. For each sequence s or c, the word vectors w i are sequentially fed into the bi-LSTM, which produces outputs from the forward pass, − → h i , and outputs ← − h i from the backward pass. The final output of the i-th word is defined as</p><formula xml:id="formula_1">h i = [ ← − h i ; − → h i ].</formula><p>To get a representation of the full sequence, h s or h c , all outputs are averaged, except for those that correspond to padding tokens. To prevent forgetting the constituent tag of the sequence, we concatenate the corresponding tag embedding with h s or h c (we call this a shortcut for the tag information). The resulting vector is fed into a feed-forward layer of exponential linear units (ELUs) <ref type="bibr" target="#b10">(Clevert et al., 2016)</ref> to produce the final representationh s orh c of the sequence.</p><p>Fromh c andh s we compute a vector h c,s = [|h c −h s |;h c h s ] <ref type="bibr" target="#b40">(Tai et al., 2015)</ref>, where |-| denotes the absolute values of the element-wise subtraction, and the element-wise multiplication. Then h c,s is fed into a feed-forward layer of ELUs to obtain the final joint representation,h c,s , of the pair (c, s). Finally, we compute the score for the pair (c, s) that represents relatedness between them, by applying a single fully connected linear layer to the joint representation:</p><formula xml:id="formula_2">score(c, s) = Wh c,s + b ∈ R,<label>(1)</label></formula><p>where W is a 1 × d weight matrix, and d the dimension of the vectorh c,s . We train the described mention-ranking model with the max-margin training objective from <ref type="bibr" target="#b43">Wiseman et al. (2015)</ref>, used for the antecedent ranking subtask. Suppose that the training set</p><formula xml:id="formula_3">D = {(a i , s i , T (a i ), N (a i )} n i=1</formula><p>, where a i is the i-th abstract anaphor, s i the corresponding anaphoric sentence, T (a i ) the set of antecedents of a i and N (a i ) the set of candidates that are not antecedents (negative candidates). Lett i = arg max t∈T (a i ) score(t i , s i ) be the highest scor-</p><formula xml:id="formula_4">VP v</formula><p>S'</p><p>x S <ref type="figure">Figure 2</ref>: A general pattern for artificially creating anaphoric sentence-antecedent pairs.</p><p>ing antecedent of a i . Then the loss is given by</p><formula xml:id="formula_5">n i=1 max(0, max c∈N (a i ) {1+score(c, s i )−score(t i , s i )}).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training data construction</head><p>We create large-scale training data for abstract anaphora resolution by exploiting a common construction, consisting of a verb with an embedded sentence (complement or adverbial) (cf. <ref type="figure">Fig.  2</ref>). We detect this pattern in a parsed corpus, 'cut off' the S constituent and replace it with a suitable anaphor to create the anaphoric sentence (AnaphS), while S yields the antecedent (Antec). This method covers a wide range of anaphoraantecedent constellations, due to diverse semantic or discourse relations that hold between the clause hosting the verb and the embedded sentence.</p><p>First, the pattern applies to verbs that embed sentential arguments. In (4), the verb doubt establishes a specific semantic relation between the embedding sentence and its sentential complement.</p><p>(4) He doubts [ S [S a Bismarckian super state will emerge that would dominate Europe], but warns of "a risk of profound change in the [..] European Community from a Germany that is too strong, even if democratic"].</p><p>From this we extract the artificial antecedent A Bismarckian super state will emerge that would dominate Europe, and its corresponding anaphoric sentence He doubts this, but warns of "a risk of profound change ... even if democratic", which we construct by randomly choosing one of a predefined set of appropriate anaphors (here: this, that, it), cf. <ref type="table">Table 1</ref>. The second row in <ref type="table">Table 1</ref> is used when the head of S is filled by an overt complementizer (doubts that), as opposed to (4). The remaining rows in <ref type="table">Table 1</ref> apply to adverbial clauses of different types.</p><p>Adverbial clauses encode specific discourse relations with their embedding sentences, often indicated by their conjunctions. In (5), for example, the causal conjunction as relates a cause (embedded sentence) and its effect (embedding sentence):  <ref type="table">Table 1</ref>: S -heads and the anaphoric types and phrases they induce (most frequent interpretation).</p><p>(5) There is speculation that property casualty firms will sell even more munis [ S as [S they scramble to raise cash to pay claims related to Hurricane Hugo [..] ]].</p><p>We randomly replace causal conjunctions because, as with appropriately adjusted anaphors, e.g. because of that, due to this or therefore that make the causal relation explicit in the anaphor. 7 Compared to the shell noun corpus of KZH13, who made use of a carefully constructed set of extraction patterns, a downside of our method is that our artificially created antecedents are uniformly of type S. However, the majority of abstract anaphora antecedents found in the existing datasets are of type S. Also, our models are intended to induce semantic representations, and so we expect syntactic form to be less critical, compared to a feature-based model. <ref type="bibr">8</ref> Finally, the general extraction pattern in <ref type="figure">Fig. 2</ref>, covers a much wider range of anaphoric types.</p><p>Using this method we generated a dataset of artificial anaphoric sentence-antecedent pairs from the WSJ part of the PTB Corpus <ref type="bibr" target="#b28">(Marcus et al., 1993)</ref>, automatically parsed using the Stanford Parser <ref type="bibr" target="#b22">(Klein and Manning, 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setup 5.1 Datasets</head><p>We evaluate our model on two types of anaphora: (a) shell noun anaphora and (b) (pro)nominal abstract anaphors extracted from ARRAU. a. Shell noun resolution dataset. For comparability we train and evaluate our model for shell noun resolution, using the original training (CSN) and test (ASN) corpus of <ref type="bibr">Kolhatkar et al. (2013a,b). 9</ref> We follow the data preparation and evaluation protocol of <ref type="bibr" target="#b25">Kolhatkar et al. (2013b)</ref> </p><formula xml:id="formula_6">(KZH13).</formula><p>The CSN corpus was constructed from the NYT corpus using manually developed patterns to identify the antecedent of cataphoric shell nouns (CSNs). In KZH13, all syntactic constituents of the sentence that contains both the CSN and its antecedent were considered as candidates for training a ranking model. Candidates that differ from the antecedent in only one word or one word and punctuation were as well considered as antecedents 10 . To all other candidates we refer to as negative candidates. For every shell noun, KZH13 used the corresponding part of the CSN data to train SVM rank .</p><p>The ASN corpus serves as the test corpus. It was also constructed from the NYT corpus, by selecting anaphoric instances with the pattern "this shell noun " for all covered shell nouns. For validation, <ref type="bibr" target="#b24">Kolhatkar et al. (2013a)</ref> crowdsourced annotations for the sentence which contains the antecedent, which KZH13 refer to as a broad region. Candidates for the antecedent were obtained by using all syntactic constituents of the broad region as candidates and ranking them using the SVM rank model trained on the CSN corpus. The top 10 ranked candidates were presented to the crowd workers and they chose the best answer that represents the ASN antecedent. The workers were encouraged to select None when they did not agree with any of the displayed answers and could provide information about how satisfied they were with the displayed candidates. We consider this dataset as gold, as do KZH13, although it may be biased towards the offered candidates. 11 b. Abstract anaphora resolution data set. We use the automatically constructed data from the WSJ corpus (Section 4) for training. <ref type="bibr">12</ref> Our test data for unrestricted abstract anaphora resolution is obtained from the ARRAU corpus <ref type="bibr" target="#b41">(Uryupina et al., 2016)</ref>. We extracted all abstract anaphoric instances from the WSJ part of ARRAU that are marked with the category abstract or plan, 13 and call the subcorpus ARRAU-AA.  Candidates extraction. Following KZH13, for every anaphor we create a list of candidates by extracting all syntactic constituents from sentences which contain antecedents. Candidates that differ from antecedents in only one word, or one word and punctuation, were as well considered as antecedents. Constituents that are not antecedents are considered as negative candidates.</p><p>Data statistics. <ref type="table" target="#tab_3">Table 2</ref> gives statistics of the datasets: the number of anaphors (row 1), the median length (in tokens) of antecedents (row 2), the median length (in tokens) for all anaphoric sentences (row 3), the median of the number of antecedents and candidates that are not antecedents (negatives) (rows 4-5), the number of pronominal and nominal anaphors (rows 6-7). Both training sets, artificial and CSN, have only one possible antecedent for which we accept two minimal variants differing in only one word or one word and punctuation. On the contrary, both test sets by design allow annotation of more than one antecedent that differ in more than one word. Every anaphor in the artificial training dataset is pronominal, whereas anaphors in CSN and ASN are nominal only. ARRAU-AA has a mixture of nominal and pronominal anaphors.</p><p>Data pre-processing. Other details can be found in Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines and evaluation metrics</head><p>Following KZH13, we report success@n (s@n), which measures whether the antecedent, or a candidate that differs in one word 14 , is in the first n ranked candidates, for n ∈ {1, 2, 3, 4}. Additionally, we report the preceding sentence baseline (PS BL ) that chooses the previous sentence for the antecedent and TAGbaseline (TAG BL ) that randomly chooses a candidate with the constituent tag label in {S, VP, ROOT, SBAR}. For TAG BL we report the average of 10 runs with 10 fixed seeds. PS BL always performs worse than the KZH13 model on the ASN, so we report it only for ARRAU-AA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training details for our models</head><p>Hyperparameters tuning. We recorded performance with manually chosen HPs and then tuned HPs with Tree-structured Parzen Estimators (TPE) <ref type="bibr">(Bergstra et al., 2011) 15</ref> . TPE chooses HPs for the next (out of 10) trails on the basis of the s@1 score on the devset. As devsets we employ the ARRAU-AA corpus for shell noun resolution and the ASN corpus for unrestricted abstract anaphora resolution. For each trial we record performance on the test set. We report the best test s@1 score in 10 trials if it is better than the scores from default HPs. The default HPs and prior distributions for HPs used by TPE are given below. The (exact) HPs we used can be found in Supplementary Materials. Input representation. To construct word vectors w i as defined in Section 3, we used 100-dim. GloVe word embeddings pre-trained on the Gigaword and Wikipedia <ref type="bibr" target="#b33">(Pennington et al., 2014)</ref>, and did not fine-tune them. Vocabulary was built from the words in the training data with frequency in {3, U(1, 10)}, and OOV words were replaced with an UNK token. Embeddings for tags are initialized with values drawn from the uniform distribution U − 1 √ d+t , 1 √ d+t , where t is the number of tags 16 and d ∈ {50, qlog-U(30, 100)} the size of the tag embeddings. <ref type="bibr">17</ref> We experimented with removing embeddings for tag, anaphor and context.</p><p>Weights initialization. The size of the LSTMs hidden states was set to {100, qlog-U(30, 150)}. We initialized the weight matrices of the LSTMs with random orthogonal matrices <ref type="bibr" target="#b16">(Henaff et al., 2016)</ref>, all other weight matrices with the initialization proposed in <ref type="bibr" target="#b15">He et al. (2015)</ref>. The first feed-forward layer size is set to a value in {400, qlog-U(200, 800)}, the second to a value in {1024, qlog-U(400, 2000)}. Forget biases in the LSTM were initialized with 1s <ref type="bibr" target="#b20">(Józefowicz et al., 2015)</ref>, all other biases with 0s.  Optimization. We trained our model in minibatches using Adam <ref type="bibr" target="#b21">(Kingma and Ba, 2015)</ref> with the learning rate of 10 −4 and maximal batch size 64. We clip gradients by global norm <ref type="bibr" target="#b32">(Pascanu et al., 2013)</ref>, with a clipping value in {1.0, U(1, 100)}. We train for 10 epochs and choose the model that performs best on the devset.</p><p>Regularization. We used the l 2 -regularization with λ ∈ {10 −5 , log-U(10 −7 , 10 −2 )}. Dropout <ref type="bibr" target="#b37">(Srivastava et al., 2014)</ref> with a keep probability k p ∈ {0.8, U(0.5, 1.0)} was applied to the outputs of the LSTMs, both feed-forward layers and optionally to the input with k p ∈ U(0.8, 1.0).</p><p>6 Results and analysis 6.1 Results on shell noun resolution dataset <ref type="table" target="#tab_5">Table 3</ref> provides the results of the mentionranking model (MR-LSTM) on the ASN corpus using default HPs. Column 2 states which model produced the results: KZH13 refers to the best reported results in <ref type="bibr" target="#b25">Kolhatkar et al. (2013b)</ref> and TAG BL is the baseline described in Section 5.2.</p><p>In terms of s@1 score, MR-LSTM outperforms both KZH13's results and TAG BL without even necessitating HP tuning. For the outlier reason we tuned HPs (on ARRAU-AA) for different variants of the architecture: the full architecture, without embedding of the context of the anaphor (ctx), of the anaphor (aa), of both constituent tag em-  bedding and shortcut (tag,cut), dropping only the shortcut (cut), using only word embeddings as input (ctx,aa,tag,cut), without the first (ffl1) and second (ffl2) layer. From <ref type="table" target="#tab_7">Table 4</ref> we observe: (1) with HPs tuned on ARRAU-AA, we obtain results well beyond KZH13, (2) all ablated model variants perform worse than the full model, (3) a large performance drop when omitting syntactic information (tag,cut) suggests that the model makes good use of it. However, this could also be due to a bias in the tag distribution, given that all candidates stem from the single sentence that contains antecedents. The median occurrence of the S tag among both antecedents and negative candidates is 1, thus the model could achieve 50.00 s@1 by picking S-type constituents, just as TAG BL achieves 42.02 for reason and 48.66 for possibility.</p><p>Tuning of HPs gives us insight into how different model variants cope with the task. For example, without tuning the model with and without syntactic information achieves 71.27 and 19.68 (not shown in table) s@1 score, respectively, and with tuning: 87.78 and 68.10. Performance of 68.10 s@1 score indicates that the model is able to learn without syntactic guidance, contrary to the 19.68 s@1 score before tuning. <ref type="table" target="#tab_9">Table 5</ref> shows the performance of different variants of the MR-LSTM with HPs tuned on the ASN corpus (always better than the default HPs), when evaluated on 3 different subparts of the ARRAU-AA: all 600 abstract anaphors, 397 nominal and 203 pronominal ones. HPs were tuned on the ASN corpus for every variant separately, without shuffling of the training data. For the best performing variant, without syntactic information (tag,cut), we report the results with HPs that yielded the best s@1 test score for all anaphors (row 4), when training with those HPs on shuffled training data (row 5), and with HPs that yielded the best s@1 all <ref type="formula" target="#formula_0">(600)</ref> nominal <ref type="formula" target="#formula_0">(397)</ref> pronominal (203) ctx aa tag cut ffl1 ffl2 s@1 s@2 s@ 3 s@ 4 s@1 s@2 s@ 3 s@ 4 s@1 s@2 s@ 3 s@ 4 24.17 43.67 54.50 63.00 <ref type="bibr">29.47 50.63 62.47 72.04 13.79 30.05 38.92 45.32 29.67 52.50 66.00 75.00 33.50 58.19 72.04 80.86 22.17 41.38 54.19 63.55 22.83 39.00 52.00 61.33 22.42 41.31 54.66 64.48 23.65 34.48 46.80 55.17 38.33 54.83 63.17 69.33 46.60 64.48 72.54 79.09 22.17 35.96 44.83 50.25 43.83 56.33 66.33 73.00 51.89 64.48 73.55 79.85 28.08 40.39 52.22 59.61 38.17 52.50 61.33 68.67 43.07 57.43 65.49 72.04 28.57 42.86 53.20 62.07 30.17 48.00 57.83 67.33 30.73 50.88 61.21 71.54 29.06 42.36 51.23 59.11 26.33 40.50 50.67 58.67 28.46 41.81 52.14 59.70 22.17 37.93 47.78 56.65 21.33 41.17 53.17 60.33 23.43 47.36 60.45 69.52 17.24 29.06 38.92 42.36 12.00 24.67 33.50 41.50 13.35 27.20 37.28 45.84 9.36 19.70 26.11</ref>   score for pronominal anaphors (row 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results on the ARRAU corpus</head><p>The MR-LSTM is more successful in resolving nominal than pronominal anaphors, although the training data provides only pronominal ones. This indicates that resolving pronominal abstract anaphora is harder compared to nominal abstract anaphora, such as shell nouns. Moreover, for shell noun resolution in KZH13's dataset, the MR-LSTM achieved s@1 scores in the range 76.09-93.14, while the best variant of the model achieves 51.89 s@1 score for nominal anaphors in ARRAU-AA. Although lower performance is expected, since we do not have specific training data for individual nominals in ARRAU-AA, we suspect that the reason for better performance for shell noun resolution in KZH13 is due to a larger number of positive candidates in ASN (cf. <ref type="table" target="#tab_3">Table 2</ref>, rows: antecedents/negatives).</p><p>We also note that HPs that yield good performance for resolving nominal anaphors are not necessarily good for pronominal ones (cf. rows 4-6 in <ref type="table" target="#tab_9">Table 5</ref>). Since the TPE tuner was tuned on the nominal-only ASN data, this suggest that it would be better to tune HPs for pronominal anaphors on a different dataset or stripping the nouns in ASN.</p><p>Contrary to shell noun resolution, omitting syntactic information boosts performance in ARRAU-AA. We conclude that when the model is provided with syntactic information, it learns to pick S-type candidates, but does not continue to learn deeper features to further distinguish them or needs more data to do so. Thus, the model is not able to point to exactly one antecedent, resulting in a lower s@1 score, but does well in picking a few good candidates, which yields good s@2-4 scores. This is what we can observe from row 2 vs. row 6 in Table 5: the MR-LSTM without context embedding (ctx) achieves a comparable s@2 score with the variant that omits syntactic information, but better s@3-4 scores. Further, median occurrence of tags not in {S, VP, ROOT, SBAR} among top-4 ranked candidates is 0 for the full architecture, and 1 when syntactic information is omitted. The need for discriminating capacity of the model is more emphasized in ARRAU-AA, given that the median occurrence of S-type candidates among negatives is 2 for nominal and even 3 for pronominal anaphors, whereas it is 1 for ASN. This is in line with the lower TAG BL in ARRAU-AA.</p><p>Finally, not all parts of the architecture contribute to system performance, contrary to what is observed for reason. For nominal anaphors, the anaphor (aa) and feed-forward layers (ffl1, ffl2) are beneficial, for pronominals only the second ffl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Exploring the model</head><p>We finally analyze deeper aspects of the model:</p><p>(1) whether a learned representation between the anaphoric sentence and an antecedent establishes a relation between a specific anaphor we want to resolve and the antecedent and (2) whether the maxmargin objective enforces a separation of the joint representations in the shared space.</p><p>(1) We claim that by providing embeddings of both the anaphor and the sentence containing the anaphor we ensure that the learned relation between antecedent and anaphoric sentence is dependent on the anaphor under consideration. <ref type="figure" target="#fig_1">Fig.  3</ref> illustrates the heatmap for an anaphoric sentence with two anaphors. The i-th column of the heatmap corresponds to absolute differences between the output of the bi-LSTM for the i-th word in the anaphoric sentence when the first vs. second anaphor is resolved. Stronger color indi- cates larger difference, the blue rectangle represents the column for the head of the first anaphor, the dashed blue rectangle the column for the head of the second anaphor. Clearly, the representations differ when the first vs. second anaphor is being resolved and consequently, joint representations with an antecedent will differ too.</p><p>(2) It is known that the max-margin objective separates the best-scoring positive candidate from the best-scoring negative candidate. To investigate what the objective accomplishes in the MR-LSTM model, we analyze the joint representations of candidates and the anaphoric sentence (i.e., outputs of ffl2) after training. For a randomly chosen instance from ARRAU-AA, we plotted outputs of ffl2 with the tSNE algorithm (v.d. <ref type="bibr" target="#b27">Maaten and Hinton, 2008)</ref>. <ref type="figure" target="#fig_2">Fig. 4</ref> illustrates that the joint representation of the first ranked candidate and the anaphoric sentence is clearly separated from other joint representations. This shows that the maxmargin objective separates the best scoring positive candidate from the best scoring negative candidate by separating their respective joint representations with the anaphoric sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We presented a neural mention-ranking model for the resolution of unconstrained abstract anaphora, and applied it to two datasets with different types of abstract anaphora: the shell noun dataset and a subpart of ARRAU with (pro)nominal abstract anaphora of any type. To our knowledge this work is the first to address the unrestricted abstract anaphora resolution task with a neural network. Our model also outperforms state-of-the-art results on the shell noun dataset.</p><p>In this work we explored the use of purely artificially created training data and how far it can bring us. In future work, we plan to investigate mixtures of (more) artificial and natural data from different sources (e.g. ASN, CSN).</p><p>On the more challenging ARRAU-AA, we found model variants that surpass the baselines for the entire and the nominal part of ARRAU-AA, although we do not train models on individual (nominal) anaphor training data like the related work for shell noun resolution. However, our model still lags behind for pronominal anaphors. Our results suggest that models for nominal and pronominal anaphors should be learned independently, starting with tuning of HPs on a more suitable devset for pronominal anaphors.</p><p>We show that the model can exploit syntactic information to select plausible candidates, but that when it does so, it does not learn how to distinguish candidates of equal syntactic type. By contrast, if the model is not provided with syntactic information, it learns deeper features that enable it to pick the correct antecedent without narrowing down the choice of candidates. Thus, in order to improve performance, the model should be enforced to first select reasonable candidates and then continue to learn features to distinguish them, using a larger training set that is easy to provide.</p><p>In future work we will design such a model, and offer it candidates chosen not only from sentences containing the antecedent, but the larger context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Mention-ranking architecture for abstract anaphora resolution (MR-LSTM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visualizing the differences between outputs of the bi-LSTM over time for an anaphoric sentence containing two anaphors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>tSNE projection of outputs of ffl2. Labels are the predicted ranks and the constituent tag.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Data statistics. For the ASN and CSN we report statistics over all shell nouns, but classifiers are trained independently.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Shell noun resolution results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>87.78 91.63 93.44 93.89 85.97 87.56 89.14 89.82 86.65 88.91 91.18 91.40 68.10 80.32 85.29 89.37 85.52 88.24 89.59 90.05 66.97 80.54 85.75 88.24 87.56 91.63 92.76 94.12 85.97 88.69 89.14 90.05</figDesc><table><row><cell>reason</cell><cell></cell></row><row><cell>ctx aa tag cut ffl1 ffl2 s@1</cell><cell>s@2 s@ 3 s@ 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Architecture ablation for reason.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results table for the ARRAU-AA test set. Refer to text for explanation of duplicated rows.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Example drawn from ARRAU (Uryupina et al., 2016). arXiv:1706.02256v2 [cs.CL] 21 Jul 2017</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We thank the authors for making their data available. 4 Their data set was not publicized.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We follow this terminology for their approach and data representation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Henceforth we refer to it as embedding of the anaphor.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">In case of ambiguous conjunctions (e.g. as interpreted as causal or temporal), we generally choose the most frequent interpretation.8  This also alleviates problems with languages like German, where (non-)embedded sentences differ in surface position of the finite verb. We can either adapt the order or ignore it, when producing anaphoric sentence -antecedent pairs.9  We thank the authors for providing the available data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">We obtained this information from the authors directly.11  The authors provided us with the workers' annotations of the broad region, antecedents chosen by the workers and links to the NYT corpus. The extraction of the anaphoric sentence and the candidates had to be redone.12  We excluded any documents that are part of ARRAU. 13 ARRAU distinguishes abstract anaphors and (mostly) pronominal anaphors referring to an action or plan, as plan.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">We obtained this information in personal communication with one of the authors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">https://github.com/hyperopt/hyperopt.16  We used a list of tags obtained from the Stanford Parser.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1. We would like to thank anonymous reviewers for useful comments and especially thank Todor Mihaylov for the model implementations advices and everyone in the Computational Linguistics Group for helpful discussion.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Pre-processing details</p><p>The CSN corpus we obtained from the authors contained tokenized sentences for antecedents and anaphoric sentences. The number of instances differed from the reported numbers in KZH13 in 9 to 809 instances for training, and 1 for testing. The given sentences still contained the antecedent, so we removed it from the sentence and transformed the corresponding shell into "this shell noun ". An example of this process is: The decision to disconnect the ventilator came after doctors found no brain activity. → This decision came after doctors found no brain activity.</p><p>To use pre-trained word embeddings we had to lowercase all the data. As we use an automatic parse to extract all syntactic constituents, due to parser errors, candidates with the same string appeared with different tags. We eliminated duplicates by checking which tag is more frequent for candidates which have the same POS tag of the first word as the duplicated candidate, in the whole dataset. In case duplicated candidates were still occurring, we chose any of them. If such duplicates occur in antecedents, we don't take such instances in the training data to eliminate noise, or choose any of them for the test data. For the training data we choose instances with an anaphoric sentence length of at least 10 tokens.</p><p>All sentences in the batch are padded with a PAD token up to the maximal sentence length in the batch and corresponding hidden states in the LSTM are masked with zeros. To implement the model efficiently in TensorFlow, batches are constructed in such a way that every sentence instance in the batch has the same number of positive candidates and the same number of negative candidates. Note that by this we do not mean that the ratio of positive and negative examples is 1:1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameter details</head><p>Tables 6 and 7 report the tuned HPs for resolution of the shell noun reason and resolution of abstract anaphors in ARRAU-AA for different model variants. Below is the list of all tunable HPs.</p><p>• the dimensionality of the hidden states in the bi-LSTM, h LST M</p><p>• the first feed-forward layer size, h f f l1</p><p>• the second feed-forward layer size, h f f l2</p><p>• the dimensionality of the tag embeddings, d T AG</p><p>• gradient clipping value, g We additionally report the number of trainable parameters (# param), the average epoch training time using one Nvidia GeForce GTX1080 gpu (t e ) and the epoch after which the best score is achieved (e).   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<editor>Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Antecedent selection for sluicing: Structure and content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1234" to="1243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reference to Abstract Objects in Discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balázs</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 35th Annual Conference on Neural Information Processing Systems (NIPS)<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1466" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Resolving pronominal reference to abstract entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Byron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>Rochester, New York</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Rochester</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Berling, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 4th International Conference on Learning Representations (ICLR)<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Together we stand: Siamese networks for similar question retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpita</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Yenala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar Chinnakotla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Berling, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Annotating Abstract Anaphora. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Dipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Zinsmeister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dialogue acts, synchronising units and anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Framewise Phoneme Classification With Bidirectional LSTM And Other Neural Network Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent orthogonal networks and long-memory tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the the 33rd International Conference on Machine Learning (ICML)<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Resolving discourse-deictic pronouns: A two-stage approach to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guerra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Fourth Joint Conference on Lexical and Computational Semantics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="299" to="308" />
		</imprint>
	</monogr>
	<note>Edgar Gonzàlez Pellicer, and Marta Recasens</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 8th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Resolving shell nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varada</forename><surname>Kolhatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="499" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Annotating anaphoric shell nouns with their antecedents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varada</forename><surname>Kolhatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Zinsmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="112" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interpreting anaphoric shell nouns using antecedents of cataphoric shell nouns as training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varada</forename><surname>Kolhatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Zinsmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="300" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Event Coreference Resolution with Multi-Pass Sieves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC)<address><addrLine>Portoroz</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3996" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Siamese recurrent architectures for learning sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 13th Conference on Artificial Intelligence (AAAI)<address><addrLine>Phoenix, Arizona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2786" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fully Automatic Resolution of It, This and That in Unrestricted Multi-Party Dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Tübingen</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universität Tübingen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Text Similarity with Siamese Recurrent Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Neculoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Versteegh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Rotaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML)<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Anaphoric Annotation in the ARRAU Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised event coreference for abstract words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016 Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods</title>
		<meeting>EMNLP 2016 Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="22" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Systematic Study of Neural Discourse Models for Implicit Discourse Relation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Attapol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Anaphoricity in Connectives: A Case Study on German</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Stede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Grishina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Coreference Resolution Beyond OntoNotes (CORBON) Workshop</title>
		<meeting>the Coreference Resolution Beyond OntoNotes (CORBON) Workshop<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A machine learning approach to pronoun resolution in spoken dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="168" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ARRAU: Linguistically-Motivated Annotation of Anaphoric Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonella</forename><surname>Bristot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Kepa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC)<address><addrLine>Portoroz</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2058" to="2062" />
		</imprint>
	</monogr>
	<note>Federica Cavicchio</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structure and ostension in the interpretation of discourse deixis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">Lynn</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="135" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam Joshua</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Matthew</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">Merrill</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
