<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Count-Based Exploration with the Successor Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marlos</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
							<email>marlosm@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google AI, Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
							<email>bellemare@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google AI, Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bowling</surname></persName>
							<email>bowlingm@google.com</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Alberta</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">DeepMind Alberta</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Count-Based Exploration with the Successor Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we introduce a simple approach for exploration in reinforcement learning (RL) that allows us to develop theoretically justified algorithms in the tabular case but that is also extendable to settings where function approximation is required. Our approach is based on the successor representation (SR), which was originally introduced as a representation defining state generalization by the similarity of successor states. Here we show that the norm of the SR, while it is being learned, can be used as a reward bonus to incentivize exploration. In order to better understand this transient behavior of the norm of the SR we introduce the substochastic successor representation (SSR) and we show that it implicitly counts the number of times each state (or feature) has been observed. We use this result to introduce an algorithm that performs as well as some theoretically sample-efficient approaches. Finally, we extend these ideas to a deep RL algorithm and show that it achieves state-of-the-art performance in Atari 2600 games when in a low sample-complexity regime.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement learning (RL) tackles sequential decision making problems by formulating them as tasks where an agent must learn how to act optimally through trial and error interactions with the environment. The goal in these problems is to maximize the (discounted) sum of the numerical reward signal observed at each time step. Because the actions taken by the agent influence not just the immediate reward but also the states and associated rewards in the future, sequential decision making problems require agents to deal with the trade-off between immediate and delayed rewards. Here we focus on the problem of exploration in RL, which aims to reduce the number of samples (i.e., interactions) an agent needs in order to learn to perform well in these tasks when the environment is initially unknown. Surprisingly, the most common approach in the field is to select exploratory actions uniformly at random, with even high-profile success stories being obtained with this strategy (e.g., <ref type="bibr" target="#b27">Tesauro 1995;</ref><ref type="bibr" target="#b17">Mnih et al. 2015)</ref>. However, random exploration often fails in environments with sparse rewards, Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. that is, environments where the agent observes a reward signal of value zero for the majority of states. In this paper we introduce an approach for exploration in RL based on the successor representation (SR; <ref type="bibr" target="#b7">Dayan 1993)</ref>. The SR is a representation that generalizes between states using the similarity between their successors, that is, the states that follow the current state given the agent's policy. The SR is defined for any problem, it can be learned with temporal-difference learning and, as we discuss below, it can be seen as implicitly estimating the transition dynamics of the environment.</p><p>The main contribution of this paper is to show that the norm of the SR can be used as an exploration bonus. We perform an extensive empirical evaluation to demonstrate this and we introduce the substochastic successor representation (SSR) to also understand, theoretically, the behavior of such a bonus. The SSR behaves similarly to the SR but it is more amenable to theoretical analyses. We show that the SSR implicitly counts state visitation, suggesting that the exploration bonus obtained from the SR, while it is being learned, might also be incorporating some notion of state visitation counts. We demonstrate this intuition empirically and we use this result to introduce algorithms that, in the tabular case, perform as well as traditional approaches with PAC-MDP guarantees. Finally, we extend the idea of using the norm of the SR as an exploration bonus to the function approximation case, designing a deep RL algorithm that achieves state-of-the-art performance in hard exploration Atari 2600 games when in a low sample-complexity regime. The proposed algorithm is also simpler than traditional baselines such as pseudo-count-based methods because it does not require domain-specific density models <ref type="bibr" target="#b2">(Bellemare et al. 2016;</ref><ref type="bibr" target="#b20">Ostrovski et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We consider an agent interacting with its environment in a sequential manner. Starting from a state S 0 ∈ S, at each step the agent takes an action A t ∈ A, to which the environment responds with a state S t+1 ∈ S according to a transition function p(s |s, a) = Pr(S t+1 = s |S t = s, A t = a), and with a reward signal R t+1 ∈ R, where r(s, a) indicates the expected reward for a transition from state s under action a, that is, r(s, a)</p><p>.</p><formula xml:id="formula_0">= E[R t+1 |S t = s, A t = a].</formula><p>arXiv:1807.11622v4 [cs.</p><p>LG] 26 Nov 2019</p><p>The value of a state s when following a policy π, v π (s), is defined to be the expected sum of discounted rewards from that state: v π (s) . = E π T k=t+1 γ k−t−1 R k S t = s , where γ is the discount factor. When the transition probability function p and the reward function r are known, we can compute v π (s) recursively by solving the system of equations below <ref type="bibr" target="#b3">(Bellman 1957)</ref>: v π (s) = a π(a|s) r(s, a) + γ s p(s |s, a)v π (s ) .</p><p>These equations can also be written in matrix form with v π , r ∈ R |S| and P π ∈ R |S|×|S| :</p><formula xml:id="formula_1">v π = r + γP π v π = (I − γP π ) −1 r,<label>(1)</label></formula><p>where P π is the state to state transition probability function induced by π, that is, P π (s, s ) = a π(a|s)p(s |s, a).</p><p>Traditional model-based algorithms learn estimates of the matrix P π and of the vector r and use them to estimate v π , for example by solving Equation 1. We useP π andr to denote empirical estimates of P π and r. Formally,</p><formula xml:id="formula_2">P π (s |s) = n(s, s ) n(s) ,r(s) = C(s, s ) n(s) ,<label>(2)</label></formula><p>wherer(i) denotes the i-th entry in the vectorr, n(s, s ) is the number of times the transition s → s was observed, n(s) = s ∈S n(s, s ), and C(s, s ) is the sum of the rewards associated with the n(s, s ) transitions (we drop the action to simplify notation). However, model-based approaches are rarely successful in problems with large state spaces due to the difficulty in learning accurate models. Because of the challenges in model learning, model-free solutions largely dominate the literature. In model-free RL, instead of estimating P π and r, we estimate v π (s) directly from samples. We often use TD learning <ref type="bibr" target="#b25">(Sutton 1988)</ref> to update our estimates of v π (s),v(s), online:</p><formula xml:id="formula_3">v(S t ) ←v(S t ) + α R t+1 + γv(S t+1 ) −v(S t ) ,<label>(3)</label></formula><p>where α is the step-size parameter. Generalization is required in problems with large state spaces, where it is unfeasible to learn an individual value for each state. We do so by parametrizingv(s) with a set of weights θ. We write, given the weights θ,v(s; θ) ≈ v π (s) andq(s, a; θ) ≈ q π (s, a), where q π (s, a) = r(s, a) + γ s p(s |s, a)v π (s ). Modelfree methods have performed well in problems with large state spaces, mainly due to the use of neural networks as function approximators (e.g., <ref type="bibr" target="#b17">Mnih et al. 2015)</ref>. The ideas presented here are based on the successor representation (SR; <ref type="bibr" target="#b7">Dayan 1993)</ref>. The successor representation with respect to a policy π, Ψ π , is defined as</p><formula xml:id="formula_4">Ψ π (s, s ) = E π,p ∞ t=0 γ t I{S t = s } S 0 = s ,</formula><p>where we assume the sum is convergent with I denoting the indicator function. This expectation can actually be estimated from samples with TD learning:</p><formula xml:id="formula_5">Ψ(S t , j) ←Ψ(S t , j) + η I{S t = j} + γΨ(S t+1 , j) −Ψ(S t , j) ,<label>(4)</label></formula><p>for all j ∈ S and η denoting the step-size. The SR also corresponds to the Neumann series of γP π :</p><formula xml:id="formula_6">Ψ π = ∞ t=0 (γP π ) t = (I − γP π ) −1 .<label>(5)</label></formula><p>Notice that the SR is part of the solution when computing a value function: v π = Ψ π r (Equation 1). We useΨ π to denote the SR computed throughP π , the approximation of P π . Successor features <ref type="bibr" target="#b0">(Barreto et al. 2017</ref>) generalize the successor representation to the function approximation setting. We use the definition for the uncontrolled case. Definition 2.1 (Successor Features). For a given 0 ≤ γ &lt; 1, policy π, and for a feature representation φ(s) ∈ R d , the successor features for a state s are:</p><formula xml:id="formula_7">ψ π (s) = E π,p ∞ t=0 γ t φ(S t ) S 0 = s .</formula><p>Alternatively, in matrix form, we can write the successor </p><formula xml:id="formula_8">features as Ψ π = ∞ t=0 (γP π ) t Φ = (I − γP π ) −1 Φ, where Φ ∈ R |S|×d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">||Ψ(s)|| as an Exploration Bonus</head><p>Recent results have shown that the SR naturally captures the diffusion properties of the environment (e.g., <ref type="bibr" target="#b14">Machado et al. 2018b;</ref><ref type="bibr" target="#b28">Wu, Tucker, and Nachum 2019)</ref>. Inspired by these results, in this section we argue that the SR can be explicitly used to promote exploration. We show that the norm of the SR, while it is being learned, behaves as an exploration bonus that rewards agents for visiting states it has visited less often. We first demonstrate this behavior empirically, in the tabular case. We then introduce the substochastic successor representation to provide some theoretical intuition that justifies this idea. In subsequent sections we show how these ideas carry over to the function approximation setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First Empirical Demonstration</head><p>To demonstrate the usefulness of the norm of the SR as an exploration bonus, we first compare the performance of traditional Sarsa <ref type="bibr" target="#b22">(Rummery and Niranjan 1994;</ref><ref type="bibr" target="#b24">Sutton and Barto 1998)</ref> to Sarsa+SR, which incorporates the norm of the SR as an exploration bonus in the Sarsa update. The update equation for Sarsa+SR iŝ</p><formula xml:id="formula_9">q(S t , A t ) ←q(S t , A t ) + α R t + β 1 ||Ψ(S t )|| 1 + γq(S t+1 , A t+1 ) −q(S t , A t ) ,<label>(6)</label></formula><p>where β is a scaling factor and, at each time step t,Ψ(S t , ·) is updated beforeq(S t , A t ) as per Equation <ref type="formula" target="#formula_5">4</ref>. We evaluated this algorithm in RIVERSWIM and SIXARMS <ref type="bibr">(Strehl and Littman 2008)</ref>, traditional domains in the PAC-MDP literature. In these domains, it is very likely that an agent will first observe a small reward generated in a 0 1 2 3 4 5 ⟨1, 0.7, 0⟩ ⟨1, 0.6, 0⟩ ⟨1, 0.6, 0⟩ ⟨1, 0.6, 0⟩ ⟨1, 0.6, 0⟩ ⟨1, 0.3, 10000⟩</p><formula xml:id="formula_10">⟨0, 1, 5⟩ ⟨1, 0.3, 0⟩ ⟨1, 0.3, 0⟩ ⟨1, 0.3, 0⟩ ⟨1, 0.3, 0⟩ ⟨1, 0.3, 0⟩ ⟨0, 1, 0⟩ ⟨1, 0.1, 0⟩ ⟨0, 1, 0⟩ ⟨1, 0.1, 0⟩ ⟨0, 1, 0⟩ ⟨1, 0.1, 0⟩ ⟨0, 1, 0⟩ ⟨1, 0.1, 0⟩ ⟨0, 1, 0⟩ ⟨1, 0.1, 0⟩ (a) RiverSwim 0 1 2 3 4 5 6 ⟨2, 1, 300⟩ ⟨1, 1, 133⟩ ⟨5, 1, 50⟩ ⟨0-3, 1, 50⟩ ⟨5, 1, 6000⟩ ⟨4, 1, 1660⟩ ⟨3, 1, 800⟩ ⟨0, 1, 0⟩ ⟨4, 1, 0⟩ ⟨1, 0.15, 0⟩ ⟨0, 1, 0⟩ ⟨2-5, 1, 0⟩ ⟨2, 0.1, 0⟩ ⟨0-1, 1, 0⟩ ⟨3-5, 1, 0⟩ ⟨5, 0.01, 0⟩ ⟨0-4, 1, 0⟩ ⟨4, 0.03, 0⟩ ⟨5, 1, 0⟩ ⟨0-3, 1, 0⟩ ⟨3, 0.05, 0⟩</formula><p>⟨0-2, 1, 0⟩ ⟨4-5, 1, 0⟩ (b) SixArms <ref type="figure">Figure 1</ref>: Domains used in the tabular case. The tuples in each transition denote action id, probability, reward . In SIXARMS, the agent starts in state 0. In RIVERSWIM, the agent starts in either state 1 or 2 with equal probability.</p><p>state that is easy to get to. If the agent does not have a good exploration policy, it is likely to converge to a suboptimal behavior, never observing larger rewards available in states that are difficult to get to. See <ref type="figure">Figure 1</ref> for more details.</p><p>We compared the performance of Sarsa and Sarsa+SR for 5,000 time steps when acting -greedily to maximize the discounted return (γ = 0.95). For Sarsa+SR, we swept over different values of α, η, γ SR , β and , with α ∈ {0.01, 0.05, 0.1, 0.25, 0.5}, η ∈ {0.01, 0.05, 0.1, 0.25, 0.5}, γ SR ∈ {0.5, 0.8, 0.95, 0.99}, β ∈ {1, 10, 100, 1000, 10000} and ∈ {0.01, 0.05, 0.1}. For Sarsa, we swept over the parameters α and . For fairness, we looked at a finer granularity for these parameters, with α ∈ i × 0.005 for i ranging from 1 to 100, and with ∈ j × 0.01 for j ranging from 1 to 15. <ref type="table" target="#tab_4">Table 6</ref>, at the end of the paper, summarizes the parameter settings that led to the best results for each algorithm in RIVERSWIM and SIXARMS. The performance of each algorithm, averaged over 100 runs, is available in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Our results show that the proposed exploration bonus has a profound impact in the algorithm's performance. Sarsa obtains an average return of approximately 25,000 while Sarsa+SR obtains an approximate average return of 1.2 million. Notice that, in RIVERSWIM, the reward that is "easy to get" has value 5, implying that, different from Sarsa+SR, Sarsa almost never explores the state space well enough. We observe the same trend in SIXARMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Justification</head><p>It is difficult to characterize the behavior of our proposed exploration bonus because it is updated at each time step with TD learning. It is hard to analyze the behavior of estimates obtained with TD learning in the interim. Also, at its fixed point, for a fixed policy, the 1 -norm of the SR is γ t 1 = 1/(1 − γ) for all states, preventing us from using the fixed point of the SR to theoretically analyze the behav- ior of this exploration bonus. In this section we introduce the substochastic successor representation (SSR) to provide some theoretical intuition, in the prediction case, of why the norm of the SR is a good exploration bonus. The SSR behaves similarly to the SR but it is simpler to analyze. Definition 3.1 (Substochastic Successor Representation). LetP π denote the substochastic matrix induced by the environment's dynamics and by the policy π such that P π (s |s) = n(s,s ) n(s)+1 . For a given 0 ≤ γ &lt; 1, the substochastic successor representation,Ψ π , is defined as:</p><formula xml:id="formula_11">Ψ π = ∞ t=0 γ tP π t = (I − γP π ) −1 .</formula><p>The SSR only differs from the empirical SR in its incorporation of an additional "phantom" transition from each state, making it underestimate the real SR. Through algebraic manipulation we show that the SSR allows us to recover an estimate of the visit counts, n(s). This result provides some intuition of why the exploration bonus we propose performs so well, as exploration bonuses based on state visitation counts are known to generate proper exploration.</p><p>As aforementioned, the SSR behaves similarly to the SR. When computing the norm of the SR, while it is being learned with TD learning, it is as if a reward of 1 was observed at each time step. 1 Thus, there is little variance in the target, with the predictions slowly approaching the true value of the SR. If pessimistically initialized, as traditionally done (i.e., initialized to zero when expecting positive rewards), the estimates of the SR approach the target from below. In this sense, the number of times a prediction has been updated in a given state is a good proxy to estimate how far this prediction is from its final target. From Definition 3.1 we can see that the SSR have similar properties. It underestimates the true target but slowly approaches it, converging to the true SR in the limit. The SSR simplifies the analysis by not taking bootstrapping into consideration.</p><p>The theorem below formalizes the idea that the norm of the SSR implicitly counts state visitation, shedding some light on the efficacy of the exploration bonus we propose. Theorem 1. Let n(s) denote the number of times state s has been visited and letΨ π denote the substochastic successor representation as in Definition 3.1. For a given 0</p><formula xml:id="formula_12">≤ γ &lt; 1, γ n(s)+1 − γ 2 1−γ ≤ (1+γ) − ||Ψ π (s)|| 1 ≤ γ n(s)+1 .</formula><p>Proof of Theorem 1. LetP π be the empirical transition matrix. We first rewriteP π in terms ofP π :</p><formula xml:id="formula_13">P π (s, s ) = n(s, s ) n(s) + 1 = n(s) n(s) + 1 n(s, s ) n(s) = n(s) n(s) + 1P π (s, s ) = 1 − 1 n(s) + 1 P π (s, s )</formula><p>This expression can also be written in matrix form:P π = (I −N )P π , where N ∈ R |S|×|S| denotes the diagonal matrix of augmented inverse counts. ExpandingΨ π we have:</p><formula xml:id="formula_14">Ψ π = γ t=0 (γP π ) t = I + γP π + γ 2P π 2Ψ π .</formula><p>The top eigenvector of a stochastic matrix is the all-ones vector, 1 (Meyn and Tweedie 2012). Using this fact and the definition ofP π with respect toP π we have:</p><formula xml:id="formula_15">Ψ π 1 = I + γ(I − N )P π 1 + γ 2P π 2Ψ π 1 = (I + γ)1 − γN 1 + γ 2P π 2Ψ π 1.<label>(7)</label></formula><p>We can now bound the term γ 2P π 2Ψ π 1 using the fact that 1 is also the top eigenvector of the successor representation and has eigenvalue 1 1−γ <ref type="bibr" target="#b14">(Machado et al. 2018b)</ref>:</p><formula xml:id="formula_16">0 ≤ γ 2P π 2Ψ π 1 ≤ γ 2 1 − γ 1.</formula><p>Plugging <ref type="formula" target="#formula_15">(7)</ref> into the definition of the SR we have (notice that Ψ(s)1 = ||Ψ(s)|| 1 ):</p><formula xml:id="formula_17">(1 + γ)1 −Ψ π 1 = γN 1 − γ 2P π 2Ψ</formula><p>π 1 ≤ γN 1. When we also use the other bound on the quadratic term we conclude that, for any state s,</p><formula xml:id="formula_18">γ n(s)+1 − γ 2 1−γ ≤ (1+γ)−||Ψ π (s)|| 1 ≤ γ n(s)+1 .</formula><p>The relationship between the SSR and the SR. Theorem 1 shows that the SSR, obtained after a slight change to the SR, can be used to recover state visitation counts. The intuition behind this result is that the phantom transition, represented by the +1 in the denominator of the SSR, serves as a proxy for the uncertainty about that state by underestimating the SR. This is due to the fact that s P π (s, s ) gets closer to 1 each time state s is visited.</p><p>This result also suggests that the exploration bonus we propose, ||Ψ(·)|| −1 , behaves as a proper exploration bonus. It rewards the agent for visiting a state for the first times and it becomes less effective as the agent continues to visit that state. Hence, our approach does not change the optimal policy because, in the limit, the norm of the SR (and the exploration bonus) converges to a constant value (see footnote 1):</p><formula xml:id="formula_19">lim t→∞ ||Ψ(s)|| 1 = 1 1 − γ .<label>(8)</label></formula><p>To validate the intuition that the SSR behaves similarly to the SR, we empirically evaluated the norm of the SR in the  <ref type="figure" target="#fig_1">Figure 2</ref> depicts the average result over the 100 runs used to generate the results in <ref type="table" target="#tab_0">Table 1</ref>. The plot shows that the norm of the SR does indeed grow as a function of state visitation counts. In this particular case, the norm of the SR grows at a rate between 1/n, as suggested by the SSR, and 1/ √ n, which is the rate at which TD often converges (Theorem 3.6; <ref type="bibr" target="#b6">Dalal et al. 2018)</ref>.</p><p>Empirical validation of ||Ψ|| 1 as an exploration bonus.</p><p>As a sanity check, we used the result in Theorem 1 to implement a simple model-based algorithm that penalizes the agent for visiting commonly visited states with the exploration bonus r int = −||Ψ π (s)|| 1 . Our agent maximizes r(s, a) + βr int (s), where β is a scaling parameter. The shift (1 + γ) in the theorem has no effect in the agent's policy because it is the same across all states. In this algorithm the agent updates its transition model and reward model with Equation 2 and its SSR estimate as in Definition 3.1. <ref type="table">Table 2</ref> depicts the performance of this algorithm, dubbed ESSR, as well as the performance of some algorithms with polynomial sample-complexity bounds. The goal with this evaluation is not to outperform these algorithms, but to evaluate how well ESSR performs when compared to algorithms that explicitly keep visitation counts to promote exploration. ESSR performs as well as R-MAX <ref type="bibr" target="#b4">(Brafman and Tennenholtz 2002)</ref> and E 3 (Kearns and Singh 2002) on RIVER-SWIM and it outperforms these algorithms on SIXARMS; while MBIE <ref type="bibr">(Strehl and Littman 2008)</ref>, which explicitly estimates confidence intervals over the expected return in each state, outperforms ESSR in these domains. These results clearly show that ESSR performs, on average, similarly to other algorithms with PAC-MDP guarantees, suggesting that the norm of the SSR is a promising exploration bonus. 2 <ref type="table">Table 2</ref>: Comparison between ESSR, R-MAX, E 3 , and MBIE. The numbers reported for R-MAX, E 3 , and MBIE were extracted from the histograms presented by <ref type="bibr">Strehl and Littman (2008)</ref>. ESSR's performance is the average over 100 runs. A 95% confidence interval is reported between parentheses. All numbers are reported in millions (i.e., ×10 6 ).</p><formula xml:id="formula_20">E 3 R-MAX MBIE ESSR RIVERSWIM 3.0 3.0 3.3 3.1 (0.06) SIXARMS 1.8 2.8 9.3 7.3 (1.2)</formula><p>4 Counting Feature Activations with the SR In large environments, where enumerating all states is not an option, directly using Sarsa+SR as described in the previous section is not viable. However, one of the reasons the results in the previous section are interesting is the fact that there is a natural extension of the SR to non-tabular settings, the successor features (Definition 2.1), and the fact that we can immediately use norms in the function approximation setting. In this section we show how one can extend the idea of using the norm of the SR as an exploration bonus to the function approximation setting, something one cannot easily do if relying on explicit state visitation counts. Because deep RL approaches often lead to state-of-the-art performance while also learning a representation from high-dimensional sensory inputs, in this section we introduce a deep RL algorithm that incorporates the ideas introduced. Our algorithm was also inspired by recent work that has shown that successor features can be learned jointly with the feature representation itself <ref type="bibr" target="#b12">(Kulkarni et al. 2016;</ref><ref type="bibr" target="#b14">Machado et al. 2018b</ref>). An overview of the neural network we used to learn the agent's value function while also learning the feature representation and the SR is depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. The layers used to compute the state-action value function,q(S t , ·), are structured as in DQN <ref type="bibr" target="#b17">(Mnih et al. 2015)</ref>, with the number of parameters (i.e., filter sizes, stride, and number of nodes) matching <ref type="bibr" target="#b18">Oh et al.'s (2015)</ref> architecture, which is known to succeed in the auxiliary task detailed below of predicting the agent's next observation. We call the part of our architecture that predictsq(S t , ·) DQN MMC e . It is trained to minimize</p><formula xml:id="formula_21">L TD = E (1 − τ )δ(s, a) + τ δ MC (s, a) 2 ,</formula><p>with δ(s, a) and δ MC (s, a) being defined as</p><formula xml:id="formula_22">δ(s, a) = R t +βr int (s; θ − ) + γ max a q(s , a ; θ − ) − q(s, a; θ), δ MC (s, a) = ∞ t=0 γ t r(S t , A t )+βr int (S t ; θ − ) −q(s, a; θ).</formula><p>This loss is known as the mixed Monte-Carlo return (MMC) and it has been used in the past by the algorithms that achieved succesful exploration in deep reinforcement learning <ref type="bibr" target="#b2">(Bellemare et al. 2016;</ref><ref type="bibr" target="#b20">Ostrovski et al. 2017</ref>). The distinction between θ and θ − is standard in the field, with θ − denoting the parameters of the target network, which is updated less often for stability purposes <ref type="bibr" target="#b17">(Mnih et al. 2015)</ref>. As before, we use r int to denote the exploration bonus obtained from the successor features of the internal representation, φ, which will be defined below. Moreover, to ensure all features are in the same range, we normalize the feature vector so that ||φ(·)|| 1 = 1. In <ref type="figure" target="#fig_2">Figure 3</ref> we highlight with φ the layer in which we normalize its output. Notice that the features are always non-negative due to the use of ReLU gates.</p><p>The successor features, ψ(S t ), at the bottom of the diagram, are obtained by minimizing the loss</p><formula xml:id="formula_23">L SR = E φ(S t ; θ − ) + γψ(S t+1 ; θ − ) − ψ(S t ; θ) 2 .</formula><p>Zero is a fixed point for the SR, which is particularly concerning in settings with sparse rewards. The agent might end up learning to set φ(·) = 0 to achieve zero loss. We address this problem by not propagating ∇L SR to φ (this is depicted in <ref type="figure" target="#fig_2">Figure 3</ref> as an open circle stopping the gradient); and by creating an auxiliary task <ref type="bibr" target="#b9">(Jaderberg et al. 2017)</ref> to encourage a representation to be learned before a non-zero reward is observed. As <ref type="bibr" target="#b14">Machado et al. (2018b)</ref>, we use the auxiliary task of predicting the next observation, learned with the architecture proposed by <ref type="bibr" target="#b18">Oh et al. (2015)</ref>, which is depicted as the top layers in <ref type="figure" target="#fig_2">Figure 3</ref>. The loss we minimize for this last part of the network is</p><formula xml:id="formula_24">L Recons = Ŝ t+1 − S t+1 2 .</formula><p>The overall loss minimized by the network is</p><formula xml:id="formula_25">L = w TD L TD + w SR L SR + w Recons L Recons .</formula><p>The last step in describing our algorithm is to define r int (S t ; θ − ), the intrinsic reward we use to encourage exploration. As in Sarsa+SR, we choose the exploration bonus to be the inverse of the 1 -norm of the vector of successor features of the current state. That is,</p><formula xml:id="formula_26">r int (S t ; θ − ) = 1 ||ψ(S t ; θ − )|| 1 ,</formula><p>where ψ(S t ; θ − ) denotes the successor features of state S t parametrized by θ − . The exploration bonus comes from the same intuition presented in the previous section (we observed in preliminary experiments not discussed here that DQN performs better when dealing with positive rewards). A complete description of the network architecture is available in <ref type="figure">Figure 4</ref>, which is at the end of the paper to allow the reader to first focus on the main concepts of the proposed idea. We initialize our network the same way <ref type="bibr" target="#b18">Oh et al. (2015)</ref> does. We use Xavier initialization <ref type="bibr" target="#b8">(Glorot and Bengio 2010)</ref> in all layers except the fully connected layers around the element-wise multiplication denoted by ⊗, which are initialized uniformly with values between −0.1 and 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation of Exploration in Deep RL</head><p>We evaluated our algorithm on the Arcade Learning Environment <ref type="bibr" target="#b1">(Bellemare et al. 2013)</ref>. Following <ref type="bibr" target="#b2">Bellemare et al.'s (2016)</ref> taxonomy, we focused on the Atari 2600 games with sparse rewards that pose hard exploration problems. They are: FREEWAY, GRAVITAR, MONTEZUMA'S REVENGE, PRIVATE EYE, SOLARIS, and VENTURE. <ref type="bibr">3</ref> We used the evaluation protocol proposed by <ref type="bibr" target="#b13">Machado et al. (2018a)</ref>. The reported results are the average over 10 seeds after 100 million frames. We evaluated our agents in the stochastic setting (sticky actions, ς = 0.25) using a frame skip of 5 with the full action set (|A| = 18). The agent learns from raw pixels i.e., it uses the game screen as input.</p><p>Our results were obtained with the algorithm described in Section 4. We set β = 0.05 after a rough sweep over values in the game MONTEZUMA'S REVENGE. We annealed in DQN's -greedy exploration over the first million steps, starting at 1.0 and stopping at 0.1 as done by <ref type="bibr" target="#b2">Bellemare et al. (2016)</ref>. We trained the network with RMSprop with a step-size of 0.00025, an value of 0.01, and a decay of 0.95, which are the standard parameters for training DQN <ref type="bibr" target="#b17">(Mnih et al. 2015)</ref>. The discount factor, γ, is set to 0.99, and w TD = 1, w SR = 1000, w Recons = 0.001. The weights w TD , w SR , and w Recons were set so that the loss functions would be roughly at the same scale. All other parameters are the same as those used by <ref type="bibr" target="#b17">Mnih et al. (2015)</ref> and <ref type="bibr" target="#b18">Oh et al. (2015)</ref>. <ref type="table" target="#tab_1">Table 3</ref> summarizes the results after 100 million frames. The performance of other algorithms is also provided for reference. Notice we are reporting learning performance for all algorithms instead of the maximum scores achieved by the algorithm. We use the superscript MMC to distinguish between the algorithms that use MMC from those that do not. When comparing our algorithm, DQN MMC e +SR, <ref type="bibr">3</ref> The code used to generate the reported results is available at: https://github.com/mcmachado/count based exploration sr/tree/ master/function approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Performance and Baselines</head><p>to DQN we can see how much our approach improves over the most traditional baseline. By comparing our algorithm's performance to DQN MMC CTS <ref type="bibr" target="#b2">(Bellemare et al. 2016</ref>) and DQN MMC PIXELCNN <ref type="bibr" target="#b20">(Ostrovski et al. 2017)</ref> we compare our algorithm to established baselines for exploration that are closer to our method. By comparing our algorithm's performance to Random Network Distillation (RND; <ref type="bibr" target="#b5">Burda et al. 2019)</ref> we compare our algorithm to the most recent paper in the field with state-of-the-art performance. Finally, we also evaluate the impact of the proposed exploration bonus by comparing our algorithm to DQN MMC e , which uses the same number of parameters our network uses (i.e., filter sizes, stride, and number of nodes), but without the additional modules (next state prediction and successor representation) and without the intrinsic reward bonus. We do this by setting w SR = w Recons = β = 0.</p><p>We can clearly see that our algorithm achieves scores much higher than those achieved by DQN, which struggles in games that pose hard exploration problems. When comparing our algorithm to DQN MMC CTS and DQN MMC PIXELCNN we observe that, on average, DQN MMC e +SR at least matches the performance of these algorithms while being simpler by not requiring a density model. Instead, our algorithm requires the SR, which is domain-independent as it is already defined for every problem since it is a component of the value function estimates, as discussed in Section 2. Finally, DQN MMC e +SR also outperforms RND <ref type="bibr" target="#b5">(Burda et al. 2019)</ref> when it is trained for 100 million frames. 4 Importantly, RND, when trained for 2 billion frames, is currently considered to be the state-of-the-art approach for exploration in Atari 2600 games. Recently <ref type="bibr" target="#b26">Taiga et al. (2019)</ref> evaluated several exploration algorithms, including those we use as baselines, and they have shown that, in these games, their performance at 100 million frames is predictive of their performance at one billion frames.</p><p>Finally, the comparison between DQN MMC e +SR and DQN MMC e shows that the provided exploration bonus has a big impact in the game MONTEZUMA'S REVENGE, which is probably known as the hardest game among those we used in our evaluation, and the only game where agents do not learn how to achieve scores greater than zero with random 4 DQN MMC e +SR outperforms DQN MMC CTS in five out of six games, it outperforms RND in four out of five games, and its performance is comparable to DQN MMC PIXELCNN 's performance.</p><p>exploration. Interestingly, the change in architecture and the use of MMC leads to a big improvement in games such as GRAVITAR and VENTURE, which we cannot fully explain. However, because the change in architecture does not have any effect in MONTEZUMA'S REVENGE, it seems that the proposed exploration bonus is essential in games with very sparse rewards.</p><p>Evaluating the Impact of the Auxiliary Task</p><p>While the results depicted in <ref type="table" target="#tab_1">Table 3</ref> allow us to see the benefit of using an exploration bonus derived from the SR, they do not inform us about the impact of the auxiliary task in the results. The experiments in this section aim at addressing this issue. We focus on MONTEZUMA'S REVENGE because it is the game where the problem of exploration is maximized, with most algorithms not being able to do anything without an exploration bonus. The first question we asked was whether the auxiliary task was necessary in our algorithm. We evaluated this by dropping the reconstruction module from the network to test whether the initial random noise generated by the SR is enough to drive representation learning. It is not. When dropping the auxiliary task, the average performance of this baseline over 4 seeds in MONT. REVENGE after 100 million frames was 100 points (σ 2 = 200; min: 0, max: 400). As comparison, our algorithm obtains 1395.4 points (σ 2 = 1121.8, min: 400, max: 2500). These results suggest that auxiliary tasks are necessary for our method to perform well.</p><p>We also evaluated whether the auxiliary task was sufficient to generate the results we observed. To do so we dropped the SR module and set β = 0.0 to evaluate whether our exploration bonus was actually improving the agent's performance or whether the auxiliary task was doing it. The exploration bonus seems to be essential. When dropping the exploration bonus and the SR module, the average performance of this baseline over 4 seeds in MONTEZUMA'S RE-VENGE after 100 million frames was 398.5 points (σ 2 = 230.1; min: 0, max: 400). Again, clearly, the auxiliary task is not a sufficient condition for the performance we report. The reported results use the same parameters as before.</p><p>Thus, while it is hard to completely disentangle the impact of the different components of DQN MMC e +SR (e.g., the exploration bonus, learning the SR, the auxiliary task), the comparisons in <ref type="table" target="#tab_1">Table 3</ref> and the results in this section suggest that the exploration bonus we introduced is essential for our approach to achieve state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluating the Impact of Using Different P-Norms</head><p>To further understand the different nuances behind the idea that the norm of the successor representation can be used to generate an exploration bonus, we also asked the question of whether this is true only for the 1 -norm of the SR. It is often said, for example, that the 2 -norm is smoother, and thus might be more amenable to the training of neural networks.</p><p>For the function approximation case, DQN MMC e +SR when using the 2 -norm has a performance comparable to DQN MMC e +SR when using the 1 -norm of the SR to generate its exploration bonus (φ is normalized with the respective norm). The actual performance of both approaches  in the Atari 2600 games we used in the previous experiment is available in <ref type="table" target="#tab_2">Table 4</ref>. We followed the same evaluation protocol described before, averaging the performance of DQN MMC e +SR with the 2 -norm over 10 runs. The parameter β is the only parameter not shared by both algorithms. While β = 0.025 when using the 2 -norm of the SR, β = 0.05 when using the 1 -norm of the SR.</p><p>We also revisited the results presented in Section 3 to evaluate, in the tabular case, the impact of the different norms in Sarsa+SR. We swept over all the parameters, as previously described. The results reported for Sarsa+SR when using the 2 -norm of the SR are the average over 100 runs. The actual numbers are available in <ref type="table" target="#tab_3">Table 5</ref>. As before, it seems that it does not make much difference which norm of the SR we use ( 1 -norm or the 2 -norm). The fact that these results are so close might suggest that the idea of using the norm of the SR for exploration is quite general, with the p-norm of the SR being effective for more than one value of p. Recall that ||x|| 1 ≤ √ 2||x|| 2 for any finite vector x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>There are multiple algorithms in the tabular, model-based case, with guarantees about their performance in terms of regret bounds (e.g., <ref type="bibr" target="#b19">Osband, Roy, and Wen 2016)</ref> or samplecomplexity (e.g., <ref type="bibr" target="#b4">Brafman and Tennenholtz 2002;</ref><ref type="bibr" target="#b11">Kearns and Singh 2002;</ref><ref type="bibr">Strehl and Littman 2008)</ref>. RIVERSWIM and SIXARMS are domains traditionally used when evaluating these algorithms. In this paper we introduced a model-free algorithm that performs particularly well in these domains.</p><p>We also introduced a model-based algorithm that performs as well as some of these algorithms with theoretical guarantees. Among these algorithms, R-MAX is the closest approach to ours. As R-MAX, the algorithm presented in Section 3 augments the state-space with an imaginary state and encourages the agent to visit that state, implicitly reducing the algorithm's uncertainty. However, R-MAX deletes the transition to this imaginary state once a state has been vis-  <ref type="figure">Figure 4</ref>: Neural network architecture used by our algorithm when learning to play Atari 2600 games.</p><p>ited a given number of times. Ours, on the other hand, lets the probability of visiting this imaginary state vanish with additional visitations. Importantly, notice that it is not clear how to apply traditional algorithms such as R-MAX and E 3 to large domains where function approximation is required.</p><p>Conversely, there are not many model-free approaches with proven sample-complexity bounds (e.g., <ref type="bibr" target="#b23">Strehl et al. 2006)</ref>, but there are multiple model-free algorithms for exploration that actually work in large domains (e.g., <ref type="bibr" target="#b2">Bellemare et al. 2016;</ref><ref type="bibr" target="#b20">Ostrovski et al. 2017;</ref><ref type="bibr" target="#b21">Plappert et al. 2018;</ref><ref type="bibr" target="#b5">Burda et al. 2019</ref>). Among these algorithms, the use of pseudo-counts through density models is the closest to ours <ref type="bibr" target="#b2">(Bellemare et al. 2016;</ref><ref type="bibr" target="#b20">Ostrovski et al. 2017)</ref>. Inspired by those papers we used the mixed Monte-Carlo return as a target in the update rule. In Section 5 we showed that our algorithm at least matches these approaches while being simpler by not requiring a density model. Importantly, <ref type="bibr" target="#b15">Martin et al. (2017)</ref> had already shown that counting activations of fixed, handcrafted features in Atari 2600 games leads to good exploration behavior. Nevertheless, by using the SR we are not only counting learned features but we are also implicitly capturing the induced transition dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>RL algorithms tend to have high sample complexity, which often prevents them from being used in the real-world. Poor exploration strategies is one of the reasons for this high sample-complexity. Despite all of its shortcomings, uniform random exploration is, to date, the most commonly used approach for exploration. This is mainly due to the fact that most approaches for tackling the exploration problem still rely on domain-specific knowledge (e.g., density models, handcrafted features), or on having an agent learn a perfect model of the environment. In this paper we introduced a general method for exploration in RL that implicitly counts state (or feature) visitation in order to guide the exploration process. It is compatible with representation learning and the idea can also be adapted to be applied to large domains.</p><p>This result opens up multiple possibilities for future work. Based on the results presented in Section 3, for example, we conjecture that the substochastic successor representation can be actually used to generate algorithms with PAC-MDP bounds (e.g., one could replace explicit state visitation counts by the norm of the SSR in traditional algorithms). Investigating to what extent different auxiliary tasks impact the algorithm's performance, and whether simpler tasks such as predicting feature activations or parts of the input (Jaderberg et al. 2017) are effective is also worth studying. Finally, it might be interesting to further investigate the connection between representation learning and exploration, since it is also known that better representations can lead to faster exploration <ref type="bibr" target="#b10">(Jiang et al. 2017</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is a matrix encoding the feature representation of each state such that φ(s) ∈ R d . This definition reduces to the SR in the tabular case, where Φ = I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Empirical evaluation of the the norm of the SR as a function of state visitation count. Each curve denotes the evolution in one of the six states of RIVERSWIM. The colors match the colors of the states inFigure 1a. Reference functions f 1 (n) = c 1 √ n and f 2 (n) = c 2 n are depicted for comparison (c 1 = 0.19; c 2 = 0.006). See text for details. six states of RiverSwim as a function of the number of times those states were visited.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Neural network architecture used by our algorithm when learning to play Atari 2600 games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between Sarsa and Sarsa+SR. A 95% confidence interval is reported between parentheses.</figDesc><table><row><cell></cell><cell>Sarsa</cell><cell>Sarsa + SR</cell></row><row><cell>RIVERSWIM</cell><cell>24,770 (196)</cell><cell>1,213,544 (540,454)</cell></row><row><cell>SIXARMS</cell><cell cols="2">247,977 (4,970) 1,052,934 (2,311,617)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Performance of the proposed algorithm, DQN MMC e +SR, compared to various agents on the "hard exploration" subset of Atari 2600 games. The DQN results reported are from Machado et al. (2018a) while the DQN MMC CTS , DQN MMC PIXELCNN and RND results were obtained through personal communication with the authors of the corresponding papers. Burda et al. did not evaluate RND in FREEWAY. When available, standard deviation is reported between parentheses. See text for details.</figDesc><table><row><cell></cell><cell>DQN</cell><cell>DQN MMC e</cell><cell>DQN MMC CTS</cell><cell>DQN MMC PIXELCNN</cell><cell>RND</cell><cell>DQN MMC e</cell><cell>+SR</cell></row><row><cell>FREEWAY</cell><cell>32.4 (0.3)</cell><cell>29.5 (0.1)</cell><cell>29.2</cell><cell>29.4</cell><cell>--</cell><cell cols="2">29.4 (0.1)</cell></row><row><cell>GRAVITAR</cell><cell>118.5 (22.0)</cell><cell>1078.3 (254.1)</cell><cell>199.8</cell><cell>275.4</cell><cell>790.0 (122.9)</cell><cell cols="2">457.4 (120.3)</cell></row><row><cell>MONT. REV.</cell><cell>0.0 (0.0)</cell><cell>0.0 (0.0)</cell><cell>2941.9</cell><cell>1671.7</cell><cell cols="3">524.8 (314.0) 1395.4 (1121.8)</cell></row><row><cell cols="2">PRIVATE EYE 1447.4 (2,567.9)</cell><cell>113.4 (42.3)</cell><cell>32.8</cell><cell>14386.0</cell><cell>61.3 (53.7)</cell><cell cols="2">104.4 (50.4)</cell></row><row><cell>SOLARIS</cell><cell>783.4 (55.3)</cell><cell>2244.6 (378.8)</cell><cell>1147.1</cell><cell>2279.4</cell><cell cols="3">1270.3 (291.0) 1890.1 (163.1)</cell></row><row><cell>VENTURE</cell><cell>4.4 (5.4)</cell><cell>1220.1 (51.0)</cell><cell>0.0</cell><cell>856.2</cell><cell cols="3">953.7 (167.3) 1348.5 (56.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Performance of the proposed algorithm, DQN MMC e +SR, when using the 1 -norm and 2 -norm of the SR to generate the exploration bonus. Standard deviation is reported between parentheses. See text for details.</figDesc><table><row><cell></cell><cell>1-norm</cell><cell>2-norm</cell></row><row><cell>FREEWAY</cell><cell>29.4 (0.1)</cell><cell>29.5 (0.1)</cell></row><row><cell>GRAVITAR</cell><cell>457.4 (120.3)</cell><cell>430.3 (109.4)</cell></row><row><cell>MONT. REV.</cell><cell cols="2">1395.4 (1121.8) 1778.6 (903.6)</cell></row><row><cell>PRIVATE EYE</cell><cell>104.4 (50.4)</cell><cell>99.1 (1.8)</cell></row><row><cell>SOLARIS</cell><cell>1890.1 (163.1)</cell><cell>2155.7 (398.3)</cell></row><row><cell>VENTURE</cell><cell>1348.5 (56.5)</cell><cell>1241.8 (236.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Performance of Sarsa+SR when using the 1 -norm and 2 -norm of the SR to generate the exploration bonus. A 95% confidence interval is reported between parentheses.</figDesc><table><row><cell></cell><cell>1-norm</cell><cell>2-norm</cell></row><row><cell cols="2">RIVERSWIM 1,213,544 (540,454)</cell><cell>1,192,052 (507,179)</cell></row><row><cell>SIXARMS</cell><cell>1,052,934 (2,311,617)</cell><cell>819,927 (2,132,003)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Parameter settings that led to the reported performance in RIVERSWIM and SIXARMS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">RIVERSWIM</cell><cell></cell><cell></cell><cell>SIXARMS</cell></row><row><cell>Algorithm</cell><cell></cell><cell></cell><cell>α</cell><cell></cell><cell>η</cell><cell cols="2">γ SR</cell><cell>β</cell><cell></cell><cell></cell><cell>α</cell><cell>η</cell><cell>γ SR</cell><cell>β</cell></row><row><cell>Sarsa</cell><cell></cell><cell></cell><cell>0.005</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="3">0.01 0.465</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.03</cell></row><row><cell cols="3">Sarsa+SR (w/ 1-norm)</cell><cell>0.25</cell><cell cols="5">0.01 0.95 100</cell><cell cols="2">0.1</cell><cell>0.1</cell><cell>0.01 0.99 100 0.01</cell></row><row><cell cols="3">Sarsa+SR (w/ 2-norm)</cell><cell>0.25</cell><cell cols="5">0.01 0.99 100</cell><cell cols="2">0.1</cell><cell>0.1</cell><cell>0.01 0.99</cell><cell>10</cell><cell>0.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>action</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>fc</cell><cell></cell><cell></cell><cell>Deconv</cell><cell>Deconv</cell><cell>Deconv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2048</cell><cell></cell><cell></cell><cell>64,6x6</cell><cell>64,6x6</cell><cell>1,6x6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pad 2,2</cell><cell>pad 2,2</cell><cell>pad 0,0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>stride 2</cell><cell>stride 2</cell><cell>stride 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>fc</cell><cell></cell><cell></cell><cell>fc</cell><cell></cell><cell>fc</cell></row><row><cell></cell><cell>pad 0,0 64,6x6 Conv</cell><cell>pad 2,2 64,6x6 Conv</cell><cell>pad 2,2 64,6x6 Conv</cell><cell></cell><cell></cell><cell>ReLU</cell><cell>2048</cell><cell></cell><cell>2048</cell><cell>1024</cell><cell>ReLU</cell><cell>6400</cell><cell>64 x 20 x 20 64 x 40 x 40</cell><cell>84 x 84</cell></row><row><cell>4 x 84 x 84</cell><cell cols="4">64 x 40 x 40 ReLU ReLU 64 x 20 x 20 64 x10 x 10 ReLU ReLU fc stride 2 stride 2 stride 2</cell><cell>1024</cell><cell>fc</cell><cell>18</cell><cell cols="2">q(S t , ·)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ReLU fc</cell><cell cols="2">fc ReLU</cell><cell cols="3">(S t )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2048</cell><cell>1024</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In vector form, when estimating the SR with TD learning, the clause I{St = j}, from Equation 4, is always true for one of the states, that is, an entry in the vector representing the SR. Thus, it is as if a reward of 1 was observed at each time step.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The code used to generate all results in this section is available at: https://github.com/mcmachado/count based exploration sr/tree/master/tabular.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Jesse Farebrother for the initial implementation of DQN used in this paper, Georg Ostrovski for the discussions and for providing us the exact results we report for DQN MMC CTS and DQN MMC PIXELCNN , and Yuri Burda for providing us the data we used to compute the performance we report for RND in Atari 2600 games. We would also like to thank Carles Gelada, George Tucker and Or Sheffet for useful discussions, as well as the anonymous reviewers for their feedback. This work was supported by grants from Alberta Innovates Technology Futures and the Alberta Machine Intelligence Institute (Amii). Computing resources were provided by Compute Canada through Cal-culQuébec. Marlos C. Machado performed this work while at the University of Alberta.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Successor Features for Transfer in Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4058" to="4068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Arcade Learning Environment: An Evaluation Platform for General Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unifying Count-Based Exploration and Intrinsic Motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1471" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-MAX -A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="213" to="231" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploration by Random Network Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finite Sample Analyses for TD(0) With Function Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Szörényi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6144" to="6160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving Generalization for Temporal Difference Learning: The Successor Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="613" to="624" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the Difficulty of Training Deep Feedforward Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>AISTATS</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reinforcement Learning with Unsupervised Auxiliary Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contextual Decision Processes with Low Bellman Rank are PAC-Learnable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1704" to="1713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Near-Optimal Reinforcement Learning in Polynomial Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep Successor Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<idno>abs/1606.02396</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="523" to="562" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Eigenoption Discovery through the Deep Successor Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Count-Based Exploration in Feature Space for Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sasikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2471" to="2478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Meyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Tweedie</surname></persName>
		</author>
		<title level="m">Markov Chains and Stochastic Stability</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human-level Control through Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action-Conditional Video Prediction using Deep Networks in Atari Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2863" to="2871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalization and Exploration via Randomized Value Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2377" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Count-Based Exploration with Neural Density Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2721" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parameter Space Noise for Exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Analysis of Model-based Interval Estimation for Markov Decision Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">; A L</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<idno>INFENG/TR 166</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1331" />
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Cambridge University Engineering Dept. Strehl,</orgName>
		</respStmt>
	</monogr>
	<note>Online Q-Learning using Connectionist Systems</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PAC Model-Free Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="881" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to Predict by the Methods of Temporal Differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<idno>abs/1908.02388</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal Difference Learning and TD-Gammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="68" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Laplacian in RL: Learning Representations with Efficient Approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
