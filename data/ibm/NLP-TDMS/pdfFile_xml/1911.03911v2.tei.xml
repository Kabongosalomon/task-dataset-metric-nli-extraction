<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Borchmann</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Applica.ai</addrLine>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawid</forename><surname>Wiśniewski</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Applica.ai</addrLine>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Gretkowski</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Applica.ai</addrLine>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izabela</forename><surname>Kosmala</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Applica.ai</addrLine>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawid</forename><surname>Jurkiewicz</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Applica.ai</addrLine>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Szałkiewicz</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Applica.ai</addrLine>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Pałka</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Applica.ai</addrLine>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kaczmarek</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Applica.ai</addrLine>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Kaliska</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Applica.ai</addrLine>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Graliński</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Applica.ai</addrLine>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new shared task of semantic retrieval from legal texts, in which a so-called contract discovery is to be performed-where legal clauses are extracted from documents, given a few examples of similar clauses from other legal acts. The task differs substantially from conventional NLI and shared tasks on legal information extraction (e.g., one has to identify text span instead of a single document, page, or paragraph). The specification of the proposed task is followed by an evaluation of multiple solutions within the unified framework proposed for this branch of methods. It is shown that state-of-the-art pretrained encoders fail to provide satisfactory results on the task proposed. In contrast, Language Model-based solutions perform better, especially when unsupervised fine-tuning is applied. Besides the ablation studies, we addressed questions regarding detection accuracy for relevant text fragments depending on the number of examples available. In addition to the dataset and reference results, LMs specialized in the legal domain were made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Processing of legal contracts requires significant human resources due to the complexity of documents, the expertise required and the consequences at stake. Therefore, a lot of effort has been made to automate such tasks in order to limit processing costs-notice that law was one of the first areas where electronic information retrieval systems were adopted <ref type="bibr" target="#b21">(Maxwell and Schafer, 2008)</ref>.</p><p>Enterprise solutions referred to as contract discovery deal with tasks, such as ensuring the inclusion of relevant clauses or their retrieval for further analysis (e.g., risk assessment). Such processes can consist of a manual definition of a few examples, followed by conventional information</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Legal SI Few-shot</p><formula xml:id="formula_0">COLIEE + − − SNLI − − − MultiNLI − − − TREC Legal Track + − − Propaganda detection − + − THUMOS (video) − + + ActivityNet (video) − + + ALBAYZIN (audio) − + −</formula><p>Contract Discovery (ours) + + + retrieval. This approach was taken recently by <ref type="bibr" target="#b23">Nagpal et al. (2018)</ref> for the extraction of fairness policies spread across agreements and administrative regulations.</p><p>2 Review of Existing Datasets <ref type="table" target="#tab_0">Table 1</ref> summarizes main differences between available challenges. It is shown that most of the related NLP tasks do not assume span identification, even those outside the legal domain. Moreover, the few-shot setting is not popular within the field of NLP yet. None of existing tasks involving semantic similarity methods, such as SNLI <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref> or multi-genre NLI <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref>, assume span identification. Instead, standalone sentences are provided to determine their entailment. It is also the case of existing shared tasks for legal information extraction, such as COLIEE <ref type="bibr" target="#b16">(Kano et al., 2017)</ref>, where one has to recognize entailment between articles and queries, as considered in the question answering problem. Obviously, the tasks aimed at retrieving documents consisting of multiple sentences, such as TREC legal track <ref type="bibr">(Baron</ref>   <ref type="figure">Figure 1</ref>: The aim of this task is to identify spans in the requested documents (referred to as target documents) representing clauses analogous to the spans selected in other documents (referred to as seed documents). <ref type="bibr" target="#b3">et al., 2006;</ref><ref type="bibr" target="#b24">Oard et al., 2010;</ref><ref type="bibr" target="#b8">Chu, 2011)</ref>, lack this component.</p><p>There are a few NLP tasks where span identification is performed. These include some of plagiarism detection competitions <ref type="bibr" target="#b27">(Potthast et al., 2010)</ref> and recently introduced SemEval task of propaganda techniques detection <ref type="bibr" target="#b10">(Da San Martino et al., 2020)</ref>. When different media are considered, NLP span identification task is equivalent to the action recognition in temporally untrimmed videos where one is expected to provide the start and end times for detected activity. These include THUMOS 14 <ref type="bibr" target="#b13">(Jiang et al., 2014)</ref> as well as ActivityNet 1.2 and ActivityNet 1.3 challenges <ref type="bibr">(Fabian Caba Heilbron and Niebles, 2015)</ref>. Another example is queryby-example spoken term detection, as considered e.g., in ALBAYZIN 2018 challenge <ref type="bibr" target="#b34">(Tejedor et al., 2019)</ref>.</p><p>In a typical business case of contract discovery one may expect only a minimal number of examples. The number of available annotations results from the fact that contract discovery is performed constantly for different clauses, and it is practically impossible to prepare data in a number required by a conventional classifier every time. When one is interested in the few-shot setting, especially querying by multiple examples, there are no similar shared tasks within the field of NLP. Some authors however experimented recently with few-shot Named Entity Recognition <ref type="bibr">(Fritzler et al., 2019)</ref> or few-shot text classification <ref type="bibr" target="#b2">(Bao et al., 2019)</ref>. The first, however, involves identification of short spans (from one to few words), whereas the second does not assume span identification at all.</p><p>What is important, existing tasks aimed at recognizing textual entailment in natural language <ref type="bibr">(Bow-man et al., 2015)</ref>, differ in terms of the domain. This also applies to a multi-genre NLI <ref type="bibr">(Williams et al., 2017)</ref>, since legal texts vary significantly from other genres. As it will be shown later, methods optimal for MultiNLI do not perform well on the proposed task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Contract Discovery: New Dataset and Shared Task</head><p>In this section, we introduce a new dataset of Contract Discovery, as well as a derived few-shot semantic retrieval shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Desiderata</head><p>We define our desiderata as follows. We wish to construct a dataset for testing the mechanisms that detect various types of regulations in legal documents. Such systems should be able to process unstructured text; that is, no legal documents segmentation into the hierarchy of distinct (sub)sections is to be given in advance. In other words, we want to provide natural language streams lacking formal structure, as in most of the real-word usage scenarios <ref type="bibr" target="#b35">(Vanderbeck et al., 2011)</ref>. What is more, it is assumed that a searched passage can be any part of the document and not necessarily a complete paragraph, subparagraph, or a clause. Instead, the process should be considered as a span identification task. We intend to develop a dataset for identifying spans in a query-by-example scenario instead of the setting where articles are being returned as an answer for the question specified in natural language.</p><p>We wish to propose using this dataset in a fewshot scenarios, where one queries the system using multiple examples rather than a single one. The intended form of the challenge following these requirements is presented in <ref type="figure">Figure 1</ref>. Roughly speaking, the task is to identify spans in the requested documents (referred to as target documents) representing clauses analogous (i.e. semantically and functionally equivalent) to the examples provided in other documents (referred to as seed documents).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Collection and Annotation</head><p>Random subsets of bond issue prospectuses and non-disclosure agreement documents from the US EDGAR database 1 , as well as annual reports of charitable organizations from the UK Charity Register 2 were annotated. Note there are no copyright issues and both datasets belong to the public domain.</p><p>Annotation was performed in such a way that clauses of the same type were selected (e.g., determining the governing law, merger restrictions, tax changes call, or reserves policy). Clause types depend on the type of a legal act and can consist of a single sentence, multiple sentences or sentence fragments. The exact type of a clause is not important during the evaluation since no full-featured training is allowed and a set of only a few sample clauses can be used during execution.</p><p>We restricted ourselves to 21 types as a result of a trade-off between annotation cost and the ability to formulate general remarks. Note that each clause type must be well-understood by the annotator (we described each very carefully in the instructions), and one must have all of the considered clauses in mind when the legal acts are being read during the process. In real-world legal applications, the clauses change in an everyday manner and depend on the problem analyzed by the layer at the moment.</p><p>Each document was annotated by two experts, and then reviewed (or resolved) by a superannotator, who also decided the gold standard. An average Soft F 1 score (Section 4.2) of the two primary annotators, when compared to the gold standard (after the super-annotation), was taken to estimate human baseline performance of 0.84.</p><p>The inter-annotator agreement was equal to 0.76 in terms of Soft F 1 metric (Section 4.2). It should be treated as an agreement between two randomly picked annotations since the total number of annotators was 10 (annotators were aligned randomly to a subset of documents in such a way that there would be two annotations and super-annotation per document). <ref type="table">Table 3</ref> presents examples of clauses annotated in the sub-group of Charity Annual Reports documents. The detailed list of clauses and their examples can be found in Appendix C.</p><p>The dataset is made publicly available. In addition, we release a large, cleaned, plain-text corpus of legal and financial texts for the purposes of unsupervised model training or fine-tuning. All the available documents of US EDGAR as for November 19, 2018 were crawled. The resulting corpus consists of approx. 1M documents and 2B words in total (1.5G of text after xz compression).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Core Statistics</head><p>More than 2,500 spans were annotated in around 600 documents representing either bond issue prospectuses, non-disclosure agreement documents or annual reports of charitable organizations (the detailed statistics regarding the dataset are presented in <ref type="table" target="#tab_3">Table 2</ref>).</p><p>Annotated clauses differ substantially from what can be found in existing sentence entailment challenges in terms of sentence length and complexity. SNLI contains less than 1% of sentences longer than 20 words, MultiNLI 5%, whereas in the case of clauses, we expect to return and consider it is 93% (and 77% of all spans in our shared task are longer than 20 words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Framework</head><p>Documents were split into halves to form validation and test sets for the purposes of few-shot semantic retrieval challenge. Evaluation is performed by means of a repeated random sub-sampling validation procedure. Sub-samples (k-combinations for each of 21 clauses, k ∈ [2, 6]) drawn from a particular set of annotations are split into k − 1 seed documents and 1 target document. Thus, clauses similar to the seed are expected to be returned from the target. We observed that the choice of input examples have an immense impact on the score. It is thus far more important to evaluate various seed configurations that various target documents. On the other hand, we wanted to keep the computational cost of evaluation reasonably small, so either the number of seed configurations had to be  reduced or the number of target documents for each configuration.</p><p>The selected k interval results in 1-shot to 5-shot learning, considered to be few-shot learning <ref type="bibr" target="#b37">(Wang et al., 2019)</ref>, whereas with the chosen number of sub-samples we expect improvements of 0.01 F 1 to be significant. Note that the 1-5 range denotes the number of annotated documents available, and it is possible that the same clause type appeared twice in one document, resulting in a higher number of clause instances.</p><p>Soft F 1 metric on character-level spans is used for the purpose of evaluation, as implemented in GEval tool <ref type="bibr">(Graliński et al., 2019)</ref>. Roughly speaking, this is the conventional F 1 measure, with precision and recall definitions altered to reflect the partial success of returning entities. In the case of the expected clause ranging between <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> characters and the answer with ranges [1, 3], <ref type="bibr">[10,</ref><ref type="bibr">15]</ref> (the system assumes a clause occurs twice within the document), recall equals 0.75 (since this is the part of the relevant item selected) and precision equals ca. 0.33 (since this is the number of selected characters which turned out to be relevant). The Hungarian algorithm <ref type="bibr" target="#b5">(Burkard et al., 2012)</ref> is employed to solve the problem of expected and returned range assignments. Soft F 1 has the desired property of being based on the widely utilized F 1 metric while abandoning the binary nature of the match, which is undesirable in the case dealt with in the task described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Competitive Baselines</head><p>Solutions based on networks consuming pairs of sequences, such as BERT in sentence pair classification task setting <ref type="bibr" target="#b11">(Devlin et al., 2018a)</ref>, are considered out of the scope of this paper since they are suboptimal in terms of performance-they require expensive encoding of all combinations from the Cartesian product between seeds and targets, making such solutions unsuitable for semantic similarity search due to the combinatorial explosion <ref type="bibr" target="#b31">(Reimers and Gurevych, 2019)</ref>. Because of the aforementioned problem and the fact that conventional classifiers require much more data than available in a few-shot setting, in this section, we describe simple k-NN-based approaches that we propose as baseline solutions to the problem stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Processing Pipeline</head><p>Evaluated solutions assume pre-encoding of all candidate segments and can be described within the unified framework consisting of segmenters, vectorizers, projectors, aggregators, scorers, and choosers ordered in a pipeline of transformations.</p><p>Segmenter is used to split a text into candidate sub-sequences to be encoded and considered in further steps. All the described solutions rely on a candidate sentence and n-grams of sentences, determined with the spaCy CNN model trained on OntoNotes. 3 Vectorizer produces vector representations of texts on either word, sub-word, or segment (e.g., sentence) level. In our case, vectorization was based on TF-IDF representations, static word embeddings, and neural sentence encoders. Projector projects embeddings into a different space (e.g., decomposition methods such as PCA or ICA). Aggregator has the capability to use word or sub-word unit embeddings to create a segment embedding (e.g., embedding mean, inverse frequency weighting, autoencoder). Scorer compares two or more embeddings and returns computed similarities. Since we often compare multiple seed embeddings with one embedding of a candidate segment, a scorer includes policies to aggregate scores obtained for multiple seeds into the final candidate score (e.g., mean of individual cosine similarities or max-pooling over Word Mover Distances). Chooser determines whether to return a candidate segment with a given score (e.g., threshold, one best per document, or a combination thereof). For the sake of simplicity, during the evaluation, we restricted ourselves to the chooser returning only one, the most similar candidate. It is not optimal (because multiple might be expected), but we consider this setting a good reference for further methods.</p><p>The proposed taxonomy is consistent with the assumptions made by <ref type="bibr">Gillick et al. (2018)</ref>. It is presented in order to highlight the similarities and differences between particular solutions when they are introduced and compared within the ablation The aim of the Scout Association is to promote the development of young people in achieving their full physical, intellectual, social and spiritual potentials, as individuals, as responsible citizens and as members of their local, national and international communities. The method of achieving the Aim of the Association is by providing an enjoyable and attractive scheme of progressive training based on the Scout Promise and Law and guided by Adult leadership.</p><p>GOVERNING DOCUMENT (160/174) Information about the legal document which represents the rule book for the way in which a charity operates (title, date of creation etc.).</p><p>The Open University Students Educational Trust (OUSET) is controlled by its governing document, a deed of trust, dated 22 May 1982 as amended by a scheme dated 9 October 1992 and constitutes an unincorporated charity.</p><p>TRUSTEE APPOINTMENT (153/168) Procedures for selecting trustees and the term of office.</p><p>As per the governing document, four of the Trustee positions are appointed by virtue of their position within the Open University Students Association (OUSA). One further position is appointed by virtue of their previous position within OUSA. One Trustee is nominated by the Vice Chancellor of the Open University (OU) and there are co-opted positions whereby the Trustees are empowered to approach up to two other persons to act as Trustees. It is envisaged that all Trustees will serve a general term of two years in line with the main election periods within OUSA.</p><p>RESERVES POLICY (170/185) What are the current financial reserves of the organization and how much these reserves should be as assumed?</p><p>The Trustees regularly reviews the amount of reserves that are required to ensure that they are adequate to fulfill the charities continuing obligations. INCOME SUMMARY (124/134) General information on income for the last year, sometimes associated with information on expenses.</p><p>Excluding the adjustments for FRS17 in respect of Pension Fund the results by way of net incoming resources accumulated f3.85m as against E6.78m in 2014, however last years performance benefited from extraordinary property sales generating a profit of F3.15m.</p><p>AUDITOR <ref type="bibr">OPINION (190/192</ref>) Summary of the opinion of an independent auditor or inspector, often in the form of a list of points.</p><p>In connection with my examination, no matter has come to my attention: 1. which gives me reasonable cause to believe that in any material respect the requirements to keep accounting records in accordance with Section 130 of the Charities Act; and to prepare accounts which accord with the accounting records and comply with the accounting requirements of the Charities Act have not been met; or 2. to which, in my opinion, attention should be drawn in order to enable a proper understanding of the accounts to be reached. <ref type="table">Table 3</ref>: Clauses annotated in Charity Annual Reports (one of three groups of documents included in the shared task). The values in parentheses indicate the number of documents with a particular clause and the total number of clause instances, respectively. More examples are available in Appendix C. studies later in this paper. The next section describes vectorizers, aggregators, and scorers used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Vectorizers</head><p>We intend to provide results of TF-IDF representations, as well as two methods that may be considered the state of the art of sentence embedding. The latter include Universal Sentence Encoder (USE) and Sentence-BERT.</p><p>USE is a Transformer-based encoder, where an element-wise sum of word representations is treated as a sentence embedding <ref type="bibr" target="#b7">(Cer et al., 2018)</ref>, trained with the multi-task objective. Sentence-BERT is a modification of the pretrained BERT network, utilizing Siamese and triplet network structures to derive sentence embeddings, trained with the explicit objective of making them comparable with cosine similarity <ref type="bibr" target="#b31">(Reimers and Gurevych, 2019)</ref>. In both cases the original models released by the authors were used for the purposes of evaluation.</p><p>In addition, multiple contextual embeddings from Transformer-based language models, as well as static (context-less) GloVe word embeddings were tested <ref type="bibr" target="#b25">(Pennington et al., 2014)</ref>. Many approaches to generating context-dependent vector representations have been proposed in recent years (e.g., <ref type="bibr" target="#b26">Peters et al. (2018)</ref>; <ref type="bibr">Vaswani et al. (2017)</ref>). One important advantage over static embeddings is the fact that every occurrence of the same word is assigned a different embedding vector based on the context in which the word is used. Thus, it is much easier to address issues arising from pre-trained static embeddings (e.g., taking into consideration polysemy of words). For the purposes of evaluation, we relied on Transformer-based models provided by authors of particular architectures, utilizing the Transformers library <ref type="bibr">(Wolf et al., 2019)</ref>. These include BERT <ref type="bibr" target="#b12">(Devlin et al., 2018b)</ref>, GPT-1 <ref type="bibr" target="#b29">(Radford, 2018)</ref>, GPT-2 , and RoBERTa . They differ substantially and introduce many innovations, though they are all based on either the encoder or the decoder from the original model proposed for sequence-to-sequence problems <ref type="bibr">(Vaswani et al., 2017)</ref>. Selected models were fine-tuned on using the next word prediction task on the Edgar corpus we release and re-evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Aggregators</head><p>In addition to conceptually simple methods such as average or max-polling operations, multiple solutions to utilizing word embeddings for comparing documents can be used. In addition to embeddings mean we evaluated the Smooth Inverse Frequency (SIF), Word Mover's Distance (WMD) and Discrete Cosine Transform (DCT).</p><p>SIF is a method proposed by <ref type="bibr" target="#b1">Arora et al. (2017)</ref>, where a representation of a document is obtained in two steps. First, each word embedding is weighted by a/(a + f r ), where f r stands for the underlying word's relative frequency, and a is the weight parameter. Then, the projections on the first tSVDcalculated principal component are subtracted, providing final representations.</p><p>WMD is a method of calculating a similarity between documents. For two documents, embeddings calculated for each word (e.g., with GloVe) are matched between documents, so that semantically similar pairs of words between documents are detected. This matching procedure generally leads to better results than simply averaging over embeddings for documents and calculating similarity between centers of mass of documents as their similarity <ref type="bibr" target="#b18">(Kusner et al., 2015)</ref>. Recently, <ref type="bibr" target="#b43">Zhao et al. (2019)</ref> showed it might be beneficial to use the method with contextual word embeddings.</p><p>DCT is a way to generate document-level representations in an order-preserving manner, adapted from image compression to NLP by <ref type="bibr" target="#b0">Almarwani et al. (2019)</ref>. After mapping an input sequence of real numbers to the coefficients of orthogonal cosine basis functions, low-order coefficients can be used as document embeddings, outperforming vector averaging on most tasks, as shown by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table 4 recapitulates the most important results of the completed evaluation.</p><p>Sentence-BERT and Universal Sentence Encoder could not outperform the simple TF-IDF approach, especially when SVD decomposition was applied (the setting commonly referred to as Latent Semantic Analysis). Static word embeddings with SIF weighting performed similarly to TF-IDF, or better, provided they were trained on a legal text corpus rather than on general English. It could not be clearly confirmed whether the use of WMD or DCT is beneficial. For the latter, the best results were achieved with c 0 , which in the case of the k-NN algorithm leads to the same answers as meanpooling and thus is not reported in the table. In case of c 0:n where n &gt; 0 constant decrease of k-NN methods performance was observed (Appendix B).</p><p>Interestingly, from all the released USE models, the multilingual ones performed best -for the monolingual universal-sentence-encoder-large model, scores were ten percentage points lower. The best Sentence-BERT model performed significantly worse than the best USE-note that the authors of Sentence-BERT compared it to monolingual models released earlier, which they indeed outperform. Moreover, Sentence-BERT does not perform better than BERT trained with whole word masking, although there is no Sentence-BERT equivalent of this model available so far. GPT-1, last layer (fine-tuned) mean cosine mean 0.47 1-3 sen.</p><p>GPT-1, last layer (fine-tuned) fICA (500) mean cosine mean 0.49 1-3 sen.</p><p>GPT-2, last layer (large, fine-tuned) mean cosine mean 0.46 1-3 sen.</p><p>GPT-2, last layer (large, fine-tuned) fICA (400) mean cosine mean 0.51 human 0.84 In cases of averaging (sub)word embeddings from the last layer of neural Language Models, the results were either comparable or inferior to TF-IDF. The best-performing language models were GPT-1 and GPT-2. Fine-tuning of these on a subsample of a legal text corpus improved the results significantly, by a factor of 3-7 points. LMs seem to benefit neither from SIF nor from the removal of a single common component; their performance can, however, be mildly improved with a conventionally used decomposition, such as ICA <ref type="bibr">(Hyvärinen and Oja, 2000)</ref>. Substantial improvement can be achieved by considering segments different from a single sentence, such as n-grams of sentences (meaning that any contiguous sequence of up to n sentences from a given text was scored and could be returned as a result). <ref type="figure" target="#fig_1">Figure 2</ref> presents how the performance of particular methods changes as a function of the number of example documents available within the simple similarity averaging scheme used in all the presented solutions. In general, the methods benefit substantially from the availability of a second exam-ple. A bigger number leads to a decreased variance but yields no improvement in the median score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The brief evaluation presented in the previous section has multiple limitations. First, it assumed retrieval of a single, most similar segment, whereas it appears that multiple clauses might be returned instead. However, we consider this restriction justifiable during a preliminary comparison of applicable methods. Multiple alternative selectors may be proposed in the future.</p><p>Secondly, all the evaluated methods assume scoring with the policy of averaging individual similarities. We encourage readers to experiment with different pooling methods or meta-learning strategies. Moreover, even the LM-based methods we had studied the most can be further studied in the proposed shared task. For example, only embeddings from the last layer were evaluated, even though it is possible that the higher layers may capture semantics better.</p><p>Finally, it is in principle possible to address the task in entirely different ways, for example, by per-forming neither segmentation nor aggregation of word embeddings at all, but by matching clauses on the word level instead, which may be an interesting direction for further research. We decided to take the most common and straightforward way, due to fact performed evaluations are to serve as baselines for other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>There is a large and varied body of work related to information retrieval in general; however, following Gillick et al. <ref type="formula">(2018)</ref> we consider the problem stated in an end-to-end manner, where the nearest neighbor search is performed on dense document representations. With this assumption, the main issue is to obtain reliable representations of documents, where by document we mean any selfcontained unit that can be returned to the user as a search result <ref type="bibr" target="#b6">(Büttcher et al., 2010)</ref>. We use the term segment with the same meaning wherever it aids clarity.</p><p>Many approaches considered in the literature rely on word embedding and aggregation strategies. Simple methods proposed include averaging, as in the continuous bag-of-words (CBOW) model <ref type="bibr" target="#b22">(Mikolov et al., 2013)</ref> or frequencyweighted averaging with the decomposition method applied <ref type="bibr" target="#b1">(Arora et al., 2017)</ref>. More sophisticated schemes include utilizing multiple weights, such as a novelty score, a significance score, and a corpuswise uniqueness  or computing a vector of locally aggregated descriptors (Ionescu and Butnaru, 2019). Most of the proposed methods are orderless, and their limitations were recently discussed by <ref type="bibr" target="#b20">Mai et al. (2019)</ref>. However, there are also pooling approaches preserving spatial information, such as a hierarchical pooling operation <ref type="bibr" target="#b32">(Shen et al., 2018)</ref>. Other methods of obtaining sentence representations from word embeddings include training an autoencoder on a large collection of unlabeled data  or utilizing random encoders <ref type="bibr" target="#b38">(Wieting and Kiela, 2019)</ref>. Despite its shortcomings and the availability of many sophisticated alternatives, the CBOW model is a common choice due to its ability to ensure strong results on many downstream tasks.</p><p>Different approaches assume training encoders through document embedding in an unsupervised or supervised manner, without the need for explicit aggregation. The former include Skip-Thought Vectors, trained with the objective of reconstruct-ing the surrounding sentences of an encoded passage <ref type="bibr" target="#b17">(Kiros et al., 2015)</ref>. Although this method was outperformed by supervised models trained on a single NLI task <ref type="bibr" target="#b9">(Conneau et al., 2017)</ref>, paraphrase corpora <ref type="bibr" target="#b15">(Jiao et al., 2018)</ref> or multiple tasks <ref type="bibr" target="#b33">(Subramanian et al., 2018)</ref>, the objective of predicting the next sentence is used as an additional objective in multiple novel models, such as the Universal Sentence Encoder <ref type="bibr" target="#b7">(Cer et al., 2018)</ref>. Even though many Transformer-based language models implement their own pooling strategy for generating sentence representations (special token pooling), they were shown to yield weak sentence embeddings, as described recently by <ref type="bibr" target="#b31">Reimers and Gurevych (2019)</ref>. The authors proposed a superior method of fine-tuning a pretrained BERT network with Siamese and triplet network structures to obtain sentence embeddings.</p><p>There were attempts to utilize semantic similarity methods explicitly in the legal domain, e.g., for a case law entailment within the COLIEE shared task. In a recent edition, <ref type="bibr" target="#b28">Rabelo et al. (2019)</ref> used a BERT model fine-tuned on a provided training set in a supervised manner, and achieved the highest F-score among all teams. However, due to the reasons discussed in Section 4, their approach is not consistent with the nearest neighbor search, which is what we are aiming for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary and Conclusions</head><p>We have introduced a new shared task of semantic retrieval from legal texts, which differs substantially from conventional NLI. It is heavily inspired by enterprise solutions referred to as contract discovery, focused on ensuring the inclusion of relevant clauses or their retrieval for further analysis. The main distinguishing characteristic of Contract Discovery shared task is conceptual, since:</p><p>• Candidate sequences are being mined from real texts. It is assumed span identification should be performed (systems should be able to return any document substring without any segmentation given in advance).</p><p>• It is suited for few-shot methods, filling the gap between conventional sentence classification and NLI tasks based on sentence pairs.</p><p>For the purposes of providing competetive baselines, we considered the problem stated in an endto-end manner, where the nearest neighbor search is performed on document representations. With this assumption, the main issue was to obtain representations of text fragments, which we referred to as segments. The description of the task was followed by the evaluation of multiple k-NN-based solutions within the unified framework, which may be used to describe future solutions. Moreover, a practical justification for handling the problem with k-NN was briefly introduced.</p><p>It has been shown that in this particular setting, pretrained, universal encoders fail to provide satisfactory results. One may suspect that this is a result of the difference between the domain they were trained on and the legal domain. During the evaluation, solutions based on the Language Models performed well, especially when unsupervised finetuning was applied. In addition to the aforementioned ability to fine-tune the method on legal texts, the most important indicator of success so far has been the involvement of multiple, sometimes overlapping substrings instead of sentences. Moreover, it has been demonstrated that the methods benefit substantially from the availability of a second example, and the presence of more leads to a decrease in variance, even when a simple similarity averaging scheme is applied.</p><p>The discussion regarding the presented methods and their limitations briefly outlined possible measures towards improving the baseline methods. In addition to the dataset and reference results, legalspecialized LMs have been made released to assist the research community in performing further experiments.</p><p>The Contract Discovery dataset, Edgar Corpus, we crawled, and all the mentioned models are publicly available on GitHub: https://github.com /applicaai/contract-discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The Smart Growth Operational Programme supported this research under project no. POIR.01.01.01-00-0605/19 (Disruptive adoption of Neural Language Modelling for automation of text-intensive work).</p><p>There are no copyright issues regarding the Contract Discovery dataset, as both sources belong to the public domain. Documents were annotated ethically by our co-workers. Moreover, the colleagues who participated in annotation are among the authors of the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A File Structure</head><p>The documents' content can be found in the reference.tsv files. The input files in.tsv consist of tab-separated fields: Target ID (e.g. 57), Clause considered (e.g. governing-law), Example #1 (e. <ref type="figure" target="#fig_1">g. 59 15215-15453)</ref>, . . . , Example #N. Each example consists of document ID and characters range. Ranges can be discontinuous. In such a case the sequences are separated with a comma, e.g. 4103-4882,12127-12971. The file with answers (expected.tsv) contains one answer per line, consisting of the entity name (to be copied from input) and characters range in the same format as described above. The reference file contains two tab-separated fields: document ID and content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Other Evaluation Results</head><p>Tables below describe evaluation results which were not included in the paper (or were included without broader context, that is without reference to different results from the same class of solutions). <ref type="table" target="#tab_7">Table 5</ref> presents results with all the evaluated Sentence-BERT models. <ref type="table" target="#tab_8">Table 6</ref> shows scores achieved by TF-IDF with different settings, including other n-gram ranges. Results of particular Universal Sentence Encoder models are presented in <ref type="table" target="#tab_9">Table 7</ref>. <ref type="table" target="#tab_10">Table 8</ref> shows results of Transformerbased Language Models not included in the paper. Finally, <ref type="table" target="#tab_11">Table 9</ref> is devoted to analysis of Discrete Cosine Transform embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Soft <ref type="formula">F1</ref> bert-base-nli-cls-token 0.29 bert-base-nli-max-tokens 0.30 bert-base-nli-mean-tokens 0.31 bert-base-nli-stsb-mean-tokens 0.32 bert-base-wikipedia-sections-mean-tokens 0.25 bert-large-nli-cls-token 0.29 bert-large-nli-max-tokens 0.30 bert-large-nli-mean-tokens 0.30 bert-large-nli-stsb-mean-tokens 0.31 roberta-base-nli-mean-tokens 0.28 roberta-base-nli-stsb-mean-tokens 0.29 roberta-large-nli-mean-tokens 0.31 roberta-large-nli-stsb-mean-tokens 0.31     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Rest of the Clauses Considered</head><p>Random subsets of bond issue prospectuses and non-disclosure agreement documents from the US EDGAR database 6 , as well as annual reports of charitable organizations from the UK Charity Register 7 were annotated, in such a way that clauses of the same type were selected (e.g. determining the governing law, merger restrictions, tax changes call or reserves policy). Clause types depend on the type of a legal act and can consist of a single sentence, multiple sentences or sentence fragments. Tables bellow present clause types annotated in each of the document groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clause (Instances) Example</head><p>GOVERNING LAW (152/160) The parties agree on which jurisdiction the contract will be subject to.</p><p>This Agreement shall be governed by and construed in accordance with the laws of the State of California without reference to its rules of conflicts of laws.</p><p>CONFIDENTIAL PERIOD (108/122) The parties undertake to maintain confidentiality for a certain period of time. You agree that for a period of eighteen months (18) from the date hereof you will not directly or indirectly recruit, solicit or hire any regional or district managers, corporate office employee, member of senior management of the Company (including store managers), or other employee of the Company identified to you.</p><p>CONFIDENTIAL INFORMATION FORM (152/174) Forms and methods of providing confidential information.</p><p>"Confidential Information" means any technical or commercial information or data, trade secrets, know-how, etc., of either Party or their respective Affiliates whether or not marked or stamped as confidential, including without limitation, Technology, Invention(s), Intellectual Property Rights, Independent Technology and any samples of products, materials or formulations including, without limitation, the chemical identity and any properties or specifications related to the foregoing. Any Development Program Technology, MPM Work Product, MSC Work Product, Hybrid Work Product, Prior End-Use Work Product and/or Shared Development Program Technology shall be Confidential Information of the Party that owns the subject matter under the terms set forth in this Agreement. DISPUTE RESOLUTION (67/68) Arrangements for how to resolve disputes (arbitration, courts).</p><p>The Parties will attempt in good faith to resolve any dispute or claim arising out of or in relation to this Agreement through negotiations between a director of each of the Parties with authority to settle the relevant dispute. If the dispute cannot be settled amicably within fourteen (14) days from the date on which either Party has served written notice on the other of the dispute then the remaining provisions of this Clause shall apply. Upon the occurrence of a Change of Control Triggering Event (as defined below with respect to the notes of a series), unless we have exercised our right to redeem the notes of such series as described above under "Optional Redemption," the indenture provides that each holder of notes of such series will have the right to require us to repurchase all or a portion (equal to $2,000 or an integral multiple of $1,000 in excess thereof) of such holder's notes of such series pursuant to the offer described below (the "Change of Control Offer"), at a purchase price equal to 101% of the principal amount thereof, plus accrued and unpaid interest, if any, to the date of repurchase, subject to the rights of holders of notes of such series on the relevant record date to receive interest due on the relevant interest payment date. CHANGE OF CONTROL NOTICE (78/79) Information about the obligation to inform bondholders (usually by mail) about the event of change of control. This clause usually follows immediately the above clause.</p><p>Within 30 days following any Change of Control, B&amp;G Foods will mail a notice to each holder describing the transaction or transactions that constitute the Change of Control and offering to repurchase notes on the Change of Control Payment Date specified in the notice, which date will be no earlier than 30 days and no later than 60 days from the date such notice is mailed, pursuant to the procedures required by the indenture and described in such notice. Holders electing to have a note purchased pursuant to a Change of Control Offer will be required to surrender the note, with the form entitled "Option of Holder to Elect Purchase" on the reverse of the note completed, to the paying agent at the address specified in the notice of Change of Control Offer prior to the close of business on the third business day prior to the Change of Control Payment Date.</p><p>CROSS DEFAULT (96/110) The company does not comply with certain conditions (event of default), so the bonds become due (e.g. when the company does not submit financial statements on time) -our clause was limited to the event of non-repayment, usually the minimum sum is given. due to our default, we (i) are bound to repay prematurely indebtedness for borrowed moneys with a total outstanding principal amount of $75,000,000 (or its equivalent in any other currency or currencies) or greater, (ii) have defaulted in the repayment of any such indebtedness at the later of its maturity or the expiration of any applicable grace period or (iii) have failed to pay when properly called on to do so any guarantee of any such indebtedness, and in any such case the acceleration, default or failure to pay is not being contested in good faith and not cured within 15 days of such acceleration, default or failure to pay; LITIGATION DEFAULT (42/51) Court verdict or administrative decision which charge the company for a significant unpaid amount (another from the series of event of default).</p><p>(8) one or more judgments, orders or decrees of any court or regulatory or administrative agency of competent jurisdiction for the payment of money in excess of $30 million (or its foreign currency equivalent) in each case, either individually or in the aggregate, shall be entered against the Company or any subsidiary of the Company or any of their respective properties and shall not be discharged and there shall have been a period of 60 days after the date on which any period for appeal has expired and during which a stay of enforcement of such judgment, order or decree, shall not be in effect;</p><p>MERGER RESTRICTIONS (188/241) A clause preventing the merger or sale of a company, etc., except under certain conditions (generally, the company should not avoid its obligations to its bondholders).</p><p>Without the consent of the holders of the outstanding debt securities under the indentures, we may consolidate with or merge into, or convey, transfer or lease our properties and assets to any person and may permit any person to consolidate with or merge into us. However, in such event, any successor person must be a corporation, partnership, or trust organized and validly existing under the laws of any domestic jurisdiction and must assume our obligations on the debt securities and under the applicable indenture. We agree that after giving effect to the transaction, no event of default, and no event which, after notice or lapse of time or both, would become an event of default shall have occurred and be continuing and that certain other conditions are met; provided such provisions will not be applicable to the direct or indirect transfer of the stock, assets or liabilities of our subsidiaries to another of our direct or indirect subsidiaries. (Section 801) BONDHOLDERS DEFAULT (191/241) A clause on the payment of the principal amount and interest -they become due as a result of an event of default, if such a declaration is made by bondholders.</p><p>If an event of default (other than an event of default referred to in clause (5) above with respect to us) occurs and is continuing, the trustee or the holders of at least 25% in aggregate principal amount of the outstanding notes by notice to us and the trustee may, and the trustee at the written request of such holders shall, declare the principal of and accrued and unpaid interest, if any, on all the notes to be due and payable. Upon such a declaration, such principal and accrued and unpaid interest will be due and payable immediately. If an event of default referred to in clause (5) above occurs with respect to us and is continuing, the principal of and accrued and unpaid interest on all the notes will become and be immediately due and payable without any declaration or other act on the part of the trustee or any holders.</p><p>TAX CHANGES CALL (48/56) A clause about the possibility of an earlier redemption of the bond by the issuer if the tax law or its interpretation changes.</p><p>If, as a result of any change in, or amendment to, the laws (or any regulations or rulings promulgated under the laws) of the Netherlands or the United States or any taxing authority thereof or therein, as applicable, or any change in, or amendments to, an official position regarding the application or interpretation of such laws, regulations or rulings, which change or amendment is announced or becomes effective on or after the date of the issuance of the notes, we become or, based upon a written opinion of independent counsel selected by us, will become obligated to pay additional amounts as described above in "Payment of additional amounts," then the Issuer may redeem the notes, in whole, but not in part, at 100% of the principal amount thereof together with unpaid interest as described in the accompanying prospectus under the caption "Description of WPC Finance Debt Securities and the Guarantee-Redemption for Tax Reasons." FINANCIAL STATEMENTS (201/317) A clause on the obligation to submit (usually to the SEC) annual reports or other reports.</p><p>Notwithstanding that the Company may not be subject to the reporting requirements of Section 13 or 15(d) of the Exchange Act, the Company will file with the SEC and provide the Trustee and Holders and prospective Holders (upon request) within 15 days after it files them with the SEC, copies of its annual report and the information, documents and other reports that are specified in Sections 13 and 15(d) of the Exchange Act. In addition, the Company shall furnish to the Trustee and the Holders, promptly upon their becoming available, copies of the annual report to shareholders and any other information provided by the Company to its public shareholders generally. The Company also will comply with the other provisions of Section 314(a) of the TIA. <ref type="table" target="#tab_0">Table 11</ref>: Clauses annotated in Corporate Bonds. The values in parentheses indicate the number of documents with a particular clause and the total number of clause instances, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Example</head><label></label><figDesc>MAIN OBJECTIVE (195/231) The main objective of a charitable organization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Performance as a function of the number of example documents available (solutions based on LMs). The methods benefit substantially from availability of a second example document and a bigger number leads to a decreased variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of existing shared tasks. Most of the related NLP tasks do not assume Span Identification (SI), even those outside the legal domain (Legal). Moreover, the few-shot setting is not popular within the field of NLP yet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>arXiv:1911.03911v2 [cs.CL] 8 Oct 2020</figDesc><table><row><cell>Target document</cell><cell></cell><cell></cell></row><row><cell>Full text to search in</cell><cell>Target spans</cell><cell></cell></row><row><cell>Input</cell><cell>Spans in target document, representing the same</cell><cell>Output</cell></row><row><cell></cell><cell>clause as seed spans</cell><cell></cell></row><row><cell>Seeds</cell><cell></cell><cell></cell></row><row><cell>Few spans in sample</cell><cell></cell><cell></cell></row><row><cell>documents</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Core statistics regarding released dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Selected results when returning a single, most similar segment, determined with given segmenters, vector- izers, projectors, scorers and aggregators. The symbol indicates only the best models from each architecture are presented here (results for the remaining ones are available in Appendix B).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles. 2015. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 961-970.</figDesc><table><row><cell>Alexander Fritzler, Varvara Logacheva, and Maksim</cell></row><row><cell>Kretov. 2019. Few-shot classification in named en-</cell></row><row><cell>tity recognition task. In Proceedings of the 34th</cell></row><row><cell>ACM/SIGAPP Symposium on Applied Computing,</cell></row><row><cell>SAC '19, pages 993-1000, New York, NY, USA.</cell></row><row><cell>ACM.</cell></row><row><cell>Daniel Gillick, Alessandro Presta, and Gaurav Singh</cell></row><row><cell>Tomar. 2018. End-to-end retrieval in continuous</cell></row><row><cell>space.</cell></row><row><cell>Filip Graliński, Anna Wróblewska, Tomasz Stani-</cell></row><row><cell>sławek, Kamil Grabowski, and Tomasz Górecki.</cell></row><row><cell>2019. GEval: Tool for debugging NLP datasets and</cell></row><row><cell>models. In Proceedings of the 2019 ACL Workshop</cell></row><row><cell>BlackboxNLP: Analyzing and Interpreting Neural</cell></row><row><cell>Networks for NLP, pages 254-262, Florence, Italy.</cell></row><row><cell>Association for Computational Linguistics.</cell></row><row><cell>N. Halko, P. G. Martinsson, and J. A. Tropp. 2011.</cell></row><row><cell>Finding structure with randomness: Probabilistic al-</cell></row><row><cell>gorithms for constructing approximate matrix de-</cell></row><row><cell>compositions. SIAM Rev., 53(2):217-288.</cell></row><row><cell>Aapo Hyvärinen and Erkki Oja. 2000. Independent</cell></row><row><cell>component analysis: algorithms and applications.</cell></row><row><cell>Neural networks : the official journal of the Inter-</cell></row><row><cell>national Neural Network Society, 13 4-5:411-30.</cell></row><row><cell>Radu Tudor Ionescu and Andrei M. Butnaru. 2019.</cell></row><row><cell>Vector of Locally-Aggregated Word Embeddings</cell></row><row><cell>(VLAWE): A Novel Document-level Representa-</cell></row><row><cell>tion.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results of Sentence-BERT models on the test-A dataset when returning the most similar sentence.</figDesc><table><row><cell>Names as in sentence-transformers library: https://</cell></row><row><cell>github.com/UKPLab/sentence-transformers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results of TF-IDF on the test-A dataset when returning the most similar sentence.</figDesc><table><row><cell>Model</cell><cell>Soft F1</cell></row><row><cell>multilingual/1</cell><cell>0.38</cell></row><row><cell>multilingual-large/1</cell><cell>0.33</cell></row><row><cell>multilingual-qa/1</cell><cell>0.28</cell></row><row><cell>large/3</cell><cell>0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Results of Universal Sentence Encoder models on the test-A dataset when returning the most similar sentence.</figDesc><table><row><cell>Model</cell><cell>Soft F1</cell></row><row><cell>bert-base-cased</cell><cell>0.25</cell></row><row><cell>bert-base-multilingual-cased</cell><cell>0.24</cell></row><row><cell>bert-base-multilingual-uncased</cell><cell>0.32</cell></row><row><cell>bert-base-uncased</cell><cell>0.26</cell></row><row><cell>bert-large-cased</cell><cell>0.21</cell></row><row><cell>bert-large-cased-whole-word-masking</cell><cell>0.31</cell></row><row><cell>bert-large-uncased</cell><cell>0.18</cell></row><row><cell></cell><cell>0.35</cell></row><row><cell>bert-large-uncased-whole-word-masking</cell><cell></cell></row><row><cell>roberta-base</cell><cell>0.25</cell></row><row><cell></cell><cell>0.32</cell></row><row><cell>roberta-large</cell><cell></cell></row><row><cell></cell><cell>0.36</cell></row><row><cell>openai-gpt</cell><cell></cell></row><row><cell>gpt2</cell><cell>0.16</cell></row><row><cell>gpt2-medium</cell><cell>0.11</cell></row><row><cell>gpt2-large</cell><cell>0.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="2">: Results of particular Transformer-based Lan-</cell></row><row><cell cols="2">guage Models (without finetuning) on the test-A dataset</cell></row><row><cell cols="2">when returning the most similar sentence. Names as in</cell></row><row><cell cols="2">transformers library: https://github.com/huggi</cell></row><row><cell>ngface/transformers</cell><cell></cell></row><row><cell>C</cell><cell>Soft F1</cell></row><row><cell>c 0</cell><cell>0.36</cell></row><row><cell>c 0:1</cell><cell>0.30</cell></row><row><cell>c 0:2</cell><cell>0.25</cell></row><row><cell>c 0:3</cell><cell>0.20</cell></row><row><cell>c 0:4</cell><cell>0.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>: Results of GloVe embeddings (300d, EDGAR)</cell></row><row><cell>on the test-A dataset when Discrete Cosine Transform</cell></row><row><cell>sentence embeddings were created. The c 0 is equiva-</cell></row><row><cell>lent to embeddings mean when k-NN methods are con-</cell></row><row><cell>sidered. The similar decrease of performance was ob-</cell></row><row><cell>served for other models.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Clauses annotated in Non-disclosure Agreements. The values in parentheses indicate the number of documents with a particular clause and the total number of clause instances, respectively. CHANGE OF CONTROL COVENANT (88/95) Information about the obligation to redeem bonds for 101% of the price in the event of change of control.</figDesc><table><row><cell>Example</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.www.sec.gov/edgar.shtml 2 http://www.gov.uk/find-charity-inform ation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://github.com/explosion/spacy-mod els/releases/tag/en_core_web_sm-2.1.0</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">TF-IDF with truncated SVD decomposition is commonly referred to as Latent Semantic Analysis(Halko et al., 2011). 5 SVD in SIF method is used to perform removal of single common component<ref type="bibr" target="#b1">(Arora et al., 2017)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://www.www.sec.gov/edgar.shtml 7 http://www.gov.uk/find-charity-information</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Efficient sentence embedding using discrete cosine transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nada</forename><surname>Almarwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Aldarmaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Few-shot text classification with distributional signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06039</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">National Archives, Records Administration, and Office Of General</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fifteenth Text REtrieval Conference (TREC 2006) Proceedings</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Trec-2006 legal track overview</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Assignment Problems. Revised reprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Burkard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Dell&amp;apos;amico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Martello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM -Society of Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Seiten</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Information Retrieval -Implementing and Evaluating Search Engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cormack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
		<idno>abs/1803.11175</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Universal sentence encoder. CoRR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Factors affecting relevance judgment: a report from trec legal track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heting</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="264" to="278" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SemEval-2020 task 11: Detection of propaganda techniques in news articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rostislav</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
		<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">THU-MOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THU" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mos14/</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural network for universal sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2470" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Overview of COLIEE 2017</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshinobu</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><forename type="middle">Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLIEE@ICAIL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">CBOW is not all you need: Combining CBOW with the compositional matrix space model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Galke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansgar</forename><surname>Scherp</surname></persName>
		</author>
		<idno>abs/1902.06423</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Concept and context in legal information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamsin</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkhard</forename><surname>Schafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JURIX</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Extracting fairness policies from legal documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Nagpal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetna</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mallika</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samiulla</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameep</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Goyal</surname></persName>
		</author>
		<idno>abs/1809.04262</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation of information retrieval for e-discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Douglas</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Jason</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Law</title>
		<imprint>
			<biblScope unit="page" from="347" to="386" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An evaluation framework for plagiarism detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><forename type="middle">Rosso</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010: Posters</title>
		<meeting><address><addrLine>Beijing, China. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="997" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combining similarity and transformer methods for case law entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Rabelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi-Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
		<idno type="DOI">10.1145/3322640.3326741</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law, ICAIL &apos;19</title>
		<meeting>the Seventeenth International Conference on Artificial Intelligence and Law, ICAIL &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="290" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Baseline needs more love: On simple wordembedding-based models and associated pooling mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning general purpose distributed sentence representations via large scale multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<idno>abs/1804.00079</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Search on Speech from Spoken Queries: The Multi-Domain International ALBAYZIN 2018 Query-by-Example Spoken Term Detection Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Tejedor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doroteo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paula</forename><surname>Toledano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Lopez-Otero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Docio-Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">Javier</forename><surname>Peñagarikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Rodriguez-Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno-Sandoval</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13636-019-0156-x</idno>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Audio Speech Music Process</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A machine learning approach to identifying sections in legal briefs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Vanderbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Bockhorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chad</forename><surname>Oldfather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MAICS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Illia Polosukhin. 2017. Attention is all you need. CoRR, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Generalizing from a few examples: A survey on few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">No training required: Exploring random encoders for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R&amp;apos;emi</forename><surname>Louf</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="m">Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Zero-training sentence embedding via orthogonal basis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1810.00438</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning universal sentence representations with mean-max attention autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
