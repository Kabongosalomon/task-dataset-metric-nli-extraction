<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relation-Aware Entity Alignment for Heterogeneous Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
							<email>z.wang@lancaster.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing and Communications</orgName>
								<orgName type="institution">Lancaster University</orgName>
								<address>
									<country key="GB">U. K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
							<email>zhaodongyan@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Relation-Aware Entity Alignment for Heterogeneous Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entity alignment is the task of linking entities with the same real-world identity from different knowledge graphs (KGs), which has been recently dominated by embedding-based methods. Such approaches work by learning KG representations so that entity alignment can be performed by measuring the similarities between entity embeddings. While promising, prior works in the field often fail to properly capture complex relation information that commonly exists in multi-relational KGs, leaving much room for improvement. In this paper, we propose a novel Relation-aware Dual-Graph Convolutional Network (RDGCN) to incorporate relation information via attentive interactions between the knowledge graph and its dual relation counterpart, and further capture neighboring structures to learn better entity representations. Experiments on three real-world cross-lingual datasets show that our approach delivers better and more robust results over the state-of-the-art alignment methods by learning better KG representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) are the building blocks for various AI applications like question-answering ], text classification <ref type="bibr" target="#b4">[Wang et al., 2016]</ref>, recommendation systems , etc. Knowledge in KGs is usually organized into triples of head entity, relation, tail entity . There are considerable works on knowledge representation learning to construct distributed representations for both entities and relations. Exemplary works are the so called transfamily methods like TransE <ref type="bibr" target="#b0">[Bordes et al., 2013]</ref>, TransH <ref type="bibr" target="#b4">[Wang et al., 2014]</ref>, and PTransE <ref type="bibr" target="#b4">[Lin et al., 2015]</ref>, which interpret a relation as the translation operating on the embeddings of its head entity and tail entity.</p><p>However, KGs are usually incomplete, and different KGs are often complementary to each other. This makes a compelling case to design a technique that can integrate heterogeneous knowledge among different KGs. An effective way for doing this is Entity Alignment. There have been existing * Corresponding author.  <ref type="bibr" target="#b4">[Li et al., 2018b]</ref>). efforts devoted to embed different KGs towards entity alignment. Most of them, like JE <ref type="bibr" target="#b3">[Hao et al., 2016]</ref>, MTransE , <ref type="bibr">JAPE [Sun et al., 2017]</ref>, IPTransE <ref type="bibr" target="#b6">[Zhu et al., 2017]</ref> and BootEA <ref type="bibr" target="#b4">[Sun et al., 2018]</ref>, rely on trans-family models to learn entity representations according to a set of prior alignments. The most recent work <ref type="bibr" target="#b4">[Wang et al., 2018]</ref>, takes a different approach by utilizing the Graph Convolutional Networks (GCNs)  to jointly represent multiple KG entities, showing a new, promising direction for entity alignment. Compared with conventional feature based methods <ref type="bibr" target="#b4">[Sarasua et al., 2012;</ref><ref type="bibr" target="#b4">Mahdisoltani et al., 2013]</ref>, embedding-based methods have the advantage of requiring less human involvement in feature construction and can be scaled to large KGs. However, there are still several hurdles that prevent wider adoption of embedding-based approaches. First, as mentioned above, most existing methods use trans-family models as the backbone to embed KGs, which are constrained by the assumption head + relation ≈ tail. This strong assumption makes it inefficient for the model to capture more complex relation information in multi-relational graphs.</p><p>As a motivation example, <ref type="figure" target="#fig_0">Figure 1</ref> shows a real-world example from the DBP15K ZH−EN  dataset. Prior study <ref type="bibr" target="#b4">[Li et al., 2018b]</ref> shows that trans-family methods cannot capture the triangular structures depicted in the diagram. For instance, for the structure of <ref type="figure" target="#fig_0">Figure 1</ref>(a), TransE requires the three formulas v 1 + r a ≈ v 2 , v 2 + r a ≈ v 3 and v 1 + r a ≈ v 3 to hold at the same time. However, to satisfy the former two equations, we would have v 1 + 2r a ≈ v 3 , which is contradictory to the third equation v 1 + r a ≈ v 3 . Accordingly, the alignment performance will inevitably be compromised if the KG representations are learned with the transfamily, since more complex structures such as triangular ones frequently appear in multi-relational graphs.</p><p>The <ref type="bibr">GCN-based model [Wang et al., 2018]</ref> represents a leap forward for embedding-based entity alignment. However, this approach is also unable to properly model relation information. Since the vanilla GCN operates on the undirected and unlabeled graphs, a GCN-based model would ignore the useful relation information of KGs. Although the Relational Graph Convolutional Networks (R-GCNs) <ref type="bibr" target="#b4">[Schlichtkrull et al., 2018]</ref> could be used to model multirelational graphs, an R-GCN simply employs one weight matrix for each relation and would require an excessive set of parameters for real-world KGs that often contain thousands of relations. This drawback makes it difficult to learn an effective R-GCN model. Dual-Primal Graph CNN (DPGCNN) <ref type="bibr" target="#b4">[Monti et al., 2018]</ref> offers a new solution for the problem. DPGCNN alternates convolution operations on the graph and its dual graph, whose vertices correspond to the edges of the original graph, and iteratively applies a graph attention mechanism to enhance primal edge representations using its dual graph. Compared with GCNs and R-GCNs, the DPGCNN can better explore complex edge structures and produce better KG representations.</p><p>Inspired by the DPGCNN, in this paper, we propose a novel Relation-aware Dual-Graph Convolutional Network (RDGCN) to tackle the challenge of proper capturing and integration for relation information. While the DPGCNN serves a good starting point, applying it to learn KG representations is not trivial. Doing so requires us to find a way to better approximate relation representations and characterize the relationship between different KG relations. We address this by extending the DPGCNN to develop a weighted model, and explore the head/tail representations initialized with entity names as a proxy to capture relation information without excessive model parameters that are often hard to train.</p><p>As a departure from GCNs and R-GCNs, our RDGCN allows multiple rounds of interactions between the primal entity graph and its dual relation graph, enabling the model to effectively incorporate more complex relation information into entity representations. To further integrate neighboring structural information, we also extend GCNs with highway gates.</p><p>We evaluate our RDGCN on three real-world datasets. Experimental results show that RDGCN can effectively address the challenges mentioned above and significantly outperforms 6 recently proposed approaches on all datasets. The key contribution of this work is a novel DPGCNN-based model for learning robust KG representations. Our work is the first to extend DPGCNN for entity alignment, which yields significantly better performance over the state-of-theart alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Convolutional Networks</head><p>Recently, there has been an increasing interest in extending neural networks to deal with graphs. There have been many encouraging works which are often categorized as spectral approaches <ref type="bibr">[Bruna et al., 2014;</ref><ref type="bibr">Defferrard et al., 2016;</ref> and spatial approaches <ref type="bibr" target="#b0">[Atwood and Towsley, 2016;</ref><ref type="bibr" target="#b2">Hamilton et al., 2017;</ref><ref type="bibr" target="#b4">Veličković et al., 2018]</ref>. The GCNs  have recently emerged as a powerful deep learning-based approach for many NLP tasks like semantic role labeling  and neural machine translation <ref type="bibr" target="#b0">[Bastings et al., 2017]</ref>. Furthermore, as an extension of GCNs, the R-GCNs <ref type="bibr" target="#b4">[Schlichtkrull et al., 2018]</ref> have recently been proposed to model relational data and have been successfully exploited in link prediction and entity classification. Recently, the graph attention networks (GATs) <ref type="bibr" target="#b4">[Veličković et al., 2018]</ref> have been proposed and achieved state-of-the-art performance. The <ref type="bibr">DPGCNN [Monti et al., 2018]</ref> discussed in Section 1 generalizes GAT model and achieves better performance on vertex classification, link prediction, and graphguided matrix completion tasks.</p><p>Inspired by the capability of DPGCNN on determining neighborhood-aware edge features, we propose the first relation-aware multi-graph learning framework for entity alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Entity Alignment</head><p>Previous approaches of entity alignment usually require intensive expert participation <ref type="bibr" target="#b4">[Sarasua et al., 2012]</ref> to design model features <ref type="bibr" target="#b4">[Mahdisoltani et al., 2013]</ref> or an external source contributed by other users <ref type="bibr" target="#b4">[Wang et al., 2017]</ref>. Recently, embedding-based methods <ref type="bibr" target="#b3">[Hao et al., 2016;</ref><ref type="bibr" target="#b6">Zhu et al., 2017;</ref><ref type="bibr" target="#b4">Sun et al., 2018;</ref><ref type="bibr" target="#b4">Wang et al., 2018]</ref> have been proposed to address this issue. In addition, NTAM <ref type="bibr" target="#b4">[Li et al., 2018a</ref>] is a non-translational approach that utilizes a probabilistic model for the alignment task. KDCoE ] is a semi-supervised learning approach for co-training multilingual KG embeddings and the embeddings of entity descriptions.</p><p>As a departure from prior work, our approach directly models the relation information by constructing the dual relation graph. As we will show later in the paper, doing so improves the learned entity embeddings which in turn lead to more accurate alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>Formally, a KG is represented as G = (E, R, T ), where E, R, T are the sets of entities, relations and triples, respectively. Let G 1 = (E 1 , R 1 , T 1 ) and G 2 = (E 2 , R 2 , T 2 ) be two heterogeneous KGs to be aligned. That is, an entity in G 1 may have its counterpart in G 2 in a different language or in different surface names. As a starting point, we can collect a small number of equivalent entity pairs between G 1 and G 2 as the alignment seeds</p><formula xml:id="formula_0">L = {(e i1 , e i2 )|e i1 ∈ E 1 , e i2 ∈ E 2 }.</formula><p>We define the entity alignment task as automatically finding more equivalent entities using the alignment seeds. Those known aligned entity pairs can be used as training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach: RDGCN</head><p>In order to better incorporate relation information to the entity representations, given the input KG (i.e., the primal graph), we first construct its dual relation graph whose vertices denote the relations in the original primal graph, and then, we utilize a graph attention mechanism to encourage interactions between the dual relation graph and the primal graph. The resulting vertex representations in primal graph are then fed to <ref type="bibr">GCN [Kipf and Welling, 2017]</ref> layers with highway gates to capture the neighboring structural information. The final entity representations will be used to determine whether two entities should be aligned. <ref type="figure" target="#fig_1">Figure 2</ref> provides an overview architecture of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Constructing the Dual Relation Graph</head><p>Without loss of generality, we put G 1 and G 2 together as the primal graph G e = (V e , E e ), where the vertex set V e = E 1 ∪ E 2 is the union of all entities in G 1 and G 2 , and the edge set E e = T 1 ∪ T 2 is the union of all edges/triples in G 1 and G 2 . Note that we do not connect the alignment seeds in G e , thus G 1 and G 2 are disconnected in G e . Given the primal graph G e , its dual relation graph G r = (V r , E r ) is constructed as follows: 1) for each type of relation r in G e , there will be a vertex v r in V r , thus V r = R 1 ∪R 2 ; 2) if two relations, r i and r j , share the same head or tail entities in G e , then we create an edge u r ij in G r connecting v r i and v r j . Different from the original design of dual graph, here we expect the dual relation graph to be more expressive about the relationship between different v r s in G r . We thus weight each edge u r ij in G r with a weight w r ij according to how likely the two relations v r i and v r j share similar heads or tails in G e , computed as:</p><formula xml:id="formula_1">w r ij = H(r i , r j ) + T (r i , r j )<label>(1)</label></formula><formula xml:id="formula_2">H(r i , r j ) = H i ∩ H j H i ∪ H j , T (r i , r j ) = T i ∩ T j T i ∪ T j<label>(2)</label></formula><p>where H i and T i are the sets of head and tail entities for relation r i in G e respectively. Here, the overhead for constructing the dual graph is proportional to the number of relation types in the primal graph. In our cases, it takes less than two minutes to construct the graphs for each evaluation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interactions between Dual and Primal Graphs</head><p>Our goal of introducing dual relation graph is to better incorporate relation information into the primal graph represen-tations. To this end, we propose to apply a graph attention mechanism (GAT) to obtain vertex representations for the dual relation graph and the primal graph iteratively, where the attention mechanism helps to prompt interactions between the two graphs. Each dual-primal interaction contains two layers, the dual attention layer and the primal attention layer. Note that we can stack multiple interactions for mutual improvement on both graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual Attention Layer</head><p>Let X r ∈ R m×2d denote the input dual vertex representation matrix, where each row corresponds to a vertex in the dual relation graph G r . Different from the vanilla <ref type="bibr">GAT [Veličković et al., 2018]</ref>, we compute the dual attention scores using the primal vertex featuresX e (computed by Eq. 8) produced by the primal attention layer from previous interaction module:</p><formula xml:id="formula_3">x r i = σ r ( j∈N r i α r ij x r j ),<label>(3)</label></formula><formula xml:id="formula_4">α r ij = exp(η(w r ij a r [c i c j ])) k∈N r i exp(η(w r ik a r [c i c k ])) ,<label>(4)</label></formula><p>wherex r i denotes the d -dimensional output representation at dual vertex v r i (corresponding to relation r i ∈ G e ); x r j denotes the dual representation of vertex v r j ; N r i is the set of neighbor indices of v r i ; α r ij is the dual attention score; a r is a fully connected layer mapping the 2d -dimensional input into a scalar; σ r is the activation function, ReLU; η is the Leaky ReLU; is the concatenation operation; c i is the relation representation for relation r i in G e obtained from the previous primal attention layer.</p><p>Note that within our graph embedding based framework, we are not able to provide relation representations directly, due to limited training data. We thus approximate the relation representation for r i by concatenating its averaged head and tail entity representations in G e as:</p><formula xml:id="formula_5">c i = [ k∈Hix e k |H i | l∈Tix e l |T i | ],<label>(5)</label></formula><p>wherex e k andx e l are the output representations of the k-th head entity and l-th tail entity of relation r i from the previous primal attention layer.</p><p>A special case is when the current dual attention layer is the first layer of our model, we do not have x r j in Eq. 3 produced by the previous dual attention layer, therefore, use an initial dual vertex representation produced by Eq. 5 with the initial primal vertex representations X e init . Similarly, c i will be obtained with the initial primal X e init as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primal Attention Layer</head><p>In this layer, when applying GAT on the primal graph, we can compute the primal attention scores using the dual vertex representations in G r , which actually correspond to the relations in the primal graph G e . In this way, we are able to influence the primal vertex embeddings using the relation representations produced by the dual attention layer.</p><p>Specifically, we use X e ∈ R n×d to denote the input primal vertex representation matrix. For an entity e q in primal graph G e , its representationx e q can be computed by:</p><formula xml:id="formula_6">x e q = σ e ( t∈N e q α e qt x e t ),<label>(6)</label></formula><p>α e qt = exp(η(a e (x r qt ))) k∈N e q exp(η(a e (x r qk ))) ,</p><p>wherex r qt denotes the dual representation for r qt (the relation between entity e q and e t ) obtained from G r ; α e qt is the primal attention score; N e q is the set of neighbor indices of entity e q in G e ; a e is a fully connected layer mapping the d -dimensional input into a scalar and σ e is the primal layer activation function.</p><p>In our model, the initial representation matrix for the primal vertices, X e init , can be initialized using entity names, which provide important evidence for entity alignment. We therefore preserve the evidence explicitly by mixing the initial representations with the output of primal attention layer:</p><formula xml:id="formula_8">x e q = β s * x e q + x e init q ,<label>(8)</label></formula><p>wherex e q denotes the final output representation of the interaction module for entity e q in G e ; β s is a weighting parameter for the s-th primal attention layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Incorporating Structural Information</head><p>After multiple rounds of interaction between the dual relation graph and the primal graph, we are able to collect relationaware entity representations from the primal graph. Next, we apply two-layer GCNs [Kipf and Welling, 2017] with highway gates to the resulting primal graph to further incorporating evidence from their neighboring structures.</p><p>In each GCN layer l with entity representations X (l) as input, the output representations X (l+1) can be computed as:</p><formula xml:id="formula_9">X (l+1) = ξ(D − 1 2ÃD − 1 2 X (l) W (l) ),<label>(9)</label></formula><p>whereÃ = A + I is the adjacency matrix of the primal graph G e with added self-connections and I is an identity matrix; D jj = kÃ jk and W (l) ∈ R d (l) ×d (l+1) is a layer-specific trainable weight matrix; ξ is the activation function ReLU. We treat G e as an undirected graph when constructing A, in order to allow the information to flow in both directions.</p><p>In addition, to control the noise accumulated across layers and preserve useful relation information learned from interactions, following the method described in <ref type="bibr" target="#b4">[Rahimi et al., 2018]</ref>, we introduce layer-wise gates between GCN layers, which is similar in spirit to the highway networks <ref type="bibr" target="#b4">[Srivastava et al., 2015]</ref>:</p><formula xml:id="formula_10">T (X (l) ) = σ(X (l) W (l) T + b (l) T ),<label>(10)</label></formula><formula xml:id="formula_11">X (l+1) = T (X (l) ) · X (l+1) + (1 − T (X (l) )) · X (l) ,<label>(11)</label></formula><p>where X (l) is the input to layer l + 1; σ is a sigmoid function; · is element-wise multiplication; W (l)</p><formula xml:id="formula_12">T and b (l)</formula><p>T are the weight matrix and bias vector for the transform gate T (X (l) ). Alignment. With the final entity representationsX collected from the output of GCN layers, entity alignment can be performed by simply measuring the distance between two entities. Specifically, the distance, d(e 1 , e 2 ), between two entities, e 1 from G 1 and e 2 from G 2 can be calculated as:</p><formula xml:id="formula_13">d(e 1 , e 2 ) = x e1 −x e2 L1 .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>For training, we expect the distance between aligned entity pairs to be as close as possible, and the distance between negative entity pairs to be as far as possible. We thus utilize a margin-based scoring function as the training objective:</p><formula xml:id="formula_14">L = (p,q)∈L (p ,q )∈L max{0, d(p, q) − d(p , q ) + γ},<label>(13)</label></formula><p>where γ &gt; 0 is a margin hyper-parameter; L is our alignment seeds and L is the set of negative instances. Rather than random sampling, we look for challenging negative samples to train our model. Given a positive aligned pair (p, q), we choose the K-nearest entities of p (or q) according to Eq. 12 in the embedding space to replace q (or p) as the negative instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Datasets. We evaluate our approach on three large-scale cross-lingual datasets from DBP15K . These datasets are built upon Chinese, English, Japanese and French versions of DBpedia. Each dataset contains data from two KGs in different languages and provides 15K pre-aligned entity pairs. <ref type="table" target="#tab_0">Table 1</ref> gives the statistics of the datasets. We use the same training/testing split with previous works <ref type="bibr" target="#b4">[Sun et al., 2018]</ref>, 30% for training and 70% for testing. Our source code and datasets are freely available online 1 . Comparison models. We compare our approach against 6 more recent alignment methods that we have mentioned in Section 1: JE <ref type="bibr" target="#b3">[Hao et al., 2016]</ref>, MTransE , <ref type="bibr">JAPE [Sun et al., 2017]</ref>, IPTransE <ref type="bibr" target="#b6">[Zhu et al., 2017]</ref>, BootEA <ref type="bibr" target="#b4">[Sun et al., 2018]</ref> and <ref type="bibr">GCN [Wang et al., 2018]</ref>, where the BootEA achieves the best performance on DBP15K. Model variants. To evaluate different components of our model, we provide four implementation variants of RDGCN for ablation studies, including (1) GCN-s: a two-layered GCN with entity name initialization but no highway gates; (2) R-GCN-s: a two-layered R- <ref type="bibr">GCN [Schlichtkrull et al., 2018]</ref> with entity name initialization; (3) HGCN-s: a two-layered GCN with entity name initialization and highway gates; (4) RD: an implementation of two dual-primal interaction modules, but without the subsequent GCN layers. Implementation details. The configuration we used is: β 1 = 0.1, β 2 = 0.3, and γ = 1.0. The dimensions of hidden representations in dual and primal attention layers are d = 300, d = 600, andd = 300. All dimensions of hidden representations in GCN layers are 300. The learning rate is set to 0.001 and we sample K = 125 negative pairs every 10 epochs. In order to utilize entity names in different KGs for better initialization, we use Google Translate to translate Chinese, Japanese, and French entity names into English, and then use pre-trained English word vectors glove.840B.300d 2 to construct the input entity representations for the primal graph. Note that Google Translate can not guarantee accurate translations for named entities without any context. We manually check 100 English translations for Japanese/Chinese entity names, and find around 20% of English translations as incorrect, posing further challenges for our model. Metrics. We use Hits@k, a widely used metric <ref type="bibr" target="#b4">[Sun et al., 2018;</ref><ref type="bibr" target="#b4">Wang et al., 2018]</ref> in our experiments. A Hits@k score (higher is better) is computed by measuring the proportion of correctly aligned entities ranked in the top k list.</p><p>6 Results and Discussion 6.1 Main Results <ref type="table" target="#tab_2">Table 2</ref> shows the performance of all compared approaches on the evaluation datasets. By using a bootstrapping process to iteratively explore many unlabeled data, BootEA gives the best Hits@10 score on DBP15K ZH−EN and clearly outperforms GCN and other translation-based models. It is not surprising that GCN outperforms most translation-based models, i.e., JE, MTransE, JAPE and IPTransE. By performing graph convolution over an entity's neighbors, GCN is able to capture more structural characteristics of knowledge graphs, especially when using more GCN layers, while the translation assumption in translation-based models focuses more on the relationship among heads, tails and relations.</p><p>We observe that RDGCN gives the best performance across all metrics and datasets, except for Hits@10 on  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Studies</head><p>GCN-s vs. GCN. As shown in <ref type="table" target="#tab_2">Table 2</ref>, GCN-s considerably improves GCN in all datasets, resulting in a 17.2% increase on Hits@1 on DBP15K F R−EN . As mentioned in Section 5, the three cross-lingual datasets require us to handle cross-lingual data through rough machine translations, which is likely to introduce lots of noise (∼80% accuracy in our pilot study). But our improvement over GCN shows that although noisy in nature, those rough translations can still provide useful evidence to capture, thus should not be ignored. GCN-s vs. R-GCN-s. R-GCN is an extension of GCN by explicitly modeling the KG relations, but in our experiments, we observe that GCN-s achieves better performance than R-GCN-s on all datasets. As discussed in Section 1, R-GCN usually requires much more training data to learn an effective model due to its large number of parameters, and the available training data in our evaluation might not be sufficient for fully unlocking the potential of R-GCN. HGCN-s vs. GCN-s. Comparing HGCN-s with GCN-s, we can see that HGCN-s greatly boosts the performance of GCN-s after employing the layer-wise highway gates, e.g., over 30% improvement of Hits@1 on DBP15K F R−EN . This is mainly due to their capability of preventing noisy vertices from driving the KG representations. HGCN-s vs. RDGCN. When comparing HGCN-s with RDGCN, we can see that the dual-primal interaction modules are crucial to the performance: removing the dual and primal attention layers leads to a drop of 1.1% on Hits@1 and 2.02% on Hits@10 on DBP15K ZH−EN . The interaction modules explore the relation characteristics of KGs by introducing the approximate relation information and fully integrate the relation and entity information after multiple interactions between the dual relation graph and the primal graph.</p><p>The results show that effective modeling and use of relation information is beneficial for entity alignment.</p><p>RD vs. RDGCN. Comparing RD with RDGCN, there is a significant drop in performance when removing the GCN layers from our model, e.g., the Hits@1 of RD and RDGCN differ by 8.94% on DBP15K ZH−EN . This is not surprising, because the dual-primal graph interactions are designed to integrate KG relation information, while the GCN layers can effectively capture the neighboring structural information of KGs. These two key components are, to some extent, complementary to each other, and should be combined together to learn better relation-aware representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis</head><p>Triangular structures. <ref type="figure" target="#fig_2">Figure 3(d)</ref> shows the performance of RDGCN and BootEA, the state-of-the-art alignment model, on the testing instances with triangular structures. We can see that the alignment accuracy of our RDGCN for entities with triangular structures is significantly higher than that of BootEA in all three datasets, showing that RDGCN can better deal with the complex relation information.</p><p>Impact of available prior alignments. We further compare our RDGCN with BootEA by varying the proportion of pre-aligned entities from 10% to 40% with a step of 10%. As expected, the results of both models on all three datasets gradually improve with an increased amount of prior alignment information. According to <ref type="figure" target="#fig_2">Figure 3</ref>(a-c), our RDGCN consistently outperforms BootEA, and seems to be insensitive to the proportion of prior alignments. When only using 10% of the pre-aligned entity pairs as training data, RDGCN still achieves promising results. For example, RDGCN using 10% of prior alignments achieves 86.35% for Hits@1 on DBP15K F R−EN . This result translates to a 17.79% higher Hits@1 score over BootEA when BootEA uses 40% of prior alignments. These results further confirm the robustness of our model, especially with limited prior alignments.</p><p>Case study. <ref type="figure" target="#fig_3">Figure 4</ref>  based models, including BootEA, give lower distance scores for (v ZH and v EN ), suggesting that these two entities should be aligned. This is because those models fail to address the specific relation information associated with the three aligned neighboring entities. For this example, both v 1 and v 5 indicate the person Chiang Ching-kuo, but v 1 has the relation parents with v ZH , while v 2 has the relation children with v EN . Utilizing such information, a better alignment model should produce a larger distance score for the two entities despite they have similar neighbors. By carefully considering the relation information during the dual-primal interactions, our RDGCN gives a larger distance score, leading to the correct alignment result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This paper presents a novel Relation-aware Dual-Graph Convolutional Network for entity alignment over heterogeneous KGs. Our approach is designed to explore complex relation information that commonly exists in multi-relational KGs. By modeling the attentive interactions between the primal graph and dual relation graph, our model is able to incorporate relation information with neighboring structural information through gated GCN layers, and learn better entity representations for alignment. Compared to the state-of-the-art methods, our model uses less training data but achieves the best alignment performance across three real-world datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of triangular structures (reproduced from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture of our RDGCN. G r 1 and G r 2 are the dual relation graphs of G e 1 and G e 2 , respectively. In our RDGCN model, G e consists of G e 1 and G e 2 , and G r consists of G r 1 and G r 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a), (b) and (c) show the performance of RDGCN and BootEA using different proportions of prior entity alignments on the DBP15K datasets. The x-axes are the proportions of prior alignments, and the y-axes are Hits@1 scores. (d) shows the performance of RDGCN and BootEA on triangular structures. The x-axis is the datasets and y-axis is the number of correctly predicted pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>shows an example in DBP15K ZH−EN and the target entity pair, (v ZH and v EN ), should not be aligned. The competitive translation-An example in DBP15KZH−EN , where the blue dash lines indicate the connected entities should be aligned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the DBP15K datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>42.77 18.92 39.97 15.38 38.84 MTransE 30.83 61.41 27.86 57.45 24.41 55.55 JAPE 41.18 74.46 36.25 68.50 32.39 66.68 IPTransE 40.59 73.47 36.69 69.26 33.30 68.54 BootEA 62.94 84.75 62.23 85.39 65.30 87.44 GCN 41.25 74.38 39.91 74.46 37.29 74.49</figDesc><table><row><cell>Models</cell><cell>ZH-EN Hits@1 Hits@10</cell><cell>JA-EN Hits@1 Hits@10</cell><cell>FR-EN Hits@1 Hits@10</cell></row><row><cell cols="4">JE 21.27 GCN-s 50.82 79.15 53.09 82.96 54.49 84.73</cell></row><row><cell cols="4">R-GCN-s 46.57 74.29 48.68 77.82 51.11 80.07</cell></row><row><cell>HGCN-s</cell><cell cols="3">69.65 82.53 75.54 87.87 88.09 95.27</cell></row><row><cell>RD</cell><cell cols="3">61.81 73.83 68.54 80.22 84.64 91.98</cell></row><row><cell>RDGCN</cell><cell cols="3">70.75 84.55 76.74 89.54 88.64 95.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The overall alignment performance for all models on the DBP15K datasets. Numbers in bold indicate the best performance.</figDesc><table><row><cell>DBP15K ZH−EN where the performance of RDGCN is sec-</cell></row><row><cell>ond to BootEA with a marginally lower score (84.55 vs</cell></row><row><cell>84.75). While BootEA serves a strong baseline by show-</cell></row><row><cell>ing what can be achieved by exploiting many unlabeled data,</cell></row><row><cell>our RDGCN has the advantage of requiring less prior align-</cell></row><row><cell>ment data to learn better representations. We believe that a</cell></row><row><cell>bootstrapping process can further improve the performance</cell></row><row><cell>of RDGCN, and we leave this for future work. Later in Sec-</cell></row><row><cell>tion 6.3, we show that RDGCN maintains consistent per-</cell></row><row><cell>formance and significantly outperforms BootEA when the</cell></row><row><cell>training dataset size is reduced. The good performance of</cell></row><row><cell>RDGCN is largely attributed to its capability for learning</cell></row><row><cell>relation-aware embeddings.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/StephanieWyt/RDGCN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported in part by the NSFC (Grant No.  61672057, 61672058, 61872294), the National Hi-Tech R&amp;D Program of China (No. 2018YFC0831905), and a UK Royal Society International Collaboration Grant (IE161012). For any correspondence, please contact Yansong Feng.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towsley ; James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Towsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bastings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<editor>IJ-CAI</editor>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
	<note>ICLR. Multilingual knowledge graph embeddings for cross-lingual knowledge alignment</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering</title>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3998" to="4004" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A joint embedding method for entity alignment of knowledge bases</title>
	</analytic>
	<monogr>
		<title level="m">CCKS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph Attention Networks</title>
		<idno type="arXiv">arXiv:1805.06197</idno>
		<idno>arXiv:1505.00387</idno>
		<ptr target="Cross-lingualknowledgegraphalign-mentviagraphconvolutionalnetworks" />
	</analytic>
	<monogr>
		<title level="m">Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks</title>
		<editor>Wang, Qingsong Lv, Xiaohan Lan, and Yu Zhang</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="349" to="357" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational reasoning for question answering with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<editor>Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative entity alignment via joint knowledge embeddings</title>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4258" to="4264" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
