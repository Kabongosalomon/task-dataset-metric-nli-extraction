<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prototype Mixture Models for Few-shot Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
							<email>liuchang615@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
							<email>jiaojb@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
							<email>qxye@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Prototype Mixture Models for Few-shot Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic Segmentation</term>
					<term>Few-shot Segmentation</term>
					<term>Few-shot Learning</term>
					<term>Mixture Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot segmentation is challenging because objects within the support and query images could significantly differ in appearance and pose. Using a single prototype acquired directly from the support image to segment the query image causes semantic ambiguity. In this paper, we propose prototype mixture models (PMMs), which correlate diverse image regions with multiple prototypes to enforce the prototype-based semantic representation. Estimated by an Expectation-Maximization algorithm, PMMs incorporate rich channel-wised and spatial semantics from limited support images. Utilized as representations as well as classifiers, PMMs fully leverage the semantics to activate objects in the query image while depressing background regions in a duplex manner. Extensive experiments on Pascal VOC and MS-COCO datasets show that PMMs significantly improve upon state-of-the-arts. Particularly, PMMs improve 5-shot segmentation performance on MS-COCO by up to 5.82% with only a moderate cost for model size and inference speed. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Substantial progress has been made in semantic segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. This has been broadly attributed to the availability of large datasets with mask annotations and convolutional neural networks (CNNs) capable of absorbing the annotation information. However, annotating object masks for large-scale datasets is laborious, expensive, and can be impractical <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. It is also not consistent with cognitive learning, which can build a model upon few-shot supervision <ref type="bibr" target="#b12">[13]</ref>.</p><p>Given a few examples, termed support images, and the related segmentation masks <ref type="bibr" target="#b13">[14]</ref>, few-shot segmentation aims to segment the query images based on a feature representation learned on training images. It remains a challenging  problem when we consider that target category is not included in the training data while objects within the support and query image significantly differ in appearance and pose.</p><p>By introducing the metric learning framework, Shaban et al. <ref type="bibr" target="#b14">[15]</ref>, Zhang et al. <ref type="bibr" target="#b15">[16]</ref>, and Dong et al. <ref type="bibr" target="#b16">[17]</ref> contributed early few-shot semantic segmentation methods. They also introduced the concept of "prototype" which refers to a weight vector calculated with global average pooling guided by ground-truth masks embedded in feature maps. Such a vector squeezing discriminative information across feature channels is used to guide the feature comparison between support image(s) and query images for semantic segmentation.</p><p>Despite clear progress, we argue that the commonly used prototype model is problematic when the spatial layout of objects is completely dropped by global average pooling, <ref type="figure" target="#fig_0">Fig. 1(upper)</ref>. A single prototype causes semantic ambiguity around various object parts and deteriorates the distribution of features <ref type="bibr" target="#b17">[18]</ref>. Recent approaches have alleviated this issue by prototype alignment <ref type="bibr" target="#b18">[19]</ref>, feature boosting <ref type="bibr" target="#b13">[14]</ref>, and iterative mask refinement <ref type="bibr" target="#b19">[20]</ref>. However, the semantic ambiguity problem caused by global average pooling remains unsolved.</p><p>In this paper, we propose prototype mixture models (PMMs) and focus on solving the semantic ambiguity problem in a systematic manner. During the training procedure, the prototypes are estimated using an Expectation-Maximization (EM) algorithm, which treats each deep pixel (a feature vector) within the mask region as a positive sample. PMMs are primarily concerned with representing the diverse foreground regions by estimating mixed prototypes for various object parts, <ref type="figure" target="#fig_0">Fig. 1</ref>(lower). They also enhance the discriminative capacity of features by modeling background regions.</p><p>The few-shot segmentation procedure is implemented in a metric learning framework with two network branches (a support branch and a query branch), <ref type="figure" target="#fig_1">Fig. 2</ref>. In the framework, PMMs are utilized in a duplex manner to segment a query image. On the one hand, they are regarded as spatially squeezed representation, which match (P-Match) with query features to activate feature channels related to the object class. On the other hand, each vector is regarded as a C-dimensional linear classifier, which multiplies (P-Conv) with the query features in an element-wised manner to produce a probability map. In this way, the channel-wised and spatial semantic information of PMMs is fully explored to segment the query image.</p><p>The contributions of our work are summarized as follows:</p><p>-We propose prototype mixture models (PMMs), with the target to enhance few-shot semantic segmentation by fully leveraging semantics of limited support image(s). PMMs are estimated using an Expectation-Maximization (EM) algorithm, which is integrated with feature learning by a plug-andplay manner. -We propose a duplex strategy, which treats PMMs as both representations and classifiers, to activate spatial and channel-wised semantics for segmentation. -We assemble PMMs to RPMMs using a residual structure and significantly improve upon the state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Semantic Segmentation. Semantic segmentation, which performs per-pixel classification of a class of objects, has been extensively investigated. State-ofthe-art methods, such as UNet <ref type="bibr" target="#b1">[2]</ref>, PSPNet <ref type="bibr" target="#b0">[1]</ref>, DeepLab <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, are based on fully convolutional networks (FCNs) <ref type="bibr" target="#b20">[21]</ref>. Semantic segmentation has been updated to instance segmentation <ref type="bibr" target="#b7">[8]</ref> and panoptic segmentation <ref type="bibr" target="#b21">[22]</ref>, which shared useful modules, e.g., Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b3">[4]</ref> and multi-scale feature aggregation <ref type="bibr" target="#b0">[1]</ref>, with few-shot segmentation. The clustering method used in SegSort <ref type="bibr" target="#b6">[7]</ref>, which partitioned objects into parts using a divide-and-conquer strategy, provides an insight for this study. Few-shot Learning. Existing methods can be broadly categorized as either: metric learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b17">18]</ref>, meta-learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, or data argumentation. Metric learning based methods train networks to predict whether two images/regions belong to the same category. Meta-learning based approaches specify optimization or loss functions which force faster adaptation of the parameters to new categories with few examples. The data argumentation methods learn to generate additional examples for unseen categories <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>In the metric learning framework, the effect of prototypes for few-shot learning has been demonstrated. With a simple prototype, e.g., a linear layer learned on top of a frozen CNN <ref type="bibr" target="#b30">[31]</ref>, state-of-the-art results can be achieved based on a simple baseline. This provides reason for applying prototypes to capture representative and discriminative features.</p><p>Few-shot Segmentation. Existing few-shot segmentation approaches largely followed the metric learning framework, e.g., learning knowledge using a prototype vector, from a set of support images, and then feed learned knowledge to a metric module to segment query images <ref type="bibr" target="#b18">[19]</ref>.</p><p>In OSLSM <ref type="bibr" target="#b14">[15]</ref>, a two-branch network consisting of a support branch and a query branch was proposed for few-shot segmentation. The support branch is devoted to generating a model from the support set, which is then used to tune the segmentation process of an image in the query branch. In PL <ref type="bibr" target="#b16">[17]</ref>, the idea of prototypical networks was employed to tackle few-shot segmentation using metric learning. SG-One <ref type="bibr" target="#b15">[16]</ref> also used a prototype vector to guide semantic segmentation procedure. To obtain the squeezed representation of the support image, a masked average pooling strategy is designed to produce the prototype vector. A cosine similarity metric is then applied to build the relationship between the guidance features and features of pixels from the query image. PANet <ref type="bibr" target="#b18">[19]</ref> further introduced a prototype alignment regularization between support and query branches to fully exploit knowledge from support images for better generalization. CANet <ref type="bibr" target="#b19">[20]</ref> introduced a dense comparison module, which effectively exploits multiple levels of feature discriminativeness from CNNs to make dense feature comparison. With this approach comes an iterative optimization module which renes segmentation masks. The FWB approach <ref type="bibr" target="#b13">[14]</ref> focused on discriminativeness of prototype vectors (support features) by leveraging foregroundbackground feature differences of support images. It also used an ensemble of prototypes and similarity maps to handle the diversity of object appearances.</p><p>As a core of metric learning in few-shot segmentation, the prototype vector was commonly calculated by global average pooling. However, such a strategy typically disregards the spatial extent of objects, which tends to mix semantics from various parts. This unintended mixing seriously deteriorates the diversity of prototype vectors and feature representation capacity. Recent approaches alleviated this problem using iterative mask refinement <ref type="bibr" target="#b19">[20]</ref> or model ensemble <ref type="bibr" target="#b13">[14]</ref>. However, issues remain when using single prototypes to represent object regions and the semantic ambiguity problem remains unsolved.</p><p>Our research is inspired by the prototypical network <ref type="bibr" target="#b31">[32]</ref>, which learns a metric space where classification is performed using distances to the prototype of each class. The essential differences are twofold: (1) A prototype in prototypical network <ref type="bibr" target="#b31">[32]</ref> represents a class of samples while a prototype in our approach represents an object part; (2) The prototypical network does not involve mixing prototypes for a single sample or a class of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The few-shot segmentation task is to classify pixels in query images into foreground objects or backgrounds by solely referring to few labeled support images containing objects of the same categories. The goal of the training procedure is to learn a segmentation model that is trained by numbers of images different from the task query image categories. The training image set is split into many small subsets and within every subset one image serves as the query and the other(s) as the support image(s) with known ground-truth(s). Once the model is trained, the segmentation model is fixed and requires no optimization when tested on a new dataset <ref type="bibr" target="#b19">[20]</ref>. The proposed few-shot segmentation model follows a metric learning framework, <ref type="figure" target="#fig_1">Fig. 2</ref>, which consists of two network branches i.e., the support branch (above) and the query branch (below). Over the support branch, PMMs are estimated for the support image(s). In the support and query branches, two CNNs with shared weights are used as the backbone to extract features. Let S ∈ R W ×H×C denote the features of the support image where W ×H denotes the resolution of feature maps and C the number of feature channels. The features for a query image are denoted as Q ∈ R W ×H×C . Without loss of generality, the network architecture and models are illustrated for 1-shot setting, which can be extended to 5-shot setting by feeding five support images to the PMMs to estimate prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prototype Mixture Models</head><p>During training, features S ∈ R W ×H×C for the support image are considered as a sample set with W × H C-dimensional samples. S is spatially partitioned into foreground samples S + and background samples S − , where S + corresponds to feature vectors within the mask of the support image and S − those outside the mask. S + is used to learn foreground PMMs + corresponding to object parts, <ref type="figure" target="#fig_2">Fig. 3</ref>  Models. PMMs are defined as a probability mixture model which linearly combine probabilities from base distributions, as</p><formula xml:id="formula_0">p(s i |θ) = K k=1 w k p k (s i |θ)<label>(1)</label></formula><p>where w k denotes the mixing weights satisfying 0 ≤ w k ≤ 1 and K k=1 w k = 1. θ denotes the model parameters which are learned when estimating PMMs. s i ∈ S denotes the i th feature sample and p k (s i |θ) denotes the k th base model, which is a probability model based on a Kernel distance function, as</p><formula xml:id="formula_1">p k (s i |θ) = β(θ)e Kernel(si,µ k ) ,<label>(2)</label></formula><p>where β(θ) is the normalization constant. µ k ∈ θ is one of the parameter. For the Gaussian mixture models (GMMs) with fixed co-variance, the Kernel function is a radial basis function (RBF),</p><formula xml:id="formula_2">Kernel(s i , µ k ) = −||(s i − µ k )|| 2 2 .</formula><p>For the von Missies-Fisher (VMF) model <ref type="bibr" target="#b32">[33]</ref>, the kernel function is defined as a co-</p><formula xml:id="formula_3">sine distance function, as Kernel(s i , µ k ) = µ T k si ||µ k ||2||si||2 ,</formula><p>where µ k is the mean vector of the k th model. Considering the metric learning framework used, the vector distance function is more appropriate in our approach, as is validated in experiments. Based on the vector distance, PMMs are defined as</p><formula xml:id="formula_4">p k (s i |θ) = β c (κ)e κµ T k si ,<label>(3)</label></formula><p>where θ = {µ, κ}. β c (κ) = κ c/2−1 (2π) c/2 I c/2−1 (κ) is the normalization coefficient, and I ν (·) denotes the Bessel function. κ denotes the concentration parameter, which is empirically set as κ = 20 in experiments.</p><p>Model Learning. PMMs are estimated using the EM algorithm which includes iterative E-steps and M-steps. In each E-step, given model parameters and sample features extracted, we calculate the expectation of the sample s i as</p><formula xml:id="formula_5">E ik = p k (s i |θ) K k=1 p k (s i θ) = e κµ T k si K k=1 e κµ T k si .<label>(4)</label></formula><p>In each M-step, the expectation is used to update the mean vectors of PMMs, as  <ref type="figure">Fig. 4</ref>; Predict a segmentation mask and calculate the segmentation loss; Update α to minimize the cross-entropy loss at the query branch, <ref type="figure" target="#fig_1">Fig. 2</ref>. end for where N = W × H denotes the number of samples.</p><formula xml:id="formula_6">µ k = N i=1 E ik s i N i=1 E ik ,<label>(5)</label></formula><p>After model learning, the mean vectors µ + = {µ + k , k = 1, ..., K} and µ − = {µ − k , k = 1, ..., K} are used as prototype vectors to extract convolutions features for the query image. The mixture coefficient w k is ignored so that each prototype vectors have same importance for semantic segmentation. Obviously, each prototype vector is the mean of a cluster of samples. Such a prototype vector can represent a region around an object part in the original image for the reception field effect, <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Few-shot Segmentation</head><p>During inference, the learned prototype vectors µ + = {µ + k , k = 1, ..., K} and µ − = {µ − k , k = 1, ..., K} are duplexed to activate query features for semantic segmentation, <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>PMMs as Representation (P-Match). Each positive prototype vector squeezes representative information about an object part and all prototypes incorporate representative information about the complete object extent. Therefore, prototype vectors can be used to match and activate the query features Q, as Q = P-Match(µ + k , Q), k = 1, ..., K,</p><p>where P-Match refers to an activation operation consists of prototype upsampling, feature concatenation, and semantic segmentation using convolution, <ref type="figure">Fig.  4</ref>. The convolution operation on concatenated features implements a channelwise comparison, which activates feature channels related to foreground while suppressing those associated with backgrounds. With P-Match, semantic information about the extent of the complete object is incorporated into the query features for semantic segmentation.</p><p>PMMs as Classifiers (P-Conv). On the other hand, each prototype vector incorporating discriminative information across feature channels can be seen as a classifier, which produces probability maps M k = {M + k , M − k } using the P-Conv operation, as</p><formula xml:id="formula_8">M k = P-Conv(µ + k , µ − k , Q), k = 1, ..., K.<label>(7)</label></formula><p>As shown in <ref type="figure">Fig. 4</ref>, P-Conv first multiplies each prototype vector with the query feature Q in an element-wise manner. The output maps are then converted to probability maps M k by applying Softmax across channels. After P-Conv, the produced probability maps M + k , k = 1, ..., K and M − k , k = 1, ..., K are respectively summarized to two probability maps, as</p><formula xml:id="formula_9">M + p = k M + k , M − p = k M − k ,<label>(8)</label></formula><p>which are further concatenated with the query features to activate objects of interest, as</p><formula xml:id="formula_10">Q = M + p ⊕ M − p ⊕ Q ,<label>(9)</label></formula><p>where ⊕ denotes the concatenation operation. After the P-Match and P-Conv operations, the semantic information across channels and discriminative information related to object parts are collected from the support feature S to activate the query feature Q. in a dense comparison manner <ref type="bibr" target="#b19">[20]</ref>. The activated query features Q are further enhanced with Atrous Spatial Pyramid Pooling (ASPP) and fed to a convolutional module to predict the segmentation mask, <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Segmentation Model Learning. The segmentation model is implemented as an end-to-end network, <ref type="figure" target="#fig_1">Fig. 2</ref>. The learning procedure for the segmentation model is described in Algorithm 1. In the feed forward procedure, the support features are partitioned into backgrounds and foreground sample sets S + and S − according to the ground-truth mask. PMMs are learned on S + and S − and the learned prototype vectors µ + and µ − are leveraged to activate query features to predict segmentation mask of the query image. In the back-propagation procedure, the network parameters θ are updated to optimize the segmentation loss at the query branch. With multiple training iterations, rich feature representation about diverse object parts is absorbed into the backbone network. During the inference procedure, the learned feature representation together with PMMs of the support image(s) is used to segment the query image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Residual Prototype Mixture Models</head><p>To further enhance the model representative capacity, we implement model ensemble by stacking multiple PMMs, <ref type="figure" target="#fig_3">Fig. 5</ref>. Stacked PMMs, termed residual PMMs (RPMMs), leverage the residual from the previous query branch to supervise the next query branch for fine-grained segmentation. This is computationally easier as it pursuits the minimization of residuals between branches rather than struggling to combine multiple models to fit a ground-truth mask. RPMMs not only further improve the performance but also defines a new model ensemble strategy. This incorporates the advantages of model residual learning, which is inspired by the idea of side-output residual <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> but has the essential difference to handle models instead of features. This is also different from the ensemble of experts <ref type="bibr" target="#b13">[14]</ref>, which generates an ensemble of the support features guided by the gradient of loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental settings</head><p>Implementation Details. Our approach utilizes CANet <ref type="bibr" target="#b19">[20]</ref> without iterative optimization as the baseline, which uses VGG16 or ResNet50 as backbone CNN for feature extraction. During training, four data augmentation strategies including normalization, horizontal flipping, random cropping and random resizing are used <ref type="bibr" target="#b19">[20]</ref>. Our approach is implemented upon the PyTorch 1.0 and run on Nvidia 2080Ti GPUs. The EM algorithm iterates 10 rounds to calculate PMMs for each image. The network with a cross-entropy loss is optimized by SGD with the initial learning of 0.0035 and the momentum of 0.9 for 200,000 iterations with 8 pairs of support-query images per batch. The learning rate reduces following the "poly" policy defined in DeepLab <ref type="bibr" target="#b3">[4]</ref>. For each training step, the categories in the train split are randomly selected and then the support-query pairs are randomly sampled in the selected categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prototype Mixture Models (PMMs)</head><p>CANet (a single prototpye) <ref type="figure">Fig. 6</ref>. Activation maps by PMMs and CANet <ref type="bibr" target="#b19">[20]</ref>. PMMs produce multiple probability maps and fuse them to a mixed map, which facilities activating and segmenting complete object extent (first two rows) or multiple objects (last row). CANet that uses a single prototype to segment object tends to miss object parts. (Best viewed in color)</p><p>Datasets. We evaluate our model on Pascal-5 i and COCO-20 i . Pascal-5 i is a dataset specified for few-shot semantic segmentation in OSLSM <ref type="bibr" target="#b14">[15]</ref>, which consists of the Pascal VOC 2012 dataset with extra annotations from extended SDS <ref type="bibr" target="#b35">[36]</ref>. 20 object categories are partitioned into four splits with three for training and one for testing. At test time, 1000 support-query pairs were randomly sampled in the test split <ref type="bibr" target="#b19">[20]</ref>. Following FWB <ref type="bibr" target="#b13">[14]</ref>, we create COCO-20 i from MSCOCO 2017 dataset. The 80 classes are divided into 4 splits and each contains 20 classes and the val dataset is used for performance evaluation. The other setting is the same as that in Pascal-5 i .</p><p>Evaluation Metric. Mean intersection over-union (mIoU) which is defined as the mean IoUs of all image categories was employed as the metric for performance evaluation. For each category, the IoU is calculated by IoU= TP TP+FP+FN , where TP, FP and FN respectively denote the number of true positive, false positive and false negative pixels of the predicted segmentation masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Analysis</head><p>In <ref type="figure">Fig. 6</ref>, we visualize probability maps produced by positive prototypes of PMMs. We also visualize and compare the activation maps and segmentation masks produced by PMMs and CANet. PMMs produce multiple probability maps and fuse them to a mixed probability map, which facilities activating complete object extent (first two rows). The advantage in terms of representation capacity is that PMMs perform better than CANet when segmenting multiple objects within the same image (last row). By comparison, CANet using a single prototype to activate object tends to miss object parts or whole objects. The probability maps produced by PMMs validate our idea, i.e., prototypes correlated to multiple regions and alleviate semantic ambiguity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Image Baseline PMMs RPMMs PMMs (P-Match) Ground Truth</head><p>True Positive False Negative False Positive <ref type="figure">Fig. 7</ref>. Semantic segmentation results. 'Baseline' refers to the CANet method <ref type="bibr" target="#b19">[20]</ref> without iterative optimization. (Best viewed in color) In <ref type="figure">Fig. 7</ref>, we compare the segmentation results by the baseline method and the proposed modules. The segmentation results show that PMMs + (P-Match) can improve the recall rate by segmenting more target pixels. By introducing background prototypes, PMMs reduce the false positive pixels, which validates that the background mixture models can improve the discriminative capability of the model. RPMMs further improve the segmentation results by refining object boundaries about hard pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>PMMs. In <ref type="table" target="#tab_4">Table 1</ref>, with P-Match modules, PMMs improve segmentation performance by 2.70% (54.63% vs. 51.93%), which validates that the prototypes generated by the PMMs perform better than the prototype generated by global average pooling. By introducing the duplex strategy, PMMs further improve the performance by 0.64% (55.27% vs. 54.63%), which validates that the probability map generated by the combination of foreground and background prototypes can suppress backgrounds and reduce false segmentation. In total, PMMs improve the performance by 3.34% (55.27% vs. 51.93%), which is a significant margin in semantic segmentation. This clearly demonstrates the superiority of the proposed PMMs over previous prototype methods.</p><p>RPMMs. RPMMs further improve the performance by 1.07% (56.34% vs. 55.27%), which validates the effectiveness of the residual ensemble strategy. Residual from the query prediction output of the previous branch of PMMs can be used to supervise the next branch of PMMs, enforcing the stacked PMMs to reduce errors, step by step.</p><p>Number of Prototypes. In <ref type="table" target="#tab_5">Table 2</ref>, ablation study is carried out to determine the number of prototypes using PMMs + with P-Match. K = 2 significantly outperforms K = 1, which validates the plausibility of introducing mixture prototypes. The best Pascal-5 i performance occurs at K = 2, 3, 4. When K = 3 the best mean performance is obtained. When K = 4, 5, the performance slightly decreases. One reason lies in that the PMMs are estimated on a single support image, which includes limited numbers of samples. The increase of K substantially decreases the samples of each prototype and increases the risk of over-fitting.</p><p>Kernel Functions. In <ref type="table">Table 3</ref>, we compare the Gaussian and VMF kernels for sample distance calculation when estimating PMMs. The better results from VMF kernel show that the cosine similarity defined by VMF kernel is preferable.</p><p>Inference Speed. The size of PMMs model is 19.5M, which is slightly larger than that of the baseline CANet <ref type="bibr" target="#b19">[20]</ref> (19M) but much smaller than that of OSLSM <ref type="bibr" target="#b14">[15]</ref> (272.6M). Because the prototypes are 1×1×C dimensional vectors, they do not significantly increase the model size or computational cost. In one shot setting, with K = 3, our inference speed on single 2080Ti GPU is 26 FPS, which is slightly lower than that of CANet <ref type="bibr">(29 FPS)</ref>. With RPMMs the speed decreases to 20 FPS while the model size (19.6M) does not significantly increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance</head><p>PASCAL-5 i . In <ref type="table">Table 4</ref> and <ref type="table">Table 5</ref>, PMMs and RPMMs are compared with the state-of-the-art methods. They outperform state-of-the-art methods in both 1-shot and 5-shot settings. With the 1-shot setting and a Resnet50 backbone, RPMMs achieve 2.38% (56.34% vs. 53.96%) performance improvement over the state-of-the-art, which is a significant margin. With the 5-shot setting and a Resnet50 backbone, RPMMs achieve 1.50% (57.30% vs. 55.80%) performance improvement over the state-of-the-art, which is also significant. With the VGG16 backbone, our approach is comparable with the state-of-the-arts. Note that the PANet and FWB used additional k-shot fusion strategy while we do not use any post-processing strategy to fuse the predicted results from five shots. MS COCO. <ref type="table" target="#tab_7">Table 6</ref> displays the evaluation results on MS COCO dataset following the evaluation metric on COCO-20 i <ref type="bibr" target="#b13">[14]</ref>. Baseline is achieved by running CANet without iterative optimization. PMMs and RPMMs again outperform state-of-the-art methods in both 1-shot and 5-shot settings. For the 1-shot setting, RPMMs improves the baseline by 4.47%, respectively outperforms the PANet and FWB methods by 9.68% and 9.39%.</p><p>For the 5-shot setting, it improves the baseline by 7.66%, and respectively outperforms the PANet and FWB by 5.82% and 11.87%, which are large margins for the challenging few-shot segmentation problem. Compared to PASCAL VOC, MS COCO has more categories and images for training, which facilities learning richer representation related to various object parts and backgrounds. Thereby, the improvement on MS COCO is larger than that on Pascal VOC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed prototype mixture models (PMMs), which correlate diverse image regions with multiple prototypes to solve the semantic ambiguity problem. During training, PMMs incorporate rich channel-wised and spatial semantics from limited support images. During inference, PMMs are matched with query features in a duplex manner to perform accurate semantic segmentation. On the large-scale MS COCO dataset, PMMs improved the performance of few-shot segmentation, in striking contrast with state-of-the-art approaches. As a general method to capture the diverse semantics of object parts given few support examples, PMMs provide a fresh insight for the few-shot learning problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The single prototype model (upper) based on global average pooling causes semantic ambiguity about object parts. In contrast, prototype mixture models (lower) correlate diverse image regions, e.g., object parts, with multiple prototypes to enhance few-shot segmentation model. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The proposed approach consists of two branches i.e., the support branch and the query branch. During training, the feature set S of a support image is partitioned into a positive sample set S + and a negative sample set S − guided by the ground-truth mask. S + and S − are respectively used to train µ + and µ − , which are used to activate query features in a duplex way (P-Conv and P-Match) for semantic segmentation. "ASPP" refers to the Atrous Spatial Pyramid Pooling (ASPP)<ref type="bibr" target="#b3">[4]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Foreground sample distribution of support images. The black points on tSNE maps denote positive prototypes correlated to object parts. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Network architecture of residual prototype mixture models (RPMMS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Prediction Global Average Pooling Prototype Mixture Models</head><label></label><figDesc></figDesc><table><row><cell cols="2">Support Mask</cell><cell></cell></row><row><cell>Support Image</cell><cell>Prototype Vector</cell><cell></cell></row><row><cell>Query Image</cell><cell>Feature Extractor</cell><cell></cell></row><row><cell cols="2">Support Mask</cell><cell></cell></row><row><cell>Support Image</cell><cell>Prototype vectors</cell><cell></cell></row><row><cell>Query Image</cell><cell>Feature Extractor</cell><cell>Prediction</cell></row><row><cell></cell><cell>Object Parts</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, and S − to learn background PMMs − . Without loss generality, the models and learning procedure are defined for PMMs, which represent either PMMs + or PMMs − .</figDesc><table><row><cell>Support Image tSNE of Samples</cell><cell>Object Parts</cell><cell>Support Image tSNE of Samples</cell><cell>Object Parts</cell><cell>Support Image tSNE of Samples</cell><cell>Object Parts</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Ablation study. 'Mean' denotes mean mIoU on Pascal-5 i with PMMs using three prototypes. The first row is the baseline method without using the PMMs or the RPMMs method.</figDesc><table><row><cell>PMMs + (P-Match) PMMs(P-Match &amp; P-Conv) RPMMs Mean</cell></row><row><cell>51.93</cell></row><row><cell>54.63</cell></row><row><cell>55.27</cell></row><row><cell>56.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Performance (mIoU%) on prototype number K.</figDesc><table><row><cell cols="6">K Pascal-5 0 Pascal-5 1 Pascal-5 2 Pascal-5 3 Mean</cell></row><row><cell cols="2">1 49.38</cell><cell>66.42</cell><cell>51.29</cell><cell>47.68</cell><cell>53.69</cell></row><row><cell cols="2">2 50.85</cell><cell>66.65</cell><cell>51.89</cell><cell>48.25</cell><cell>54.41</cell></row><row><cell cols="2">3 51.88</cell><cell>66.72</cell><cell>51.14</cell><cell>48.80</cell><cell>54.63</cell></row><row><cell cols="2">4 51.89</cell><cell>66.96</cell><cell>51.36</cell><cell>47.91</cell><cell>54.53</cell></row><row><cell cols="2">5 50.76</cell><cell>66.89</cell><cell>50.76</cell><cell>47.94</cell><cell>54.09</cell></row><row><cell cols="6">Table 3. Performance comparison of Kernel functions.</cell></row><row><cell>Kernal</cell><cell cols="5">Pascal-5 0 Pascal-5 1 Pascal-5 2 Pascal-5 3 Mean</cell></row><row><cell cols="2">Gaussian 50.94</cell><cell>66.70</cell><cell>50.59</cell><cell>47.91</cell><cell>54.04</cell></row><row><cell>VMF</cell><cell>51.88</cell><cell>66.72</cell><cell>51.14</cell><cell>48.80</cell><cell>54.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Performance of 1-way 1-shot semantic segmentation on Pascal-5 i . CANet reports multi-scale test performance. The single-scale test performance is obtained from github.com/icoz69/CaNet/issues/4 for a fair comparison. Performance of 1-way 5-shot semantic segmentation on Pascal-5 i .</figDesc><table><row><cell cols="2">Backbone Method</cell><cell cols="5">Pascal-5 0 Pascal-5 1 Pascal-5 2 Pascal-5 3 Mean</cell></row><row><cell></cell><cell>OSLSM [15]</cell><cell>33.60</cell><cell>55.30</cell><cell>40.90</cell><cell>33.50</cell><cell>40.80</cell></row><row><cell></cell><cell>co-FCN [37]</cell><cell>36.70</cell><cell>50.60</cell><cell>44.90</cell><cell>32.40</cell><cell>41.10</cell></row><row><cell>VGG16</cell><cell>SG-One [16]</cell><cell>40.20</cell><cell>58.40</cell><cell>48.40</cell><cell>38.40</cell><cell>46.30</cell></row><row><cell></cell><cell>PANet [19]</cell><cell>42.30</cell><cell>58.00</cell><cell>51.10</cell><cell>41.20</cell><cell>48.10</cell></row><row><cell></cell><cell>FWB [14]</cell><cell>47.04</cell><cell>59.64</cell><cell>52.51</cell><cell>48.27</cell><cell>51.90</cell></row><row><cell></cell><cell cols="2">RPMMs (ours) 47.14</cell><cell>65.82</cell><cell>50.57</cell><cell>48.54</cell><cell>53.02</cell></row><row><cell>Resnet50</cell><cell>CANet [20] PMMs (ours)</cell><cell>49.56 51.98</cell><cell>64.97 67.54</cell><cell>49.83 51.54</cell><cell>51.49 49.81</cell><cell>53.96 55.22</cell></row><row><cell></cell><cell cols="2">RPMMs (ours) 55.15</cell><cell>66.91</cell><cell>52.61</cell><cell>50.68</cell><cell>56.34</cell></row><row><cell cols="2">Backbone Method</cell><cell cols="5">Pascal-5 0 Pascal-5 1 Pascal-5 2 Pascal-5 3 Mean</cell></row><row><cell></cell><cell>OSLSM [15]</cell><cell>35.90</cell><cell>58.10</cell><cell>42.70</cell><cell>39.10</cell><cell>43.95</cell></row><row><cell></cell><cell>SG-One [16]</cell><cell>41.90</cell><cell>58.60</cell><cell>48.60</cell><cell>39.40</cell><cell>47.10</cell></row><row><cell>VGG16</cell><cell>FWB [14]</cell><cell>50.87</cell><cell>62.86</cell><cell>56.48</cell><cell>50.09</cell><cell>55.08</cell></row><row><cell></cell><cell>PANet [19]</cell><cell>51.80</cell><cell>64.60</cell><cell>59.80</cell><cell>46.05</cell><cell>55.70</cell></row><row><cell></cell><cell>RPMMs (ours)</cell><cell>50.00</cell><cell>66.46</cell><cell>51.94</cell><cell>47.64</cell><cell>54.01</cell></row><row><cell>Resnet50</cell><cell>CANet [20] PMMs (ours)</cell><cell>-55.03</cell><cell>-68.22</cell><cell>-52.89</cell><cell>-51.11</cell><cell>55.80 56.81</cell></row><row><cell></cell><cell cols="2">RPMMs (ours) 56.28</cell><cell>67.34</cell><cell>54.52</cell><cell>51.00</cell><cell>57.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Performance of 1-shot and 5-shot semantic segmentation on MS COCO. FWB uses the ResNet101 backbone while other approaches use the ResNet50 backbone. COCO-20 1 COCO-20 2 COCO-20 3 Mean</figDesc><table><row><cell cols="3">Settings Method PANet [19] FWB [14] COCO-20 0 1-shot -16.98 Baseline 25.08</cell><cell>-17.98 30.25</cell><cell>-20.96 24.45</cell><cell>-28.85 24.67</cell><cell>20.90 21.19 26.11</cell></row><row><cell></cell><cell>PMMs (ours)</cell><cell>29.28</cell><cell>34.81</cell><cell>27.08</cell><cell>27.27</cell><cell>29.61</cell></row><row><cell></cell><cell cols="2">RPMMs (ours) 29.53</cell><cell>36.82</cell><cell>28.94</cell><cell>27.02</cell><cell>30.58</cell></row><row><cell></cell><cell>FWB [14]</cell><cell>19.13</cell><cell>21.46</cell><cell>23.93</cell><cell>30.08</cell><cell>23.65</cell></row><row><cell></cell><cell>PANet [19]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.70</cell></row><row><cell>5-shot</cell><cell>Baseline</cell><cell>25.95</cell><cell>32.38</cell><cell>26.11</cell><cell>26.98</cell><cell>27.86</cell></row><row><cell></cell><cell>PMMs (ours)</cell><cell>33.00</cell><cell>40.55</cell><cell>30.29</cell><cell>33.27</cell><cell>34.28</cell></row><row><cell></cell><cell cols="2">RPMMs (ours) 33.82</cell><cell>41.96</cell><cell>32.99</cell><cell>33.33</cell><cell>35.52</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at github.com/Yang-Bob/PMMs. *Qixiang Ye is the corresponding author.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="833" to="851" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segsort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7334" to="7344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="386" to="397" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">C-mil: Continuation multiple instance learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="2199" to="2208" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Min-entropy latent model for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2395" to="2409" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="147" to="155" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning compositional representations for few-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6372" to="6381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="622" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1810.09091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collect and select: Semantic alignment metric learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8460" to="8469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="622" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="5217" to="5226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="3630" to="3638" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="1199" to="1208" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111719" to="111727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3037" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="7278" to="7286" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Yu-ChiangWang: A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="4077" to="4087" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Clustering on the unit hypersphere using von mises-fisher distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1345" to="1382" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SRN: side-output residual network for object symmetry detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RSRN: rich side-output residual network for medial axis detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops, ICCV Workshops</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="1739" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conditional networks for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
