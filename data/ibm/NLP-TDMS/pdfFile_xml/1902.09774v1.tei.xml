<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image-Question-Answer Synergistic Network for Visual Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
							<email>c.xu@</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image-Question-Answer Synergistic Network for Visual Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The image, question (combined with the history for dereferencing), and the corresponding answer are three vital components of visual dialog. Classical visual dialog systems integrate the image, question, and history to search for or generate the best matched answer, and so, this approach significantly ignores the role of the answer. In this paper, we devise a novel image-question-answer synergistic network to value the role of the answer for precise visual dialog. We extend the traditional one-stage solution to a two-stage solution. In the first stage, candidate answers are coarsely scored according to their relevance to the image and question pair. Afterward, in the second stage, answers with high probability of being correct are re-ranked by synergizing with image and question. On the Visual Dialog v1.0 dataset, the proposed synergistic network boosts the discriminative visual dialog model to achieve a new state-of-the-art of 57.88% normalized discounted cumulative gain. A generative visual dialog model equipped with the proposed technique also shows promising improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual dialog is an emerging research theme lying at the intersection between computer vision and natural language processing. Given the capacities of reasoning, grounding, recognition, and translating, a visual dialog agent is expected to answer questions based on an image, caption, and history. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, the agent first reasons what the word 'their' refers to in the current question based on history, locates the bounding boxes of the five people in the picture, recognizes the color of their jackets, and then translates the visual information into human language. Hence, a visual dialog task can also be regarded as: (i) visual grounding <ref type="bibr" target="#b32">[33]</ref>, which further converts visual information in located bounding boxes into human language; (ii) visual question answering (VQA) <ref type="bibr" target="#b1">[2]</ref>, which includes extra dialog history and caption as the input; and (iii) image captioning <ref type="bibr" target="#b24">[25]</ref>, which generates a description not only based on visual information but also the history and question. A general visual dialog model has two components: an encoder to embed inputs (e.g., images and questions) into vectors and fuse them to create a unified representation, and a decoder either to translate the encoded vector directly into words for an answer or to rank the given candidate answers. Both VQA and visual dialog involve the fusion job of multiple modalities. However, as a multi-turn VQA task, in each turn, a visual dialog system must also integrate the caption and dialog history from past turns. Visual dialog systems can be grouped into two main categories according to different decoders: generative models and discriminative models. Generative models usually employ seq2seq <ref type="bibr" target="#b23">[24]</ref> or advanced reinforcement learning <ref type="bibr" target="#b28">[29]</ref> techniques to generate the answer set, where the highest probability answer is cho-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primary Stage</head><p>The color of their jackets are black and grey The color of their jackets are dark blue The color of their jackets are red green grey and black The color of their jackets are grey, black and white The color of their jackets are blue, red, black, grey and green sen as the output. Discriminative models tend to calculate the similarity between the latent output of the encoder and the embedding of candidate answers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29]</ref>, with the correct answer expected to have the highest score. However, existing generative models aim to generate a single word of high probability at each step but omit the meaning of the whole answer sentence. The discriminative models are beneficial to understanding the answer sentence through long short-term memory (LSTM) <ref type="bibr" target="#b9">[10]</ref>, but the scoring method is insufficient to capture the similarity between inputs and answers, since the vector of inputs and answers have been separately learned without deep fusion. Furthermore, both generative and discriminative models tend to give short and safe answer, such as 'Yes' or 'No', as their fusion methods focus on the major signal in short answer but will not look into details in a longer one.</p><p>To highlight the role of answer and its integration with other ingredients (e.g., images and questions) in visual dialog, we propose an image-question-answer synergistic network. However, not all answers are plausible for the image and question. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, only answers about color in the image are related to the question, while the others are unreasonable and probably lead to network side effects. Hence, we extend the traditional one-stage model composed of an encoder and a decoder into a two-stage model containing a primary stage and a synergistic stage. The primary stage can be any existing model that coarsely scores all candidate answers or generates some high probability candidates. The synergistic stage selects answers of high probability based on certain policies, finely synergizes them with the question and then re-ranks the candidate answers according to the relevance of the synergies to the image, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The proposed method is consistent with human practice. In multiple choice examinations, we usually first exclude obviously wrong answers before paying more effort to compare the remaining answers that are more likely to be correct. We fill each answer into the question blank and judge whether the complete sentence is more suitable than the others. In addition, we address the class imbalance problem in the primary stage for a strong discriminative model. As a large number of easy negative candidate answers dominate the loss function, a temperature factor is considered in the loss function to discount the contribution of easy samples.</p><p>Our model is evaluated on the Visual Dialog v1.0 dataset <ref type="bibr" target="#b3">[4]</ref>. In validation, our primary stage with a loss-balanced discriminative model improves the mean reciprocal rank (MRR) by 0.71% compared to the non-balanced model, and the synergistic stage gives additional 0.91% improvement on MRR. Furthermore, the synergistic stage in our generative model improves MRR by 4.7% and recall on top-5 responses (R@5) by 9.2% compared to the primary stage, which provides a different way to generate descriptive answers other than GAN and reinforcement learning <ref type="bibr" target="#b28">[29]</ref>. On the test-standard dataset, our two-stage model outperforms the baselines and achieves state-of-art performance, higher than the other entries in the Visual Dialog Challenge 2018 with a 57.88% normalized discounted cumulative gain (NDCG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Question Answering (VQA): VQA is the first task undertaken when querying images for text answer generation. It is a classification problem in which candidate answers are restricted to the most common answers appearing in the dataset. Current models can be classified into three main categories: early fusion models, later fusion models, and external knowledge-based models. In early fusion models, the input queries are regarded as parameters of conditional batch normalization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref> in the detection network <ref type="bibr" target="#b8">[9]</ref>, which leads a pre-trained ResNet to the proposed MODERN architecture; this method affects less than 1% of parameters in the pre-trained model, which reduces the risk of over-fitting. Later fusion models mainly concentrate on how to represent the answer vector by jointing the question and global image feature <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref>. However, a lot of visual information is irrelevant to the input query, resulting in significant noise during reasoning. Thus, the attention mechanism is introduced to solve this problem. This starts with the linear combination of two features, such as the stacked attention network <ref type="bibr" target="#b29">[30]</ref> by learning the attention using multi-step reasoning and the dual attention network <ref type="bibr" target="#b19">[20]</ref> that learns both the visual and textual attention. Then, a bilinear pooling method like multi-modal compact bilinear pooling (MCB) <ref type="bibr" target="#b6">[7]</ref> is applied by projecting the outer product of two features into the high dimension of quadratic expansion. However, MCB needs to sample features, which is computationally intractable and has very large projected dimension, so low-rank bilinear pool-ing (MLB) <ref type="bibr" target="#b14">[15]</ref> and multi-factor bilinear pooling (MFB) <ref type="bibr" target="#b31">[32]</ref> have been proposed to project the two features into a common low-rank space, and the bilinear attention network (BAN) <ref type="bibr" target="#b13">[14]</ref> builds the interaction attention between multimodal inputs. In contrast to early and later fusion models, external knowledge-based models suppose that common sense or information not given in the image is required to infer the right answer. <ref type="bibr" target="#b27">[28]</ref> uses DBpedia <ref type="bibr" target="#b2">[3]</ref> to broaden the range of answers. <ref type="bibr" target="#b20">[21]</ref> queries the triplet (visual concept, relation, attribute) in Fvqa <ref type="bibr" target="#b25">[26]</ref> to score the retrieved facts. However, this method is not good enough to reason complex facts and requires further development.</p><p>Visual Dialog: Extending the single turn dialog task (VQA) to a multi-turn one, we introduce visual dialog. Before the dataset proposed by <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> used a dataset that located the object in an image in which one person gave a question about the image and the other provided a 'Yes/No/NA' answer based on the truth. The current dataset expands the range of questions and answers. Question types about image can be various including color, number, relationships, etc., while the answer could be simple as a 'No' or a complex description of the image. Three encoding methods were provided in <ref type="bibr" target="#b3">[4]</ref> as baselines, namely late fusion, the hierarchical recurrent encoder, and the memory network, as well as two decoding methods: LSTM and softmax. Inspired by the generative adversarial networks (GANs) <ref type="bibr" target="#b7">[8]</ref> and the performance gap between discriminative and generative networks, <ref type="bibr" target="#b17">[18]</ref> transferred knowledge from a pre-trained discriminative network to a generative network with a Gumbel-softmax <ref type="bibr" target="#b12">[13]</ref> LSTM encoder using perceptual loss. <ref type="bibr" target="#b28">[29]</ref> combined GAN and reinforcement learning <ref type="bibr" target="#b30">[31]</ref> to train the generator with a co-attention encoder, thereby allowing the discriminator to directly access the generated response to evaluate its quality, and used Monte Carlo (MC) searching with a roll-out policy to compute the intermediate reward for each word. <ref type="bibr" target="#b11">[12]</ref> merged the fusion step and the scoring step into a single step, which is similar to our synergistic stage, but involved all the image information containing noise and the whole history containing topics irrelevant to current question. Furthermore, it simply arrayed the isolated vectors of the image, question, history, and answer for their representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Synergistic Network</head><p>In a candidate answer set, some of the answers are hard samples that are close or equal to the correct answer, while others are easy samples. Our framework shown in <ref type="figure" target="#fig_2">Figure 3</ref> has two stages, the primary stage and the synergistic stage. In the primary stage, we learn representative vectors of the image, history, and question using a co-attention module and then calculates the score of each candidate answer to separate hard answers from easy ones. In the synergistic stage, we select hard answers together with their questions to form question-answer pairs. These pairs further coordinate with the image and history to predict scores.</p><p>We first formally define the visual dialog problem and then introduce our new synergistic strategy. Given an image I and caption C, we collect historical questions and their corresponding answers as H. At turn t, our model gives a score for each answer a t,i in candidate set A t based on question q t . To describe input image I, we detect objects and their features using the Faster-RCNN model <ref type="bibr" target="#b0">[1]</ref> and apply CNN to encode them into V = (v 1 , . . . , v n ), where v i ∈ R d and n is the number of objects. Question q t is a sequence of words, which can be encoded using LSTM, i.e., m q t = LSTM(q t ). We also organize the previous dialog (including caption) as history</p><formula xml:id="formula_0">H = (H 0 , . . . , H t−1 ), where H 0 = C, and H i = (q i , a i,gt ) for i ∈ {1, . . . , t − 1},</formula><p>which is the concatenation of the question and correct answer at each turn before time t. Similar to the question, we use another LSTM to extract the history features as</p><formula xml:id="formula_1">U = (u 0 , . . . , u t−1 ), where u i = LSTM(H i ). m q t ∈ R d and u i ∈ R d correspond to the last state of LSTM.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Primary Stage</head><p>An encoder-decoder solution is adopted in the primary stage <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18]</ref>. The encoder contains two main tasks, one is how to de-reference in the multi-turn conversations (98% of dialogs contain at least one pronoun), and the other is to locate the objects in the image mentioned in the current question. The attention mechanism <ref type="bibr" target="#b17">[18]</ref> is commonly used to tackle the tasks. Instead of linear concatenation, we use multi-modal factorized bilinear pooling (MFB) <ref type="bibr" target="#b31">[32]</ref>, as it can overcome the difference between distributions of the two features (two LSTMs to encode question and history, respectively; LSTMs for text feature and CNNs for image feature). MFB is expected to provide a richer representation than other bilinear methods, such as MLB <ref type="bibr" target="#b14">[15]</ref> and MCB <ref type="bibr" target="#b6">[7]</ref>. In MFB, the fusion of two features X, Y ∈ R d is calculated by:</p><formula xml:id="formula_2">z = MFB(X, Y ) = k i=1 (U i X • V i Y ),<label>(1)</label></formula><p>where U, V ∈ R d×l×k are the parameters to be learned, k is the number of factors, l is the hidden size, and • is the Hadamard product (element-wise multiplication). However, Y sometimes represents multiple channel input, e.g., detected objects or history in our model, so the formula becomes:</p><formula xml:id="formula_3">z = MFB(X, Y ) = k i=1 ((U i X · 1 ) • (V i Y )),<label>(2)</label></formula><p>where 1 ∈ R φ is the vector with all elements equal to one, and φ is the channel number of Y . To stabilize the output neurons, we use power normalization (z ← sign(z)|z| 0.5 ) and 2 normalization (z ← z/ z ). We utilize MFB to learn the unified vector of the question and history, denoted as z h t ∈ R l×t . Then, we learn the attention weight and vector by:</p><formula xml:id="formula_4">z h t = MFB h (m q t , U ), where</formula><formula xml:id="formula_5">α h t = softmax(w α z h t ),<label>(3)</label></formula><formula xml:id="formula_6">m h t = t−1 i=0 (α h t,i u i ),<label>(4)</label></formula><p>w α ∈ R l is a learned parameter, and α h t ∈ R t is the calculated weight implying which history the question should refer to. The attended history vector m h t can be concatenated with the question vector and then fused with image features,</p><formula xml:id="formula_7">z v t = MFB v ([m q t : m h t ], V ).</formula><p>The image attention vector m v t can be obtained in a similar approach as Eq.(3) and Eq.(4) by taking z v t as the input. Finally, we learn the representation of text and visual features with</p><formula xml:id="formula_8">e p t = MFB e ([m q t : m h t ], m v t ).</formula><p>The decoder encodes each candidate answer a t,i ∈ A t to m a t,i using LSTM and calculates the dot similarity score by:</p><formula xml:id="formula_9">s d t,i = e p t f d (m a t,i ),<label>(5)</label></formula><p>where f d is a one-layer MLP with activation tanh to project the answer encoding m a t,i ∈ R d to the space of input embedding e p t . The correct answer a t,gt should have a higher score than the others. Thus, we use N-pair loss <ref type="bibr" target="#b22">[23]</ref> to measure the error. Most of the 100 candidate answers are easy samples, which are irrelevant to the inputs, and contribute to no useful learning signal in this loss (score difference less than zero in <ref type="figure" target="#fig_6">Figure 5</ref>). To solve the imbalance problem, we employ temperature τ to reduce the imbalance impact:</p><formula xml:id="formula_10">L D = log( 100 i=1 exp s d t,i − s d t,gt τ ),<label>(6)</label></formula><p>where τ ≤ 1.</p><p>If the candidate answer a t,i is correctly scored lower than that of the ground truth answer a t,gt , loss</p><formula xml:id="formula_11">l t,i = s d t,i − s d t,</formula><p>gt will be less than 0, and τ can reduce the contribution of answer a t,i . For instance, with τ = 0.25 and l t,i = −1, about 20 such items make the same loss as the normal N-pair loss. Otherwise, it amplifies the loss of incorrectly scored answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Synergistic Stage</head><p>In the primary stage, some answers are improperly scored due to the limitations of the scoring method. In this synergistic stage, answers are coordinated with the question and image for re-rank. However, easy candidate answers are not needed in further analysis, and we want our second stage model to fully focus on hard answers within its modeling capacity. Therefore, we select the answers with higher probability of being correct based on the predicted scores from the primary stage. As seen from the recall of our best method in <ref type="table">Table 1</ref>, the top ten answers predicted in the primary stage covered nearly 90% of the ground truth, which means that the remaining 90 answers are of lower probability and can easily be discriminated. Based on this phenomenon, we first pick the top N answers from A t to organize a new candidate set B t , where</p><formula xml:id="formula_12">B t = (b t,1 , . . . , b t,N ), B t ⊂ A t .</formula><p>The selected answers are often ambiguous for describing the whole meaning of sentences (such as 'No' and 'Black and grey' in <ref type="figure" target="#fig_0">Figure 1</ref>), so they must work with the corresponding question to make complete sentences. Thus, we append question q t to each answer b t,j , j ∈ {1, . . . , N } as a question-answer pair and encode it using LSTM to obtain a vector:</p><formula xml:id="formula_13">m b t,j = LSTM([q t : b t,j ]).<label>(7)</label></formula><p>Extra history is required to remedy the reference problem of the question. Therefore, we use m b t,j as a question vec-tor combined with attended history m h t to learn the image's attention parameters:</p><formula xml:id="formula_14">z r t,j = MFB a ([m b t,j : m h t ], V )<label>(8)</label></formula><p>and the attended image feature m r t,j using Eq.(3) and Eq.(4) for selected answer b t,j . Similar to the primary stage, we obtain the fusion embedding e r t,j = MFB r ([m b t,j : m h t ], m r t,j ), which represents the answer vector synergized with the image, question, and history, which is directly used to calculate the score by:</p><formula xml:id="formula_15">s r t,j = f r (e r t,j ),<label>(9)</label></formula><p>where f r can be a one-layer MLP. An answer in candidate set containing more details and better matching the inputs should earn a higher score than ordinary ones.</p><p>Here, we reuse the attended history vector m h t from the primary stage, since the co-reference problem in the question can be resolved without knowing the answer. For example, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we can refer 'their' to the 'five people' regardless of the colors of their jackets. The image feature V is shared with the primary stage, because we want the image feature to be represented universally in the two stages. But the attention weights are learned at each stage, since each candidate answer, as well as the question, depicts its own attention map.</p><p>We treat this stage as a classification problem, where the correct answer should have the highest probability:</p><formula xml:id="formula_16">p r t = softmax(s r t ),<label>(10)</label></formula><formula xml:id="formula_17">L R = N j=1 −y j log(p r t,j ),<label>(11)</label></formula><p>where y gt is equal to 1 and the others are zeros. We note that this formula can be easily extended to soft cross entropy, where y i is the probability marking this answer as correct if a dense annotation dataset is available in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Extension to the Generative Model</head><p>Besides discriminative model in the primary stage, the generative model can also be used to score the candidate answers and seamlessly works with the proposed imagequestion-answer synergistic method. The encoder of the generative model is the same as that of the discriminative model in the primary stage in Section 3.1. Accordingly, we still use e p t to represent the common vector of image I, history H, and question q t at turn t. The decoder interprets e p t into answers and calculates the probability of answer a t,i by:</p><p>s g t,i = p(w 1 t,i , . . . , w T t,i |q t , H, I)</p><formula xml:id="formula_18">= T j=1 p(w j t,i |(w 1 t,i , . . . , w j−1 t,i ), e p t ),<label>(12)</label></formula><p>The University of Sydney  which is also regarded as score in the primary stage as shown in <ref type="figure" target="#fig_4">Figure 4</ref>, where w 1 t,i , . . . , w T t,i is the word sequence of the answer a t,i , and T is word number. For each word, its probability is given by:</p><formula xml:id="formula_19">p(w j t,i |(w 1 t,i , . . . , w j−1 t,i ), e p t ) = f g (MFB g (h j , e p t )),<label>(13)</label></formula><p>where h 0 is the last hidden state of LSTM for the question, h j = LSTM(h j−1 , w j−1 t,i ) is the state of LSTM, and f g maps the fusion vector to the word space. Instead of initializing the LSTM of decoder with encoded vector e p t <ref type="bibr" target="#b3">[4]</ref>, we regard it as a context vector <ref type="bibr" target="#b18">[19]</ref>. This is for three main reasons: first, the encoded vector and the LSTM decoder have different distributions, and all gates and the hidden state in LSTM are learned by linear combination; second, the salient objects that should be attended by each token are already chosen by the question, and the aim of the decoder is to translate the visual information of salient objects into text, such that the context vector is fixed for each token; and third, instead of learning a joint vector with h j−1 , the context vector could be considered as compensatory information to the current hidden state h j , which reduces the uncertainty for next word prediction.</p><p>To make the correct answer a t,gt score higher in the primary stage, we maximize the conditional probability p(w 1 t,gt , . . . , w T t,gt |q t , H, I). Thus, the loss function is the sum of the negative log likelihood of the correct word in each step:</p><formula xml:id="formula_20">L G = − T i=1 log p(a t,gt ).<label>(14)</label></formula><p>If the candidate answers are given in the generative model as in the discriminative model, we can collect the score for each answer using Eq. <ref type="bibr" target="#b11">(12)</ref>. Otherwise, we can generate some candidate answers with high probability by beam search <ref type="bibr" target="#b23">[24]</ref>. After collecting the score for each answer in the primary stage, we follow the strategy in Section 3.2 to pick the top N answers and then synergize them with the image, question, and history to learn a better representation for re-scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate our synergistic strategy on a visual dialog dataset. We introduce the dataset and evalua-tion metric, before describing our experimental setting and results, and finally the qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset and Evaluation Metric</head><p>Our model is trained on the Visual Dialog v1.0 dataset <ref type="bibr" target="#b3">[4]</ref>, which contains about 120k images from COCO-trainval <ref type="bibr" target="#b16">[17]</ref>. Each image has one caption and 10 turn dialogs, i.e., about 1.2 million question-answer pairs. To organize this dataset, two people chatted on the Amazon Mechanical Turk. The questioner could not see the image and asked a question based on the given caption and previous context to better understand the scene, while the answerer could see both the image and caption and replied to the question as naturally and conversationally as possible. Each question has 100 candidate answers containing one correct answer, 50 answers to similar questions, 30 popular answers, and some randomly picked answers from the dataset. For the validation and test datasets, 10K COCO-like images were collected from Flickr. The test dataset was densely annotated in v1.0 by four people to allow application for the more robust evaluation metric NDCG <ref type="bibr" target="#b26">[27]</ref> rather than traditional retrieval metrics, such as mean rank, R@1,5,10, and MRR. There could be more than one correct answer to each question in the candidate set, such as 'yeah' and 'yes'. In this situation, NDCG is invariant to the order of options with identical relevance. For each candidate answer, its relevance is annotators who marked answer as relevent</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>. The metric is given by:</p><formula xml:id="formula_21">DCG@k = k i=1 relevance i log 2 (i + 1) ,<label>(15)</label></formula><p>NDCG@k = DCG@k for submited ranking DCG@k for ideal ranking , <ref type="bibr" target="#b15">(16)</ref> where k is the number of answer options whose relevance is greater than zero. Of these metrics, a higher score is better for NDCG, MRR, and R@1,5,10, but a lower score is better for mean rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We first constructed the vocabulary, which contains the words appearing in the questions, correct answers, and captions more than four times in the train dataset. This made 11,213 words with the padding word 'PAD', out-ofvocabulary word 'UNK', start symbol 'START', and end symbol 'END'. Then, each word was embedded within a 300-dimension vector shared across caption, history, question, and answer. The maximum lengths of the caption, question, answer, and history were 40, 20, 20, and 40, respectively. For each candidate answer, we inserted 'START' at the head and appended 'END' at the tail. The LSTMs of the question and history are two layered, while it is one layered for the answer in the primary stage and the question-answer pair in the synergistic stage. The hid- den state dimension d for all LSTMs and CNN is 512. For bilinear pooling, we set k to 5 and l to 1000 as Yu <ref type="bibr" target="#b31">[32]</ref>. We start training the primary stage with loss L D (L G ) for 7 epochs to arrange the top ranked answers relative to the inputs. The synergistic stage follows with loss L D (L G ) + L R for another 15 epochs. When training the synergistic stage, our policy is to randomly sample N − 1 answers from the top M ranked in the first stage combined with the right answer a t,gt to organize B t . During testing, we only select the top N answers. For this dataset, we choose N = 10 or 15 and M varying from 10 to 40 for the discriminative model, then N = 10, 20, 30 and M fixed to 30 for the generative model, and we analyze the effect of different N and M on performance in Section 5.4. Our model is trained using the Adam solver <ref type="bibr" target="#b15">[16]</ref> with β 1 = 0.9, β 2 = 0.99, initial learning rate 10 −3 , and decay every 7 epochs with an exponential rate of 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with the State-of-the-art</head><p>We compare our discriminative model with baselines <ref type="bibr" target="#b3">[4]</ref> and other methods: Later Fusion (LF), which encodes the question, image, and history respectively and projects their concatenation into a joint embedding; Hierarchical Recurrent Encoder (HRE), which uses a hierarchical architecture to encode the dialog history; Memory Network (MN), which maintains a memory bank storing previous dialog, which is attended by a question; and MN-att and LF-att, which add an attention mechanism for image to their base methods. From <ref type="table">Table 1</ref>, it can be seen that our best single model improves NDCG by 7.56% and MRR by 5.13% compared with LF-att. To improve accuracy, we ensemble 10 models with different seeds and M . We rank the answers by summing scores of all models and achieve the highest NDCG on the test-standard server of Visual Dialog Challenge 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>We conduct several ablation studies to verify the contribution of each module to our discriminative model. The first three lines in <ref type="table">Table 2</ref> show the performance of the primary stage with different τ varying from 1 to 0. <ref type="bibr" target="#b24">25</ref>   performance improves as τ decays, because most candidate answers are easily discriminated. The summed loss of these easy negative answers whose scores are lower than the correct answer by one consumes almost 30% of the model's energy at τ = 1.00, as shown in <ref type="figure" target="#fig_6">Figure 5</ref>. This kind of loss is reduced as τ decreases and is nearly zero when τ = 0.25 , saturating our primary stage. This makes the model pay more attention to incorrect answers scored near or higher than the correct answer. The last five lines show the results of the synergistic stage with different settings. MRR of this stage drops at N = 10 and M = 10, as the top answers of the primary stage become stable after several epochs, resulting in the synergistic stage learning bias. Feeding more samples by increasing M improves performance to produce the best model at M = 30, showing that synergy can learn a better representation of the image, question, and answer. What of interest is that the performance drops at M = 40 or N = 15, possibly because the selected answers become less relevant to the inputs, and the second stage model's ability is to score nearly correct answers but is sensitive to unrelated ones. Conversely, it also addresses the importance of the primary stage to the synergistic stage. And the primary stage is also necessary to balance the memory cost, since each answer learns its own attention map and fusion with the image in the synergistic stage, while in the primary stage, only one attention map is required for the question and image.</p><p>For the generative model, the first line in <ref type="table" target="#tab_3">Table 3</ref> shows the performance of the memory network initialized by the common vector of inputs, and the second line shows the result of our model using only the primary stage, leaving other three lines for the synergistic stage. It can be seen  that the primary stage outperforms baseline by 1.1% with respect to MRR, which generates a strong candidate set for the next stage. Our synergistic stage further improves MRR by 2.6% when N = 10, since the primary stage focuses on each word but lacks the understanding of whole answer sequence. In contrast to the discriminative model in which about 90% of correct answers are top ten ranked in the primary stage, only 2 out of 3 correct answers are top sorted by the generative model, so we increase the selected answer number N from 10 to 30 in the second stage, which further increases MRR by 2.1%. Furthermore, our model improves R@5 by 9.2% and R@10 by 8.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Analysis</head><p>To further demonstrate the effects of our synergistic model, we present some examples from the validation dataset. <ref type="figure" target="#fig_7">Figure 6</ref> shows the results of the discriminative model ranked by only the primary stage and our two-stage network. The answer in red is correct, while the others are top-ranked candidate answers. It can be seen that the one-stage model tries to give a safe answer such as 'No' to the top-left and middle-left images, while our model depicts more details by adding 'only the giraffe' and 'another pair of feed'. Biased answers are given in middle-right image, since for a binary question with choices, the answer word is always contained in the question. There is also bias in bottom-right image, because only humans tend to wear hats. Our model gives unbiased answers based on the images, and furthermore, can detect the discrepancies between similar words in the bottom-left image.</p><p>In order to apply our method to realistic applications, we abandon the prepared candidate set in the first stage and generate another within our primary model by beam search <ref type="bibr" target="#b23">[24]</ref>, which maintains a partial sequence list of size B. At each step, all partial sequences are extended with the whole vocabulary, and only the top B sequences with the highest probability are retained for next step. Sentences meeting the 'END' symbol are moved from the partial list to the complete one. Starting with the 'START' symbol and iterating for a maximum 20 steps, we obtain a candidate set of size B with all complete sentences complemented by some partial ones. In <ref type="figure" target="#fig_8">Figure 7</ref>, we show the top generated answers with B = 15 for the primary and synergistic stage, the preset answer is below the question. The primary stage always ranks short answers having one or two words higher than the long sequences that depict images with more informa-   tion, because the generative method calculates the score for each answer by product probability of its words. In the synergistic stage, this problem is overcome, since the extra attribute information, e.g., 'a white visor' (middle-left image) and 'white with a blue tail' (top-right image), can have a higher score than a simple answer. Surprisingly, our model can sometimes even generate better answers than those provided, such as 'Just the legs of someone' vs. 'Part of a person' in the bottom-left image and 'behind the truck' vs. 'In the background' in the bottom-right image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>The limitations of previous input-answer fusion methods mean that they cannot correctly represent the common vector of these features. As a result, they omit detailed in-formation and focus on short and safe answers. In this paper, we develop a synergistic network that jointly learns the representation of the image, question, answer, and history in a single step. We also improve the N-pair loss function to solve the class-imbalanced problem in the discriminative model. Our final proposed discriminative model achieves state-of-the-art performance on the Visual Dialog v1.0 teststandard server with the robust NDCG evaluation metric. The results of our generative model are also encouraging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A general visual dialog task. The correct answer is picked from a candidate set by investigating the given image, caption, history, and question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Candidate answers synergized with image and question are re-scored. The synergistic stage refers the answers back to the question and re-matches the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of our model. All candidate answers are scored in the primary stage, and some selected answers are re-scored in the synergistic stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Primary stage of our generative model. The score of each answer is its probability of word sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Cumulative normalized loss for different τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparison for discriminative model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>the legs of someone A1:Just the legs of someone A2:Just the legs of 1 person A3:I can see the legs of 1 person A4:Just legs A5:Just the legs of 1 , there is 1 behind the truck A2:Yes, I see a few A3:Yes, there is a building behind it A4:Yes, there is 1 behind the bus A5:Yes, I see 1PrimarySynergistic Qualitative comparison for generative model without candidate answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>38.54 59.82 66.94 16.69 Synergistic (ours) 10 30 51.62 40.77 63.58 67.00 16.51 Synergistic (ours) 20 30 53.23 41.42 67.22 72.91 15.87 Synergistic (ours) 30 30 53.73 41.28 69.01 75.85 15.12</figDesc><table><row><cell>Model</cell><cell cols="3">N M MRR R@1 R@5 R@10 Mean</cell></row><row><cell>MN-att [4]</cell><cell>-</cell><cell>-</cell><cell>47.94 37.48 58.56 65.57 17.61</cell></row><row><cell>Primary (ours)</cell><cell>-</cell><cell>-</cell><cell>49.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance of our generative model on validation dataset with candidate set.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>This work was supported in part by the Australian Research Council under Project FL-170100117, Project DP-180103424, Project IH180100002, and Project DE-180101438.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="727" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Two can play this game: Visual dialog with discriminative question generation and answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07932</idno>
		<title level="m">Bilinear attention networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00471</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Straight to the facts: Learning knowledge base retrieval for factual visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01124</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning visual reasoning without strong priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03017</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class npair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fvqa: Fact-based visual question answering. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A theoretical analysis of ndcg ranking measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013)</title>
		<meeting>the 26th Annual Conference on Learning Theory (COLT 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">den Hengel. Ask me anything: Free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4622" to="4630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reid, and A. van den Hengel. Are you talking to me? reasoned visual dialog generation through adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07613</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rethinking diversified and discriminative proposal generation for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03508</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
