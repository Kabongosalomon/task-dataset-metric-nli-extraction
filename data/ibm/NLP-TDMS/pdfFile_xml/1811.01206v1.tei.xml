<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DUNet: A deformable network for retinal vessel segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">DUNet: A deformable network for retinal vessel segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Retinal blood vessel</term>
					<term>segmentation</term>
					<term>DUNet</term>
					<term>U- Net</term>
					<term>deformable convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic segmentation of retinal vessels in fundus images plays an important role in the diagnosis of some diseases such as diabetes and hypertension. In this paper, we propose Deformable U-Net (DUNet), which exploits the retinal vessels' local features with a U-shape architecture, in an end to end manner for retinal vessel segmentation. Inspired by the recently introduced deformable convolutional networks, we integrate the deformable convolution into the proposed network. The DUNet, with upsampling operators to increase the output resolution, is designed to extract context information and enable precise localization by combining low-level feature maps with highlevel ones. Furthermore, DUNet captures the retinal vessels at various shapes and scales by adaptively adjusting the receptive fields according to vessels' scales and shapes. Three public datasets DRIVE, STARE and CHASE DB1 are used to train and test our model. Detailed comparisons between the proposed network and the deformable neural network, U-Net are provided in our study. Results show that more detailed vessels are extracted by DUNet and it exhibits state-of-the-art performance for retinal vessel segmentation with a global accuracy of 0.9697/0.9722/0.9724 and AUC of 0.9856/0.9868/0.9863 on DRIVE, STARE and CHASE DB1 respectively. Moreover, to show the generalization ability of the DUNet, we used another two retinal vessel data sets, one is named WIDE and the other is a synthetic data set with diverse styles, named SYNTHE, to qualitatively and quantitatively analyzed and compared with other methods. Results indicates that DUNet outperforms other state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE morphological and topographical changes of retinal vessels may indicate some pathological diseases, such as diabetes and hypertension. Diabetic Retinopathy (DR) caused by elevated blood sugar levels, is a complication of diabetes in which retinal blood vessels leak into the retina, accompanying <ref type="bibr">Manuscript</ref>  with the swelling of the retinal vessels <ref type="bibr" target="#b0">[1]</ref>. It must be noticeable if a diabetic patient appears in a swelling of the retinal vessels. Hypertensive Retinopathy (HR) is another commonly seen retina disease caused by high blood pressure <ref type="bibr" target="#b1">[2]</ref>. An increased vascular tortuosity or narrowing of vessels can be observed in a patient with high blood pressure <ref type="bibr" target="#b2">[3]</ref>. Therefore, retinal blood vessels extracted from fundus images can be applied to the early diagnosis of some severe disease. This inspires the proposal of more accurate retinal blood vessel detection algorithms in order to facilitate the early diagnosis of pathological diseases.</p><p>However, the retinal blood vessels present extremely complicated structures, together with high tortuosity and various shapes <ref type="bibr" target="#b3">[4]</ref>, which makes the blood vessel segmentation task quite challenging. Different approaches have been proposed for blood vessel detection. They are mainly divided into two categories: manual segmentation and algorithmic segmentation. The manual way is time-consuming and in high-demand of skilled technical staff. Therefore, automated segmentation of retinal vessels, which can release the intense burden of manual segmentation, is highly demanded. However, due to the uneven intensity distribution of the retinal vascular images, the subtle contrast between the target vessels and the background of the images, high complexity of the vessel structures, coupled with image noise pollution, it is quite challenging to segment the retinal blood vessels in an accurate and efficient way.</p><p>Deep learning has shown its excellence in medical imaging tasks. Recently, the Fully Conventional Neural Network (FCN) based network such as U-Net <ref type="bibr" target="#b4">[5]</ref> has attracted more attention compared with the traditional Convolutional Neural Network (CNN) due to its ability to obtain a coarse-to-fine representation. In this study, we proposed an FCN-based network named Deformable U-Net (DUNet) that greatly enhances deep neural networks' capability of segmenting vessels in an endto-end and pixel-to-pixel manner. It is designed to have a U-shape similar to U-Net <ref type="bibr" target="#b4">[5]</ref> where upsampling operators with a large number of feature channels are stacked symmetrically to the conventional CNN, so context information is captured and propagated to higher resolution layers and thus a more precise segmentation is obtained. Furthermore, inspired by the recently proposed deformable convolutional networks (Deformable-ConvNet) <ref type="bibr" target="#b5">[6]</ref>, we stacked deformable convolution blocks both in the encoder and decoder to capture the geometric transformations. Therefore, the receptive fields are adaptively adjusted according to the objects' scales and shapes and complicated vessel structures can be well detected. Deformable-ConvNet and U-Net are used for comparison. All arXiv:1811.01206v1 [cs.CV] 3 Nov 2018 the networks were trained from scratch and detailed analysis of the experimental results was provided. In the next section, we give a brief literature review of related work. Section III explains the architecture of DUNet and systematic retinal blood vessel segmentation method. Deformable-ConvNet and U-Net are also introduced briefly in this section. Experimental results are presented in Section IV, where we evaluate the proposed method on three different retinal blood vessel datasets. Conclusions and discussions are given in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The goal of retinal blood vessel segmentation is to locate and identify retinal vessel structures in the fundus images. With the development of imaging technology, various intelligent algorithms have been applied to retinal vessel segmentation. According to the learning patterns, segmentation methods can be divided into supervised method and unsupervised method. Supervised learning learns from training data to generate a model and predict test data from that model, it automatically finds the probable category of data. Next, a brief overview of vessel segmentation from these two aspects is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unsupervised method</head><p>The unsupervised method has no training samples in advance, and it constructs models directly in most cases. Zana et al. presented an algorithm based on mathematical morphology and curvature evaluation for the detection of vessel-like patterns in a noisy environment and they obtained an accuracy of 0.9377 <ref type="bibr" target="#b6">[7]</ref>. Fraz et al. combined vessel centerlines detection and morphological bit plane slicing to extract vessel from retinal images <ref type="bibr" target="#b7">[8]</ref>. Martinez-Perez et al. proposed a method to automatically segment retinal blood vessels based on multiscale feature extraction <ref type="bibr" target="#b8">[9]</ref>. Niemeijer et al. compared a number of vessel segmentation algorithms <ref type="bibr" target="#b9">[10]</ref>. According to this study, the highest accuracy of those compared algorithms reached 0.9416. Zhang et al. presented a retinal vessel segmentation algorithm using an unsupervised texton dictionary, where vessel textons were derived from responses of a multi-scale Gabor filter bank <ref type="bibr" target="#b10">[11]</ref>. A better performance would be obtained if a proper pre-processing was carried out. Hassan et al. proposed a method which combined mathematical morphology and kmeans clustering to segment blood vessels <ref type="bibr" target="#b11">[12]</ref>. However, this method was not good at dealing with vessels of various widths. Tiny structures might be lost using this method. Oliveira et al. used a combined matched filter, Frangi's filter, and Gabor Wavelet filter to enhance the vessels <ref type="bibr" target="#b12">[13]</ref>. They took the average of a few performance metrics to enhance the contrast between vessels and background. Jouandeau et al. presented an algorithm which was based on an adaptive random sampling algorithm <ref type="bibr" target="#b13">[14]</ref>. Garg et al. proposed a segmentation approach which modeled the vessels as trenches <ref type="bibr" target="#b14">[15]</ref>. They corrected the illumination, detected trenches by high curvature, and oriented the trenches in a particular direction first. Then they used a modified region growing method to extract the complete vessel structure. A threshold of mean illumination level that was set empirically might bring bias in this method. Zardadi et al.</p><p>presented a faster unsupervised method for automatic detection of blood vessels in fundus images <ref type="bibr" target="#b15">[16]</ref>. They enhanced the blood vessels in various directions; Then they presented an activation function on cellular responses; Next, they classified each pixel via an adaptive thresholding algorithm; Finally, a morphological post-processing was carried out. However, several spots were falsely segmented into vessels which affected the final performance of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Supervised method</head><p>Different from unsupervised learning, supervised learning requires hand-labeled data in order to build an optimally predictive model. All the inputs are mapped to the corresponding outputs using the built model. It has been widely applied to the segmentation tasks. In order to reach the goal of segmentation, two processors are needed: One is an extractor to extract the feature vectors of pixels; The other one is a classifier to map extracted vectors to the corresponding labels. A number of feature extractors have been proposed, for instance, the Gabor filter <ref type="bibr" target="#b16">[17]</ref>, the Gaussian filter <ref type="bibr" target="#b17">[18]</ref> etc. Various classifiers such as k-NN classifier <ref type="bibr" target="#b18">[19]</ref>, support vector machine (SVM) <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b20">[21]</ref>, artificial neural networks (ANN) <ref type="bibr" target="#b21">[22]</ref>, AdaBoost <ref type="bibr" target="#b22">[23]</ref> etc, have been proposed to deal with different tasks.</p><p>Supervised methods were used widely in retinal vessel segmentation. Aslani et al. proposed a new segmentation method which characterized pixels with a vector of hybrid features calculated via a different extractor. They trained a Random Forest classifier with the hybrid feature vector to classify vessel/non-vessel pixels <ref type="bibr" target="#b23">[24]</ref>. In order to simplify the model and increase the efficiency, the number of Gabor features should be reduced as small as possible. Marn et al. used Neural Network (NN) scheme for pixel classification and they computed a 7-D vector composed of gray-level and moment invariants-based features for pixel representation <ref type="bibr" target="#b24">[25]</ref>. Yet the calculation cost was high and needed to be optimized.</p><p>For these traditional supervised methods, what features are used for classification greatly influence the final results of the prediction. However, they are often defined empirically, which requires the human intervention and may cause bias. Therefore, an automated and effective feature extractor is highly demanded to achieve higher efficiency.</p><p>Deep learning is an architecture referring to an algorithm set which can solve the image, text and other tasks based on backpropagation and multi-layer neural network. One of the most significant contributions of deep learning is that it can replace handcrafted features with features automatically learned from deep hierarchical feature extraction method <ref type="bibr" target="#b25">[26]</ref>.</p><p>In a number of fields such as image processing, bioinformatics, and natural language processing, various deep learning architectures such as Deep Neural Networks, Convolutional Neural Networks, Deep Belief Networks and Recurrent Neural Networks have been widely used and have shown that they could produce state-of-the-art results on various tasks. Recently, there are some studies that investigated the vessel segmentation problems based on deep learning. <ref type="bibr">Wang</ref>   two superior classifiers, Convolutional Neural Network (CNN) and Random Forest (RF) together to carry out the segmentation <ref type="bibr" target="#b26">[27]</ref>. Fu et al. used the deep learning architecture, formulated the vessel segmentation to a holistically-nested edge detection (HED) problem, and utilized the fully convolution neural networks to generate vessel probability map <ref type="bibr" target="#b27">[28]</ref>. Maji et al. used a ConvNet-ensemble based framework to process color fundus images and detect blood vessels <ref type="bibr" target="#b28">[29]</ref>. Jiang et al. proposed a method which defined and computed pixels as primary features for segmentation, then a Neural Network (NN) classifier was trained using selected training data <ref type="bibr" target="#b29">[30]</ref>. In this method, each pixel was represented by an 8-D vector. Then the unlabeled pixels were classified based on the vector. Azemin et al. estimated the impact of aging based on the results of the supervised vessel segmentation using artificial neural network <ref type="bibr" target="#b30">[31]</ref>. It showed that different age groups affected different aspect of segmentation results. Liskowski et al. proposed a supervised segmentation architecture that used a Deep Neural Network with a large training dataset which was preprocessed via global contrast normalization, zero-phase whitening, geometric transformations and gamma corrections <ref type="bibr" target="#b31">[32]</ref>. And the network classified multiple pixels simultaneously using a variant structured prediction method. Fu et al. regarded the segmentation as a boundary detection problem and they combined the Convolution Neural Networks (CNN) and Conditional Random Field (CRF) layers into an integrated deep network to achieve their goal <ref type="bibr" target="#b32">[33]</ref>.</p><p>Overall, it is expected that deep learning approaches can overcome the difficulties existed in the traditional unsupervised and supervised methods. In our study, we developed a systematic framework using the fully convolutional based methods to finish the effective and automatic segmentation task of retinal blood vessels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The goal of our work is to build deep learning models to segment retinal vessels in fundus images. Inspired by U-Net <ref type="bibr" target="#b4">[5]</ref> and deformable convolutional network (Deformable-ConvNet) <ref type="bibr" target="#b5">[6]</ref>, we propose a new network named Deformable U-Net (DUNet) for retinal vessel segmentation task. The proposed approach is designed to integrate the advantages of both deformable unit and U-Net architecture. We will introduce our proposed method in details while giving a brief explanation of the two other networks as well. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an overview of the proposed DUNet, U-Net and Deformable-ConvNet. The raw images are preprocessed and cropped into small patches to establish training and validation dataset. During contrastive experiments, different models will be set with corresponding patch size. Since DUNet and U-Net are both end-to-end deep learning frameworks for segmentation, a 48 × 48 patch size is used to trade off between computing complexity and efficiency. Meanwhile, Deformable-ConvNet is a model for vessel classification, a 29 × 29 patch size is chosen for training. After the inference of an image from the test dataset, all outputs from different models are re-composed to form a complete segmentation map respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and material</head><p>Performance was evaluated on three public datasets: DRIVE, STARE and CHASE DB1 (CHASE) dataset. The DRIVE (Digital Retinal Images for Vessel Extraction) contains 40 colored fundus photographs which were obtained from a diabetic retinopathy (DR) screening program in the Netherlands <ref type="bibr" target="#b33">[34]</ref>. The plane resolution of DRIVE is 565 × 584. STARE (Structured Analysis of the Retina) dataset, which contains 20 images, is proposed to assist the ophthalmologist  to diagnose eye disease <ref type="bibr" target="#b34">[35]</ref>. The plane resolution of STARE is 700×605. The CHASE dataset contains 28 images corresponding to two per patient for 14 children in the program Child Hear And Health Study in England <ref type="bibr" target="#b35">[36]</ref>. The plane resolution of CHASE is 999 × 960. Experts' manual annotations of the vascular are available as the ground truth ( <ref type="figure" target="#fig_1">Fig. 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Image preprocessing and dataset preparation</head><p>Deep neural network has the ability to learn from unpreprocessed image data effectively. While it tends to be much more efficient if appropriate preprocessing has been applied to the image data. In this study, three image preprocessing strategies were employed. Single channel images show the better vessel-background contrast than RGB images <ref type="bibr" target="#b36">[37]</ref>. Thus, raw RGB images were converted into single channel ones. Normalization and Contrast Limited Adaptive Histogram Equalization <ref type="bibr" target="#b37">[38]</ref> (CLAHE) were used over the whole data set to enhance the foreground-background contrast. Finally, gamma correction was introduced to improve the image quality much further. Intermediate images after each preprocessing step are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>To reduce overtting problem, our models were trained on small patches which were randomly extracted from the images. In order to reduce the calculation complexity and ensure the surrounding local features, we set the size of the patch to 48×48 for DUNet and U-Net. The corresponding label for that patch was decided based on the ground truth images <ref type="figure" target="#fig_3">(Fig. 4</ref>).</p><p>All the datasets were divided into training set, validation set, and test set. The training set is used for adjusting the weights. Validation set is used for selecting the best weight while test set is used for performance evaluation. For DRIVE dataset, 20  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deformable U-Net (DUNet)</head><p>Inspired by U-Net <ref type="bibr" target="#b4">[5]</ref> and deformable convolutional network (Deformable-ConvNet) <ref type="bibr" target="#b5">[6]</ref>, we proposed a network, named Deformable U-Net (DUNet) for retinal vessel segmentation task. The proposed network has a U-shaped architecture with encoders and decoders on two sides, and the original convolutional layer was replaced by the deformable convolutional block. The new model is trained to integrate the low-level feature with the high-level features, and the receptive field and sampling locations are trained to adaptive to vessels' scale and shape, both of which enable precise segmentation. DUNet builds on top of U-Net and uses the deformable convolutional block as encoding and decoding unit. <ref type="figure">Fig. 5</ref> illustrates the network architecture. Detailed design of the deformable convolutional block is shown in the dashed window. The architecture consists of a convolutional encoder (left side) and a decoder (right side) in a U-Net framework. In each encoding and decoding phase, deformable convolutional blocks are used to model retinal vessels of various shapes and scales through learning local, dense and adaptive receptive fields. Each deformable convolutional block consists of a convolution offset layer, which is the kernel concept of deformable convolution, a convolution layer, a batch normalization layer <ref type="bibr" target="#b38">[39]</ref> and an activation layer. During the decoding phase, we additionally insert a normal convolution layer after merge operation to adjust filter numbers for convolution offset layer. With this architecture, DUNet can learn discriminative features and generate the detailed retinal vessel segmentation results.  1) U-Net as the basic architecture: Our U-Net architecture has an overall architecture similar to the standard U-Net, consisting of an encoder and a decoder symmetrically on the two sides of the architecture. The encoding phase is used to encode input images in a lower dimensionality with richer filters, while the decoding phase is designed to do the inverse process of encoding by upsampling and merging low dimensional feature maps, which enables the precise localization. Besides, in the upsampling part, a larger number feature channels are used in order to propagate the context to higher resolution layers. In order to solve the internal covariate shift problem and speed up the processing, a batch normalization layer was inserted after each unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv</head><p>2) Deformable Convolutional Blocks: A big challenge in vessel segmentation is to model the vessels with various shapes and scales <ref type="bibr" target="#b5">[6]</ref>. Traditional methods such as the steerable filter <ref type="bibr" target="#b39">[40]</ref>, Frangi filter <ref type="bibr" target="#b40">[41]</ref> exploit the vessel features through linear combination of responses at multiple scales or direction, which may bring bias. The deformable convolutional network (Deformable-ConvNet) solved this problem by introducing deformable convolutional layers and deformable ROI pooling layers into the traditional neural networks. We were inspired by the idea from Deformable-ConvNet that the various shapes and scales can be captured via deformable receptive fields, which are adaptive to the input features. Therefore, we integrated the deformation convolution into the proposed network.</p><p>In the deformable convolution, offsets were added to the grid sampling locations which are normally used in the standard convolution. The offsets were learned from the preceding feature maps produced by the additional convolutional layers. Therefore, the deformation is able to adapt to different scales, shapes, orientation, etc. We take the 5 × 5 deformable convolution as an example in <ref type="figure">Fig. 6</ref>.</p><p>As <ref type="figure">Fig. 6</ref> shows, for a 5 × 5 sized kernel with grid size 1, the normal convolution grid G can be formalized as:</p><formula xml:id="formula_0">G = {(−2, 2), (−2, −1), ..., ( 2, 1), (2, 2)}<label>(1)</label></formula><p>Thus, each location m 0 from output feature map y can be formalized as:</p><formula xml:id="formula_1">y(m 0 ) = mi∈G w(m i ) · x(m 0 + m i )<label>(2)</label></formula><p>Where x denotes the input feature map, w represents the weights of sampled value and m i means the locations in G. While in deformable convolution, normal grid G is enhanced by the offset ∆m i , we have  </p><p>Because offset ∆m i is usually not an integer, bilinear interpolation is applied to determine the value of the sampled points after migration. As mentioned above, the offset ∆m i is learned by an additional convolution layer. This procedure is illustrated in <ref type="figure" target="#fig_5">Fig. 7</ref>. Compared to the regular U-Net, DUNet may incur some computation cost in order to perform in a more local and adaptive manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Compare with U-Net and Deformable-ConvNet</head><p>We compared our proposed model with two state-of-the-art works. One is the normal U-Net, which we have introduced above; The other is the deformable convolutional network (Deformable-ConvNet). Deformable-ConvNet was originally introduced to distinguish whether a pixel belongs to vessel or not. In this model, vessel segmentation was considered as a classification task. A pixel's class can be determined based on its neighborhood defined as the patch centered on this pixel. For a selected pixel, which needs to be classified, we used pixel values in a patch centered on that selected pixel to capture the local information at a high level. In order to reduce the calculation complexity and to maximally capture the local features, the size of the patch was set to 29×29. The architecture of the Deformable-ConvNet is shown in <ref type="figure" target="#fig_6">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Performance evaluation metrics</head><p>We evaluated our model using several metrics: Accuracy (ACC), Positive Predictive Value (PPV), True Positive Rate (TPR), True Negative Rate (TNR) and the Area Under Curve (AUC) of Receiver Operating Characteristic (ROC). ACC is a metric for measuring the ratio between the correctly classified pixels and the total pixels in the dataset. PPV, which is also called precision, indicates the proportion of the true positive samples among all the predicted positive samples. TPR, also known as sensitivity, measures the proportion of positives that are correctly identified. TNR, or specificity, measures the proportion of negatives that are correctly identified. These metrics have the forms as following: Additionally, performance was evaluated with F-measure (F 1 ) <ref type="bibr" target="#b41">[42]</ref> and Jaccard similarity (JS) <ref type="bibr" target="#b42">[43]</ref> to compare the similarity and diversity of testing datasets. Here GT refers to the ground truth and SR refers to the segmentation result.</p><formula xml:id="formula_3">F 1 = 2 · PPV · TPR PPV + TPR (8) JS = |GT SR| |GT SR|<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULT</head><p>The proposed DUNet has upsampling layers to increase output resolution. It enables propagation of the context information to the higher resolution layers and detection of vessels in various shapes and scales, thus presents an accurate segmentation result. In this section, we systematically compared the DUNet with Deformable-ConvNet and U-Net. We firstly show the results based on the validation set, which is used for parameter selection. Then the results on the test set are presented. We also briefly compared DUNet with some other recently published approaches, most of which are under deep neural network framework and the others are standard segmentation approaches. All experiments were conducted under the Tensorflow <ref type="bibr" target="#b43">[44]</ref> and Keras <ref type="bibr" target="#b44">[45]</ref> frameworks using an NVIDIA GeForce GTX 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparisons with Deformable-ConvNet and U-Net</head><p>We compared the three models, Deformable-ConvNet, U-Net and DUNet based on the DRIVE, STARE and CHASE datasets. As described in Section III, we split the data into training set, validation set, and test set. We trained the three models from scratch using the training set and initialized the weights with random values. We set the batch size to 60, total training epochs to 100, Adam as optimizer and binary crossentropy as our loss function. To ensure a quick convergence and avoid overfitting, we used a dynamic method to set the learning rate values. The initial learning rate was set to 0.001. If the loss values remained stable after m e epochs, the learning rate was reduced 10 times. Additionally, the training process would be ceased if loss value stayed almost unchanged after n e epochs.</p><p>Here m e and n e are set to 4 and 20 empirically. The validation accuracy and loss values were recorded during the training phase. Performance on validation dataset is reflected in <ref type="table" target="#tab_4">Table I</ref>.</p><p>From <ref type="table" target="#tab_4">Table I</ref>, it shows that DUNet achieved the highest validation accuracy of 0.9650 and got the lowest loss value of 0.0919 on DRIVE dataset. On STARE dataset, it had the second highest accuracy and lowest loss value. And on CHASE dataset, it had the highest validation accuracy of 0.9704 and got the lowest loss value of 0.0833. Bar chart of the performance is shown in <ref type="figure">Fig. 9</ref>.</p><p>We further evaluated the model using the test data. PPV, TPR, TNR, ACC, F 1 -scores, JS and AUC were compared and shown in <ref type="table" target="#tab_4">Table II, Table III and Table IV</ref>. It shows from the tables that the DUNet achieves the best performance in terms of most of the metrics. To be noticed, the DUNet achieves the highest accuracy among the three models. The global accuracy for Deformable-ConvNet, U-Net, and DUNet is 0.9642/0.9681/0.9697 on DRIVE, 0.9673/0.9705/0.9729 on STARE and 0.9659/0.9728/0.9724 on CHASE, respectively.</p><p>We further evaluated the models using ROC curves, which is shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. The closer the ROC curve to the top-left border is in the ROC coordinates, the more accurate a model is. It can be seen that the curves of DUNet are the most top-left one among the three models while the Deformable-ConvNet curve is the lowest one of the three. Besides, figures also show that the DUNet obtains the largest area under the ROC curve (AUC).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Retinal vessel segmentation results</head><p>We display the retinal vessel segmentation results in <ref type="figure" target="#fig_0">Fig. 11</ref>, <ref type="figure" target="#fig_0">Fig. 12</ref> and <ref type="figure" target="#fig_0">Fig. 13</ref>. From figures, it can be observed that DUNet produces more distinct vessel segmentation results. The proposed DUNet can detect weak vessel or vessels that are tied up which may be lost in U-Net and Deformable-ConvNet, thus it is more powerful to preserve more details.</p><p>We show the details of the segmentation results of the three models in <ref type="figure" target="#fig_0">Fig. 14, it</ref> shows the local magnification view of vascular junction, where several vessels are tied up and close to each other, and tiny vessels of DRIVE, STARE and CHASE respectively. Due to the complicated vascular tree, segmentation algorithms are difficult to proceed precisely with such complicated structures. In the junction region of vessel, Deformable-ConvNet and U-Net extracted coarse information due to the limitation of network. It is worth mentioning that Deformable-ConvNet extracted more detailed vessel than U-Net in some junction regions, which showed its ability to capture retinal vessels of various shapes. With the help of deformable convolutional blocks, the DUNet successfully segmented the tied vessels. In the tiny vessel regions, U-Net showed its limitation in handling details. However, Deformable-ConvNet picked them up somewhere. As a result, the DUNet got a desiring segmentation results in those tiny and weak vessels.</p><p>With this structure, the DUNet is able to distinguish different vessels and present a better performance than the other models. Experimental results arrival at a conclusion that DUNet architecture has a more desirable performance in dealing with complicated and weak vessel structures among  the three models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison against existing methods</head><p>We also compared our method with several state-of-theart approaches. Among them, some are standard segmentation algorithms (denoted with STA) while the others are all based on deep neural networks (denoted with DNN). Additionally, we compared our method with Dasgupta et al.'s method and Alom et al.'s method on the other two datasets for qualitative and quantitative analysis. The first dataset named WIDE, used for tree topology estimation, contains 15 high-resolution, wide-field, RGB images. Each retinal image was taken from a different individual and captured as an un-compressed TIFF file at the widest setting <ref type="bibr" target="#b51">[52]</ref>. The WIDE dataset does not contain ground truth for retinal vessel segmentation. We used the WIDE for qualitative analysis and compared the proposed method with the other two methods. The second dataset (denoted with SYNTHE) is synthesized from generative adversarial nets <ref type="bibr" target="#b52">[53]</ref>. The dataset contains 20 retinal images at 565 × 584 resolution, which includes DRIVE <ref type="bibr" target="#b18">[19]</ref>, STARE <ref type="bibr" target="#b34">[35]</ref>, Kaggle and HRF <ref type="bibr" target="#b53">[54]</ref> style. Each of these styles contains 5 retinal images generated by 5 corresponding ground truth images. <ref type="figure" target="#fig_0">Fig. 15</ref> shows the SYNTHE dataset, four distinct style retinal images are generated from the same vessel map.</p><p>To show the generalization of these three models, we used the weights well-trained on DRIVE and predicted on WIDE and SYNTHE dataset. We preprocessed and cropped these images in patches in the same way. From <ref type="figure" target="#fig_0">Fig. 16</ref>, it qualitatively indicates that DUNet produces competitive results.</p><p>To further validate quantitatively the performance of these models, we also used the well-trained weights from DRIVE and tested on the SYNTHE datasets. We mixed the four distinct style images together, preprocessed and cropped SYN-THE in patches in the same way. The performances of three models are summarized in <ref type="table" target="#tab_4">Table VIII</ref>, which prove quantitatively that the DUNet gets the best performance among all these three models overall. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Deep neural networks, which uses hierarchical layers of learned features to accomplish high-level tasks, has been applied to a wide range of medical processing tasks. In this study, we propose a fully convolutional neural network, named DUNet to handle the retinal vessel segmentation task in a pixel-wise manner. DUNet is an extension of the U-Net with convolutional layers replaced by the deformable convolution blocks. With the symmetric U-shape architecture, DUNet is designed to capture context by the encoder and enable precise localization by the decoder through combining the low-level feature maps with the high-level ones. It also allows the context being propagated to the higher resolution layers through a larger number of feature channels in the upsampling part. Furthermore, with the deformable convolution blocks, DUNet is able to capture the retinal blood vessels at various shapes and scales by adaptively adjusting the receptive fields according to the vessels' scales and shapes. By adding offsets to the regular sampling grids of standard convolution, the receptive fields are deformable and augmented. While it does bring some extra costs of computation resources from convolution offset layer. In order to test the performance of the proposed network, we have trained Deformable-ConvNet and U-Net from scratch for comparison. This is also the first time that DUNet being used to conduct the retinal segmentation. Besides, a comparison with several standard segmentation algorithms and some other deep neural network based approaches are introduced here. We train and test the models on three public datasets: DRIVE, STARE and CHASE DB1. To validate the generalization of our model, we tested the DUNet on WIDE and SYNTHE datasets, and analyze qualitatively and quantitatively. Results show that with the help of deformable convolutional blocks, more detailed vessels are extracted, and the DUNet exhibits state-of-the-art performance in segmenting the retinal vessels.</p><p>In the future, more retinal vessel data will be incorporated to validate the proposed end-to-end model. We also plan to extend our DUNet architecture to three dimensions, aiming to obtain more accurate results in medical image analysis tasks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The pipeline of the three networks. (a) Original image; (b) Training samples; (c) Snapshots of proposed DUNet and compared models. Note that blue blocks refer to deformable convolution and the white ones represent regular convolution; (d) Inference results; (e) Re-composition of segmentation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Original retinal images (upper row) and corresponding ground truth (bottom row) examples from DRIVE, STARE and CHASE sequentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Typical images after each preprocessing step. (a) Original image; (b) Normalized image; (c) Image after CLAHE operation; (d) Image after Gamma correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Typical 48 × 48 patches selected for model training. (a) shows the patches from the original images; (b) shows the patches from the preprocessed image; (c) shows the patches from the corresponding ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>DUNet architecture with convolutional encoder and decoder using deformable convolutional block based on U-Net architecture. Output size of feature map is listed beside each layer. Illustration of the sampling locations in 5 × 5 normal and deformable convolutions. The upper row stands for normal convolution and the corresponding deformable convolution is in the bottom row. Each sampling location has an offset to generate new sampling location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Illustration of a 5 × 5 deformable convolution. Offset field comes from the input patches and features while the channel dimension is 2N corresponding to N 2D offsets. Deformable convolutional kernel has the same resolution as the current convolution layer. The convolution kernels and the offsets are learned at the same time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>The architecture of the Deformable-ConvNet. It is mainly composed of convolution layers (Conv), deformable convolutional layers (ConvOffset), batch normalization layers and activation layers (ReLU). y(m 0 ) = mi∈G w(m i ) · x(m 0 + m i + ∆m i )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Where TP represents the number of the true positive samples; TN stands for the number of the true negative samples; FP means the number of the false positive samples; FN means number of the false negative samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .Fig. 10 .Fig. 11 .Fig. 12 .Fig. 13 .</head><label>910111213</label><figDesc>Performance comparisons using three models using the validation dataset. (a) validation performance on DRIVE; (b) validation performance on STARE; (c) validation performance on CHASE. ROC curves of different models. (a) ROC curves on DRIVE; (b) ROC curves on STARE; (c) ROC curves on CHASE. Segmentation results using the different models on DRIVE. Segmentation results using the different models on STARE. Segmentation results using the different models on CHASE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Magnified view of green-boxed patches predicted by different models on DRIVE (two rows above), STARE (two rows middle) and CHASE (two rows below).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 16 .</head><label>16</label><figDesc>Detailed view of four images on WIDE. Red boxes show segmentation cases that DUNet perform better than the other two methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>submitted Oct. 18, 2018. (Corresponding author: Ran Su). Qiangguo Jin is with School of Computer Software, College of Intelligence and Computing, Tianjin University, Tianjin, China (e-mail: qgk-ing@tju.edu.cn). Zhaopeng Meng is with School of Computer Software, College of Intelligence and Computing, Tianjin University, Tianjin, China (e-mail: mengzp@tju.edu.cn). Tuan D. Pham is with Department of Biomedical Engineering, Linkoping University, Sweden (e-mail: tuan.pham@liu.se). Qi Chen is with School of Computer Software, College of Intelligence and Computing, Tianjin University, Tianjin, China (e-mail: joannax-iaoqi@tju.edu.cn). Leyi Wei is with School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China (e-mail: weileyi@tju.edu.cn)</figDesc><table /><note>Ran Su is with School of Computer Software, College of Intelligence and Computing, Tianjin University, Tianjin, China (e-mail: ran.su@tju.edu.cn).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>et al. preprocessed the retinal vessel images and then combined ...... ...... ...... vessel non-vessel ...... ...... ...... ...... ......</figDesc><table><row><cell></cell><cell></cell><cell>DUNet</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>U-Net</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Deformable-ConvNet</cell><cell></cell><cell></cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>images were used for training and validating purpose and the rest for testing. Since no splitting of training or test is provided for STARE/CHASE, we manually separated the first 10/14 images for training and validating and the remaining 10/14 for</figDesc><table /><note>testing. From each training/validating image on DRIVE, 10000 patches were randomly sampled including 8,000 for training and 2000 for validating. From each training/validating im- age on STARE/CHASE, 20000/15000 patches were randomly sampled including 16000/12000 for training and 4000/3000 for validating. Therefore, DRIVE and STARE both had 160000 patches as training set and 40000 patches as validation set. Meanwhile, CHASE had 168000 patches as training set and 42000 patches as validation set. The test set consists of the whole rest images. Since the capacity of patch dataset is large enough, data augmentation is not token into consideration.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>OF THE THREE SCRATCHED-TRAINED MODELS ON DRIVE, STARE AND CHASE DATASETS</figDesc><table><row><cell>Models</cell><cell cols="4">DRIVE ACC LOSS ACC LOSS ACC LOSS STARE CHASE</cell></row><row><cell cols="5">Deformable-ConvNet 0.9622 0.1101 0.9501 0.1593 0.9651 0.0962</cell></row><row><cell>U-Net</cell><cell cols="4">0.9648 0.1413 0.9573 0.2659 0.9664 0.1366</cell></row><row><cell>DUNet</cell><cell cols="4">0.9650 0.0919 0.9543 0.1477 0.9704 0.0833</cell></row><row><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">PERFORMANCE OF THE THREE MODELS TESTED ON DRIVE</cell><cell></cell></row><row><cell>Models</cell><cell>DRIVE PPV TPR TNR ACC</cell><cell>F 1</cell><cell>JS</cell><cell>AUC</cell></row><row><cell cols="5">Deformable-ConvNet 0.8180 0.7618 0.9837 0.9642 0.7889 0.9642 0.9745</cell></row><row><cell>U-Net</cell><cell cols="4">0.8795 0.7373 0.9903 0.9681 0.8021 0.9681 0.9830</cell></row><row><cell>DUNet</cell><cell cols="4">0.8537 0.7894 0.9870 0.9697 0.8203 0.9697 0.9856</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>OF THE THREE MODELS TESTED ON STARE</figDesc><table><row><cell>Models</cell><cell>STARE PPV TPR TNR ACC</cell><cell>F 1</cell><cell>JS</cell><cell>AUC</cell></row><row><cell cols="5">Deformable-ConvNet 0.8447 0.7036 0.9892 0.9673 0.7677 0.9674 0.9742</cell></row><row><cell>U-Net</cell><cell cols="4">0.9225 0.6712 0.9953 0.9705 0.7770 0.9705 0.9813</cell></row><row><cell>DUNet</cell><cell cols="4">0.8856 0.7428 0.9920 0.9729 0.8079 0.9729 0.9868</cell></row><row><cell></cell><cell>TABLE IV</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">PERFORMANCE OF THE THREE MODELS TESTED ON CHASE</cell><cell></cell></row><row><cell>Models</cell><cell>CHASE PPV TPR TNR ACC</cell><cell>F 1</cell><cell>JS</cell><cell>AUC</cell></row><row><cell cols="5">Deformable-ConvNet 0.7024 0.7727 0.9786 0.9659 0.7359 0.9659 0.9772</cell></row><row><cell>U-Net</cell><cell cols="4">0.8211 0.7124 0.9898 0.9728 0.7629 0.9728 0.9830</cell></row><row><cell>DUNet</cell><cell cols="4">0.7510 0.8229 0.9821 0.9724 0.7853 0.9724 0.9863</cell></row><row><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">COMPARISONS AGAINST EXISTING APPROACHES ON DRIVE DATASET</cell></row><row><cell>Method</cell><cell cols="4">Type Year PPV TPR TNR ACC AUC</cell></row><row><cell>Azzopardi et al. [46]</cell><cell cols="4">STA 2015 -0.7655 0.9704 0.9442 0.9614</cell></row><row><cell>Li et al. [47]</cell><cell cols="4">DNN 2015 -0.7569 0.9816 0.9527 0.9738</cell></row><row><cell>Liskowski et al. [32]</cell><cell cols="4">DNN 2016 -0.7811 0.9807 0.9535 0.9790</cell></row><row><cell>Fu et al. [33]</cell><cell cols="4">DNN 2016 -0.7603 -0.9523 -</cell></row><row><cell>Dasgupta et al. [48]</cell><cell cols="4">DNN 2017 0.8498 0.7691 0.9801 0.9533 0.9744</cell></row><row><cell cols="5">Roychowdhury et al. [49] STA 2017 -0.7250 0.9830 0.9520 0.9620</cell></row><row><cell>Chen et al. [50]</cell><cell cols="4">DNN 2017 -0.7426 0.9735 0.9453 0.9516</cell></row><row><cell>Alom et al. [51]</cell><cell cols="4">DNN 2018 -0.7792 0.9813 0.9556 0.9784</cell></row><row><cell>DUNet</cell><cell cols="4">DNN 2018 0.8537 0.7894 0.9870 0.9697 0.9856</cell></row><row><cell></cell><cell>TABLE VI</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">COMPARISONS AGAINST EXISTING APPROACHES ON STARE DATASET</cell></row><row><cell>Method</cell><cell cols="4">Type Year PPV TPR TNR ACC AUC</cell></row><row><cell>Azzopardi et al. [46]</cell><cell cols="4">STA 2015 -0.7716 0.9701 0.9497 0.9563</cell></row><row><cell>Li et al. [47]</cell><cell cols="4">DNN 2015 -0.7726 0.9844 0.9628 0.9879</cell></row><row><cell>Liskowski et al. [32]</cell><cell cols="4">DNN 2016 -0.8554 0.9862 0.9729 0.9928</cell></row><row><cell>Fu et al. [33]</cell><cell cols="4">DNN 2016 -0.7412 -0.9585 -</cell></row><row><cell cols="5">Roychowdhury et al. [49] STA 2017 -0.7720 0.9730 0.9510 0.9690</cell></row><row><cell>Chen et al. [50]</cell><cell cols="4">DNN 2017 -0.7295 0.9696 0.9449 0.9557</cell></row><row><cell>Alom et al. [51]</cell><cell cols="4">DNN 2018 -0.8298 0.9862 0.9712 0.9914</cell></row><row><cell>DUNet</cell><cell cols="4">DNN 2018 0.8856 0.7428 0.9920 0.9729 0.9868</cell></row><row><cell></cell><cell>TABLE VII</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">COMPARISONS AGAINST EXISTING APPROACHES ON CHASE DATASET</cell></row><row><cell>Method</cell><cell cols="4">Type Year PPV TPR TNR ACC AUC</cell></row><row><cell>Azzopardi et al. [46]</cell><cell cols="4">STA 2015 -0.7585 0.9587 0.9387 0.9487</cell></row><row><cell>Li et al. [47]</cell><cell cols="4">DNN 2015 -0.7507 0.9793 0.9581 0.9716</cell></row><row><cell>Fu et al. [33]</cell><cell cols="4">DNN 2016 -0.7130 -0.9489 -</cell></row><row><cell cols="5">Roychowdhury et al. [49] STA 2017 -0.7201 0.9824 0.9530 0.9532</cell></row><row><cell>Alom et al. [51]</cell><cell cols="4">DNN 2018 -0.7759 0.9820 0.9634 0.9715</cell></row><row><cell>DUNet</cell><cell cols="4">DNN 2018 0.7510 0.8229 0.9821 0.9724 0.9863</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table V ,</head><label>V</label><figDesc>VI, VII summarize the type of algorithm, year of publication, and performance on DRIVE, STARE and CHASE dataset.From the results, it shows that DUNet architecture performs the best among those methods on DRIVE and CHASE. It achieves the highest global accuracy of 0.9697/0.9724 and the highest AUC of 0.9856/0.9863 with a small quantity of</figDesc><table><row><cell></cell><cell>Ground Truth</cell><cell></cell><cell></cell></row><row><cell>DRIVE</cell><cell>STARE</cell><cell>Kaggle</cell><cell>HRF</cell></row><row><cell cols="4">Fig. 15. Four distinct style of retinal images synthesized by generative</cell></row><row><cell>adversarial nets.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">training samples, which shows that the DUNet exhibits state-</cell></row><row><cell cols="4">of-the-art performance comparing both standard segmentation</cell></row><row><cell cols="4">methods and deep neural network based methods. Although</cell></row><row><cell cols="4">DUNet performs not better than Liskowski et al.'s method [32]</cell></row><row><cell cols="4">and Alom et al.'s method [51] on STARE, DUNet uses less</cell></row><row><cell cols="4">training patch samples than their methods while reaches a</cell></row><row><cell>desiring results.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII PERFORMANCES</head><label>VIII</label><figDesc>OF THE THREE MODELS TESTED ON SYNTHE USING WELL-TRAINED WEIGHTS ON DRIVE Models SYNTHE PPV TPR TNR ACC F 1 JS AUC Dasgupta et al. [48] 0.8485 0.7660 0.9868 0.9675 0.8052 0.9675 0.9822 Alom et al. [51] 0.8509 0.7728 0.9870 0.9682 0.8100 0.9682 0.9831 DUNet 0.8537 0.7894 0.9870 0.9697 0.8203 0.9697 0.9855</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study of red blood cell deformability in diabetic retinopathy using optical tweezers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pavesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Trapping and Optical Micromanipulation XII</title>
		<imprint>
			<biblScope unit="volume">9548</biblScope>
			<biblScope unit="page">954825</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification of retinal vessels into arteries and veins for detection of hypertensive retinopathy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Irshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Akram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biomedical Engineering Conference (CIBEC)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Retinal vascular tortuosity, blood pressure, and cardiovascular risk factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">P</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="812" to="818" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blood Vessel Segmentation in Pathological Retinal Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="960" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation of vessel-like patterns using mathematical morphology and curvature evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1010" to="1019" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An approach to localize the retinal blood vessels using bit planes and centerline detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Fraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uyyanonvara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Rudnicka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods &amp; Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="600" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation of blood vessels from red-free and fluorescein retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adthom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="61" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparative study of retinal vessel segmentation methods on a new publicly available database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2004: Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5370</biblScope>
			<biblScope unit="page" from="648" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using multi-scale textons derived from keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Retinal Blood Vessel Segmentation Approach Based on Mathematical Morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>El-Bendary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fahmy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Snasel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="612" to="622" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised Retinal Vessel Segmentation Using Combined Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Cavalcanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sijbers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">149943</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Retinal Vessel Segmentation Based on Adaptive Random Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jouandeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Greussay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical and Bioengineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="202" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised curvaturebased retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging: From Nano To Macro</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="344" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised Segmentation of Retinal Blood Vessels Using the Human Visual System Line Detection Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zardadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mehrshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems &amp; Telecommunication</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="125" to="133" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A gabor filter-based method for recognizing handwritten numerals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tomita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="400" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Application of the 2D Gaussian Filter for Enhancing Feature Extraction in Off-line Signature Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blumenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="339" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Retinal blood vessel segmentation using line operators and support vector classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perfetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1357" to="65" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segmentation of retinal blood vessels using the radial projection and semi-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2314" to="2324" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated localisation of the optic disc, fovea, and retinal blood vessels from digital colour fundus images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sinthanayothin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Boyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">902</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">AdaBoost with SVM-based component classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="785" to="795" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new supervised retinal vessel segmentation method based on robust hybrid features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aslani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sarnel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing &amp; Control</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new supervised method for blood vessel segmentation in retinal images by using gray-level and moment invariants-based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gegundezarias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="146" to="158" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical Representation Using NMF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="466" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical retinal blood vessel segmentation based on feature and ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="708" to="717" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation via deep learning network and fully-connected conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="698" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ensemble of Deep Convolutional Neural Networks for Learning to Detect Retinal Vessels in Fundus Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04833</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A supervised method for retinal image vessel segmentation by embedded learning and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent and Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2305" to="2315" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervised Retinal Vessel Segmentation Based on Neural Network Using Broader Aging Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z C</forename><surname>Azemin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I B M</forename><surname>Tamrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H M</forename><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IWBBIO</title>
		<imprint>
			<biblScope unit="page" from="1235" to="1242" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Segmenting Retinal Blood Vessels With Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liskowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krawiec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2369" to="2380" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepVessel: Retinal Vessel Segmentation via Deep Learning and Conditional Random Field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Measuring retinal vessel tortuosity in 10-year-old children: validation of the Computer-Assisted Image Analysis of the Retina (CAIAR) program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Rudnicka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Monekosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Whincup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investigative Ophthalmology &amp; Visual Science</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using the 2-D Gabor wavelet and supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leandro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1214" to="1222" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive histogram equalization and its variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Amburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cromartie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geselowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision Graphics &amp; Image Processing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="368" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The design and use of steerable filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="891" to="906" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Muliscale Vessel Enhancement Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Vincken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1496</biblScope>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The truth of the F-measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Teach Tutor Mater</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Distribution of the Flora in the Alpine Zone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Phytologist</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keras</forename><surname>Others</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Github</surname></persName>
		</author>
		<ptr target="https://github.com/keras-team/keras" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Trainable COS-FIRE filters for vessel delineation with application to retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strisciuglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Cross-Modality Learning Approach for Vessel Segmentation in Retinal Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A fully convolutional neural network based structured prediction approach towards the retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="248" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Blood Vessel Segmentation of Fundus Images by Major Vessel Extraction and Subimage Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Koozekanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Parhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical &amp; Health Informatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1118" to="1128" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A Labeling-Free Approach to Supervising Deep Neural Networks for Retinal Blood Vessel Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07502</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06955</idno>
		<title level="m">Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Tree Topology Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Schmidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1688" to="1701" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Synthesizing retinal and neuronal images with generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maurerstroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="14" to="26" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automatic no-reference quality assessment for retinal fundus images using vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Budai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Odstrčilik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Michelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Computer-Based Medical Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
