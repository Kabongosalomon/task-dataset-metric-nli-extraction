<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relation-Shape Convolutional Neural Network for Point Cloud Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
							<email>yongcheng.liu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
							<email>bfan@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
							<email>smxiang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
							<email>chpan@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Relation-Shape Convolutional Neural Network for Point Cloud Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud analysis is very challenging, as the shape implied in irregular points is difficult to capture. In this paper, we propose RS-CNN, namely, Relation-Shape Convolutional Neural Network, which extends regular grid CNN to irregular configuration for point cloud analysis.</p><p>The key to RS-CNN is learning from relation, i.e., the geometric topology constraint among points. Specifically, the convolutional weight for local point set is forced to learn a high-level relation expression from predefined geometric priors, between a sampled point from this point set and the others. In this way, an inductive local representation with explicit reasoning about the spatial layout of points can be obtained, which leads to much shape awareness and robustness. With this convolution as a basic operator, RS-CNN, a hierarchical architecture can be developed to achieve contextual shape-aware learning for point cloud analysis. Extensive experiments on challenging benchmarks across three tasks verify RS-CNN achieves the state of the arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, the analysis of 3D point cloud has drawn a lot of attention, as it has many applications such as autonomous driving and robot manipulation. However, this task is very challenging, since it is difficult to infer the underlying shape formed by these irregular points (see <ref type="figure" target="#fig_0">Fig. 1</ref> for detail).</p><p>For this issue, much effort is focused on replicating the remarkable success of convolutional neural network (CNN) on regular grid data (e.g., image) analysis <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>, to irregular point cloud processing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>. Some works transform point cloud to regular voxels <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3]</ref> or multi-view images <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref> for easy application of classic grid CNN. These transformations, however, usually lead to much loss of inherent geometric information in 3D point cloud, as well as high complexity.</p><p>To directly process point cloud, PointNet <ref type="bibr" target="#b25">[26]</ref> independently learns on each point and gathers the final features * Corresponding author: Bin Fan for a global representation. Though impressive, this design ignores local structures that have been proven to be important for abstracting high-level visual concepts in image CNN <ref type="bibr" target="#b51">[52]</ref>. To solve this problem, some works partition point cloud into several subsets by sampling <ref type="bibr" target="#b27">[28]</ref> or superpoint <ref type="bibr" target="#b19">[20]</ref>. Then a hierarchy is built to learn contextual representation from local to global. Nevertheless, this extremely relies on effective inductive learning of local subsets, which is quite intractable to achieve.</p><p>Generally, there are mainly three challenges for learning from point set P ⊂ R 3 : (1) P is unordered, thus requiring the learned representation being permutation invariant;</p><p>(2) P distributes in 3D geometric space, thus demanding the learned representation being robust to rigid transformation (e.g., rotation and translation); (3) P forms an underlying shape, therefore, the learned representation should be of discriminative shape awareness. The issue <ref type="bibr" target="#b0">(1)</ref> has been well resolved by symmetric function <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b50">51]</ref>, while (2) and (3) still demand for a full exploration. The goal of this work is to extend regular grid CNN to irregular configuration for handling these issues together.</p><p>To this end, we propose a relation-shape convolutional neural network (aliased as RS-CNN). The key to RS-CNN is learning from relation, i.e., the geometric topology constraint among points, which in our view can encode meaningful shape information in 3D point cloud.</p><p>Specifically, each local convolutional neighborhood is constructed by taking a sampled point x as the centroid and the surrounding points as its neighbors N (x). Then, the convolutional weight is forced to learn a high-level relation expression from predefined geometric priors, i.e., intuitive low-level relation between x and N (x). By convoluting in this way, an inductive representation with explicit reasoning about the spatial layout of points can be obtained. It discriminatively reflects the underlying shape that irregular points form thus is shape-aware. Furthermore, it can benefit from geometric priors, including the invariance to points permutation and the robustness to rigid transformation (e.g., translation and rotation). With this convolution as a basic operator, a hierarchical CNN-like architecture, i.e., RS-CNN, can be developed to achieve contextual shape-aware learning for point cloud analysis.</p><p>The key contributions are highlighted as follows:</p><p>• A novel learn-from-relation convolution operator called relation-shape convolution is proposed. It can explicitly encode geometric relation of points, thus resulting in much shape awareness and robustness;</p><p>• A deep hierarchy equipped with the relation-shape convolution, i.e., RS-CNN, is proposed. It can extend regular grid CNN to irregular configuration for achieving contextual shape-aware learning of point cloud;</p><p>• Extensive experiments on challenging benchmarks across three tasks, as well as thorough empirical and theoretical analysis, demonstrate RS-CNN achieves the state of the arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>View-based and volumetric methods. View-based methods represent a 3D shape as a group of 2D views from different angles. Recently, many works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27]</ref> have been proposed to recognize these view images with deep neural networks. They often finetune a pre-trained image-based architecture for accurate recognition. However, 2D projections could cause loss of shape information due to self-occlusions, and it often demands a huge number of views for decent performance. Volumetric methods convert the input 3D shape into a regular 3D grid, over which classic CNN can be employed <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3]</ref>. The main limitation is the quantization loss of the shape due to the low resolution enforced by 3D grid. Recent space partition methods like K-d trees <ref type="bibr" target="#b17">[18]</ref> or octrees <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30]</ref> rescue some resolution issues but still rely on the subdivision of a bounding volume rather than a local geometric shape. In contrast to these methods, our work aims to process 3D point cloud directly. Deep learning on point cloud. PointNet <ref type="bibr" target="#b25">[26]</ref> pioneers this route by independently learning on each point and gathering the final features with max pooling. Yet this design neglects local structures, which have been proven important for the success of CNN. To remedy this, PointNet++ <ref type="bibr" target="#b27">[28]</ref> suggests a hierarchical application of PointNet to multiple subsets of point cloud. Local structure exploitation with PointNet is also investigated in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>. In addition, Superpoint <ref type="bibr" target="#b19">[20]</ref> is proposed to partition point cloud into geometric elements. Graph convolution network is applied on a local graph created by neighboring points <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22]</ref>. However, these methods do not explicitly model the local spatial layout of points, thus acquiring less shape awareness. By contrast, our work captures the spatial layout of points by learning a high-level relation expression among points.</p><p>Some works map point cloud to a high-dimensional space to facilitate the application of classic CNN. SPLAT-Net <ref type="bibr" target="#b35">[36]</ref> maps the input points onto a sparse lattice, then processing with bilateral convolution <ref type="bibr" target="#b14">[15]</ref>. PCNN <ref type="bibr" target="#b0">[1]</ref> extends the function over point cloud to a continuous volumetric function over ambient space. These methods could cause loss of geometric information, while our method directly operates on point cloud without introducing such loss.</p><p>Another key issue is the irregularity of points. Some works focus on analyzing symmetric functions that are equivariant to point sets learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b20">21]</ref>. Some other works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b22">23]</ref> develop alignment network for the robustness to rigid transformation in 3D space. However, the alignment learning is a suboptimal solution for this issue. Some traditional descriptors like Fast Point Feature Histograms can be invariant to translation and rotation, yet they are often less effective for high-level shape understanding. Our method that learns on geometric relation among points is naturally robust to rigid transformation, whilst being highly effective due to the powerfulness of deep nets. Relation learning. To learn a data-dependent weight from relation has been explored in the field of image and video analysis. Spatial transformer <ref type="bibr" target="#b13">[14]</ref> learns a transition matrix to align 2D images. Non-local network <ref type="bibr" target="#b42">[43]</ref> learns longterm relation across video frames. Relation networks <ref type="bibr" target="#b9">[10]</ref> learn position relation across objects. DFN <ref type="bibr" target="#b15">[16]</ref> proposes general dynamic filters that inspire many subsequent works.</p><p>There are also some works focusing on the relation learning in 3D point cloud. DGCNN <ref type="bibr" target="#b43">[44]</ref> captures similar local shapes by learning point relation in a high-dimensional feature space, yet this relation could be unreliable in some cases. Wang et al. <ref type="bibr" target="#b41">[42]</ref> propose a parametric continuous convolution that is based on computable relation among points, but they do not explicitly learn from local to global like classic CNN. By contrast, our method learns a highlevel relation expression from geometric priors in 3D space, and performs contextual local-to-global shape learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Shape-Aware Representation Learning</head><p>The core of point cloud analysis is to discriminatively represent the underlying shape with robustness. Here we learn contextual shape-aware representation for this goal, by extending regular grid CNN to irregular configuration with a novel relation-shape convolution (RS-Conv). The key is to learn from relation. Specifically, the convolutional weight for xj is converted to wij, which learns a mapping M (Eq. (2)) on predefined geometric relation vector hij. In this way, the inductive convolutional representation σ A({wij · f x j , ∀xj}) (Eq. (3)) can expressively reason the spatial layout of points, resulting in discriminative shape awareness. As in image CNN <ref type="bibr" target="#b33">[34]</ref>, further channel-raising mapping is conducted for a more powerful shape-aware representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Relation-Shape Convolution</head><p>Local-to-global learning, which has gained remarkable success in image CNN <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>, is a promising solution for contextual shape representation. However, it extremely relies on shape-aware inductive learning from irregular point subsets, which remains a quite intractable problem.</p><p>Modeling. To overcome this issue, we model local point subset P sub ⊂ R 3 to be a spherical neighborhood, with a sampled point x i as the centroid and surrounding points as its neighbors x j ∈ N (x i ). The left-most part of <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates this modeling. Then, our goal is to learn an inductive representation f Psub of this neighborhood, which should discriminatively encode the underlying shape information. To this end, we formulate a general convolutional operation as</p><formula xml:id="formula_0">f Psub = σ A({T (f xj ), ∀x j }) 1 , d ij &lt; r ∀x j ∈ N (x i ),</formula><p>(1) where x is a 3D point and f is a feature vector. d ij is the Euclidean distance between x i and x j , and r is the sphere radius. Here f Psub is obtained by first transforming the features of all the points in N (x i ) with function T , and then aggregating them with function A followed by a nonlinear activator σ. In this formulation, the two functions A and T are the key to f Psub . That is, the permutation invariance of point set can be achieved only when A is symmetric (e.g., summation) and T is shared over each point in N (x i ).</p><p>Limitations of classic CNN. In classic CNN, T is implemented as T (f xj ) = w j · f xj , where w j is learnable weight and "·" denotes element-wise multiplication. There are mainly two limitations of this convolution when applied on point cloud: 1) w j is not shared over each point in N (x i ), resulting in variance to point permutation and incapability to process irregular P sub (e.g., different number); 2) the gradient of w j in backpropagation is only relevant to the isolated point x j , leading to an implicit learning strategy, which could not bring much shape awareness and ro- <ref type="bibr" target="#b0">1</ref> In this paper, the bias term is omitted for clarity. bustness to f Psub . This issue can be partly alleviated by some techniques like performing various data augmentations or using lots of convolutional filters, yet they are suboptimal.</p><p>Conversion: Learn from relation. We argue that the above limitations can be mitigated by learning from relation. In the neighborhood of 3D space, the geometric relation between x i and all its neighbors N (x i ) is an explicit expression about the spatial layout of points, which further discriminatively reflects the underlying shape. To capture this relation, we replace w j in classical CNN with w ij , which learns a mapping M of a relation vector h ij , i.e., the predefined geometric priors between x i and x j . We call h ij as low-level relation. This process can be described as</p><formula xml:id="formula_1">T (f xj ) = w ij · f xj = M(h ij ) · f xj .<label>(2)</label></formula><p>The goal of mapping M is to abstract high-level relation expression between two points, which can encode their spatial layout. Here we implement M with a shared multi-layer perceptron (MLP) due to its powerful mapping ability. This process is illustrated in the middle part of <ref type="figure" target="#fig_1">Fig. 2</ref>. In this way, w j is neatly converted to w ij , whose gradient (determined by h ij ) is relevant to both x i and x j . Meanwhile, M is exactly shared over all the points in N (x i ), making it independent to the irregularity of points. It can also be robust to rigid transformation that will be clarified in Sec 3.2.</p><p>As a consequence, f Psub in Eq.</p><p>(1) becomes</p><formula xml:id="formula_2">f Psub = σ A({M(h ij ) · f xj , ∀x j }) .<label>(3)</label></formula><p>This convolutional representation, with all the relation between x i and N (x i ) aggregated, can achieve explicit reasoning about the spatial layout of points, thus resulting in discriminative shape awareness. For geometric priors, one can use 3D Euclidean distance as an intuitive description of low-level relation h ij . Moreover, h ij can also be defined flexibly since M can map it to a high-dimensional relation vector for channel alignment with f xj for easy multiplication. We will discuss h ij in detail in the experiment section.</p><p>Channel-raising mapping. In Eq. <ref type="formula" target="#formula_2">(3)</ref>, the channel number of f Psub is the same as the input feature f xj . This is inconsistent with classic image CNN that increases channel number while decreasing image resolution for a more abstract representation. For example, the channel number of 64-128-256-512 is set in VGG network <ref type="bibr" target="#b33">[34]</ref>. Accordingly, we add a shared MLP on f Psub for further channel-raising mapping. It is illustrated in the middle part of <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Properties</head><p>RS-Conv in Eq. <ref type="formula" target="#formula_2">(3)</ref> can maintain four decent properties:</p><p>Permutation invariance. In the inner mapping function M(h), both the low-level relation h and the shared MLP M are invariant to the input order of points. Therefore, with the outer aggregation function A being symmetric, the permutation invariance can be satisfied.</p><p>Robustness to rigid transformation. This property is well held in the high-level relation encoding M(h). It can be robust to rigid transformation, e.g., translation and rotation, when a suitable h (e.g., 3D Euclidean distance) is defined.</p><p>Points interaction. Points are not isolated and nearby points form a meaningful shape in geometric space. Thus their inherent interaction is critical for discriminative shape awareness. Our solution of relation learning explicitly encode the geometric relation among points, naturally capturing the interaction of points.</p><p>Weight sharing. This is the key property that allows applying the same learning function over different irregular point subsets for robustness, as well as low complexity. In Eq. <ref type="formula" target="#formula_2">(3)</ref>, the symmetric A, the shared MLP M and the predefined geometric priors h are all independent to the irregularity of points. Hence, this property is also satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Revisiting 2D Grid Convolution</head><p>The proposed RS-Conv is a generic formulation of 2D grid convolution for relation reasoning. We clarify this with a neighborhood (convolution kernel) of 3 × 3 on a 2D-grid feature map, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Specifically, the summation function is a specific instance of the aggregation function A. Moreover, note that w j always implies a fixed positional relation between x i and its neighbor x j in the regular grid. For example, w 1 always implies the top-left relation with x i , and w 2 implies the right-above relation with x i . In other words, w j is actually constrained to encode one kind of regular grid relation in the learning process. Therefore, our RS-Conv with relation learning is more general and can be applied to model 2D grid spatial relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">RS-CNN for Point Cloud Analysis</head><p>Using RS-Conv <ref type="figure" target="#fig_1">(Fig. 2)</ref> as a basic operator and adopting a uniform sampling strategy, a hierarchical shape-aware  learning architecture like classic CNN, namely, RS-CNN, can be developed for point cloud analysis as</p><formula xml:id="formula_3">F P N = RS-CONV(F −1 P N −1 ),<label>(4)</label></formula><p>where F P N , features in layer of the sampled point set P N with number N , are obtained by applying RS-Conv on the features in the previous layer − 1.</p><p>Our RS-CNN applied in the classification and segmentation of point cloud is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. In both tasks, RS-CNN is used for learning a group of hierarchical shapeaware representation. The final global representation followed by three fully connected (FC) layers is configured for classification. For segmentation, the learned multi-level representation is successively upsampled by feature propagation <ref type="bibr" target="#b27">[28]</ref> to generate per-point predictions. Both of them can be trained in an end-to-end manner. <ref type="figure" target="#fig_2">Eq. (3)</ref>. Symmetric function max pooling is applied as aggregation function A. ReLU <ref type="bibr" target="#b24">[25]</ref> is used as nonlinear activator σ. For mapping function M, a threelayer shared MLP is deployed since theoretically it can fit arbitrary continuous mappings <ref type="bibr" target="#b8">[9]</ref>. Low-level relation h ij is defined as a compact vector with 10 channels, i.e., (3D Euclidean distance, x i − x j , x i , x j ). The channel-raising mapping is achieved by a single-layer shared MLP. Batch normalization <ref type="bibr" target="#b12">[13]</ref> is applied in each MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RS-Conv in</head><p>RS-CNN for points analysis. The farthest points are picked from point cloud for sampling local subsets to perform RS-Conv. In each neighborhood, a fixed number of neighbors are randomly sampled for batch processing, and they are normalized to take the centroid as the origin. To capture more sufficient geometric relation, we force RS-CNN to learn over three-scale neighborhoods centered on a sampled point with a shared weight. This is different from multi-scale grouping (MSG) <ref type="bibr" target="#b27">[28]</ref> that learns multiscale features using multiple groups of weight. RS-CNN with 3 layers and 4 layers is deployed for classification and segmentation, respectively. Note that only 3D coordinates xyz are used as the input features to RS-CNN.</p><p>Our RS-CNN is implemented using Pytorch 2 . The Adam optimization algorithm is employed for training, with a mini-batch size of 32. The momentum for BN starts with 0.9 and decays with a rate of 0.5 every 20 epochs. The learning rate begins with 0.001 and decays with a rate of 0.7 every 20 epochs. The weight of RS-CNN is initialized using the techniques introduced by He et al. <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we arrange comprehensive experiments to validate the proposed RS-CNN. First, we evaluate RS-CNN for point cloud analysis on three tasks (Sec 4.1). We then provide detailed experiments to carefully study RS-CNN (Sec 4.2). Finally, we visualize the shape features that RS-CNN captures and analyze the complexity (Sec 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Point Cloud Analysis</head><p>Shape classification. We evaluate RS-CNN on Model-Net40 classification benchmark <ref type="bibr" target="#b44">[45]</ref>. It is composed of 9843 train models and 2468 test models in 40 classes. The point cloud data is sampled from these models by <ref type="bibr" target="#b25">[26]</ref>. We uniformly sample 1024 points and normalize them to a unit sphere. During training, we augment the input data with random anisotropic scaling in the range [-0.66, 1.5] and translation in the range [-0.2, 0.2], as in <ref type="bibr" target="#b17">[18]</ref>. Meanwhile, dropout technique <ref type="bibr" target="#b34">[35]</ref> with 50% ratio is applied in FC layers. During testing, similar to <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>, we perform ten voting tests with random scaling and average the predictions.</p><p>The quantitative comparisons with the state-of-the-art point-based methods are summarized in <ref type="table" target="#tab_0">Table 1</ref>, where RS-CNN outperforms all the xyz-input methods. Specifically, RS-CNN reduces the error rate of PointNet++ <ref type="bibr" target="#b27">[28]</ref> by 31.2%, and surpasses its advanced version that uses additional normal data as well as very dense points (5k). Moreover, even using only xyz as the input, RS-CNN can also achieve a superior result (93.6%) compared with the best additional-input method SO-Net <ref type="bibr" target="#b20">[21]</ref> (93.4%). This convincingly verifies the effectiveness of our RS-CNN.</p><p>We test the robustness of RS-CNN on sampling density, by using sparser points of number 1024, 512, 256, 128 and 64 as the input to a model trained with 1024 points. As in <ref type="bibr" target="#b27">[28]</ref>, random input dropout technique is applied for a fair comparison. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the test results, where   <ref type="bibr" target="#b20">[21]</ref> xyz 2k 90.9 Kd-Net(depth=15) <ref type="bibr" target="#b17">[18]</ref> xyz 32k 91.8 O-CNN <ref type="bibr" target="#b40">[41]</ref> xyz, nor -90.6 Spec-GCN <ref type="bibr" target="#b39">[40]</ref> xyz, nor 1k 91.8 PointNet++ <ref type="bibr" target="#b27">[28]</ref> xyz, nor 5k 91.9 SpiderCNN <ref type="bibr" target="#b47">[48]</ref> xyz, nor 5k 92.4 SO-Net <ref type="bibr" target="#b20">[21]</ref> xyz, nor 5k 93.4</p><p>the compared methods are PointNet <ref type="bibr" target="#b25">[26]</ref>, PointNet++ <ref type="bibr" target="#b27">[28]</ref>, PCNN <ref type="bibr" target="#b0">[1]</ref> and DGCNN <ref type="bibr" target="#b43">[44]</ref>. As can be seen, it is more difficult for shape recognition when points get sparser. Even so, RS-CNN is still considerably robust. It achieves nearly consistent robustness as PointNet++, whilst showing superior performance on each density.</p><p>Shape part segmentation. Part segmentation is a challenging task for fine-grained shape analysis. We evaluate RS-CNN for this task on ShapeNet part benchmark <ref type="bibr" target="#b48">[49]</ref> and follow the data split in <ref type="bibr" target="#b25">[26]</ref>. This dataset contains 16881 shapes with 16 categories, and is labeled in 50 parts in total. As in <ref type="bibr" target="#b25">[26]</ref>, we randomly pick 2048 points as the input and concatenate the one-hot encoding of the object label to the last feature layer. During testing, we also apply ten voting tests using random scaling. Except for standard IoU (Interover-Union) on each category, we also report two types of mean IoU (mIoU) that are averaged across all classes and all instances, respectively.  <ref type="figure">Figure 6</ref>. Segmentation examples on ShapeNet part benchmark. <ref type="table" target="#tab_1">Table 2</ref> summarizes the quantitative comparisons with the state-of-the-art methods, where RS-CNN achieves the best performance with class mIoU of 84.0% and instance mIoU of 86.2%. This considerably surpasses the second best xyz-based methods, i.e., DGCNN <ref type="bibr" target="#b43">[44]</ref> with 82.3% (1.7↑) in class mIoU and PCNN <ref type="bibr" target="#b0">[1]</ref> with 85.1% (1.1↑) in instance mIoU, respectively. Noticeably, RS-CNN sets new state of the arts in the xyz-based methods over ten categories. These improvements demonstrate the robustness of RS-CNN to diverse shape structures. <ref type="figure">Fig. 6</ref> shows some segmentation examples. One can see that although the part shapes implied in irregular points are varied and they may be very confusing to recognize, RS-CNN can also segment them out with decent accuracy.</p><p>Normal estimation. Normal estimation in point cloud is a crucial step for numerous applications, such as surface reconstruction and rendering. This task is very challenging since it requires a higher level of reasoning, which goes beyond the underlying shape recognition. We take normal estimation as a supervised regression task, and achieve it using the segmentation network. The cosine-loss between the normalized output and ground truth normal is applied for regression training. ModelNet40 dataset is used for evaluation, with uniformly sampled 1024 points as the input.</p><p>The quantitative results are summarized in <ref type="table" target="#tab_2">Table 3</ref>. RS-CNN outperforms other advanced methods on this task with   <ref type="figure" target="#fig_5">Fig. 7</ref> shows some normal estimation examples, where our RS-CNN with geometric relation learning can obtain more decent predictions. However, RS-CNN could also be less effective for some intractable shapes, such as spiral stairs and intricate plants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">RS-CNN Design Analysis</head><p>In this section, we first perform a detailed ablation study on RS-CNN. Then, we discuss the choices of aggregation function A, mapping function M and low-level relation h in Eq. (3). Finally, we validate the robustness of RS-CNN on point permutation and rigid transformation. All experiments are conducted on ModelNet40 classification dataset.</p><p>Ablation study. The results are summarized in <ref type="table" target="#tab_3">Table 4</ref>. The baseline (model A) is set to learn without geometric relation encoding, but with a shared three-layer MLP as feature transformation function T in Eq. (1).</p><p>The baseline only gets an accuracy of 87.2%. Yet with geometric relation learning, it is significantly improved to 89.9% (model B). This convincingly verifies the effectiveness of our RS-CNN. Then, a great improvement of 2% is gained after using BN (model C), maybe because it can greatly ease the network training. Moreover, dropout technique improves the result by 0.3% (model D). As mentioned in Sec 3.5, RS-CNN should be able to benefit from sufficient geometric relation. This is verified by model E (92.5%) and model F (92.9%) that perform two-scale and three-scale relation learning, respectively. Eventually, with ten voting tests, an impressive accuracy of 93.6% (model G) can be obtained with only xyz features.</p><p>To investigate the impact of the number of input points on RS-CNN, we also train the network with 2048 points but find no improvement (model H). In addition, to compare with the baseline (model A) more fairly, we set a new baseline (model I) that works with all the techniques but relation learning. It gets an accuracy of 90.1%, which RS-CNN can also surpass by 3.5%. We speculate that RS-CNN with geometric relation reasoning can acquire more discriminative shape awareness, and this awareness can be greatly enhanced by multi-scale relation learning.</p><p>Aggregation function A. Three symmetric functions: max pooling (max), average pooling (avg.) and summation (sum), are employed to study the effect of A on RS-CNN. <ref type="table" target="#tab_4">Table 5</ref> summarizes the results. As can be seen, with M using three layers, max pooling achieves the best performance while average pooling and summation get the same </p><formula xml:id="formula_4">, x nor j ) 7 92.8 E (2D-Ed, x i − x j , x i , x j ) 10 ≈ 92.2</formula><p>accuracy. The reason may be that max pooling can select the biggest feature response, thus keeping the most expressive representation and removing redundant information.</p><p>Mapping function M. The results of M deployed with different layers are summarized in the first three rows of <ref type="table" target="#tab_4">Table 5</ref>. One can see that the best accuracy of 93.6% is obtained by a shared three-layer MLP, and it decreases by 0.9% when increasing the number of layers. The reason might be that M with four layers brings some difficulty for network training. Noticeably, RS-CNN can also get a decent accuracy of 92.4% with M using only two layers. This verifies the powerfulness of relation learning for underlying shape capturing from point cloud.</p><p>Low-level relation h. The key to RS-CNN is learning from relation, thus how to define h is an issue worth exploring. Actually, h can be defined flexibly, as long as it could discriminatively reflect the underlying shape. To validate this claim and facilitate the understanding, we experiment with five intuitive relation definitions as examples, whose results are summarized in <ref type="table">Table 6</ref>.</p><p>As can be seen, using only 3D Euclidean distance as h, the accuracy can also reach 92.5% (model A). This demonstrates the effectiveness of our RS-CNN for highlevel geometric relation learning. Moreover, the performance is gradually improved with additional relation, including coordinates difference (model B) and coordinates themselves (model C). We also utilize the normal vectors of two points and their cosine distance as h, the result (model D) is 92.8%. This indicates RS-CNN is also able to abstract shape information from the relation in normals. Intuitively, the relation among points in the 2D view of point cloud can also reflect the underlying shape. Therefore, to validate our RS-CNN for shape abstraction on 2D relation, we forcibly set the value of one dimension in 3D coordinates to be zero, i.e., projecting 3D points onto a 2D plane of XY, XZ and YZ. The results are all around 92.2% (model E), which is quite impressive. This further verifies the effectiveness of the proposed relation learning method.</p><p>Robustness to point permutation and rigid transformation. We compare the robustness of our RS-CNN with PointNet <ref type="bibr" target="#b25">[26]</ref> and PointNet++ <ref type="bibr" target="#b27">[28]</ref>. Note that all the models are trained without related data augmentations, e.g., translation or rotation, to avoid confusion in this test. In addition, although relation learning in RS-CNN is robust to rotation, the initial input features of 3D coordinates are affected. We address this issue by normalizing each sampled point subset to corresponding local coordinate system, which is determined by each sampled point and its normal. For a fair comparison, we also perform this normalization for Point-Net++, as it learns over local subsets as well. The 3D Euclidean distance is applied as geometric relation h in RS-CNN for this test. <ref type="table" target="#tab_5">Table 7</ref> summarizes the test results.</p><p>As can be seen, all the methods are invariant to permutation. However, PointNet is vulnerable to both translation and rotation while PointNet++ is sensitive to rotation. By contrast, our RS-CNN with geometric relation learning is invariant to these perturbations, making it powerful for robust shape recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visualization and Complexity Analysis</head><p>Visualization. <ref type="figure">Fig. 8</ref> visualizes the shape features learned by the first two layers of RS-CNN on ModelNet40 dataset. As it shows, the features learned by the first layer mostly respond to edges, corners and arcs, while the ones in the second layer capture more semantical shape parts like airfoils and heads. This verifies RS-CNN can learn progressive shape-aware representation for point cloud analysis.</p><p>Complexity Analysis. <ref type="table" target="#tab_6">Table 8</ref> summarizes the space <ref type="figure">Figure 8</ref>. Visualization of the shape features learned by the first two layers of RS-CNN on ModelNet40 dataset. The features learned by the first layer mostly respond to edges, corners and arcs, while the ones in the second layer capture more semantical shape parts like airfoils and heads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, RS-CNN, namely, Relation-Shape Convolutional Neural Network, which extends regular grid CNN to irregular configuration for point cloud analysis, has been proposed. The core to RS-CNN is a novel convolution operator, which learns from relation, i.e., the geometric topology constraint among points. In this way, explicit reasoning about the spatial layout of points can be made to obtain discriminative shape awareness. Moreover, the decent properties of geometric relation can also be acquired, such as robustness to rigid transformation. As a consequence, RS-CNN equipped with this operator can achieve contextual shape-aware learning, making it highly effective. Extensive experiments on challenging benchmarks across three tasks, as well as thorough empirical and theoretical analysis, have demonstrated RS-CNN achieves the state of the arts. <ref type="table">Table 9</ref>. The results (%) of two selection strategies on N (xi). Both of them are trained with a single-scale neighborhood. For a fair comparison, the number of neighbors is set to be equal in each layer between the two models. method acc. k-NN 90.5 Random-PIB 92.2 <ref type="table" target="#tab_0">Table 10</ref>. The results (%) of learning with relation in different proportions. "ratio" indicates the cut off relation accounts for the proportion of all the relation between the centroid and the neighbors. ratio 0 0.1 0.2 0.3 0.4 0.5 acc. 93.6 92.8 92.9 93.2 92.5 92.1 <ref type="table" target="#tab_0">Table 11</ref>. The results (%) of three selection approaches and one fusion strategy of the centroid. The approach of picking in N (xi) is performed randomly in each neighborhood. Note that the weight in M is shared over these approaches in the fusion process. centroid acc. sampled point x i 93.6 average of N (x i ) 93.6 random picking in N (x i ) 92.8 fusion of above 93.4 picking in N (x i ), are studied for this issue. Besides, a strategy that fuses all of them is also studied. The results are summarized in <ref type="table" target="#tab_0">Table 11</ref>, where the first two strategies obtain the same decent accuracy while random picking performs less well. The reason may be that random picking requires RS-CNN to reason the spatial layout of points from various topological connections, which is quite difficult. Another promising strategy is fusing a group of relations that are centered on different centroids. This can be achieved by performing element-wise summation of f Psub in Eq. <ref type="formula" target="#formula_2">(3)</ref>, with the relation centered on the above three kinds of centroids. However, it does not perform better, with an accuracy of 93.4% that is lower than the best single-centroid version of 93.6%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Low-Level Relation h</head><p>More details of the relation learning on 2D views of point cloud (the fourth part in Sec 4.2) are provided in this section. As illustrated in <ref type="figure" target="#fig_6">Fig. 9</ref> in this material, the relation among points in the 2D view can also reflect the underlying shape. Therefore, we are interested in how powerfully the proposed RS-CNN to acquire shape awareness from only 2D-view relation of points.</p><p>To validate this, the value of one dimension in 3D coordinates is forcibly set to be zero, that is, 3D points are projected onto the 2D plane of XY, XZ and YZ for three 2D views. In addition, a strategy with fusion of these views is also studied. Note that the projection operation is only conducted for the definition of h, the initial input features for x j in Eq. (3) is still intact 3D coordinates. <ref type="table" target="#tab_0">Table 12</ref> summarizes the results. As can be seen, all single-view relation can achieve an accuracy around 92.2%, which is quite impressive. After fusing them, the result is improved by 0.3%. This shows RS-CNN can also capture the underlying shape well even with relation learning from 2D view (potentially, a group of 2D views) of 3D point cloud, further verifying its effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Left part: Point cloud. Right part: Underlying shape formed by this point cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of relation-shape convolution (RS-Conv).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of 2D grid convolution with a kernel of 3 × 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The architectures of RS-CNN applied in the classification (a) and segmentation (b) of point cloud. N is the number of points and C is the channel number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2Figure 5 .</head><label>5</label><figDesc>https://github.com/Yochengliu/Relation-Shape-CNN Left part: Point cloud with random point dropout. Right part: Test results of using sparser points as the input to a model trained with 1024 points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Normal estimation on ModelNet40 dataset. For clearness, we only show predictions with angle less than 30 • in blue, and angle greater than 90 • in red between ground truth normals. a lower error of 0.15. This significantly reduces the error of PointNet++ (0.29) by 48.3%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>The projection of 3D point cloud onto the 2D plane of XY, XZ and YZ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell cols="4">Shape classification results (%) on ModelNet40 bench-</cell></row><row><cell cols="2">mark (nor: normal, "-": unknown).</cell><cell></cell><cell></cell></row><row><cell>method</cell><cell>input</cell><cell cols="2">#points acc.</cell></row><row><cell>Pointwise-CNN [11]</cell><cell>xyz</cell><cell>1k</cell><cell>86.1</cell></row><row><cell>Deep Sets [51]</cell><cell>xyz</cell><cell>1k</cell><cell>87.1</cell></row><row><cell>ECC [33]</cell><cell>xyz</cell><cell>1k</cell><cell>87.4</cell></row><row><cell>PointNet [26]</cell><cell>xyz</cell><cell>1k</cell><cell>89.2</cell></row><row><cell>SCN [47]</cell><cell>xyz</cell><cell>1k</cell><cell>90.0</cell></row><row><cell>Flex-Conv [4]</cell><cell>xyz</cell><cell>1k</cell><cell>90.2</cell></row><row><cell>Kd-Net(depth=10) [18]</cell><cell>xyz</cell><cell>1k</cell><cell>90.6</cell></row><row><cell>PointNet++ [28]</cell><cell>xyz</cell><cell>1k</cell><cell>90.7</cell></row><row><cell>KCNet [32]</cell><cell>xyz</cell><cell>1k</cell><cell>91.0</cell></row><row><cell>MRTNet [3]</cell><cell>xyz</cell><cell>1k</cell><cell>91.2</cell></row><row><cell>Spec-GCN [40]</cell><cell>xyz</cell><cell>1k</cell><cell>91.5</cell></row><row><cell>PointCNN [23]</cell><cell>xyz</cell><cell>1k</cell><cell>91.7</cell></row><row><cell>DGCNN [44]</cell><cell>xyz</cell><cell>1k</cell><cell>92.2</cell></row><row><cell>PCNN [1]</cell><cell>xyz</cell><cell>1k</cell><cell>92.3</cell></row><row><cell>Ours</cell><cell>xyz</cell><cell>1k</cell><cell>93.6</cell></row><row><cell>SO-Net</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Shape part segmentation results (%) on ShapeNet part benchmark (nor: normal, "-": unknown). 86.4 84.1 78.2 90.4 69.3 91.4 87.0 83.5 95.4 66.0 92.6 81.8 56.1 75.8 82.2 SCN [47] 1k 81.8 84.6 83.8 80.8 83.5 79.3 90.5 69.8 91.7 86.5 82.9 96.0 69.2 93.8 82.5 62.9 74.4 80.88.8 79.6 91.2 81.1 91.6 88.4 86.0 96.0 73.7 94.1 83.4 60.5 77.7 83.6</figDesc><table><row><cell>method</cell><cell>input</cell><cell>class</cell><cell>instance</cell><cell>air</cell><cell>bag cap car chair ear</cell><cell>guitar knife lamp laptop motor</cell><cell>mug pistol rocket skate</cell><cell>table</cell></row><row><cell></cell><cell></cell><cell>mIoU</cell><cell>mIoU</cell><cell>plane</cell><cell>phone</cell><cell>bike</cell><cell>board</cell><cell></cell></row><row><cell>Kd-Net [18]</cell><cell>4k</cell><cell>77.4</cell><cell>82.3</cell><cell cols="5">80.1 74.6 74.3 70.3 88.6 73.5 90.2 87.2 81.0 94.9 57.4 86.7 78.1 51.8 69.9 80.3</cell></row><row><cell>PointNet [26]</cell><cell>2k</cell><cell>80.4</cell><cell>83.7</cell><cell cols="5">83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6</cell></row><row><cell>RS-Net [12]</cell><cell>-</cell><cell>81.4</cell><cell>84.9</cell><cell cols="5">82.7 8</cell></row><row><cell>PCNN [1]</cell><cell>2k</cell><cell>81.8</cell><cell>85.1</cell><cell cols="5">82.4 80.1 85.5 79.5 90.8 73.2 91.3 86.0 85.0 95.7 73.2 94.8 83.3 51.0 75.0 81.8</cell></row><row><cell>SPLATNet [36]</cell><cell>-</cell><cell>82.0</cell><cell>84.6</cell><cell cols="5">81.9 83.9 88.6 79.5 90.1 73.5 91.3 84.7 84.5 96.3 69.7 95.0 81.7 59.2 70.4 81.3</cell></row><row><cell>KCNet [32]</cell><cell>2k</cell><cell>82.2</cell><cell>84.7</cell><cell cols="5">82.8 81.5 86.4 77.6 90.3 76.8 91.0 87.2 84.5 95.5 69.2 94.4 81.6 60.1 75.2 81.3</cell></row><row><cell>DGCNN [44]</cell><cell>2k</cell><cell>82.3</cell><cell>85.1</cell><cell cols="5">84.2 83.7 84.4 77.1 90.9 78.5 91.5 87.3 82.9 96.0 67.8 93.3 82.6 59.7 75.5 82.0</cell></row><row><cell cols="9">Ours 83.5 84.8 PointNet++ [28] 2k 84.0 86.2 2k,nor 81.9 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6</cell></row><row><cell>SyncCNN [50]</cell><cell>mesh</cell><cell>82.0</cell><cell>84.7</cell><cell cols="5">81.6 81.7 81.9 75.2 90.2 74.9 93.0 86.1 84.7 95.6 66.7 92.7 81.6 60.6 82.9 82.1</cell></row><row><cell>SO-Net [21]</cell><cell cols="2">1k,nor 80.8</cell><cell>84.6</cell><cell cols="5">81.9 83.5 84.8 78.1 90.8 72.2 90.1 83.6 82.3 95.2 69.3 94.2 80.0 51.6 72.1 82.6</cell></row><row><cell>SpiderCNN [48]</cell><cell cols="2">2k,nor 82.4</cell><cell>85.3</cell><cell cols="5">83.5 81.0 87.2 77.5 90.7 76.8 91.1 87.3 83.3 95.8 70.2 93.5 82.7 59.7 75.8 82.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Normal estimation error on ModelNet40 dataset.</figDesc><table><row><cell>dataset</cell><cell>method</cell><cell cols="3">#points error</cell></row><row><cell>ModelNet40</cell><cell cols="2">PointNet [1]</cell><cell>1k</cell><cell>0.47</cell></row><row><cell></cell><cell cols="2">PointNet++ [1]</cell><cell>1k</cell><cell>0.29</cell></row><row><cell></cell><cell>PCNN [1]</cell><cell></cell><cell>1k</cell><cell>0.19</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell>1k</cell><cell>0.15</cell></row><row><cell>desk</cell><cell></cell><cell></cell><cell></cell></row><row><cell>airplane</cell><cell></cell><cell></cell><cell></cell></row><row><cell>person</cell><cell></cell><cell></cell><cell></cell></row><row><cell>stairs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ground truth</cell><cell>ours</cell><cell cols="2">pointnet</cell><cell>pointnet++</cell></row><row><cell>ground truth</cell><cell cols="2">&lt; 30° normal</cell><cell></cell><cell>&gt; 90° normal</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of RS-CNN (%). "DP" indicates the dropout technique in FC layers of the classification network.</figDesc><table><row><cell>model</cell><cell cols="2">#points relation BN DP scale voting</cell><cell>acc.</cell></row><row><cell>A</cell><cell>1k</cell><cell>1</cell><cell>87.2</cell></row><row><cell>B</cell><cell>1k</cell><cell>1</cell><cell>89.9</cell></row><row><cell>C</cell><cell>1k</cell><cell>1</cell><cell>91.9</cell></row><row><cell>D</cell><cell>1k</cell><cell>1</cell><cell>92.2</cell></row><row><cell>E</cell><cell>1k</cell><cell>2</cell><cell>92.5</cell></row><row><cell>F</cell><cell>1k</cell><cell>3</cell><cell>92.9</cell></row><row><cell>G</cell><cell>1k</cell><cell>3</cell><cell>93.6</cell></row><row><cell>H</cell><cell>2k</cell><cell>3</cell><cell>93.6</cell></row><row><cell>I</cell><cell>1k</cell><cell>3</cell><cell>90.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The results (%) of different designs on aggregation function A and mapping function M (Eq. (3)) (M (k) : k-layer MLP).</figDesc><table><row><cell></cell><cell>A</cell><cell cols="2">M (2) M (3) M (4) acc.</cell></row><row><cell></cell><cell>max</cell><cell></cell><cell>92.4</cell></row><row><cell></cell><cell>max</cell><cell></cell><cell>93.6</cell></row><row><cell></cell><cell>max</cell><cell></cell><cell>92.7</cell></row><row><cell></cell><cell>avg.</cell><cell></cell><cell>91.6</cell></row><row><cell></cell><cell>sum</cell><cell></cell><cell>91.6</cell></row><row><cell cols="5">Table 6. The results (%) of five intuitive low-level relations h (Ed:</cell></row><row><cell cols="5">Euclidean distance, cosd: cosine distance, x nor : normal of x, x :</cell></row><row><cell cols="5">2D projection of x). Model A applies only 3D Euclidean distance</cell></row><row><cell cols="5">as h; Model B adds the coordinates difference to model A; Model</cell></row><row><cell cols="5">C adds the coordinates of two points to model B; Model D utilizes</cell></row><row><cell cols="5">the normals of two points and their cosine distance as h; Model E</cell></row><row><cell cols="5">projects 3D points onto a 2D plane of XY, XZ and YZ.</cell></row><row><cell cols="3">model low-level relation h</cell><cell>channels</cell><cell>acc.</cell></row><row><cell>A</cell><cell cols="2">(3D-Ed)</cell><cell>1</cell><cell>92.5</cell></row><row><cell>B</cell><cell cols="2">(3D-Ed, xi − xj)</cell><cell>4</cell><cell>93.0</cell></row><row><cell>C</cell><cell cols="2">(3D-Ed, xi − xj, xi, xj)</cell><cell>10</cell><cell>93.6</cell></row><row><cell>D</cell><cell cols="2">(3D-cosd, x nor i</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Robustness to point permutation and rigid transformation (%). During testing, we perform random permutation (perm.) of points, add a small translation of ±0.2 and counterclockwise rotate the input point cloud by 90 • and 180 • around Y axis. The accuracy drops a lot mainly because the forcible normalization of each local point subset could bring difficulty for shape recognition.</figDesc><table><row><cell>method</cell><cell>acc.</cell><cell cols="2">perm. +0.2</cell><cell>-0.2</cell><cell>90 •</cell><cell>180 •</cell></row><row><cell>PointNet [26]</cell><cell>88.7</cell><cell>88.7</cell><cell>70.8</cell><cell cols="2">70.6 42.5</cell><cell>38.6</cell></row><row><cell>PointNet++ [28]</cell><cell>88.2  †</cell><cell>88.2</cell><cell>88.2</cell><cell cols="2">88.2 47.9</cell><cell>39.7</cell></row><row><cell>Ours</cell><cell>90.3  †</cell><cell>90.3</cell><cell>90.3</cell><cell cols="2">90.3 90.3</cell><cell>90.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>†</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Complexity of RS-CNN in point cloud classification. CNN reduces the params by 59.7% and the FLOPs by 32.9%, which shows its great potential for real-time applications, e.g., scene parsing in autonomous driving.</figDesc><table><row><cell>method</cell><cell cols="2">#params #FLOPs/sample</cell></row><row><cell>PointNet [26]</cell><cell>3.50M</cell><cell>440M</cell></row><row><cell>PointNet++ [23]</cell><cell>1.48M</cell><cell>1684M</cell></row><row><cell>PCNN [23]</cell><cell>8.20M</cell><cell>294M</cell></row><row><cell>Ours</cell><cell>1.41M</cell><cell>295M</cell></row><row><cell cols="3">(number of params) and the time (floating point opera-</cell></row><row><cell cols="3">tions/sample) complexity of RS-CNN in classification with</cell></row><row><cell cols="3">1024 points as the input. Compared with PointNet [26],</cell></row><row><cell>RS-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 12 .</head><label>12</label><figDesc>The results (%) of RS-CNN with the low-level relation h defined on 2D views (XY-Ed: Euclidean distance in XY plane, x xy : 2D coordinates of x in XY plane, i.e., the value of z is set to be zero). The fusion strategy is achieved by performing elementwise summation of f P sub in Eq. (3), with h defined on three 2D views. Note that the weight in M is shared over these three views in the fusion process.</figDesc><table><row><cell>low-level relation h</cell><cell cols="2">channels acc.</cell></row><row><cell>(XY-Ed, x xy i − x xy j , x xy i , x xy j ) (XZ-Ed, x xz i − x xz j , x xz i , x xz j ) (YZ-Ed, x yz i − x yz j , x yz i , x yz j )</cell><cell>10 10 10</cell><cell>92.1 92.1 92.2</cell></row><row><cell>fusion of above three views</cell><cell></cell><cell>92.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Outline</head><p>This supplementary material provides further investigations for the proposed RS-CNN. Specifically, three issues on the construction of local neighborhood are discussed in Sec B. More details of the relation learning on 2D views of 3D point cloud are presented in Sec C. All the experiments are conducted on ModelNet40 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Construction of Local Neighborhood</head><p>In the main paper, the local point subset P sub in Eq. (3) is modeled to be a spherical neighborhood with a sampled point x i as the centroid, and the surrounding points as its neighbors N (x i ) (see the left-most part in <ref type="figure">Fig. 2</ref>). Then, the inductive representation f Psub , which is expected to reason the spatial layout of points in this neighborhood, is obtained by performing the proposed relation-shape convolution to aggregate all the relation between x i and N (x i ).</p><p>In the above process, there are mainly three issues worth further investigation: (1) How should N (x i ) be selected? (2) Is it suitable to simply aggregate all the relation between x i and N (x i )? (3) Is it reasonable to select the sampled point x i as the centroid? They are explored as follows.</p><p>(1) Selection of the neighbors N (x i ). Two strategies, k-nearest neighbor (k-NN) and random picking in the ball (Random-PIB), are investigated for this issue. <ref type="table">Table 9</ref> summarizes the results. Note that the number of neighbors is set to be equal for a fair comparison. As it shows, Random-PIB obtains better classification accuracy. The reason may be k-NN would suffer selection inhomogeneity in some cases, which is adverse to shape-aware learning (the aggregated relation may only focus on dense points and ignore sparse points that are essential for the underlying shape). By contrast, Random-PIB can have a better coverage of points even in the case of inhomogeneous distribution.</p><p>(2) Relation aggregation issue. To verify this issue, we randomly cut off some relation between x i and N (x i ) during training, i.e., randomly setting the learned high-level relation expression M(h ij ) in Eq. (3) to be a zero vector, but using all the relation during testing. This operation is similar to the dropout technique. <ref type="table">Table 10</ref> summarizes the results. As can be seen, the best approach is training with all the relation while the second best one is training with relation cut ratio of 0.3. This indicates the dropout-like technique is not suitable for relation learning, probably because RS-CNN can automatically encode the strength of the relation in the learning process.</p><p>(3) Selection of the centroid. Three types of the centroid: the sampled point x i , the average of N (x i ) and random</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GVCNN: Group-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiresolution tree networks for 3D point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Flex-convolution (million-scale point-cloud learning beyond grid-worlds)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Lensch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07289</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PCPNet: Learning local shape properties from raw point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="85" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view 3D object retrieval with deep embedding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5526" to="5537" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SeqViews2SeqLabels: Learning 3D global features via aggregating sequential views by RNN with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="658" to="672" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3D segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning sparse high dimensional filters: Image filtering, dense CRFs and bilateral neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4452" to="4461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">PointSIFT: A SIFT-like network module for 3D point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00652</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep Kd-Networks for the recognition of 3D point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SO-Net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3546" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on X-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning with sets and point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6620" to="6629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalized convolutional neural networks for point cloud data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savchenkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="930" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3D outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2107" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">RGCNN: Regularized graph CNN for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="746" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">O-CNN: octree-based convolutional neural networks for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<idno>72:1-72:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph CNN for learning on point clouds</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DeepShape: Deep-learned shape descriptor for 3D shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1335" to="1345" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attentional ShapeCon-textNet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SpiderCNN: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3D shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>210:1-210:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6584" to="6592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
