<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EpO-Net: Exploiting Geometric Constraints on Dense Trajectories for Motion Saliency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Faisal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Information Technology University</orgName>
								<address>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">KeepTruckin, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ali</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Information Technology University</orgName>
								<address>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EpO-Net: Exploiting Geometric Constraints on Dense Trajectories for Motion Saliency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The existing approaches for salient motion segmentation are unable to explicitly learn geometric cues and often give false detections on prominent static objects. We exploit multiview geometric constraints to avoid such shortcomings. To handle the nonrigid background like a sea, we also propose a robust fusion mechanism between motion and appearance-based features. We find dense trajectories, covering every pixel in the video, and propose trajectory-based epipolar distances to distinguish between background and foreground regions. Trajectory epipolar distances are dataindependent and can be readily computed given a few features' correspondences between the images. We show that by combining epipolar distances with optical flow, a powerful motion network can be learned. Enabling the network to leverage both of these features, we propose a simple mechanism, we call input-dropout. Comparing the motion-only networks, we outperform the previous state of the art on DAVIS-2016 dataset by 5.2% in the mean IoU score. By robustly fusing our motion network with an appearance network using the input-dropout mechanism, we also outperform the previous methods on DAVIS-2016, 2017 and Seg-trackv2 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Segmenting object(s) with significant motion in a video is called Salient Motion Segmentation. In contrast, segmenting the most prominent object(s) in an image (or a video) is Salient Appearance Segmentation. While the datadriven approaches have been quite successful for the later, we argue, that the former suffers from the scarcity of the video-based training data and remains ill-posed. Specifically, for a moving camera, it remains hard to learn, whether the 2D projected motion field corresponds to a static object in the video, or the one having independent motion. To segment out the rigid background from the independently mov-Ground truth LVO <ref type="bibr" target="#b42">[43]</ref> STP <ref type="bibr" target="#b16">[17]</ref> MotAdapt <ref type="bibr" target="#b39">[40]</ref> AGS <ref type="bibr" target="#b48">[49]</ref> Our <ref type="figure">Figure 1</ref>: Existing methods fail to automatically learn geometric cues between the foreground objects and the rigid background. As a result, they often give false detections on prominent static objects, as shown here in an example from DAVIS <ref type="bibr" target="#b31">[32]</ref>. Whereas by exploiting these constraints over the whole video, we avoid making such mistakes.</p><p>ing foreground objects, we exploit extensively studied geometric constraints <ref type="bibr" target="#b13">[14]</ref>, over the complete video, in a learning paradigm. Unlike the data-dependent learning, these constraints have closed-form solutions and can be computed very efficiently. In <ref type="figure">Fig. 1</ref> we give an example from DAVIS <ref type="bibr" target="#b31">[32]</ref>, showing that the previous approaches give false detections on prominent static objects; whereas the proposed approach can disambiguate static and nonstatic objects. This clearly shows that the existing deep-networks are unable to automatically learn the geometric cues even when the optical flow was provided as an input.</p><p>To exploit multiview geometric constraints, we convert optical flow between consecutive frames into dense trajectories, covering every pixel in the video, and then use trifocal tensors to find epipolar distances <ref type="bibr" target="#b13">[14]</ref> for them. The trajectory epipolar distance serves as a measure of (non)rigidity: a small distance corresponds to the rigid background, and a large distance implies a foreground object(s).</p><p>Trajectory epipolar distances, capture temporally global constraint on the foreground and background region, whereas optical flow only captures local temporal information. In essence, they both are complementary and by combining them, powerful features for motion saliency can be learned. Given trajectory epipolar distances and optical flow as an input, we build an encoder-decoder based network <ref type="bibr" target="#b35">[36]</ref>, called EpO-Net. We devise a strategy called input-dropout, enabling the network to learn robust motion features and handle failure cases of one of the two inputs.</p><p>EpO-Net brings two key advantages over the existing motion network, Mp-Net <ref type="bibr" target="#b41">[42]</ref>. 1) EpO-Net exploits geometric constraints over a large temporal window, whereas Mp-Net makes suboptimal decisions based on temporally local optical flow information. Consequently, as we show, EpO-Net can be trained on smaller training data, while having better generalization than Mp-Net. 2) In contrast to Mp-Net, EpO-Net does not require any objectness score on top of the estimated motion saliency map. The main reason for this is, we prepare and train our network on a more realistic but synthetic data consisting of real backgrounds and synthetic foreground objects, called Real Background and Synthetic Foreground (RBSF) dataset. Whereas, Mp-Net was trained on unrealistic synthetic 3D flying objects <ref type="bibr" target="#b27">[28]</ref>.</p><p>Being a motion-only network, EpO-Net cannot handle a nonrigid background. To handle this case, we exploit appearance <ref type="bibr" target="#b2">[3]</ref> along with motion-based features in the form of a joint network, EpO-Net+. Using the proposed input-dropout strategy, we show that the EpO-Net+ is robust against the failure cases of individual motion and appearance-based features.</p><p>To the best of our knowledge, ours is the first method to combine geometric constraints in a learning paradigm for motion segmentation. Our paper has three main contributions. 1) A motion only network based on trajectory epipolar distance and optical flow. 2) Our RBSF dataset, that can be used to train salient motion segmentation. Applications like video annotation <ref type="bibr" target="#b9">[10]</ref>, object tracking <ref type="bibr" target="#b53">[54]</ref>, and video anomaly detection <ref type="bibr" target="#b51">[52]</ref>, can use our network and the dataset, to exploit geometric constraints on the rigid world. The source code of our method, as well as the dataset, is publicly released 1 .</p><p>3) The input-dropout technique, which can be used to robustify early or late fusion of features in deep architectures. Our motion network outperforms Mp-Net on DAVIS-2016 <ref type="bibr" target="#b31">[32]</ref> by a significant margin of 5.2% in mean IoU score and is quite close to other recent methods exploiting additional appearance features. The proposed joint network also demonstrates significant improvement over the previous methods on DAVIS <ref type="bibr">(2016 [32]</ref> &amp; 2017 <ref type="bibr" target="#b34">[35]</ref>) and Segtrack-v2 <ref type="bibr" target="#b22">[23]</ref>. <ref type="bibr" target="#b0">1</ref> https://github.com/mfaisal59/EpONet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, video object segmentation (VOS) has been gaining interest <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref>, much credit to the new challenging benchmark datasets. One of the factors to categorize existing approaches could be the degree of supervision. Supervised approaches <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b4">5]</ref> or interactive ones assume user input, in the form of scribbles, is available at multiple instances, helping algorithm refine the results. Semi-Supervised methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25]</ref>, assume that at least for the first frame, segmentation is given, reducing the problem to label propagation. For brevity, we discuss only a few prominent unsupervised methods.</p><p>In unsupervised settings, to make the problem tractable the motion-saliency constraint is enforced. Many methods try to capture motion information across the multiple frames, mostly by constructing long sparse point trajectories <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39]</ref>. Salient object segmentation is then reduced to clustering these trajectories <ref type="bibr" target="#b19">[20]</ref> and converting them into dense points <ref type="bibr" target="#b29">[30]</ref>. Among the other early methods, few methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b32">33]</ref> extract object proposals <ref type="bibr" target="#b7">[8]</ref> and try to build the connection between the proposals temporally. These trajectory based methods are not robust because they heavily rely on feature matching, that may fail due to occlusion, fast motion, and appearance change.</p><p>Recently deep learning based methods have been used to solve the VOS problem. Broadly, these techniques have three components: 1) network to capture the motion, 2) extract appearance information, 3) a temporal memory so that the decision made at one frame is propagated to the others <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b40">41]</ref>. Among all these approaches, Mp-Net <ref type="bibr" target="#b41">[42]</ref> and LVO <ref type="bibr" target="#b42">[43]</ref> are very close to our method. Mp-Net constructs an encoder/decoder based network to segment the optical flow into the salient and non-salient one. Encoder/decoder network is trained on large synthetic dataset <ref type="bibr" target="#b27">[28]</ref> and then fine-tuned on DAVIS <ref type="bibr" target="#b31">[32]</ref>. Since motion information they learn is not sufficient, they rely on an objectness score <ref type="bibr" target="#b33">[34]</ref> to clean their results. LVO, builds on Mp-Net, using bi-directional ConvGRU to propagate the information across the other frames. Their results improve drastically (LSMO <ref type="bibr" target="#b43">[44]</ref>) by just using a better optical flow estimation and appearance model (DeepLabv2 instead of Deep Lab v1). MotAdapt <ref type="bibr" target="#b39">[40]</ref> used the teacher-student learning paradigm, where the teacher provides pseudo labels using the optical flow and the image as input.</p><p>AGS <ref type="bibr" target="#b48">[49]</ref> explores the concepts of video saliency or dynamic fixation prediction, with an argument that unsupervised VOS is closely related to the video saliency <ref type="bibr" target="#b46">[47]</ref>. Authors trained a visual attention module on the dynamic fixation data, collected by tracking viewers' eyes while they watch videos. Unlike AGS which required the data gathered by tracking the viewer's gaze, we try to model the concept of motion-saliency by exploiting the geometric constraints An early method by Torr <ref type="bibr" target="#b44">[45]</ref>, Sheikh et. al. <ref type="bibr" target="#b37">[38]</ref>, and Tron and Vidal <ref type="bibr" target="#b45">[46]</ref>, try to exploit motion models. <ref type="bibr" target="#b37">[38]</ref>, and <ref type="bibr" target="#b45">[46]</ref> exploited trajectory information to separate out the foreground and background objects. Many recent methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17]</ref> have relied on the previous trajectorybased segmentation work, using the deep features for image saliency and optical flow for motion saliency to construct a neighborhood graph. <ref type="bibr" target="#b36">[37]</ref> used optical flow-based point trajectories to propagate the user input scribbles. <ref type="bibr" target="#b47">[48]</ref> clustered neighboring trajectories to create super-trajectories, and tracked the mask, provided as input, in the first frame of the video. However, they have not exploited the geometrybased constraints, rather rely on the heuristics and complex pipeline.</p><p>Our work relies on all the three techniques. We use optical flow to build trajectories and geometry-based technique to penalize the trajectories not following the geometric constraint. To make our deep learning models robust, we design the input-dropout technique for the training. To the best of our knowledge, we are the first one to combine CNNs and geometrical constraints for VOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Epipolar Constraints on Dense Trajectories</head><p>Existing methods for salient motion segmentation, use appearance, and optical flow based features to distinguish foreground from background. These features are not geometry inspired, learned from the data and alone do not provide enough constraints for the rigid background. We propose geometry inspired features and leverage them in a learning pipeline. We use trifocal tensors to constraint the rigid background in the video and propose epipolar distances for the <ref type="figure">Figure 3</ref>: An illustration of exploiting the complete trajectories to find epipolar distances. Part of the bear remains static in this and the previous frame, giving small epipolar distance (middle). Since trajectories aggregate these distances over their full time-span, the trajectory-based epipolar distances are still high for almost the complete bear (right). dense trajectories as a measure of nonrigidity (See <ref type="figure" target="#fig_0">Fig. 2</ref>).</p><p>We first find forward and backward optical flow of F frames, each of height h and width w, using <ref type="bibr" target="#b3">[4]</ref> and then convert it into T dense trajectories covering every pixel in the video. Each trajectory, X i , where i ∈ {1, . . . , T }, is an F × 1 vector of 2D image coordinates and may consists of missing values due to pixels' occlusion. T hw, because for every occlusion new pixels appear. We use forward and backward optical flow consistency to find occluding regions. We stack all the trajectories into a F × T sparse matrix, X.</p><p>Once trajectories are found, we estimate the dominant rigid background, by finding the trifocal tensors in every three consecutive frames, using the six-point algorithm <ref type="bibr" target="#b13">[14]</ref> 2 , and RANSAC. We convert the trifocal tensor to the corresponding six pair-wise fundamental matrices, F 12 , F 21 , F 13 , F 31 , F 23 , F 32 <ref type="bibr" target="#b13">[14]</ref> 3 . When the camera is static and optical flow is zero for the background, the estimation of the trifocal tensor can become degenerate. Any skew-symmetric matrix, in this case, would be a valid fundamental matrix. To avoid degeneracy, we first detect if the camera remains static, by checking if at least 50% of the pixels have zero optical flow, in the current triplet of frames. Then we initialize fundamental matrices to arbitrary skewsymmetric matrices.</p><p>We find the epipolar distances for the triplet as follows. Let x j1 , x j2 and x j3 denote the homogenous 2D coordinates of the selected three frames in the j th trajectory. We find the distance between x j1 and x j2 as,</p><formula xml:id="formula_0">l 21 = F 21 x j1 ,<label>(1)</label></formula><formula xml:id="formula_1">d j12 = x T j2 l 21 / l 21 (1) 2 + l 21 (2) 2 ,<label>(2)</label></formula><p>where l 21 is the epipolar line in frame 2 corresponding to the frame 1, l 21 (i), its i th component and d j12 is the distance between the line and x j2 . By normalizing the line w.r.t its magnitude, gives the normlize epipolar distance. The triplet epipolar distance would be</p><formula xml:id="formula_2">d j123 = d j12 + d j21 + d j13 + d j31 + d j23 + d j32 . (3)</formula><p>The epipolar distance for the trajectory j is computed as the mean of all triplet epipolar distances along this trajectory. Concatenating all the trajectory epipolar distances gives a 1 × T matrix, D.</p><p>We assign the epipolar distance of a trajectory to all the constituent pixels. Hence, the proposed approach can deal with parts of the foreground object that remain static for a few frames but were in motion otherwise. As we show in <ref type="figure">Fig. 3</ref> the epipolar distance estimated based on the current and the previous frame is quite small for the static part of the bear, whereas the trajectory-based epipolar distance can detect a significant part of the bear. Trajectory epipolar distances help us find powerful motion features for video segmentation, as we show in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>The proposed pipeline consists of three distinct stages as illustrated in <ref type="figure" target="#fig_1">Fig 4.</ref> 1) Our motion network, EpO-Net takes optical flow and epipolar distances as input, and outputs motion-saliency-map. 2) Parallel to this, we have a network to compute the appearance features to extract scene context and object information <ref type="bibr" target="#b2">[3]</ref>. 3) Our joint network, EpO-Net+ fuses the appearance features and the motionsaliency-map with a bidirectional-ConvGRU and outputs saliency mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motion Images</head><p>Given an input video, we compute optical flow, convert it into dense trajectories, find trajectory epipolar distances and convert them into per-frame Epipolar Distances (ED). Having a temporally bigger receptive field, ED assigns a large weight to the foreground and lower to the background. However, it is sensitive towards optical flow errors because, during trajectory estimation, optical flow errors accumulate over time, affecting all the constituent pixels and their corresponding epipolar distances. Whereas, optical flow captures temporally local but relatively robust information containing motion patterns to distinguish foreground from background. Both of them are complementary and should be exploited jointly. We concatenate 2-channel optical flow vectors with ED, to get a 3-channel image, we call motionimages, as shown in <ref type="figure">Fig 5.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Epipolar Optical flow Network (EpO-Net)</head><p>Given the motion image as input, we design an encoderdecoder architecture, in the fashion of UNet <ref type="bibr" target="#b35">[36]</ref> that outputs motion-saliency-map. The encoder latent space captures the context of the whole motion image, by jointly exploiting motion patterns and their relationship with ED. The decoding part on the other-hand has unraveled the context to decide about each pixel. The use of skip layers gives decoder access to local information ( <ref type="bibr" target="#b50">[51]</ref>) collected from the lower layers of the encoding-network and helps to exploit the context to decide the pixel level labels.</p><p>In our network, we use four encoders followed by four decoders, where each block consists of a convolution layer, followed by batch normalization, ReLU activation, and max-pooling layers. Different from Mp-Net, our much informative input allows us to have fewer channels before the final classification layer (128 instead of 512). Motionsaliency-map is produced using a sigmoid layer in the end. CRF is used to clean the output. A detailed architecture diagram showing the parameters of EpO &amp; EpO+ is shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Joint Network (EpO-Net+)</head><p>Any algorithm solely based on motion information will struggle with defining object boundaries and be confused by the non-rigid background. To exploit the additional appearance information, we use the pre-trained Deep-Lab <ref type="bibr" target="#b2">[3]</ref> features and fuse them with our motion network, similar to LVO <ref type="bibr" target="#b42">[43]</ref>. Although the FC6 layer of Deep-Lab is just 1/8th of the spatial size of the original image, it still captures important information about the objects, their boundaries, and nonrigid background. Although customized appearance networks for video segmentation can produce better segmentation results, we choose to use rather generic appearance-based features, to demonstrate the significance of the proposed motion network.</p><p>We train the bottleneck layer to reduce the appearance features from 1024 to 128 and concatenate it with the downsampled output of EpO-Net. To exploit temporal continuity in the joint-features and build a global context, we concatenate the bi-directional Convolutional Gated Recurrent Unit at the end of our network. To robustly handle motion network failures in the case of nonrigid background, we introduce input-dropout, discussed in Sec. 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Challenges in Training</head><p>The proposed architecture contains a fusion of mixed features, encapsulating information at varied spatial and temporal receptive fields, at different stages of the network. To enable the network to properly learn the concept of motion saliency, and robustly fuse these features, required contribution both in the training methodology and dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">RBSF Dataset</head><p>Training sequences in the DAVIS 2016 are too few to train a robust motion network. We find that F3DT <ref type="bibr" target="#b27">[28]</ref> and PHAV <ref type="bibr" target="#b6">[7]</ref> datasets are not very useful for us. F3DT has holes and the objects' motion is quite fast. PHAV is low resolution than DAVIS and the ground-truth optical flow is noisy because of jpeg compression. We create our own synthetic dataset, called RBSF (Real Background, Synthetic Foreground), by mixing 20 different foreground objects performing various movements with 5 different real background videos. With fairly large size objects (size: 30% to 50% of the frame) and reasonably fast motion, RBSF allows us to compute accurate optical flow and long trajectories. We observe that generating more data does not improve results, thanks to the well-constrained epipolar distances. After training on RBSF, we fine-tune EpO-Net on DAVIS-2016 <ref type="bibr" target="#b31">[32]</ref>. For more details of the dataset, please see the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Feature Fusion &amp; Input-Dropout</head><p>Robustly fusing optical flow and epipolar distances is a challenging task. Ideally, the network should be able to learn which feature to rely on the pixel-level granularity. But this requires contextual information that is only available in the deeper layers of the network, where the resolution is usually very small and the network has already mixed the input channels. In such a scenario, training with more data or for more iterations usually does not improve the results.</p><p>This problem is usually solved by introducing a mix of early and late fusion, requiring complex network designs, where skip layers are going from one part of the network to the others. Our proposed solution is rather quite simple, which we call Input-Dropout. While training EpO-Net, we randomly make complete ED-channel zero, for some of the sequences which have erroneous ED-maps (sequences with large motion and a considerable occlusion). For the rest, motion-images are unaltered. This is done for the initial 10 epochs, allowing the filters to give more importance to the optical flow. After that, we repeat the same procedure but instead of zero, we assign random values, forcing the network to learn the diverse enough filters to capture the motion information from the optical flow, ED and their combination, separately. With input-dropout EpO's mean  IoU increases from 72.7 to 75.2 ( <ref type="table" target="#tab_5">Table 6</ref>). We exploit the same input-dropout strategy for the late fusion of appearance and motion features in our joint network. We randomly set the motion-saliency-map to zero for a few frames of the sequences, where the motion network fails (sequences with dynamic background and occlusion). Using input-dropout, mean IoU improves from 79.4 to 80.6. The complete network, containing all the above stages and layers is called EpO-Net+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We train and evaluate on RBSF (Sec. 5.1), DAVIS2016 <ref type="bibr" target="#b31">[32]</ref>, DAVIS2017 <ref type="bibr" target="#b34">[35]</ref> and Segtrack-v2 <ref type="bibr" target="#b22">[23]</ref>. Below we detail our training parameters and evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation Details</head><p>EpO is trained using the mini-batch SGD with a batch size of 12, the initial learning rate is set to 0.001, with a momentum of 0.9, and a weight decay of 0.005. The network is trained from scratch for 50 epochs, with the learning rate decay factor set to 0.1, after every 5 epochs. The images are down-sampled by a factor of 0.5 to fit a batch size of 12 images in the GPU memory.</p><p>We train EpO in two stages: training on a synthetic dataset, RBSF (Sec. 5.1), and then fine-tuning on DAVIS-2016. For both of these training, we perform input-dropout for epipolar channel for only 20% of training data i.e. we randomly assign zero and add small random Gaussian noise in the epipolar channel. We call this final trained model,  <ref type="figure">Figure 5</ref>: Qualitative Comparison of our EpO-Net with Mp-Net <ref type="bibr" target="#b41">[42]</ref>.</p><p>EpO and the one trained on RBSF, EpO-RBSF. The fusion network is fully trained only on the DAVIS-2016's training set, resulting in EpO+. We use the batch size of 12 and an initial learning rate set to 0.001, which is decreased after every epoch with a factor epoch 50 . The model is trained using the back-propagation through time <ref type="bibr" target="#b49">[50]</ref> using binary cross-entropy loss and RMSProp optimizer. The weights of all the layers in the fusion network are initialized using the Xavier <ref type="bibr" target="#b11">[12]</ref>, except for those in ConvGRU, that is initialized using MSR initialization <ref type="bibr" target="#b14">[15]</ref>. We clip the gradients to the [-50, 50], before each update step <ref type="bibr" target="#b12">[13]</ref> to avoid numerical issues. For robust fusion, we again use the input-dropout mechanism by setting the motion-saliencymap to zero, for 20% frames of the sequence with fast motion and dynamic background. We also perform the random cropping and flipping of sequences during the training. The fusion network is trained for 50 epochs. The final output is refined using CRF, during inference. To test on DAVIS-2017, we fine-tine EpO-RBSF and EpO on the DAVIS-2017's training-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation</head><p>We follow the standard training &amp; validation split, to train and evaluate using the protocol proposed in <ref type="bibr" target="#b31">[32]</ref> and compute intersection-over-union J , F-measures F, and temporal stability T , contour accuracy and smoothness of segmentation overtime respectively. The evaluation results are summarized in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Motion Network</head><p>By exploiting geometric constraints in salient motion segmentation, our EpO (motion-only) network scores mean J of 0.752 over DAVIS-2016 validation set. This is much higher than 0.70 score of Mp-Net <ref type="bibr" target="#b41">[42]</ref>, which also relies on non-motion features (objectness score). MP-Net is trained on 45K frames using ground-truth optical flow, whereas EpO uses only 20K frames and an estimated optical flow on them. We observe that using more data does not improve the performance, thanks to the well-constrained epipolar distances. Moreover, our EpO score is competitive to LVO, which is using a bi-directional ConvGRU and the appearance information in addition to optical flow. Whereas EpO only uses motion-images (optical flow &amp; ED).</p><p>Qualitative comparison of EpO-Net with Mp-Net is given in <ref type="figure">Fig. 5</ref>. It's evident from the 2 nd to 4 th columns that ED and optical flow are complimenting each other, and the results are robust against the failure of one of these inputs. In the case of optical flow being too small, or if the object motion is in the same direction as the camera motion (row-2), ED helps distinguish the object. Similarly, when the ED score is sporadically bad (row-1 &amp; 3), optical flow information helps to distinguish the object, much due to the robust motion features learned with input-dropout training. Whereas Mp-Net makes local decisions, unable to recover from the optical flow errors (row 3 &amp; 5). Their results also degrade when the camera and object have similar motion (row-3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">EpO+</head><p>Combining the motion-saliency map obtained from EpO with the appearance features and adding temporal memory, EpO+ outperforms its direct competitors LVO and LSMO, by a significant margin of 4.7% and 2.4% over mean IoU. EpO+ outperforms even recently published works, like AGS <ref type="bibr" target="#b48">[49]</ref>, which requires training on dynamic fixation Measure EpO+ EpO AGS <ref type="bibr" target="#b48">[49]</ref> MOA <ref type="bibr" target="#b39">[40]</ref> LSMO <ref type="bibr" target="#b43">[44]</ref> STP <ref type="bibr" target="#b16">[17]</ref> PDB <ref type="bibr" target="#b40">[41]</ref> ARP <ref type="bibr" target="#b20">[21]</ref> LVO <ref type="bibr" target="#b42">[43]</ref> Mp-Net <ref type="bibr" target="#b41">[42]</ref> FSeg <ref type="bibr" target="#b17">[18]</ref>    <ref type="table">Table 3</ref>: Attribute-based analysis of top performing methods on DAVIS-2016 dataset. The mean IoU on all sequences with attributes: appearance cahnge (AC), dynamic background (DB), fast motion (FM), motion blur (MB), and occlusion (OCC), is computed. The smaller font values indicate the change in performance (gain or loss) for the method on the remaining sequences without that respective attribute.</p><p>dataset collected by tracking the gaze of viewers, both in mean IoU and its recall. Important to note is mean temporal stability, which is substantially better than rest explicitly indicating the effectiveness of our formulation. Our attribute analysis is given in <ref type="table">Table 3</ref>. EpO+ outperforms the baselines in all categories except the occlusion.</p><p>Qualitative comparison of EpO+ with SOTA is presented in <ref type="figure" target="#fig_3">Fig. 6</ref>. AGS has failed to properly segment moving objects (2 nd and 3 rd row). Most of the errors in the previous methods are over-segmenting and are due to overexploitation of appearance information. This we can attribute to the very basic reason of not being able to exploit/learn enough constraints for motion saliency.</p><p>While the proposed method, due to more informative proposed motion features (based on geometric constraints) and input-dropout training procedure, is being able to learn how to balance appearance and motion cues. For details see supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Evaluation on other datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS-2017:</head><p>We fine-tune EpO-RBSF and EpO+ on the DAVIS-2017's training sequences. We could not find the comparative results, but we are reporting ours for future comparison in <ref type="table">Table 5</ref>.  <ref type="table">Table 4</ref>: EpO+ results on SegTrack-v2 dataset <ref type="bibr" target="#b22">[23]</ref>. We only perform bad on one sequence (birdfall). Removing this increase our Mean IoU to 72.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segtrack-v2: Evalaution resutls of EpO+ and EpO on</head><p>SegTrack-v2 <ref type="bibr" target="#b22">[23]</ref> dataset have been presented in <ref type="table">Table 4</ref>. Our results are better than existing methods, including STP <ref type="bibr" target="#b16">[17]</ref>. Although, it's with a small margin of 0.8%; this could be attributed to the difference in resolution of SegTrack-v2 videos vs that of DAVIS-2016. Removing birdfall, the only sequence we perform poorly, the results improves to 72.8%. AGS <ref type="bibr" target="#b48">[49]</ref> uses both SegTrackv2 and DAVIS for training, therefore, do not evaluate on this. Note that, since NLC <ref type="bibr" target="#b8">[9]</ref> reports results only on subset of sequences in their paper, results in <ref type="table">Table 4</ref> were taken from <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method AC</head><p>DB FM MB OCC J Mean EpO 0.67 -0.02 0.56 0.10 0.62 0.04 0.57 0.11 0.59 0.08 0.652 EpO+ 0.79 -0.04 0.72 0.05 0.74 0.03 0.72 0.06 0.66 0.13 0.763 <ref type="table">Table 5</ref>: Results on DAVIS 2017 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation Study</head><p>In this section, we present the study on the impact and effectiveness of different design choices. We first analyze the influence of different input modalities and depth of the  network architecture by training and validating on DAVIS-2016 dataset. Specifically, we use the single-channel ED, 2 channel optical flow i.e. X-Y displacement, and the combination of the both as 3 channel motion images. For each input modality, we train and validate EpO network with 2, 3 and 4-layer encoders/decoders to study which modality needs the deeper network.</p><p>In <ref type="table" target="#tab_5">Table 6</ref>, we observe that ED being a very simple yet informative feature, the epipolar alone network requires fewer parameters to learn, implying that they should not require (i) deep network, ii) large datasets. In contrast, optical flow, being complex information for motion saliency, requires more encoders and decoders. Since small errors in optical flow, get accumulated in trajectories estimation and result in quite noisy epipolar distances, optical flow with 4 encoders/decoders architecture beats the epipolar network, with 63.3% mean IoU. However, when we combine both, in the form of motion images, the accuracy further improves by 4.2%. This shows that the combination can exploit both the global temporal geometric information and local temporal motion information distinguishing the foreground and background. Note that all the experiments are performed using the same hyper-parameters stated in Sec. 6.1, the input-dropout strategy is not used, and all models are trained for 30 epochs only.</p><p>Next, we demonstrate the effectiveness of our dataset RBSF and the input-dropout in <ref type="table" target="#tab_5">Table 6</ref>. The mean IoU on DAVIS-2016 with the proposed dataset was 48.5%. That increases to 72.7% with fine-tuning on DAVIS. Comparing this with our Ep+OF's, the increase is 5.3%, showing the significance of the proposed dataset. With the proposed dropout the results further improve by 2.5%, showing the effectiveness of the input-dropout.</p><p>We also study the effect of GRU-sequence length. As expected, when we increase sequence length, from 6 to 12, the mean IoU improves from 77.3 to 79.4. A considerable improvement comes in the videos having occlusion. Finally, we observe that instead of the angle-magnitude representation of optical flow, the velocity representation gives better results. A qualitative review of the dataset, made us realize that the channel representing angle information is not robust to optical flow errors. Even for humans, inferring motion patterns by just looking at them, is quite difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We exploit multiview geometric constraints to define motion saliency and propose trajectory epipolar distances, as a measure of non-rigidity. By combining epipolar distances with optical flow, we train a powerful motion network and demonstrate significant improvement over the previous motion networks. Unlike previous methods, the learned motion features avoid over-reliance on appearancebased features. Even without using RNNs and appearance features, our motion network is competitive to the existing state of the art. With them, our method gives state of the art results. An input-dropout mechanism has been proposed that allows network to learn robust feature fusion. The proposed learning paradigm, involving the strong geometric constraints, should be useful for a number of related applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of multiview geometric constraints on rigid points. A 3D rigid line (red) is viewed by a moving camera. The projections of its 2D projections in 3D should meet at the actual line. In contrast, the 2D projections of a 3D nonrigid point (orange) are not constrained to lie on any 3D lines. This relationship can be captured in the form of trifocal tensors (or fundamental matrices) in the frames. In contrast to rigid points, the nonrigid point may not lie on the corresponding epipolar lines and their epipolar distances can serve as a measure of nonrigidity.inside the video itself and do not require extra data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Flow diagram depicting different parts &amp; information transition in the algorithm. Top Row: steps to compute the motion trajectories &amp; Epipolar Distance. Bottom row: (Left) Deep-Lab based Appearance Network trained to compute the Appearance Features. (Right) Motion-Images (Optical Flow &amp; Epipolar Distance) fed to EpO, which outputs motion saliency map. (Middle) Motion-saliency map concatenated with appearance features are fed into the bidirectional convGRU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>0.71 -0.02 0.58 0.14 0.68 0.04 0.65 0.10 0.69 0.01 0.700 EpO 0.77 -0.03 0.63 0.14 0.72 0.06 0.67 0.14 0.67 0.11 0.752</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparison with state-of-the-art methods on DAVIS-2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>EpO-Net vs. Mp-Net [42] on DAVIS-2016 dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our motion (EpO) and fusion network (EpO+), with state-of-the-art on DAVIS-2016 with intersection over union J , F-measure F, and temporal stability T . Best &amp; second best scores have been bold and are underlined respectively. AGS uses eye-gaze data to train their network, whereas we only exploit information existent in the videos itself by enforcing the geomatrical constraints.-0.04 0.80 -0.01 0.78 -0.01 0.78 +0.00 0.72 +0.07 DB 0.72 +0.10 0.66 +0.16 0.61 +0.20 0.55 +0.27 0.66 +0.15 FM 0.78 +0.04 0.77 +0.04 0.74 +0.05 0.73 +0.08 0.75 +0.04 MB 0.78 +0.06 0.74 +0.10 0.71 +0.10 0.73 +0.10 0.74 +0.06 OCC 0.75 +0.08 0.76 +0.05 0.78 -0.02 0.74 +0.06 0.81 -0.05</figDesc><table><row><cell cols="2">Attribute EpO+ AGS[49] MOA[40] LSMO[44] STP[17]</cell></row><row><cell>AC</cell><cell>0.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Method KEY NLC FSG LVO LSMO STP EpO EpO+ Mean IoU 57.3 67.2 61.4 57.3 59.1 70.1 68.3 70.9</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Left: Studying the effects of different input modalities against network depth. Right: Effect of dropout in epipolar channel of motion images, R and D denote RBSF and DAVIS dataset respectively.</figDesc><table><row><cell cols="2">#enc/dec Input Modality</cell><cell cols="2">EpO Variant Mean IoU</cell></row><row><cell></cell><cell>Ep OF Ep+OF</cell><cell>EpO(R)</cell><cell>48.5</cell></row><row><cell>2</cell><cell>57.2 54.7 62.7</cell><cell>EpO(D)</cell><cell>72.7</cell></row><row><cell>3</cell><cell>58.9 59.7 64.4</cell><cell>EpO(R)+Drop</cell><cell>50.6</cell></row><row><cell>4</cell><cell>49.2 63.3 67.5</cell><cell cols="2">EpO(D)+Drop 75.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Algorithm 20.1 page 511, Hartley &amp; Zisserman (2nd Ed) 3 Algorithm 15.1, page 375, Hartley &amp; Zisserman (2nd Ed)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cnn in mrf: Video object segmentation via inference in a cnn-based higher-order spatio-temporal mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5977" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Full flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4706" to="4714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Procedural generation of videos to train deep action recognition networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Lopez</forename><surname>Pena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Category independent object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="575" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiple bernoulli relevance models for image and video annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1002" to="1009" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1846" to="1853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="56" to="73" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised video object segmentation using motion saliency-guided spatiotemporal propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05384</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequential clique optimization for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="517" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3271" to="3279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1995" to="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Instance embedding transfer to unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6526" to="6535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Premvos: Proposalgeneration, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>565-580. 2</idno>
	</analytic>
	<monogr>
		<title level="m">14th Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maximum weight cliques with mutex constraints for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="670" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1515" to="1530" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3235" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object segmentation in video: a hierarchical variational approach for turning point trajectories into dense regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1583" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Higher order motion models and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="614" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3227" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<title level="m">The 2018 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shankar Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE ICCV</title>
		<meeting>the IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3235" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Background subtraction for freely moving cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1219" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Motion segmentation and tracking using normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1154" to="1160" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Video segmentation using teacherstudent adaptation in a human robot interaction (HRI) setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jägersand</surname></persName>
		</author>
		<idno>abs/1810.07733</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Geometric motion segmentation and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1321" to="1340" />
			<date type="published" when="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A benchmark for the comparison of 3-d motion segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2007" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Revisiting video saliency: A large-scale benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4894" to="4903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Super-trajectory for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1671" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning unsupervised video object segmentation through visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3064" to="3074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The devil is in the decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Video behavior profiling for anomaly detection. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="893" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1140" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
