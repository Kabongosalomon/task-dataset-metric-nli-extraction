<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory Enhanced Global-Local Aggregation for Video Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
							<email>chenyihong@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center of Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Zhejiang Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<email>yuecao@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<email>wanglw@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center of Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">MOE</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Memory Enhanced Global-Local Aggregation for Video Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How do humans recognize an object in a piece of video? Due to the deteriorated quality of single frame, it may be hard for people to identify an occluded object in this frame by just utilizing information within one image. We argue that there are two important cues for humans to recognize objects in videos: the global semantic information and the local localization information. Recently, plenty of methods adopt the self-attention mechanisms to enhance the features in key frame with either global semantic information or local localization information. In this paper we introduce memory enhanced global-local aggregation (MEGA) network, which is among the first trials that takes full consideration of both global and local information. Furthermore, empowered by a novel and carefully-designed Long Range Memory (LRM) module, our proposed MEGA could enable the key frame to get access to much more content than any previous methods. Enhanced by these two sources of information, our method achieves state-of-the-art performance on ImageNet VID dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>What differs detecting objects in videos from detecting them in static images? A quick answer is the information lies in the temporal dimension. When isolated frame may suffer from problems such as motion blur, occlusion or out of focus, it is natural for humans to find clues from the entire video to identify objects.</p><p>When people are not certain about the identity of an object, they would seek to find a distinct object from other frames that shares high semantic similarity with current object and assign them together. We refer this clue as global semantic information for every frame in the video could be reference to. But semantic information alone fails in the case if we are not sure about whether an object exists or not, e.g., a black cat walking in the dark. We could not rely on the semantic information to tell us where it is, as the existence of the instance has not been approved in key frame yet. This problem could be alleviated if nearby frames were given. With information such as motion calculated by the difference between nearby frames, we could localize the object in key frame. We refer this source of information as local localization information. To be general, people identify objects mainly with these two sources of information.</p><p>Consequently, it is direct to borrow this idea from human to enhance the video object detection methods with the information inside the whole video, just as shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>. However, due to the huge amount of boxes exist in the whole video, enhancement with the information in the entire video is infeasible. This motivates us to perform approximation while keeping a balance between efficiency and accuracy. Recent methods towards solving video object detection could be viewed as different approaches of approximation and can be classified into two main categories: local aggregation methods and global aggregation methods. Methods like <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref> consider utilizing both semantic and localization information in a short local range just as shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. On the other hand, <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">23]</ref> just consider the semantic impact between boxes, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(c). Unfortunately, none of these methods takes a joint view of both local and global information. We refer this as the ineffective problem.</p><p>Another problem existing in recent works is the size of frames for aggregation, which means the amount of information the key frame could gather from. In previous stateof-the-art methods <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">30]</ref>, only 20-30 reference frames are selected, which lasts only 1-2 seconds, for feature aggregation. This is also illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(b) and 1(c). We argue that the size of aggregation at this scale is an insufficient approximation of either local influence or global influence, to say nothing of <ref type="figure" target="#fig_0">Figure 1</ref>(a).</p><p>In this paper, we present Memory Enhanced Global-Local Aggregation (MEGA) to overcome the aforementioned ineffective and insufficient problem. Specifically, MEGA augments candidate box features of key frame by effectively aggregating global and local information.</p><p>MEGA is instantiated as a multi-stage structure. In the first stage, MEGA aims to solve the ineffective problem by aggregating both global and local information to key frames. However, just as shown in the top half of <ref type="figure" target="#fig_0">Figure  1(d)</ref>, the available content is still quite limited. So in the second stage, we introduce a novel Long Range Memory (LRM) module which enables key frame to get access to much more content than any previous methods. In particular, instead of computing features from scratch for current key frame, we reuse the precomputed features obtained in the detection process of previous frames. These precomputed features are cached in LRM and builds up a recurrent connection between current frame and previous frames. Unlike traditional memory, please note that these cached features are first enhanced by global information, which means current key frame is able to access more information not only locally, but also globally. The aggregation size is shown in the bottom half of <ref type="figure" target="#fig_0">Figure 1(d)</ref>. By introducing LRM, a great leap towards solving the insufficient problem is achieved while remaining simple and fast.</p><p>Thanks to the enormous aggregation size of MEGA empowered by LRM, we achieve 85.4% mAP on ImageNet VID dataset, which is to-date the best reported result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Detection from Images. Current leading object detectors are build upon deep Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3]</ref> and can be classified into two main familys, namely, anchor-based detectors (e.g., R-CNN <ref type="bibr" target="#b10">[11]</ref>, Fast(er) R-CNN <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>, Cascade R-CNN <ref type="bibr" target="#b1">[2]</ref>) and anchor-free detectors (e.g., CornerNet <ref type="bibr" target="#b17">[18]</ref>, ExtremeNet <ref type="bibr" target="#b34">[34]</ref>). Our method is built upon Faster-RCNN with ResNet-101, which is one of the state-of-the-art object detector.</p><p>Object Detection in Videos. Due to the complex manner of video variation, e.g., motion blur, occlusion and out of focus, it is not trivial to generalize the success of image detector into the video domain. The main focus of recent methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">30]</ref> towards solving video object detection is improving the performance of per-frame detection by exploiting information in the temporal dimension. These methods could be categorized into local aggregation methods and global aggregation methods.</p><p>Local aggregation methods <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b6">7]</ref> primarily utilize information in local temporal range to aid the detection of current frame. For example, FGFA <ref type="bibr" target="#b36">[36]</ref>, MANet <ref type="bibr" target="#b27">[27]</ref> utilize optical flow predicted by FlowNet <ref type="bibr" target="#b7">[8]</ref> to propagate features across frames. Methods like STSN <ref type="bibr" target="#b0">[1]</ref>, STMN <ref type="bibr" target="#b31">[31]</ref> directly learn to align and aggregate features without optical flow. Besides of these pixel-level aggregation methods, RDN <ref type="bibr" target="#b6">[7]</ref> based on Relation Network <ref type="bibr" target="#b14">[15]</ref> directly learns the relation among candidate boxes of different frames in a local range to enhance the box-level features.</p><p>Global aggregation methods <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">23]</ref> seek to enhance the pixel or box features directly with semantic information. Unlike optical flow or position relationship between boxes that depend on locality in temporal range to some extent, semantic similarity is somewhat independent of temporal distance. However, on the one hand getting rid of locality could enable model utilizing rich information beyond a fixed temporal window, on the other hand the lack of locality information would introduce weakness when localizing.</p><p>Unlike these methods that separately view video object detection globally or locally, MEGA seeks to take full merit of both local and global aggregation to enhance the feature representation. Moreover, enabled by memory, information from longer content could be utilized.</p><p>Information Aggregation beyond Local Range. Like our method that tries to aggregate information beyond a small local range, <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">33]</ref> share a similar spirit and shows superior results in different areas. <ref type="bibr" target="#b29">[29]</ref> also tries to aggregate information both globally and locally, however their "global" is just a larger local range while our "global" is the whole video. <ref type="bibr" target="#b19">[20]</ref> keeps a carefully designed memory to aid video segmentation while ours memory is simpler and already efficient. <ref type="bibr" target="#b28">[28]</ref> shares a similar relation aggregation module with ours. However how this module is instantiated is not the main focus of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we will elaborate how we devise MEGA to enable the whole architecture to fully utilize both global and local information. To be specific, MEGA first aggregates selected global features to local features and afterwards, these global-enhanced local features together with a novel Long Range Memory (LRM) module aggregate longer content of global and local information into key frame for better detection. An overview is depicted in <ref type="figure" target="#fig_2">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>The goal of video object detection is to give detection results for every frame of video {I t } T t=1 . Suppose the current frame to be detected is I k and B t = {b i t } denotes the candidate boxes generated by RPN at each frame I t . All candidate boxes in adjacent frames {I t } k+τ t=k−τ are grouped together to form the local pool, that is,</p><formula xml:id="formula_0">L = {B t } k+τ t=k−τ .</formula><p>For global features, we randomly shuffle the ordered index sequence {1, . . . , T } to obtain a shuffled index sequence S, then we sequentially select T g frames and group all boxes in them to form the global pool. It could be denoted as</p><formula xml:id="formula_1">G = {B Si } k+Tg−1 i=k</formula><p>. At last, a novel long range memory module M stores intermediate features produced in the detection process of previous frame is introduced to allow the key frame to exploit cached information, leading to an ability of modeling longer-term global and local dependency. Our ultimate goal is to give classification and regression results to all candidate boxes B k in the key frame with the help of L, G and M.</p><p>Furthermore, we represent each box b i with its semantic feature f i and localization feature g i . g i represents both the spatial information (i.e., height, width, center location) and the temporal information (i.e., the frame number).</p><p>Relation Module. The operator that we choose to mine the relationship between boxes is the relation module introduced in <ref type="bibr" target="#b14">[15]</ref> which is inspired by the multi-head attention <ref type="bibr" target="#b26">[26]</ref>. Given a set of boxes B = {b i }, object relation module is devised to enhance each box b i by computing M relation features as a weighted sum of semantic features from other boxes, where M denotes the number of heads. Technically, the m-th relation features of b i is computed as</p><formula xml:id="formula_2">f m, * R (bi, B) = j ω m, * ij · (W m V · fj) , m = 1, · · · , M ,<label>(1)</label></formula><p>where W m V is a linear transformation matrix. The relation weight ω m, * ij indicates the impact between b i and b j measured with semantic features f and probably localization features g. Here * ∈ {L, N } denotes whether the localization features g is incorporated in ω, where L means incorporated and N means not. As localization features between two distant boxes in the temporal dimension are redundant and may harm the overall performance, we design the location-free version urging the relation module to focus only on semantic features. Notice that unlike <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref>, temporal information is also contained in our localization features to distinguish the impact of boxes from different frames. This temporal information is incorporated in ω in a relative manner as in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Finally, by concatenating all M relation features and its original feature, we obtain the output augmented feature:</p><formula xml:id="formula_3">f * rm (b i , B) = f i + concat f m, * R (b i , B) M m=1 , (2)</formula><p>where * ∈ {L, N } and the meaning is the same as before.</p><p>After the augmented feature is produced, we additionally append a non-linear transformation function h(·) implemented as a fully-connected layer and ReLU after it.</p><p>Further, We could extend the relation module to modeling the relationship between two sets of boxes. For convenience, we use notation f * rm (B, P ) to represent the collection of all augmented proposal features, i.e., {f * rm (b i , P )}, which means all the bounding boxes in B are augmented via the features of bounding boxes in P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory Enhanced Global-Local Aggregation</head><p>Global-Local Aggregation for ineffective problem. First we will elaborate how we design the network by aggregating global and local features together to address the ineffective problem, which means separately considering global or local information. We denote this architecture as base model and it is depicted in <ref type="figure" target="#fig_2">Figure 2</ref>(a).</p><p>Specifically, the global features from G are aggregated into L at first. The update function could be formulated as:</p><formula xml:id="formula_4">L g = N g (L, G),<label>(3)</label></formula><p>where N g (·) is a function composed of stacked locationfree relation modules and L g denotes the final globalenhanced version of L after this function. As our aim is to fully exploit the potential of global features to enhance local features, we iterate the relation reasoning in a stacked manner equipped with N g relation modules to better characterize the relations between G and L. To be specific, the calculation process in the k-th relation module is given as:</p><formula xml:id="formula_5">L g,k = f N rm (L g,k−1 , G), k = 1, ..., N g ,<label>(4)</label></formula><p>where f N rm (·) denotes location-free relation module, defined in Eq <ref type="formula">(2)</ref>  After that, N l location-based relation modules are utilized to mine the complex spatial-temporal information underlies L (in orange box).</p><p>As can be seen, one frame is only capable to gather information from T l local reference frames and Tg global reference frames. (b) our proposed MEGA with memory size Tm = 3: the introduced novel long range memory module M (in gray box) enables the key frame could get information from far more frames than base model. M caches precomputed intermediate features of local aggregation stack of previous frames. By utilizing these cached information and the recurrent connection empowered by M, one frame is able to access information from totally N l × Tm + T l = 10 local reference frames and N l × Tm + Tg = 10 global reference frames at this time, which is a great improvement over T l = 4 and Tg = 4 of base model. Additionally, as the cached information do not need any update, this makes the introduced computation overhead low. After the final enhanced features of current frame are produced, they will be propagated into traditional RCNN head to give classification and regression result.</p><p>previous relation module as input. Finally, the output of the N g -th relation module is taken as L g . After the global features are aggregated into local features, we then seek to utilize semantic and localization information underlies these local features to further enhance themselves. To achieve this, a stack of N l location-based relation modules is adopted. Technically, the overall function could be summarized as:</p><formula xml:id="formula_6">L l = N l (L g ),<label>(5)</label></formula><p>where L l represents the final enhanced version of local pool. We decompose the whole procedure of N l (·) in below. The computation pipeline in the k-th relation module is similar with its counterpart in N g (·):</p><formula xml:id="formula_7">L l,k = f L rm (L l,k−1 , L l,k−1 ), k = 1, ..., N l ,<label>(6)</label></formula><p>where f L rm (·) denotes location-based relation module and we adopt L g , the global-enhanced version of L, as the input of the first location-based relation module. L l,N l is taken as the output enhanced pool L l . After the final update is done, all box features in L l that belong to the key frame will be extracted and be propagated through traditional RCNN head to give classification and regression result. These extracted features is denoted as C.</p><p>Long Range Memory for insufficient problem. With base model, a single frame is able to aggregate totally T g frames of global features and T l frames of local features as shown in <ref type="figure" target="#fig_2">Figure 2(a)</ref>, which is a big step towards solving the ineffective problem. But the insufficient problem, which stands for the size of frames for key frame to aggregate is too small, is still leaving untouched until now. This problem can be naively solved by increasing T g and T l to approach the length of the video if infinite memory and computation is available. However, this is infeasible because the resource is limited in practice.</p><p>So how could we solve the insufficient problem while keeping the computation cost affordable? Inspired by the recurrence mechanism introduced in [5], we design a novel module named Long Range Memory (LRM) to fulfill this target. To summarize, LRM empowers base model to capture content from much longer content, both globally and locally, by taking full use of precomputed features. We name this memory enhanced version as MEGA. We refer readers to <ref type="figure" target="#fig_2">Figure 2</ref> to recompute everything from the very start. This motivates us to memorize precomputed features to allow current frame to exploit more information in the history. In practice, besides utilizing information from adjacent frames {I t } k+τ t=k−τ , long range memory M of size T m would additionally provide features of T m frames of information ahead of adjacent frames, namely {I t } k−τ −1 t=k−τ −Tm , to help detecting on I k .</p><p>To be specific, after detection process for I k−1 is done, Unlike in base model where we discard all features computed in the detection process, at this time intermediate features of I k−τ −1 , the first frame of adjacent frames of I k−1 , would be cached in long range memory M. That means, as our local aggregation function N l (·) defined in Eq <ref type="formula" target="#formula_6">(5)</ref> is composed of a stack of N l relation modules and L l,i , i ∈ {0, N l } is the features enhanced after the i-th relation module (i = 0 denotes the input), we would extract and store all level of features of</p><formula xml:id="formula_8">I k−τ −1 , namely L l,i k−τ −1 , i ∈ {0, N l } in M. Concretely, M is of N l + 1 levels where M i caches L l,i k−τ −1 .</formula><p>Every time the detection process is done for a new frame, features corresponding to the first frame of adjacent frames of this new frame would be added into M.</p><p>By introducing M and reusing these precomputed features cached in it, we could enhance our local aggregation stage to incorporate the information from M for detection at I k and latter frames. The enhanced version of local aggregation could be summarized as</p><formula xml:id="formula_9">L l = N El (L g , M)<label>(7)</label></formula><p>Like N l (·), N El (·) is also build upon a stack of N l location-based relation module while taking the impact of M into consideration:</p><formula xml:id="formula_10">L l,k = f L rm (L l,k−1 , [L l,k−1 , M k−1 ]), k = 1, ..., N l ,<label>(8)</label></formula><p>where [·, ·] indicates the concatenation of two information pools. Compared to the standard update function, the critical difference lies in the formation of the reference pool. Like in base model, after the final update is done, C will be extracted and be propagated through traditional RCNN head to give classification and regression result of current key frame. The detailed inference process of MEGA is given in Algorithm 1.</p><p>To what extent does LRM address the ineffective and insufficient approximation problem? By appending long range memory M of size T m , a direct increase of T m in feature number is obvious. But the increase of sight is far beyond this number. Please note that our enhanced local stage is of N l stacks, thanks to the recurrent connection introduced by M, the key frame could gather information from additional T m frames every time we iterate the relation reasoning just as shown in <ref type="figure" target="#fig_2">Figure 2</ref>(b). The last and most importantly, as the cached features of every frame are first enhanced by different set of global features, long range memory does not only increase the aggregation size locally, but also globally. To summarize, model with N l -level enhanced local aggregation stage could gather information from totally N l × T m + T l local reference frames and N l × T m + T g global reference frames, where T l , T g , T m denotes the size of local pool, global pool and memory, respectively. It is a great leap towards T l and T g in the base model. With this hugely enlarged aggregation size, we argue the ineffective and insufficient problem is better addressed by our model while not increasing the running time too much. This is further justified by the superior experimental results shown in <ref type="table">Table 1</ref> in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Setup</head><p>We evaluate our proposed methods on ImageNet VID dataset <ref type="bibr" target="#b21">[22]</ref>. The ImageNet VID dataset is a large scale benchmark for video object detection task consisting of 3,862 videos in the training set and 555 videos in the validation set. 30 object categories are contained in this dataset. Following protocols widely adopted in <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b35">35]</ref>, we evaluate our method on the validation set and use mean average precision (mAP) as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Architecture</head><p>Feature Extractor. We mainly use ResNet-101 <ref type="bibr" target="#b13">[14]</ref> and ResNeXt-101 <ref type="bibr" target="#b32">[32]</ref> as the feature extractor. As a common practice in <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">30]</ref>, we enlarge the resolution of feature maps by modifying the stride of the first convolution block in last stage of convolution, namely conv5, from 2 to 1. To retain the receptive field size, the dilation of these convolutional layers is set as 2.</p><p>Detection Network. We use Faster R-CNN <ref type="bibr" target="#b20">[21]</ref> as our detection module. The RPN head is added on the top of conv4 stage. In RPN, the anchors are of 3 aspect ratios {1:2, 1:1, 2:1} and 4 scales {64 2 , 128 2 , 256 2 , 512 2 }, resulting in 12 anchors for each spatial location. During training and inference, N = 300 candidate boxes are generated for each frame at an NMS threshold of 0.7 IoU. After boxes are generated, we apply RoI-Align <ref type="bibr" target="#b12">[13]</ref> and a 1024-D fullyconnected layer after conv5 stage to extract RoI feature for each box.</p><p>MEGA. At training and inference stages, the local temporal window size is set to T l = 25 (τ = 12). Note that the temporal span could be different on two sides of key frame in practice. For the sake of efficiency, we do not keep all candidate boxes produced by RPN for each local reference frame, instead 80 candidate boxes with highest objectness scores are selected. And in the latter stack of local aggregation stage, the number of boxes are further reduced to 20. As for the global reference frame, we totally select T g = 10 frames and 80 proposals with highest objectness scores each frame. The size T m of Long Range Memory is set to 25.</p><p>As for the global and local aggregation stage, the number of relation modules is set to N g = 1 and N l = 3, respectively. For each relation module, the hyperparameter setting is the same as <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Following common protocols in <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">30]</ref>, we train our model on a combination of ImageNet VID and DET datasets. For the DET dataset, we select images that are of the same 30 classes as in the VID dataset. We implement MEGA mainly on maskrcnn-benchmark <ref type="bibr" target="#b18">[19]</ref>. The iuput images are resized to have their shorter side to be 600 pixels. The whole architecture is trained on 4 RTX 2080ti GPUs with SGD. Each GPU holds one mini-batch and each mini-batch contains one set of images or frames. We train our network for totally 120K iteration, with learning rate 10 −3 and 10 −4 in the first 80K and in the last 40K iterations, respectively. At inference, an NMS of 0.5 IoU threshold is adopted to suppress reduplicate detection boxes.</p><p>Training. As the number of boxes prohibits the training process to be the same as the inference process, we adopt the strategy of temporal dropout <ref type="bibr" target="#b36">[36]</ref> to train our model. Given key frame I k , we randomly sample two frames from {I t } k+τ t=k−τ and two frames from the whole video to approximately form L, G. Additional two frames from {I t } k−τ −1 t=k−τ −Tm are selected to construct M. For convenience, we name them asL,Ĝ andL M . We first apply base model onĜ andL M and store all intermediate features produced asM. After that,L,Ĝ,M are propagated through full MEGA to generate C. Finally, the whole model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Backbone local global mAP(%) FGFA <ref type="bibr" target="#b36">[36]</ref> ResNet-101 76.3 MANet <ref type="bibr" target="#b27">[27]</ref> ResNet-101 78.1 THP <ref type="bibr" target="#b35">[35]</ref> ResNet-101+DCN 78.6 STSN <ref type="bibr" target="#b0">[1]</ref> ResNet-101+DCN 78.9 OGEMN <ref type="bibr" target="#b5">[6]</ref> ResNet-101+DCN 80.0 SELSA <ref type="bibr" target="#b30">[30]</ref> ResNet-101 80.3 RDN <ref type="bibr" target="#b6">[7]</ref> ResNet-101 81.8 RDN <ref type="bibr" target="#b6">[7]</ref> ResNeXt-101 83.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MEGA (ours)</head><p>ResNet-101 82.9 ResNeXt-101 84.1 <ref type="table">Table 1</ref>. Performance comparison with state-of-the-art end-to-end video object detection models on ImageNet VID validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Backbone mAP(%) FGFA <ref type="bibr" target="#b36">[36]</ref> ResNet-101 78.4 ST-Lattice <ref type="bibr" target="#b3">[4]</ref> ResNet-101 79.6 MANet <ref type="bibr" target="#b27">[27]</ref> ResNet-101 80.3 D&amp;T <ref type="bibr" target="#b8">[9]</ref> ResNet-101 80.2 STSN <ref type="bibr" target="#b0">[1]</ref> ResNet-101+DCN 80.4 STMN <ref type="bibr" target="#b31">[31]</ref> ResNet-101 80.5 SELSA <ref type="bibr" target="#b30">[30]</ref> ResNet-101 80.5 OGEMN <ref type="bibr" target="#b5">[6]</ref> ResNet-101+DCN 81.6 RDN <ref type="bibr" target="#b6">[7]</ref> ResNet-101 83.8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MEGA (ours)</head><p>ResNet-101 84.5 FGFA <ref type="bibr" target="#b36">[36]</ref> Inception-ResNet 80.1 D&amp;T <ref type="bibr" target="#b8">[9]</ref> Inception-v4 82.0 RDN <ref type="bibr" target="#b6">[7]</ref> ResNeXt-101 84.7 MEGA (ours)</p><p>ResNeXt-101 85.4 <ref type="table">Table 2</ref>. Performance comparison with state-of-the-art video object detection models with post-processing methods (e.g. Seq-NMS, Tube Rescoring, BLR).</p><p>are trained with classification and regression losses over C.</p><p>One thing needed to be pointed out is that we stop the gradient flow in the construction ofM. This behavior is similar to <ref type="bibr" target="#b4">[5]</ref> but with different motivation: we would like the model to pay more attention on most adjacent frames. <ref type="table">Table 1</ref> shows the result comparison between stateof-the-art end-to-end models without any post-processing. Among all methods, MEGA achieves the best performance and is the only method that takes full use of both global and local information. With ResNet-101 backbone, MEGA can achieve 82.9% mAP, 1.1% absolute improvement over the strongest competitor RDN. By replacing the backbone feature extractor from ResNet-101 to a stronger backbone ResNeXt-101, our method achieves better performance of 84.1% mAP just as expected. Among all competitors, RDN is the most representative method in local aggregation scheme while SELSA is the one in global aggregation scheme. RDN only models the relationship within a short local temporal range, while SELSA only builds sparse global connection. As discussed in Section 1, these methods may suffer from the ineffective and insufficient approxima-  tion problems which lead to inferior results than ours. Like many previous methods that could get further improvement by post processing, it could also benefit ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Main Results</head><p>Here the post processing technique we adopt is BLR <ref type="bibr" target="#b6">[7]</ref>, which is done by finding optimal paths in the whole video and then re-score detection boxes in each path. <ref type="table">Table 2</ref> summarizes the results of state-of-the-art methods with different post-processing techniques. With no doubt, our method still performs the best, obtaining 84.5% and 85.4% mAP with backbone ResNet-101 and ResNeXt-101, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>To examine the impact of the key components in our MEGA, we conduct extensive experiments to study how they contribute to our final performance.</p><p>Long Range Memory. We would first explore the effect of LRM as it plays the key role in our full model. We show the performance comparison results between base model and MEGA in <ref type="table" target="#tab_1">Table 3</ref>. As shown in the table, a gap of 1.5% mAP exists between these two models, which is a significant improvement. Gap of 1% mAP still exists after increasing base model's local span τ to τ + Tm 2 , while running at a much slower speed. We argue the superior performance of MEGA is brought by the novel LRM module which enables one frame could gather information efficiently from much longer content, both globally and locally. <ref type="table" target="#tab_2">Table 4</ref> shows the results of removing global or local information from MEGA. In the default setting, the number of relation modules in the global aggregation stage and local aggregation stage is N g = 1 and N l = 3, respectively. To study the effect of global features, We simply set N g to 0 to remove the influence from it. As shown in the table, MEGA experiences a sheer performance loss of 1.6% mAP by removing the global features. We also conduct an experiment by setting N l to 4 which means to keep the number of parameters as the same as MEGA though it is not fully comparable (the local range is larger). And this experiment gives 81.6% mAP which is still lower. The above results show the importance of global feature aggregation.</p><p>To see the importance of local information, we conduct an experiment by setting N g to 4 and N l to 0, also the local temporal window size T l and the number of global reference frames T g is changed to 1 and 25, respectively. Under this setting, one frame could only enhanced by global information while keeping the number of parameters the same as MEGA. The result is given in the last row of  <ref type="table" target="#tab_2">Table 4</ref>. Ablation study on the global and local feature aggregation. N l and Ng is the number of relation modules in local aggregation stage and global aggregation stage, respectively. By setting N l or Ng to 0 removes the influence of local or global information.</p><p>can be seen, this result is 1.1% lower than our full model, indicating the necessity of local features. From <ref type="table" target="#tab_1">Table 3</ref> and 4, an interesting conclusion could be drawn. It is the combination of local features, global features and memory that empowers MEGA to obtain superior result, and three components alone is not sufficient. With memory, one frame could get access to more global and local features and in turn, this enhanced frame could offer us a more compact memory. This manner justifies our intuition that a better approximation of <ref type="figure" target="#fig_0">Figure 1(a)</ref> is a promising direction to boost the performance on video object detection. <ref type="figure">Figure 3</ref> showcases one hard example in video object detection. As the lizard is presented in a rare pose for quite a while, only exploiting local information is not able to tell us what it is. The top two rows shows the result of single frame and local aggregation. They are not able to recognize the object with just local information. By taking global features into consideration, the model overcomes this difficult case by aggregating features from distinct object from global frames. The last two rows show the result of base model and MEGA. The result gets better when more information is incorporated just as expected.</p><p>Aggregation Scale. The aggregation scale here means the content one frame could gather from, globally or locally. Totally four hyperparameters, global reference frame number T g , local reference frame number T l , number of relation modules in local aggregation stage N l and memory size T m would influence the aggregation scale. The result is given in <ref type="table">Table 5</ref>. (a) The result of different number of T g . As can be seen, there is only minor difference between them. As discussed in Section 3.2, the number of global reference frames one frame could see is N l × T m + T g , which indicates the influence from T g is minor compared to N l and T m . (b) As for T l , the result is similar with T g . (c)(d) The result for N l and T m . These two parameters jointly influence the global and local range of MEGA. The performance of MEGA gets worse when N l or T m is small, which implies larger aggregation scale matters. The improvement saturates when N l or T m gets bigger, which may suggest that a proper approximation is sufficient enough.</p><p>Types of Relation Modules. As we has discussed earlier, we differentiate two types of relation modules by whether incorporating the location information into the re-  <ref type="table">Table 5</ref>. Ablation study on different global reference frame number Tg, local reference frame number T l , number of relation modules in local aggregation stage N l and memory size Tm. Default parameter is indicated by *. lation weights. By incorporating location information in global aggregation stage, we obtain inferior result of 82.5% mAP, which verifies that incorporating the location information to the global aggregation stage would harm the overall performance. Online Setting. How would our model behave if current frame could only access information from previous frames? We refer this setting as online setting, which is an often confronted situation in practice. In experiment, we still keep the number of local reference frames T l to be 25 while modifying the adjacent frames of key frame I k to be {I t } k t=k−T l +1 . As for global information, the sampling range is limited to all previous frames of current frame while keeping global sampling number T g the same as before. With this limited sight, the modified MEGA could also obtain 81.9% mAP. To the best of our knowledge, the previous state-of-the-art performance under the same setting is OGEMN's 80.0% mAP, which uses a variant of memory network to boost detection. As memory network could be treated as a way to build sparse connection among different frames, it could be classified into global aggregation scheme. This method also suffers from the ineffective and insufficient problem, which results in inferior performance than ours.</p><p>Running Time. The running time of our model is given in <ref type="table" target="#tab_1">Table 3</ref>. Compared to the base model, MEGA boosts the overall performance by a large margin while only introducing little computation overhead. This result suggests the efficiency of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present Memory Enhance Global-Local Aggregation Network (MEGA), which takes a joint view of both global and local information aggregation to solve video object detection. Particularly, we first thoroughly analyze the ineffecitve and insufficient problems existed in recent methods. Then we propose to solve these problems in a two-stage manner, in the first stage we aggregate global features into local features towards solving the ineffective problem. Afterwards, a novel Long Range Memory is introduced to solve the insufficient problem. Experiments conducted on ImageNet VID dataset validate the effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The aggregation size of different methods for addressing video object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>and L g,0 = L denotes the input of the first relation module. Latter relation module takes the output from An overview of base model and our proposed MEGA. For the purpose of illustration convenience, the key frame is placed on the very right. (a) base model with local pool size T l = 4, global pool size Tg = 4, local aggregation stage N l = 2: we first aggregate global information from G into L (in blue box). In practice, this stage is instantiated as a stack of Ng location-free relation modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b) for an overview of how MEGA works.To see the defect of base model, suppose I k−1 and I k are two consecutive frames. When we shift the detection process from I k−1 to I k , we discard all intermediate features produced by the detection on I k−1 . So the detection process of I k could not take any merit of the detection process of I k−1 , though they are consecutive in the temporal dimension. Every time we move to a new frame, we needAlgorithm 1 Inference Algorithm of MEGA 1: Input: video frames {It} of length T 2: Initialize: long range memory M to be empty 3: for t = 1 to τ + 1 do 4: Bt = N RP N (It) // generate candidate boxes of It 5: end for 6: // shuffle the frame index for random global feature selection 7: S = shuffle(1 . . . T ) 8: for t = 1 to Tg do 9: B S t = N RP N (I S t ) // generate candidate boxes of I S t 10: end for 11: for k = 1 to T do 12: L = {Bt} k+τ t=k−τ // form local pool with adjacent frames 13: G = {B S t } k+Tg −1 t=k // form global pool with random frames 14: L g = Ng(L, G) // global agg. stage with Eq (3) 15: L l = N El (L g , M) // enhanced local agg. stage with Eq (7) 16: C =Select-I k (L l ) // extract enhanced features of key frame I k 17: D k = N RCN N (C) // detect on the key frame 18: Update(M, L l, * k−τ ) // update the long range memory 19: B k+τ +1 = N RP N (I k+τ +1 ) 20: B S k+Tg = N RP N (I S k+Tg ) 21: end for 22: Output: The final detection result of the video: {D k }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Performance of base model and MEGA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>As</figDesc><table><row><cell>Method</cell><cell>Ng</cell><cell>N l</cell><cell>mAP(%)</cell></row><row><cell>MEGA</cell><cell>1</cell><cell>3</cell><cell>82.9</cell></row><row><cell>MEGA (no global stage)</cell><cell>0 0</cell><cell>3 4</cell><cell>81.3 81.6</cell></row><row><cell>MEGA (no local stage)</cell><cell>4</cell><cell>0</cell><cell>81.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>... ...Figure 3. Example detection results of methods that incorporates different amount of local and global information.</figDesc><table><row><cell cols="2">Single Frame Faster R-CNN</cell><cell></cell><cell></cell><cell></cell><cell>lizard 0.245</cell></row><row><cell>Local Agg.</cell><cell>only</cell><cell></cell><cell>...</cell><cell></cell><cell>lizard 0.302</cell><cell>...</cell></row><row><cell>Base Model</cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell>lizard 0.637 lizard</cell><cell>...</cell></row><row><cell>MEGA</cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell>0.786</cell><cell>...</cell></row><row><cell cols="5">(a) Global reference frame number Tg</cell><cell></cell></row><row><cell>Tg</cell><cell>5</cell><cell>10*</cell><cell>15</cell><cell>20</cell><cell></cell></row><row><cell>mAP (%)</cell><cell>82.7</cell><cell>82.9</cell><cell>82.9</cell><cell>83.0</cell><cell></cell></row><row><cell>runtime (ms)</cell><cell>111.6</cell><cell>114.5</cell><cell>117.4</cell><cell>124.2</cell><cell></cell></row><row><cell cols="5">(b) Local reference frame number T l</cell><cell></cell></row><row><cell>T l</cell><cell>13</cell><cell>17</cell><cell>21</cell><cell>25*</cell><cell>29</cell></row><row><cell>mAP (%)</cell><cell>82.6</cell><cell>82.7</cell><cell>82.8</cell><cell>82.9</cell><cell>82.9</cell></row><row><cell>runtime (ms)</cell><cell>99.2</cell><cell>105.9</cell><cell>109.7</cell><cell>114.5</cell><cell>122.1</cell></row><row><cell cols="5">(c) number of relation modules in local agg. stage N l</cell><cell></cell></row><row><cell>N l</cell><cell>1</cell><cell>2</cell><cell>3*</cell><cell>4</cell><cell></cell></row><row><cell>mAP (%)</cell><cell>82.1</cell><cell>82.5</cell><cell>82.9</cell><cell>83.0</cell><cell></cell></row><row><cell>runtime (ms)</cell><cell>100.6</cell><cell>108.7</cell><cell>114.5</cell><cell>122.3</cell><cell></cell></row><row><cell></cell><cell cols="3">(d) Memory size Tm</cell><cell></cell><cell></cell></row><row><cell>Tm</cell><cell>5</cell><cell>15</cell><cell>25*</cell><cell>35</cell><cell>45</cell></row><row><cell>mAP (%)</cell><cell>82.0</cell><cell>82.3</cell><cell>82.9</cell><cell>82.9</cell><cell>83.0</cell></row><row><cell>runtime (ms)</cell><cell>111.3</cell><cell>113</cell><cell>114.5</cell><cell>115.4</cell><cell>116.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by National Key R&amp;D Program of China (2018YFB1402600), BJNSF (L172037), Beijing Acedemy of Artificial Intelligence and Zhejiang Lab.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="342" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimizing video object detection via a scale-time lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7814" to="7823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object guided external memory network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanming</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongpu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relation distillation networks for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3057" to="3065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Seq-nms for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooya</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>arxiv, abs/1602.08465</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="817" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="765" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9225" to="9234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Leveraging long-range temporal relationships between proposals for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9755" to="9763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeruIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully motion-aware network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Donghyeon Cho, and In So Kweon. Linknet: Relational embedding for scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="558" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Longterm feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence level semantics aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video object detection with an aligned spatial-temporal memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="494" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatialtemporal relation networks for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3988" to="3998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards high performance video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7210" to="7218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
