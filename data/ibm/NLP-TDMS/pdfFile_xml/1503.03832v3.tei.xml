<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
							<email>fschroff@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
							<email>jphilbin@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc. Dmitry Kalenichenko</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite significant recent advances in the field of face recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.</p><p>Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face.</p><p>On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result [15] by 30% on both datasets.</p><p>We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper we present a unified system for face verification (is this the same person), recognition (who is this person) and clustering (find common people among these faces). Our method is based on learning a Euclidean embedding per image using a deep convolutional network. The network is trained such that the squared L2 distances in the embedding space directly correspond to face similarity:  faces of the same person have small distances and faces of distinct people have large distances. Once this embedding has been produced, then the aforementioned tasks become straight-forward: face verification simply involves thresholding the distance between the two embeddings; recognition becomes a k-NN classification problem; and clustering can be achieved using off-theshelf techniques such as k-means or agglomerative clustering.</p><p>Previous face recognition approaches based on deep networks use a classification layer <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref> trained over a set of known face identities and then take an intermediate bottle-neck layer as a representation used to generalize recognition beyond the set of identities used in training. The downsides of this approach are its indirectness and its inefficiency: one has to hope that the bottleneck representation generalizes well to new faces; and by using a bottleneck layer the representation size per face is usually very large (1000s of dimensions). Some recent work <ref type="bibr" target="#b14">[15]</ref> has reduced this dimensionality using PCA, but this is a linear transformation that can be easily learnt in one layer of the network.</p><p>In contrast to these approaches, FaceNet directly trains its output to be a compact 128-D embedding using a tripletbased loss function based on LMNN <ref type="bibr" target="#b18">[19]</ref>. Our triplets consist of two matching face thumbnails and a non-matching face thumbnail and the loss aims to separate the positive pair from the negative by a distance margin. The thumbnails are tight crops of the face area, no 2D or 3D alignment, other than scale and translation is performed.</p><p>Choosing which triplets to use turns out to be very important for achieving good performance and, inspired by curriculum learning <ref type="bibr" target="#b0">[1]</ref>, we present a novel online negative exemplar mining strategy which ensures consistently increasing difficulty of triplets as the network trains. To improve clustering accuracy, we also explore hard-positive mining techniques which encourage spherical clusters for the embeddings of a single person.</p><p>As an illustration of the incredible variability that our method can handle see <ref type="figure" target="#fig_0">Figure 1</ref>. Shown are image pairs from PIE <ref type="bibr" target="#b12">[13]</ref> that previously were considered to be very difficult for face verification systems.</p><p>An overview of the rest of the paper is as follows: in section 2 we review the literature in this area; section 3.1 defines the triplet loss and section 3.2 describes our novel triplet selection and training procedure; in section 3.3 we describe the model architecture used. Finally in section 4 and 5 we present some quantitative results of our embeddings and also qualitatively explore some clustering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Similarly to other recent works which employ deep networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>, our approach is a purely data driven method which learns its representation directly from the pixels of the face. Rather than using engineered features, we use a large dataset of labelled faces to attain the appropriate invariances to pose, illumination, and other variational conditions.</p><p>In this paper we explore two different deep network architectures that have been recently used to great success in the computer vision community. Both are deep convolutional networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>. The first architecture is based on the Zeiler&amp;Fergus <ref type="bibr" target="#b21">[22]</ref> model which consists of multiple interleaved layers of convolutions, non-linear activations, local response normalizations, and max pooling layers. We additionally add several 1×1×d convolution layers inspired by the work of <ref type="bibr" target="#b8">[9]</ref>. The second architecture is based on the Inception model of Szegedy et al. which was recently used as the winning approach for ImageNet 2014 <ref type="bibr" target="#b15">[16]</ref>. These networks use mixed layers that run several different convolutional and pooling layers in parallel and concatenate their responses. We have found that these models can reduce the number of parameters by up to 20 times and have the potential to reduce the number of FLOPS required for comparable performance.</p><p>There is a vast corpus of face verification and recognition works. Reviewing it is out of the scope of this paper so we will only briefly discuss the most relevant recent work.</p><p>The works of <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref> all employ a complex system of multiple stages, that combines the output of a deep convolutional network with PCA for dimensionality reduction and an SVM for classification.</p><p>Zhenyao et al. <ref type="bibr" target="#b22">[23]</ref> employ a deep network to "warp" faces into a canonical frontal view and then learn CNN that classifies each face as belonging to a known identity. For face verification, PCA on the network output in conjunction with an ensemble of SVMs is used.</p><p>Taigman et al. <ref type="bibr" target="#b16">[17]</ref> propose a multi-stage approach that aligns faces to a general 3D shape model. A multi-class network is trained to perform the face recognition task on over four thousand identities. The authors also experimented with a so called Siamese network where they directly optimize the L 1 -distance between two face features. Their best performance on LFW (97.35%) stems from an ensemble of three networks using different alignments and color channels. The predicted distances (non-linear SVM predictions based on the χ 2 kernel) of those networks are combined using a non-linear SVM.</p><p>Sun et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> propose a compact and therefore relatively cheap to compute network. They use an ensemble of 25 of these network, each operating on a different face patch. For their final performance on LFW (99.47% <ref type="bibr" target="#b14">[15]</ref>) the authors combine 50 responses (regular and flipped). Both PCA and a Joint Bayesian model <ref type="bibr" target="#b1">[2]</ref> that effectively correspond to a linear transform in the embedding space are employed. Their method does not require explicit 2D/3D alignment. The networks are trained by using a combination of classification and verification loss. The verification loss is similar to the triplet loss we employ <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>, in that it minimizes the L 2 -distance between faces of the same identity and enforces a margin between the distance of faces of different identities. The main difference is that only pairs of images are compared, whereas the triplet loss encourages a relative distance constraint.</p><p>A similar loss to the one used here was explored in Wang et al. <ref type="bibr" target="#b17">[18]</ref> for ranking images by semantic and visual similarity.  <ref type="figure">Figure 2</ref>. Model structure. Our network consists of a batch input layer and a deep CNN followed by L2 normalization, which results in the face embedding. This is followed by the triplet loss during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anchor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative</head><p>Anchor Positive Negative LEARNING <ref type="figure">Figure 3</ref>. The Triplet Loss minimizes the distance between an anchor and a positive, both of which have the same identity, and maximizes the distance between the anchor and a negative of a different identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>FaceNet uses a deep convolutional network. We discuss two different core architectures: The Zeiler&amp;Fergus <ref type="bibr" target="#b21">[22]</ref> style networks and the recent Inception <ref type="bibr" target="#b15">[16]</ref> type networks. The details of these networks are described in section 3.3.</p><p>Given the model details, and treating it as a black box (see <ref type="figure">Figure 2</ref>), the most important part of our approach lies in the end-to-end learning of the whole system. To this end we employ the triplet loss that directly reflects what we want to achieve in face verification, recognition and clustering. Namely, we strive for an embedding f (x), from an image x into a feature space R d , such that the squared distance between all faces, independent of imaging conditions, of the same identity is small, whereas the squared distance between a pair of face images from different identities is large.</p><p>Although we did not directly compare to other losses, e.g. the one using pairs of positives and negatives, as used in <ref type="bibr" target="#b13">[14]</ref> Eq. (2), we believe that the triplet loss is more suitable for face verification. The motivation is that the loss from <ref type="bibr" target="#b13">[14]</ref> encourages all faces of one identity to be projected onto a single point in the embedding space. The triplet loss, however, tries to enforce a margin between each pair of faces from one person to all other faces. This allows the faces for one identity to live on a manifold, while still enforcing the distance and thus discriminability to other identities.</p><p>The following section describes this triplet loss and how it can be learned efficiently at scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Triplet Loss</head><p>The embedding is represented by f (x) ∈ R d . It embeds an image x into a d-dimensional Euclidean space. Additionally, we constrain this embedding to live on the d-dimensional hypersphere, i.e. f (x) 2 = 1. This loss is motivated in <ref type="bibr" target="#b18">[19]</ref> in the context of nearest-neighbor classification. Here we want to ensure that an image x a i (anchor) of a specific person is closer to all other images x p i (positive) of the same person than it is to any image x n i (negative) of any other person. This is visualized in <ref type="figure">Figure 3</ref>.</p><p>Thus we want,</p><formula xml:id="formula_0">f (x a i ) − f (x p i ) 2 2 + α &lt; f (x a i ) − f (x n i ) 2 2 ,<label>(1)</label></formula><formula xml:id="formula_1">∀ (f (x a i ), f (x p i ), f (x n i )) ∈ T .<label>(2)</label></formula><p>where α is a margin that is enforced between positive and negative pairs. T is the set of all possible triplets in the training set and has cardinality N . The loss that is being minimized is then L =</p><formula xml:id="formula_2">N i f (x a i ) − f (x p i ) 2 2 − f (x a i ) − f (x n i ) 2 2 + α + .</formula><p>(3) Generating all possible triplets would result in many triplets that are easily satisfied (i.e. fulfill the constraint in Eq. <ref type="formula" target="#formula_0">(1)</ref>). These triplets would not contribute to the training and result in slower convergence, as they would still be passed through the network. It is crucial to select hard triplets, that are active and can therefore contribute to improving the model. The following section talks about the different approaches we use for the triplet selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Triplet Selection</head><p>In order to ensure fast convergence it is crucial to select triplets that violate the triplet constraint in Eq. (1). This means that, given x a i , we want to select an</p><formula xml:id="formula_3">x p i (hard pos- itive) such that argmax x p i f (x a i ) − f (x p i ) 2 2 and similarly x n i (hard negative) such that argmin x n i f (x a i ) − f (x n i ) 2 2</formula><p>. It is infeasible to compute the argmin and argmax across the whole training set. Additionally, it might lead to poor training, as mislabelled and poorly imaged faces would dominate the hard positives and negatives. There are two obvious choices that avoid this issue:</p><p>• Generate triplets offline every n steps, using the most recent network checkpoint and computing the argmin and argmax on a subset of the data.</p><p>• Generate triplets online. This can be done by selecting the hard positive/negative exemplars from within a mini-batch.</p><p>Here, we focus on the online generation and use large mini-batches in the order of a few thousand exemplars and only compute the argmin and argmax within a mini-batch.</p><p>To have a meaningful representation of the anchorpositive distances, it needs to be ensured that a minimal number of exemplars of any one identity is present in each mini-batch. In our experiments we sample the training data such that around 40 faces are selected per identity per minibatch. Additionally, randomly sampled negative faces are added to each mini-batch.</p><p>Instead of picking the hardest positive, we use all anchorpositive pairs in a mini-batch while still selecting the hard negatives. We don't have a side-by-side comparison of hard anchor-positive pairs versus all anchor-positive pairs within a mini-batch, but we found in practice that the all anchorpositive method was more stable and converged slightly faster at the beginning of training.</p><p>We also explored the offline generation of triplets in conjunction with the online generation and it may allow the use of smaller batch sizes, but the experiments were inconclusive.</p><p>Selecting the hardest negatives can in practice lead to bad local minima early on in training, specifically it can result in a collapsed model (i.e. f (x) = 0). In order to mitigate this, it helps to select x n i such that</p><formula xml:id="formula_4">f (x a i ) − f (x p i ) 2 2 &lt; f (x a i ) − f (x n i ) 2 2 .<label>(4)</label></formula><p>We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchorpositive distance. Those negatives lie inside the margin α.</p><p>As mentioned before, correct triplet selection is crucial for fast convergence. On the one hand we would like to use small mini-batches as these tend to improve convergence during Stochastic Gradient Descent (SGD) <ref type="bibr" target="#b19">[20]</ref>. On the other hand, implementation details make batches of tens to hundreds of exemplars more efficient. The main constraint with regards to the batch size, however, is the way we select hard relevant triplets from within the mini-batches. In most experiments we use a batch size of around 1,800 exemplars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deep Convolutional Networks</head><p>In all our experiments we train the CNN using Stochastic Gradient Descent (SGD) with standard backprop <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref> and AdaGrad <ref type="bibr" target="#b4">[5]</ref>. In most experiments we start with a learning rate of 0.05 which we lower to finalize the model. The models are initialized from random, similar to <ref type="bibr" target="#b15">[16]</ref>, and trained on a CPU cluster for 1,000 to 2,000 hours. The decrease in the loss (and increase in accuracy) slows down drastically after 500h of training, but additional training can still significantly improve performance. The margin α is set to 0.2.</p><p>We used two types of architectures and explore their trade-offs in more detail in the experimental section. Their practical differences lie in the difference of parameters and FLOPS. The best model may be different depending on the application. E.g. a model running in a datacenter can have many parameters and require a large number of FLOPS, whereas a model running on a mobile phone needs to have few parameters, so that it can fit into memory. All our  <ref type="bibr" target="#b21">[22]</ref> based model with 1×1 convolutions inspired by <ref type="bibr" target="#b8">[9]</ref>.</p><formula xml:id="formula_5">layer size-in size-out kernel param FLPS conv1 220×220×3 110×110×64 7×7×3, 2 9K 115M pool1 110×110×64 55×55×64 3×3×64, 2 0 rnorm1 55×55×64 55×55×64 0 conv2a 55×55×64 55×55×64 1×1×64, 1 4K 13M conv2 55×55×64 55×55×192 3×3×64, 1 111K 335M rnorm2 55×55×192 55×55×192 0 pool2 55×55×192 28×28×192 3×3×192, 2 0 conv3a 28×28×192 28×28×192 1×1×192, 1 37K 29M conv3 28×28×192 28×28×384 3×3×192, 1 664K 521M pool3 28×28×384 14×14×384 3×3×384, 2 0 conv4a 14×14×384 14×14×384 1×1×384, 1 148K 29M conv4 14×14×384 14×14×256 3×3×384, 1 885K 173M conv5a 14×14×256 14×14×256 1×1×256, 1 66K 13M conv5 14×14×256 14×14×256 3×3×256, 1 590K 116M conv6a 14×14×256 14×14×256 1×1×256, 1 66K 13M conv6 14×14×256 14×14×256 3×3×256, 1 590K 116M pool4 14×14×256 7×7×256 3×3×256, 2 0 concat 7×7×256 7×7×256 0 fc1 7×7×256 1×32×128 maxout p=2 103M 103M fc2 1×32×128 1×32×128 maxout p=2 34M 34M fc7128 1×32×128 1×1×128 524K 0.5M L2 1×1×128 1×1×128 0 total 140M 1.6B</formula><p>The input and output sizes are described in rows × cols × #f ilters.</p><p>The kernel is specified as rows × cols, stride and the maxout <ref type="bibr" target="#b5">[6]</ref> pooling size as p = 2.</p><p>models use rectified linear units as the non-linear activation function.</p><p>The first category, shown in <ref type="table" target="#tab_1">Table 1</ref>, adds 1×1×d convolutional layers, as suggested in <ref type="bibr" target="#b8">[9]</ref>, between the standard convolutional layers of the Zeiler&amp;Fergus <ref type="bibr" target="#b21">[22]</ref> architecture and results in a model 22 layers deep. It has a total of 140 million parameters and requires around 1.6 billion FLOPS per image.</p><p>The second category we use is based on GoogLeNet style Inception models <ref type="bibr" target="#b15">[16]</ref>. These models have 20× fewer parameters (around 6.6M-7.5M) and up to 5× fewer FLOPS (between 500M-1.6B). Some of these models are dramatically reduced in size (both depth and number of filters), so that they can be run on a mobile phone. One, NNS1, has 26M parameters and only requires 220M FLOPS per image. The other, NNS2, has 4.3M parameters and 20M FLOPS. <ref type="table">Table 2</ref> describes NN2 our largest network in detail. NN3 is identical in architecture but has a reduced input size of 160x160. NN4 has an input size of only 96x96, thereby drastically reducing the CPU requirements (285M FLOPS vs 1.6B for NN2). In addition to the reduced input size it does not use 5x5 convolutions in the higher layers as the receptive field is already too small by then. Generally we found that the 5x5 convolutions can be removed throughout with only a minor drop in accuracy. <ref type="figure">Figure 4</ref> compares all our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets and Evaluation</head><p>We evaluate our method on four datasets and with the exception of Labelled Faces in the Wild and YouTube Faces we evaluate our method on the face verification task. I.e. given a pair of two face images a squared L 2 distance threshold D(x i , x j ) is used to determine the classification of same and different. All faces pairs (i, j) of the same identity are denoted with P same , whereas all pairs of different identities are denoted with P diff .</p><p>We define the set of all true accepts as</p><formula xml:id="formula_6">TA(d) = {(i, j) ∈ P same , with D(x i , x j ) ≤ d} .<label>(5)</label></formula><p>These are the face pairs (i, j) that were correctly classified as same at threshold d. Similarly</p><formula xml:id="formula_7">FA(d) = {(i, j) ∈ P diff , with D(x i , x j ) ≤ d}<label>(6)</label></formula><p>is the set of all pairs that was incorrectly classified as same (false accept). The validation rate VAL(d) and the false accept rate FAR(d) for a given face distance d are then defined as</p><formula xml:id="formula_8">VAL(d) = |TA(d)| |P same | , FAR(d) = |FA(d)| |P diff | .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hold-out Test Set</head><p>We keep a hold out set of around one million images, that has the same distribution as our training set, but disjoint identities. For evaluation we split it into five disjoint sets of 200k images each. The FAR and VAL rate are then computed on 100k × 100k image pairs. Standard error is reported across the five splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Personal Photos</head><p>This is a test set with similar distribution to our training set, but has been manually verified to have very clean labels. It consists of three personal photo collections with a total of around 12k images. We compute the FAR and VAL rate across all 12k squared pairs of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Academic Datasets</head><p>Labeled Faces in the Wild (LFW) is the de-facto academic test set for face verification <ref type="bibr" target="#b6">[7]</ref>. We follow the standard protocol for unrestricted, labeled outside data and report the mean classification accuracy as well as the standard error of the mean.</p><p>Youtube Faces DB <ref type="bibr" target="#b20">[21]</ref> is a new dataset that has gained popularity in the face recognition community <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b14">15]</ref>. The setup is similar to LFW, but instead of verifying pairs of images, pairs of videos are used.  <ref type="figure">Figure 4</ref>. FLOPS vs. Accuracy trade-off. Shown is the trade-off between FLOPS and accuracy for a wide range of different model sizes and architectures. Highlighted are the four models that we focus on in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>If not mentioned otherwise we use between 100M-200M training face thumbnails consisting of about 8M different identities. A face detector is run on each image and a tight bounding box around each face is generated. These face thumbnails are resized to the input size of the respective network. Input sizes range from 96x96 pixels to 224x224 pixels in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Computation Accuracy Trade-off</head><p>Before diving into the details of more specific experiments we will discuss the trade-off of accuracy versus number of FLOPS that a particular model requires. <ref type="figure">Figure 4</ref> shows the FLOPS on the x-axis and the accuracy at 0.001 false accept rate (FAR) on our user labelled test-data set from section 4.2. It is interesting to see the strong correlation between the computation a model requires and the accuracy it achieves. The figure highlights the five models (NN1, NN2, NN3, NNS1, NNS2) that we discuss in more detail in our experiments.</p><p>We also looked into the accuracy trade-off with regards to the number of model parameters. However, the picture is not as clear in that case. For example, the Inception based model NN2 achieves a comparable performance to NN1, but only has a 20th of the parameters. The number of FLOPS is comparable, though. Obviously at some point the performance is expected to decrease, if the number of parameters is reduced further. Other model architectures may allow further reductions without loss of accuracy, just like Inception <ref type="bibr" target="#b15">[16]</ref>   <ref type="table">Table 2</ref>. NN2. Details of the NN2 Inception incarnation. This model is almost identical to the one described in <ref type="bibr" target="#b15">[16]</ref>. The two major differences are the use of L2 pooling instead of max pooling (m), where specified. I.e. instead of taking the spatial max the L2 norm is computed. The pooling is always 3×3 (aside from the final average pooling) and in parallel to the convolutional modules inside each Inception module. If there is a dimensionality reduction after the pooling it is denoted with p. 1×1, 3×3, and 5×5 pooling are then concatenated to get the final output.</p><formula xml:id="formula_9">NN2 NN1 NNS1 NNS2 1E6 1E6 1E6 1E5 1E5 1E5 1E4 1E4 1E4 1E3 1E3 1E3 1E2 1E2 1E2 1E1 1E1 1E1 1E0 1E0 1E0 1E1 1E1 1E1 5E1 5E1 5E1 1E0 1E0 1E0</formula><p>FAR FAR FAR VAL VAL VAL <ref type="figure">Figure 5</ref>. architecture VAL NN1 (Zeiler&amp;Fergus 220×220) 87.9% ± 1.9 NN2 <ref type="table">(Inception 224×224)</ref> 89.4% ± 1.6 NN3 <ref type="table" target="#tab_1">(Inception 160×160)</ref> 88.3% ± 1.7 NN4 <ref type="table">(Inception 96×96)</ref> 82.0% ± 2.3 NNS1 (mini Inception 165×165) 82.4% ± 2.4 NNS2 (tiny Inception 140×116) 51.9% ± 2.9 <ref type="table">Table 3</ref>. Network Architectures. This table compares the performance of our model architectures on the hold out test set (see section 4.1). Reported is the mean validation rate VAL at 10E-3 false accept rate. Also shown is the standard error of the mean across the five test splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effect of CNN Model</head><p>We now discuss the performance of our four selected models in more detail. On the one hand we have our traditional Zeiler&amp;Fergus based architecture with 1×1 convolutions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref> (see <ref type="table" target="#tab_1">Table 1</ref>). On the other hand we have Inception <ref type="bibr" target="#b15">[16]</ref> based models that dramatically reduce the model size. Overall, in the final performance the top models of both architectures perform comparably. However, some of our Inception based models, such as NN3, still achieve good performance while significantly reducing both the FLOPS and the model size.</p><p>The detailed evaluation on our personal photos test set is  <ref type="table">Table 4</ref>. Image Quality. The table on the left shows the effect on the validation rate at 10E-3 precision with varying JPEG quality.</p><p>The one on the right shows how the image size in pixels effects the validation rate at 10E-3 precision. This experiment was done with NN1 on the first split of our test hold-out dataset.</p><p>#dims VAL 64 86.8% ± 1.7 128 87.9% ± 1.9 256 87.7% ± 1.9 512 85.6% ± 2.0 <ref type="table">Table 5</ref>. Embedding Dimensionality. <ref type="table">This Table compares</ref> the effect of the embedding dimensionality of our model NN1 on our hold-out set from section 4.1. In addition to the VAL at 10E-3 we also show the standard error of the mean computed across five splits.</p><p>shown in <ref type="figure">Figure 5</ref>. While the largest model achieves a dramatic improvement in accuracy compared to the tiny NNS2, the latter can be run 30ms / image on a mobile phone and is still accurate enough to be used in face clustering. The sharp drop in the ROC for FAR &lt; 10 −4 indicates noisy labels in the test data groundtruth. At extremely low false accept rates a single mislabeled image can have a significant impact on the curve. <ref type="table">Table 4</ref> shows the robustness of our model across a wide range of image sizes. The network is surprisingly robust with respect to JPEG compression and performs very well down to a JPEG quality of 20. The performance drop is very small for face thumbnails down to a size of 120x120 pixels and even at 80x80 pixels it shows acceptable performance. This is notable, because the network was trained on 220x220 input images. Training with lower resolution faces could improve this range further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Sensitivity to Image Quality</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Embedding Dimensionality</head><p>We explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in <ref type="table">Table 5</ref>. One would expect the larger embeddings to perform at least as good as the smaller ones, however, it is possible that they require more training to achieve the same accuracy. That said, the differences in the performance re-#training images VAL 2,600,000 76.3% 26,000,000 85.1% 52,000,000 85.1% 260,000,000 86.2% <ref type="table">Table 6</ref>. Training Data Size. This table compares the performance after 700h of training for a smaller model with 96x96 pixel inputs. The model architecture is similar to NN2, but without the 5x5 convolutions in the Inception modules.</p><p>ported in <ref type="table">Table 5</ref> are statistically insignificant. It should be noted, that during training a 128 dimensional float vector is used, but it can be quantized to 128-bytes without loss of accuracy. Thus each face is compactly represented by a 128 dimensional byte vector, which is ideal for large scale clustering and recognition. Smaller embeddings are possible at a minor loss of accuracy and could be employed on mobile devices. <ref type="table">Table 6</ref> shows the impact of large amounts of training data. Due to time constraints this evaluation was run on a smaller model; the effect may be even larger on larger models. It is clear that using tens of millions of exemplars results in a clear boost of accuracy on our personal photo test set from section 4.2. Compared to only millions of images the relative reduction in error is 60%. Using another order of magnitude more images (hundreds of millions) still gives a small boost, but the improvement tapers off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Amount of Training Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Performance on LFW</head><p>We evaluate our model on LFW using the standard protocol for unrestricted, labeled outside data. Nine training splits are used to select the L 2 -distance threshold. Classification (same or different) is then performed on the tenth test split. The selected optimal threshold is 1.242 for all test splits except split eighth (1.256).</p><p>Our model is evaluated in two modes:</p><p>1. Fixed center crop of the LFW provided thumbnail. <ref type="bibr" target="#b2">[3]</ref>) is run on the provided LFW thumbnails. If it fails to align the face (this happens for two images), the LFW alignment is used. <ref type="figure">Figure 6</ref> gives an overview of all failure cases. It shows false accepts on the top as well as false rejects at the bottom. We achieve a classification accuracy of 98.87%±0.15 when using the fixed center crop described in (1) and the record breaking 99.63%±0.09 standard error of the mean when using the extra face alignment <ref type="bibr" target="#b1">(2)</ref>. This reduces the error reported for DeepFace in <ref type="bibr" target="#b16">[17]</ref> by more than a factor False accept False reject <ref type="figure">Figure 6</ref>. LFW errors. This shows all pairs of images that were incorrectly classified on LFW. Only eight of the 13 false rejects shown here are actual errors the other five are mislabeled in LFW. of 7 and the previous state-of-the-art reported for DeepId2+ in <ref type="bibr" target="#b14">[15]</ref> by 30%. This is the performance of model NN1, but even the much smaller NN3 achieves performance that is not statistically significantly different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A proprietary face detector (similar to Picasa</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Performance on Youtube Faces DB</head><p>We use the average similarity of all pairs of the first one hundred frames that our face detector detects in each video. This gives us a classification accuracy of 95.12%±0.39. Using the first one thousand frames results in 95.18%. Compared to <ref type="bibr" target="#b16">[17]</ref> 91.4% who also evaluate one hundred frames per video we reduce the error rate by almost half. DeepId2+ <ref type="bibr" target="#b14">[15]</ref> achieved 93.2% and our method reduces this error by 30%, comparable to our improvement on LFW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Face Clustering</head><p>Our compact embedding lends itself to be used in order to cluster a users personal photos into groups of people with the same identity. The constraints in assignment imposed by clustering faces, compared to the pure verification task, <ref type="figure">Figure 7</ref>. Face Clustering. Shown is an exemplar cluster for one user. All these images in the users personal photo collection were clustered together. lead to truly amazing results. <ref type="figure">Figure 7</ref> shows one cluster in a users personal photo collection, generated using agglomerative clustering. It is a clear showcase of the incredible invariance to occlusion, lighting, pose and even age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Summary</head><p>We provide a method to directly learn an embedding into an Euclidean space for face verification. This sets it apart from other methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>  NN2 is an improved model that performs much better than NN1. When comparing embeddings generated by NN1 to the harmonic ones generated by NN2 we can see the compatibility between the two. In fact, the mixed mode performance is still better than NN1 by itself. nation of multiple models and PCA, as well as SVM classification. Our end-to-end training both simplifies the setup and shows that directly optimizing a loss relevant to the task at hand improves performance.</p><p>Another strength of our model is that it only requires minimal alignment (tight crop around the face area). <ref type="bibr" target="#b16">[17]</ref>, for example, performs a complex 3D alignment. We also experimented with a similarity transform alignment and notice that this can actually improve performance slightly. It is not clear if it is worth the extra complexity.</p><p>Future work will focus on better understanding of the error cases, further improving the model, and also reducing model size and reducing CPU requirements. We will also look into ways of improving the currently extremely long training times, e.g. variations of our curriculum learning with smaller batch sizes and offline as well as online positive and negative mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Appendix: Harmonic Embedding</head><p>In this section we introduce the concept of harmonic embeddings. By this we denote a set of embeddings that are generated by different models v1 and v2 but are compatible in the sense that they can be compared to each other.</p><p>This compatibility greatly simplifies upgrade paths. E.g. in an scenario where embedding v1 was computed across a large set of images and a new embedding model v2 is being rolled out, this compatibility ensures a smooth transition without the need to worry about version incompatibilities. <ref type="figure" target="#fig_2">Figure 8</ref> shows results on our 3G dataset. It can be seen that the improved model NN2 significantly outper-  <ref type="figure">Figure 9</ref>. Learning the Harmonic Embedding. In order to learn a harmonic embedding, we generate triplets that mix the v1 embeddings with the v2 embeddings that are being trained. The semihard negatives are selected from the whole set of both v1 and v2 embeddings.</p><p>forms NN1, while the comparison of NN2 embeddings to NN1 embeddings performs at an intermediate level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Harmonic Triplet Loss</head><p>In order to learn the harmonic embedding we mix embeddings of v1 together with the embeddings v2, that are being learned. This is done inside the triplet loss and results in additionally generated triplets that encourage the compatibility between the different embedding versions. <ref type="figure">Figure 9</ref> visualizes the different combinations of triplets that contribute to the triplet loss.</p><p>We initialized the v2 embedding from an independently trained NN2 and retrained the last layer (embedding layer) from random initialization with the compatibility encouraging triplet loss. First only the last layer is retrained, then we continue training the whole v2 network with the harmonic loss. <ref type="figure" target="#fig_0">Figure 10</ref> shows a possible interpretation of how this compatibility may work in practice. The vast majority of v2 embeddings may be embedded near the corresponding v1 embedding, however, incorrectly placed v1 embeddings can be perturbed slightly such that their new location in embedding space improves verification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Summary</head><p>These are very interesting findings and it is somewhat surprising that it works so well. Future work can explore how far this idea can be extended. Presumably there is a limit as to how much the v2 embedding can improve over v1, while still being compatible. Additionally it would be interesting to train small networks that can run on a mobile phone and are compatible to a larger server side model.  <ref type="figure" target="#fig_0">Figure 10</ref>. Harmonic Embedding Space. This visualisation sketches a possible interpretation of how harmonic embeddings are able to improve verification accuracy while maintaining compatibility to less accurate embeddings. In this scenario there is one misclassified face, whose embedding is perturbed to the "correct" location in v2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illumination and Pose invariance. Pose and illumination have been a long standing problem in face recognition. This figure shows the output distances of FaceNet between pairs of faces of the same and a different person in different pose and illumination combinations. A distance of 0.0 means the faces are identical, 4.0 corresponds to the opposite spectrum, two different identities. You can see that a threshold of 1.1 would classify every pair correctly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 .</head><label>8</label><figDesc>who use the CNN bottleneck layer, or require additional post-processing such as concate-Harmonic Embedding Compatibility. These ROCs show the compatibility of the harmonic embeddings of NN2 to the embeddings of NN1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>NN1.</figDesc><table /><note>This table show the structure of our Zeiler&amp;Fergus</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>did in this case.</figDesc><table><row><cell>type</cell><cell>output size</cell><cell cols="2">depth #1×1</cell><cell>#3×3 reduce</cell><cell>#3×3</cell><cell>#5×5 reduce</cell><cell>#5×5</cell><cell>pool proj (p)</cell><cell cols="2">params FLOPS</cell></row><row><cell cols="2">conv1 (7×7×3, 2) 112×112×64</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9K</cell><cell>119M</cell></row><row><cell>max pool + norm</cell><cell>56×56×64</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>m 3×3, 2</cell><cell></cell><cell></cell></row><row><cell>inception (2)</cell><cell>56×56×192</cell><cell>2</cell><cell></cell><cell>64</cell><cell>192</cell><cell></cell><cell></cell><cell></cell><cell>115K</cell><cell>360M</cell></row><row><cell>norm + max pool</cell><cell>28×28×192</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>m 3×3, 2</cell><cell></cell><cell></cell></row><row><cell>inception (3a)</cell><cell>28×28×256</cell><cell>2</cell><cell>64</cell><cell>96</cell><cell>128</cell><cell>16</cell><cell>32</cell><cell>m, 32p</cell><cell>164K</cell><cell>128M</cell></row><row><cell>inception (3b)</cell><cell>28×28×320</cell><cell>2</cell><cell>64</cell><cell>96</cell><cell>128</cell><cell>32</cell><cell>64</cell><cell>L2, 64p</cell><cell>228K</cell><cell>179M</cell></row><row><cell>inception (3c)</cell><cell>14×14×640</cell><cell>2</cell><cell>0</cell><cell>128</cell><cell>256,2</cell><cell>32</cell><cell>64,2</cell><cell>m 3×3,2</cell><cell>398K</cell><cell>108M</cell></row><row><cell>inception (4a)</cell><cell>14×14×640</cell><cell>2</cell><cell>256</cell><cell>96</cell><cell>192</cell><cell>32</cell><cell>64</cell><cell>L2, 128p</cell><cell>545K</cell><cell>107M</cell></row><row><cell>inception (4b)</cell><cell>14×14×640</cell><cell>2</cell><cell>224</cell><cell>112</cell><cell>224</cell><cell>32</cell><cell>64</cell><cell>L2, 128p</cell><cell>595K</cell><cell>117M</cell></row><row><cell>inception (4c)</cell><cell>14×14×640</cell><cell>2</cell><cell>192</cell><cell>128</cell><cell>256</cell><cell>32</cell><cell>64</cell><cell>L2, 128p</cell><cell>654K</cell><cell>128M</cell></row><row><cell>inception (4d)</cell><cell>14×14×640</cell><cell>2</cell><cell>160</cell><cell>144</cell><cell>288</cell><cell>32</cell><cell>64</cell><cell>L2, 128p</cell><cell>722K</cell><cell>142M</cell></row><row><cell>inception (4e)</cell><cell>7×7×1024</cell><cell>2</cell><cell>0</cell><cell>160</cell><cell>256,2</cell><cell>64</cell><cell>128,2</cell><cell>m 3×3,2</cell><cell>717K</cell><cell>56M</cell></row><row><cell>inception (5a)</cell><cell>7×7×1024</cell><cell>2</cell><cell>384</cell><cell>192</cell><cell>384</cell><cell>48</cell><cell>128</cell><cell>L2, 128p</cell><cell>1.6M</cell><cell>78M</cell></row><row><cell>inception (5b)</cell><cell>7×7×1024</cell><cell>2</cell><cell>384</cell><cell>192</cell><cell>384</cell><cell>48</cell><cell>128</cell><cell>m, 128p</cell><cell>1.6M</cell><cell>78M</cell></row><row><cell>avg pool</cell><cell>1×1×1024</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>fully conn</cell><cell>1×1×128</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>131K</cell><cell>0.1M</cell></row><row><cell>L2 normalization</cell><cell>1×1×128</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7.5M</cell><cell>1.6B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Network Architectures. This plot shows the complete ROC for the four different models on our personal photos test set from section 4.2. The sharp drop at 10E-4 FAR can be explained by noise in the groundtruth labels. The models in order of performance are: NN2: 224×224 input Inception based model; NN1: Zeiler&amp;Fergus based network with 1×1 convolutions; NNS1: small Inception style model with only 220M FLOPS; NNS2: tiny Inception model with only 20M FLOPS.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Johannes Steffens for his discussions and great insights on face recognition and Christian Szegedy for providing new network architectures like <ref type="bibr" target="#b15">[16]</ref> and discussing network design choices. Also we are indebted to the DistBelief <ref type="bibr" target="#b3">[4]</ref> team for their support especially to Rajat Monga for help in setting up efficient training schemes.</p><p>Also our work would not have been possible without the support of Chuck Rosenberg, Hartwig Adam, and Simon Han.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<editor>P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1989-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Surpassing human-level face verification performance on LFW with gaussianface. CoRR, abs/1404</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3840</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a distance metric from relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>S. Thrun, L. Saul, and B. Schölkopf</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The CMU pose, illumination, and expression (PIE) database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FG</title>
		<meeting>FG</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep learning face representation by joint identification-verification. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4773</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1412.1265</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions. CoRR, abs/1409</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">4842</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1404.4661</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The general inefficiency of batch training for gradient descent learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1429" to="1451" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>CoRR, abs/1311.2901, 2013. 2, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Recover canonicalview faces in the wild with deep neural networks. CoRR, abs/1404</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3543</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
