<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton Based Action Recognition Using Translation-Scale Invariant Image Mapping And Multi-Scale Deep CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Skeleton Based Action Recognition Using Translation-Scale Invariant Image Mapping And Multi-Scale Deep CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-skeleton</term>
					<term>CNN</term>
					<term>image mapping</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an image classification based approach for skeleton-based video action recognition problem. Firstly, A dataset independent translation-scale invariant image mapping method is proposed, which transformes the skeleton videos to colour images, named skeleton-images. Secondly, A multi-scale deep convolutional neural network (CNN) architecture is proposed which could be built and fine-tuned on the powerful pre-trained CNNs, e.g., AlexNet, VGGNet, ResNet et al. Even though the skeleton-images are very different from natural images, the fine-tune strategy still works well. At last, we prove that our method could also work well on 2D skeleton video data. We achieve the state-of-the-art results on the popular benchmard datasets e.g. NTU RGB+D, UTD-MHAD, MSRC-12, and G3D. Especially on the largest and challenge NTU RGB+D, UTD-MHAD, and MSRC-12 dataset, our method outperforms other methods by a large margion, which proves the efficacy of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Action recognition is an important research area in computer vision, which has a wide range of applications, e.g., human computer interaction, video surveillance, robotics, and etc. Recent years, the cost-effective depth sensor combining with real-time skeleton estimation algorithms can provide reliable joint coordinates <ref type="bibr" target="#b0">[1]</ref>. As an intrinsic high level representation, 3D skeleton is valuable and comprehensive for summarizing a series of human dynamics in the video, and thus benefits general action analysis <ref type="bibr" target="#b1">[2]</ref>. Besides its succinctness and effectiveness, it has a significant advantage of great robustness to illumination, clustered background, and camera motion. Based on these advantages, 3D skeleton based activity analysis has drawn great attentions <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b13">[14]</ref>.</p><p>Very recently, human pose estimation from 2D RGB videos have also been studied with deep CNN method <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>. Rather accurate 2D skeleton joints could be evaluated from RGB videos. However, it seems still lack effective method to deal with this kind of 2D skeleton video data for recognition purpose.</p><p>Previously, hand-crafted skeleton features have been devised <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. However, these hand-crafted features are always shallow and dataset-dependent, thus limiting their performance.</p><p>Recently, deep learning based methods have achieved great success in high-level computer vision tasks such as image recognition, classification, detection, and semantic segmentation etc. As for the 3D skeleton based action recogni-tion problem, Recurrent Neural Networks (RNNs) have been widely adopted <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b13">[14]</ref>. RNNs could effectively extract the temporal information and learn the contextural information well. However, RNNs tend to overemphasize the temporal information especially when the training data is insufficient, thus leading to over-fitting <ref type="bibr" target="#b20">[21]</ref>.</p><p>Convolutional Neural Networks (CNNs) have also been applied to this problem <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Different from the RNNs, how to effectively represent 3D skeleton data and feed into deep CNNs is still an open problem. Wang et al. <ref type="bibr" target="#b20">[21]</ref> proposed the Joint Trajectory Maps (JTM), which represents both spatial configuration and dynamics of joint trajectories into three texture images through color encoding, and then fed these texture images to CNNs for classification. However, this kind of JTM is a little complicated, and may lose some important information when projecting 3D skeleton into 2D image. Du et al. <ref type="bibr" target="#b21">[22]</ref> proposed to represent each skeleton sequence as an image, where the temporal dynamics of the sequence are encoded as changes in columns and the spatial structure of each frame is represented as column. Their encoding method is dataset dependent and translation-scale variant which means their encoding method need dataset information and human's translation and action scale may influence the final mapping results.</p><p>To tackle the above shortcomings in skeleton-based video recognition problem, in this paper, we present a new framework consisting of translation-scale invariant image mapping and multi-scale deep CNN classifier. The overall flowchart of our method is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. We propose to map the 3D skeleton video to a color image, where the color image achieves translation and scale invariance and dataset independent. The proposed mapping could easily handle the translation and scale changes in 3D skeleton data, thus is more distinctive, and dataset independent.</p><p>Although the skeleton images are very different from natural images, the widely used pre-trained deep CNN model e.g., AlexNet <ref type="bibr" target="#b22">[23]</ref>, VGGNet <ref type="bibr" target="#b23">[24]</ref>, ResNet <ref type="bibr" target="#b24">[25]</ref> could still be transferred to it well. It is especially valuable when there is insufficient annotated skeleton videos. The fine-tune strategy could avoid training millions of parameters afresh and improve the performance significantly <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>. More importantly, due to the special property of this kind of skeleton images, we propose a simple yet effective multi-scale deep CNN to enhance the frequency adjustment ability of our method.</p><p>In addition, we extend our method to deal with 2D skeleton-arXiv:1704.05645v2 [cs.CV] 13 Jun 2017</p><p>based video recognition problem. Surprisingly, our method could also work well. Experimental results on the popular benchmark dataset like NTU RGB-D, UTD-MHAD, MSRC-12, and G3D demonstrate the effectiveness our proposed framework. We also give extensive analysis experiments to show the propoerties of our method.</p><p>In conclusion, our main contributions are summarized as following:</p><p>• We propose a translation-scale invariant image mapping method for 3D skeleton video data. This mapping could avoid the translation and scale influence of the skeleton data, thus is more distinctive and dataset independent. • A multi-scale deep CNN is proposed to enhance the frequency adjustment ability of our method. • We test our method to 2D skeleton data and achieve excellent results which shows our method could also work on 2D skeleton data well. • We achieve the state-of-the-art results on the widely used benchmarks like NTU RGB-D, UTD-MHAD, MSRC-12, and G3D dataset. In addition, extensive component analysis experiments are conducted. This paper are organized as following. Related works are summarized and presented in Sec. II. We present our method in Sec. III, including our Translation-scale invariant image mapping method, multi-scale deep CNN and data augmentation method. Experiments on the popular benchmarks are presented in Sec. V. At last, more analysis experiments and results on 2D skeleton data are presented in Sec. VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Tranditionaly, hand-crafted skeleton features have been devised to capture the spatial-tempory information. There have been great amount of hand-craft features proposed for 3D skeleton-based video recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. Generally speaking, spatial descriptor, geometric descriptor, or key poses are extensively studied. We would like suggest the readers refer to <ref type="bibr" target="#b26">[27]</ref> for more summary. However, these hand-crafted features are always shallow and dataset-dependent, thus limiting their performance.</p><p>Recently, deep learning methods have been adopted on this field. Du et al. <ref type="bibr" target="#b7">[8]</ref> divided the human skeleton into 5 parts and a hierarchical recurrent neural network is proposed for this problem. Veeriah et al. <ref type="bibr" target="#b8">[9]</ref> proposed a kind of differential recurrent neural network which emphasizes on the change in information gain caused by the salient motions between the successive frames. Zhu et al. <ref type="bibr" target="#b9">[10]</ref> proposed a kind of regularized deep LSTM network which take the skeleton as the input at each time slot and introduce a novel regularization scheme to learn the co-occurrence features of skeleton joints. Liu et al. <ref type="bibr" target="#b11">[12]</ref> propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, they introduce new gating mechanism within LSTM to learn the reliability of the sequential input data.</p><p>Convolutional Neural Networks (CNNs) have also been applied to this problem <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Wang et al. <ref type="bibr" target="#b20">[21]</ref> proposed the Joint Trajectory Maps (JTM), which represents both spatial configuration and dynamics of joint trajectories into three texture images through color encoding, and then fed these texture images to CNNs for classification. Du et al. <ref type="bibr" target="#b21">[22]</ref> proposed to represent each skeleton sequence as an image, where the temporal dynamics of the sequence are encoded as changes in columns and the spatial structure of each frame is represented as column. How to effectively represent 3D skeleton data and feed into deep CNNs is still an open problem.</p><p>In recent years, human pose estimation from 2D RGB videos have been studied with deep CNN method <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>. Rather accurate 2D skeleton joints could be evaluated from RGB videos.</p><p>Our work is also related to recent works on transfer learning and deep learning. In <ref type="bibr" target="#b22">[23]</ref>, Krizhevsky et al. trained a large deep CNN on the ImageNet dataset and achieved a performance leap on the classification problem. Recently, more and more work show that pre-trained deep CNN features can be transferred to new classification or recognition problems and boost performance <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, a very deep CNN is proposed, which achieved state-of-the-art classification results in ImageNet challenge 2014. In <ref type="bibr" target="#b24">[25]</ref>, a kind of residual structure is proposed which makes the hundreds layers CNN trainable.</p><p>This paper is an extended version of our conference paper published in ICMEW [], In which, we achieve the 3rd place in the "Large Scale 3D Human Activity Analysis Challenge in Depth Videos". However, this journal version substantively improves the performance and gives more insights by more experiments and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Our framework consist of 3 parts (1) Translation-scale invariant image mapping. (2) A multi-scale deep CNN for classification. (3) Data augmentation methods we utilized for 3D skeleton data. A conceptual illustration of our framework is presented in <ref type="figure" target="#fig_0">Fig.1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Translation-scale invariant image mapping</head><p>Following the work of <ref type="bibr" target="#b21">[22]</ref>, we divide all human skeleton joints in each frame into five main parts according to human physical structure, i.e. two arms, two legs and a trunk. To preserve the local motion characteristics, joints in each part are concatenated as a vector by their physical connections. Then the five parts are concatenated as the representation of each frame. To map the 3D skeleton video to an image, a natural and direct way is to represent the three coordinate components (x, y, z) of each joint as the corresponding three components (R, G, B) of each pixel in an image. Specially, each row of the action image is defined as</p><formula xml:id="formula_0">R i = [x i1 , x i2 , ..., x iN ], G i = [y i1 , y i2 , ..., y iN ], B i = [z i1 , z i2 , ..., z iN ],</formula><p>where i denotes the joint index and N indicates the number of frames in a sequence. By concatenating all joints together, we obtain the resultant action image representation of the original skeleton video.</p><p>Due to the coordinate difference in 3D skeleton and image, proper normalization is needed. Du et al. <ref type="bibr" target="#b21">[22]</ref> proposed to quantify the float matrix to discrete image representation with  respect to the entire training dataset. Specifically, given the joint coordinate c jk (x, y, z), the corresponding pixel value p jk is defined as</p><formula xml:id="formula_1">p jk = f loor 255 * c jk − c min c max − c min ,<label>(1)</label></formula><p>where c max and c min are the maximum and minimum of all joint coordinates in the training set respectively. jk represent the k-th channel (x, y, z) of the j-th skeleton video sequence. To tackle the above problems in encoding the skeleton video, we propose a simple and yet effective translation-scale invariant image mapping method, which is presented in Equ. 2</p><formula xml:id="formula_2">p jk = f loor   255 * c jk − c jk min max k (c jk max − c jk min )   ,<label>(2)</label></formula><p>where c jk max and c jk min are the maximum and minimum coordinate value of the k-th channel (x, y, z) of the j-th skeleton video sequence.</p><p>Compared with Du et al. <ref type="bibr" target="#b21">[22]</ref>, our image mapping method owns the following properties:</p><p>• Translation invariant: Our image mapping transforms the 3D coordinates with respect to the minimum coordinate in each sequence rather than the entire training dataset, thus the translation in 3D does not affect the action video as illustrated in <ref type="figure">Fig. 9</ref>. • Scale invariant: Similarly, by normalizing the 3D skeleton coordinates with respect to the 3D variation along the three axis in each sequence, the scale change has also been eliminated. In addition, our normalization is isometric to each coordinate x, y, z, thus the relative scale in different axis has been well preserved. • Dataset independent: As the minimum and maximum coordinates are extracted from each sequence independently, the normalization is thus independent to each specific skeleton video dataset.  <ref type="figure">Fig. 3</ref>: Comparison between the mapping results from <ref type="bibr" target="#b21">[22]</ref> and our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action recognition through multi-scale CNN</head><p>Our overall multi-scale CNN architecture is presented in <ref type="figure" target="#fig_3">Fig. 4</ref>. Our CNN architecture could be built on the fully-CNN based pre-trained model like AlexNet <ref type="bibr" target="#b22">[23]</ref>, VGGNet <ref type="bibr" target="#b23">[24]</ref>, and ResNet <ref type="bibr" target="#b24">[25]</ref>.</p><p>The multi-scale structure is motivated by the frequency variant of this kind of skeleton-images. Under our skeleton mapping framework, if we fix the size of the convolution kernel, different input size will bring different frequency variance. The multi-scale (multi-frequency) input often includes rich cues to the activity recognition problem.</p><p>In order to reduce the amount of the parameters in our model, the weights of the Fully-CNN parts are shared by all the different resolution inputs. Then global pooling is performed on the correspondent feature maps which is critical to result in the same size feature vectors. In order to further regularize the training, we put on the softmax loss on the all output of different resolution as well as the average of them.</p><p>We train our network with the multinomial logistic loss:</p><formula xml:id="formula_3">E = − 1 N N n=1 log(p n k )<label>(3)</label></formula><p>p is the output of the softmax layer:</p><formula xml:id="formula_4">p i = exp x i m i =1 exp x i<label>(4)</label></formula><p>Here, N is the number of training samples, k the correspondent label of sample n, m is the number of classes.</p><p>x is the input of the softmax layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data augmentation</head><p>Data augmentation has been proved as an effective way in deep CNN based image classification. In this paper, we have encoded the 3D skeleton data to RGB images. In order to augment the dataset and leap the classification performance, we have specially designed different data augmentation strategies, such as 3D coordinate random rotation <ref type="bibr" target="#b20">[21]</ref>, Gaussian noise, video crop etc.</p><p>The augmentation methods we utilized including:  </p><formula xml:id="formula_5">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION DETAILS</head><p>Before proceeding to the experimental results, we give implementation details for our method. The proposed network is trained by using stochastic gradient decent momentum of 0.9, and weight decay of 0.0004. Weights are initialized by the pre-trained model from AlexNet <ref type="bibr" target="#b22">[23]</ref>, VGGNet <ref type="bibr" target="#b23">[24]</ref>, and ResNet <ref type="bibr" target="#b24">[25]</ref>. The network is trained with fixed learning rate 0.001 in the first 8 epoches, then divided by 10 every 5 epoches.</p><p>Our implementation is based on the efficient CNN toolbox: caffe <ref type="bibr" target="#b28">[29]</ref> with an NVIDIA Tesla Titian X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we evaluate the proposed method on public benchmark datasets: the large NTU RGB+D Dataset, UTD-MHAD, MSRC-12 Kinect Gesture Dataset, and G3D. The final recognition results were compared with the state-of theart reporteds on the same datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. NTU RGB+D Dataset</head><p>To the best of our knowledge, NTU RGB-D dataset <ref type="bibr" target="#b10">[11]</ref> is the largest action recognition dataset. We adopt the same train-test protocol as in <ref type="bibr" target="#b20">[21]</ref>.</p><p>The dataset has more than 56 thousands sequences and 4 million frames, containing 60 actions performed by 40 subjects aging between 10 and 35. It consists of front view, two side views and one left, right 45 degree views. This dataset is very challenging due to the large intra-class and viewpoint variations.</p><p>For a fair comparison and evaluation, the same protocol as that in <ref type="bibr" target="#b10">[11]</ref> was used. It has both cross-subject and cross view evaluation. In the cross-subject evaluation, samples of subjects <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35</ref> and 38 were used as training samples and the remaining subjects were reserved for testing. In the cross-view evaluation, samples taken by camera 2 and 3 were used as training, while testing set includes the samples of camera 1. We further augment the training dataset by 2 times.</p><p>In the dataset of NTU, there are some samples consisted of more than 1 person. We choose the simplest strategy to deal with this kind of situation, that, we just concat these two person's coordinate and present them in one image. An sample is presented in <ref type="figure" target="#fig_6">Fig. 6</ref>.</p><p>In <ref type="table" target="#tab_1">Table I</ref>, we report the performance comparison between our method and the state-of-the-art methods. Clearly, our proposed method achieves the best performance in both crosssubject and cross-view evaluation. Our method outperforms  Method Cross subject Cross view Lie Group <ref type="bibr" target="#b4">[5]</ref> 50.08% 52.76% Dynamic Skeletons <ref type="bibr" target="#b6">[7]</ref> 60.23% 65.22% HBRNN <ref type="bibr" target="#b7">[8]</ref> 59.07% 63.97% Part-aware LSTM <ref type="bibr" target="#b10">[11]</ref> 62.93% 70.27% ST-LSTM + Trust Gate <ref type="bibr" target="#b11">[12]</ref> 69.20% 77.70% JTM+CNN <ref type="bibr" target="#b20">[21]</ref> 76.32% 81.08% STA-LSTM <ref type="bibr" target="#b12">[13]</ref> 73.4% 81.2% Proposed Method 85.02% 92.3%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. UTD-MHAD</head><p>UTD-MHAD <ref type="bibr" target="#b29">[30]</ref> is a multimodal action dataset, captured by one Microsoft Kinect camera and one wearable inertial sensor. This dataset contains 27 actions performed by 8 subjects (4 females and 4 males) with each subject performing each action 4 times. After removing three corrupted sequences, the dataset has 861 sequences. The actions are: right arm swipe to the left, right arm swipe to the right, right hand wave, two hand front clap, right arm throw, cross arms in the chest, basketball shoot, right hand draw x, right hand draw circle (clockwise), right hand draw circle (counter clockwise), draw triangle, bowling (right hand), front boxing, baseball swing from right, tennis right hand forehand swing, arm curl (two arms), tennis serve, two hand push, right hand know on door, right hand catch an object, right hand pick up and throw, jogging in place,walking in place, sit to stand, stand to sit, forward lunge (left foot forward) and squat (two arms stretch out).It covers sport actions (e.g. bowling, tennis serve and baseball swing), hand gestures (e.g. draw X, draw triangle, and draw circle), daily activities (e.g. knock on door, sit to stand and stand to sit) and training exercises(e.g. arm curl, lung and squat). For this dataset, cross-subjects protocol was adopted as in <ref type="bibr" target="#b29">[30]</ref>, namely, the data from the subjects numbered 1, 3, 5, 7 were used for training while subjects 2, 4, 6, 8 were used for testing. <ref type="table" target="#tab_1">Table VIII</ref> compares the performance of the proposed method and those reported in <ref type="bibr" target="#b29">[30]</ref>.  <ref type="bibr" target="#b10">[11]</ref> Method Accuracy ELC-KSVD <ref type="bibr" target="#b30">[31]</ref> 76.19% kinect &amp; Inertial <ref type="bibr" target="#b29">[30]</ref> 79.10% Cov3DJ <ref type="bibr" target="#b31">[32]</ref> 85.58% SOS <ref type="bibr" target="#b32">[33]</ref> 86.97% JTM <ref type="bibr" target="#b20">[21]</ref> 87.90% Proposed Method 96.27% <ref type="figure">Fig. 7</ref>: Confusion matrix of UTD-MHAD C. MSRC-12 Kinect Gesture Dataset MSRC-12 <ref type="bibr" target="#b33">[34]</ref> is a relatively large dataset for gesture/action recognition from 3D skeleton data captured by a Kinect sensor. The dataset has 594 sequences, containing 12 gestures by 30 subjects, 6244 gesture instances in total. The 12 gestures are: lift outstretched arms, duck, push right, goggles,wind it up, shoot, bow, throw, had enough, beat both, change weapon and kick. For this dataset, cross-subjects protocol was adopted, that is, odd subjects were used for training and even subjects were for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy HGM <ref type="bibr" target="#b34">[35]</ref> 66.25% Pose-Lexicon <ref type="bibr" target="#b35">[36]</ref> 85.86% ELC-KSVD <ref type="bibr" target="#b30">[31]</ref> 90.22% Cov3DJ <ref type="bibr" target="#b31">[32]</ref> 91.70% SOS <ref type="bibr" target="#b32">[33]</ref> 94.27% JTM <ref type="bibr" target="#b20">[21]</ref> 94.86% Proposed Method 99.41% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. G3D Dataset</head><p>Gaming 3D Dataset (G3D) <ref type="bibr" target="#b36">[37]</ref> focuses on real-time action recognition in a gaming scenario. It contains 10 subjects performing 20 gaming actions: punch right, punch left, kick right, kick left, defend, golf swing, tennis swing forehand, tennis swing backhand, tennis serve, throw bowling ball, aim and fire gun, walk, run, jump, climb, crouch, steer a car, wave, flap and clap. For this dataset, the first 4 subjects were used for training, the fifth for validation and the remaining 5 subjects were for testing as configured in <ref type="bibr" target="#b37">[38]</ref>. <ref type="table" target="#tab_1">Table VII</ref> compared the performance of the proposed method and those reported in <ref type="bibr" target="#b37">[38]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy ELC-KSVD <ref type="bibr" target="#b30">[31]</ref> 82.37% Cov3DJ <ref type="bibr" target="#b31">[32]</ref> 71.95% LRBM <ref type="bibr" target="#b37">[38]</ref> 90.50% SOS <ref type="bibr" target="#b32">[33]</ref> 95.45% JTM <ref type="bibr" target="#b20">[21]</ref> 96.02% Proposed Method 93.9%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. MORE ANALYSIS A. Effect of our mapping method</head><p>To demonstrate the effectiveness of our translation-scale invariant image mapping and our data augmentation method, we compared with other skeleton data image mapping method and the results are presented in <ref type="table">Table.</ref>V, which clearly demonstrates the effectiveness of our image mapping method. For a fair comparison, all the encoded action images are fine-tuned on the Alexnet <ref type="bibr" target="#b22">[23]</ref>. and the result of Wang et al. is quoted from <ref type="bibr" target="#b20">[21]</ref> directly, in order to avoid the hyper-parameter setting influence. It is worth noting that the results of Wang etal <ref type="bibr" target="#b20">[21]</ref> utilized data augmentation. For fair comparison, we also give the results of our method with data augmentation.</p><p>It is clear from the Tab ??, our mapping method outperform <ref type="bibr" target="#b21">[22]</ref> in all the dataset with a clearly margin, which prove our translation-scale property is important. In addtion, our <ref type="figure">Fig. 9</ref>: Confusion matrix of G3D dataset method outperform <ref type="bibr" target="#b20">[21]</ref> in most dataset except G3D. More importantly, our advantage is obvious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of our multi-scale architecture</head><p>We compared the performance of different pre-trained CNN net in <ref type="table" target="#tab_1">Table VI</ref>. It is obvious that our multi-scale network structure also improves the performance significately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Adapted to 2D skeleton</head><p>As for 2D skeleton, we set the missing coordinate to 0. The correspondent results are presented in Tab. VII. Surprisingly, the 2D skeleton is just a little worse than the 3D skeleton. We would like to argu that, to our best knowledge, we are the first to conduct this kind of experiment. This promissing results show that our method could also work well on the 2D skeleton data.</p><p>In addition, we also give the result of 1D skeleton data in Tab. VII. The performance of 1D skeleton data decrease sharply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION AND FUTURE WORK</head><p>In this paper, we present a skeleton based action recognition method by using both translation-scale invariant image mapping and multi-scale deep CNNs. Experiments on the large scale challenging NTU RGB-D, UTD-MHAD MSRC-12 dataset show that our method outperforms the state-of-theart methods by a large margin. In addition, we extend our method to 2D skeleton-based video recogntion problem and it performs well.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Flowchart of the proposed method. The 3D skeleton video is first mapped to action image via translation-scale invariant transformation. Image classification is conducted on the action image by using our multi-scale CNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>f loor is the rounding down function. However, as the normalization is conducted with respect to the entire training dataset, the resultant action image is dataset dependent and translationscale variant. 1. This encoding method need c min , c max which need statistic on the dataset. Different dataset may have different c min , c max , thus this kind of encoding method is dataset dependent. 2. The same action conducted on the different position(caused by the translation of the person or camera) will result in a different action image, thus is translation variant. 3. The same action conducted by different subjects may have some scale variant, while this normalization also could not guarantee scale invariant in encoding the skeleton video. Illustration of the translation-scale invariant image mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Our</head><label></label><figDesc>Du et al. [22]    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Our multi-scale pretrained CNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>3D coordinate rotation: 3D coordinate are randomly rotated in the range of [−30 o , 30 o ], along the x, y, z axis. Some examples are presented in Fig. 5. • Gaussian noise: We randomly add gaussian noise in the 3D coordinate with the θ = 0, σ = 0.01. • Video crop: We randomly crop the videos by the range of [0.7, 1] in the random locations of the video .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Illustration of skeleton image results from different rotation settings along x axis. (a) −30 o , (b) −15 o , (c) 0 o , (d) 15 o , (e) 30 o mapping results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Illustration of the skeleton images for 1 person and 2 person in NTU RGB+D dataset the current state-of-the-art methods with a margin of 12% in cross-subject evaluation and 11% in cross-view evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Confusion matrix of MSRC 12 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell>: Performance comparison on the NTU RGB-D</cell></row><row><cell>dataset [11]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Performance comparison on the UTD-MHAD dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Performance comparison on the MSRC-12 Kinect Gesture Dataset<ref type="bibr" target="#b10">[11]</ref> </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Performance comparison on the G3D dataset<ref type="bibr" target="#b10">[11]</ref> </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Performance comparison of different mapping methods on the popular benchmard dataset</figDesc><table><row><cell>Method</cell><cell cols="3">NTU-CS NTU-CV UTD-MHAD</cell><cell>MSRC</cell><cell>G3D</cell></row><row><cell>Du [22]</cell><cell>76.0%</cell><cell>84.7%</cell><cell>83.55%</cell><cell>99.30%</cell><cell>92.5%</cell></row><row><cell>Our</cell><cell>80.2%</cell><cell>85.0%</cell><cell>87.7%</cell><cell>99.4%</cell><cell>92.6%</cell></row><row><cell>JTM [21] + Aug</cell><cell>76.3%</cell><cell>81.1%</cell><cell>87.90%</cell><cell cols="2">94.86% 96.02%</cell></row><row><cell>Our + Aug</cell><cell>81.3%</cell><cell>87.2%</cell><cell>90.5%</cell><cell>99.4%</cell><cell>93.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Performance of different pre-trained CNN model and our multi-scale network structure</figDesc><table><row><cell>Pre-trained CNN</cell><cell cols="2">Cross subject Cross view</cell></row><row><cell>AlexNet</cell><cell>81.3%</cell><cell>87.2%</cell></row><row><cell>VGGNet</cell><cell>84.6%</cell><cell>90.1%</cell></row><row><cell>ResNet101</cell><cell>83.8%</cell><cell>89.9%</cell></row><row><cell>ResNet152</cell><cell>84.3%</cell><cell>90.1%</cell></row><row><cell>ResNet101 + 2scale</cell><cell>84.6%</cell><cell>90.9%</cell></row><row><cell>ResNet101 + 3scale</cell><cell>84.6%</cell><cell>92.1%</cell></row><row><cell>ResNet152 + 3scale</cell><cell>85.02%</cell><cell>92.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Performance of 2D skeleton data. The experiments are conducted on NTU RGB+D dataset with fine-tuned on AlexNet</figDesc><table><row><cell cols="3">coordinate Cross subject Cross view</cell></row><row><cell>X-Y-Z</cell><cell>80.2%</cell><cell>85.0%</cell></row><row><cell>X-Y</cell><cell>77.8%</cell><cell>83.5%</cell></row><row><cell>Y-Z</cell><cell>77.4%</cell><cell>83.9%</cell></row><row><cell>X-Z</cell><cell>76.6%</cell><cell>82.5%</cell></row><row><cell>X</cell><cell>54.3%</cell><cell>70.2%</cell></row><row><cell>Y</cell><cell>53.6%</cell><cell>69.2%</cell></row><row><cell>Z</cell><cell>49.5%</cell><cell>68.3%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported in part by Natural Science Foundation of China grants (61420106007, 61671387) and Australian Research Council grants (DE140100180).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient human pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2821</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Histogram of oriented displacements (hod): describing trajectories of human joints for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1351" to="1357" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rolling rotations for recognizing human actions from 3d skeletal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4471" to="4479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint angles similarities and HOG2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohnbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4041" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cooccurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representation learning of temporal dynamics for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3010" to="3022" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="page" from="4966" to="4975" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3073" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ongoing human action recognition with motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barnachon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouakaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boufama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guillou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">R3dg features: Relative 3d geometry-based skeletal representations for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d skeleton-based human action classification: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Presti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Cascia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="130" to="147" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative key pose extraction using extended lc-ksvd for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Lmage Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="639" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Skeleton optical spectra based action recognition using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Instructing people for training gestural interactive systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fothergill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mentis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sigchi Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A hierarchical model based on latent dirichlet allocation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2613" to="2618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a pose lexicon for semantic action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">G3d: A gaming action dataset and real time action recognition evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A generative restricted Boltzmann machine based method for high-dimensional motion data modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Elsevier Science Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
