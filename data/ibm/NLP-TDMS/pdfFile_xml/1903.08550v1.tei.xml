<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OCGAN: One-class Novelty Detection Using GANs with Constrained Latent Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
							<email>pperera3@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
							<email>rnallapa@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
							<email>bxiang@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OCGAN: One-class Novelty Detection Using GANs with Constrained Latent Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel model called OCGAN for the classical problem of one-class novelty detection, where, given a set of examples from a particular class, the goal is to determine if a query example is from the same class. Our solution is based on learning latent representations of in-class examples using a denoising auto-encoder network. The key contribution of our work is our proposal to explicitly constrain the latent space to exclusively represent the given class. In order to accomplish this goal, firstly, we force the latent space to have bounded support by introducing a tanh activation in the encoder's output layer. Secondly, using a discriminator in the latent space that is trained adversarially, we ensure that encoded representations of in-class examples resemble uniform random samples drawn from the same bounded space. Thirdly, using a second adversarial discriminator in the input space, we ensure all randomly drawn latent samples generate examples that look real. Finally, we introduce a gradient-descent based sampling technique that explores points in the latent space that generate potential out-of-class examples, which are fed back to the network to further train it to generate in-class examples from those points. The effectiveness of the proposed method is measured across four publicly available datasets using two one-class novelty detection protocols where we achieve state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One-class novelty detection tackles the problem of quantifying the probability that a test example belongs to the distribution defined by training examples <ref type="bibr" target="#b18">[19]</ref>. Different from other machine learning tasks, in one-class novelty detection, examples of only a single class are observed at training time. During inference, the trained model is expected to accept in-class examples and reject out-of-class exam- ples. Since the problem formulation assumes unavailability of any negative training data, it is a difficult problem to solve in practice. Nevertheless, it has a number of applications including abnormality detection <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b12">[13]</ref> intruder detection <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, bio-medical data processing <ref type="bibr" target="#b17">[18]</ref> and imbalance learning <ref type="bibr" target="#b9">[10]</ref>.</p><p>With the advent of deep learning, one-class novelty detection has received considerable amount of attention in the literature. Contemporary works in one-class novelty detection focus on learning a representative latent space for the given class <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>. Once such a space is learned, novelty detection is performed based on the projection of a query image onto the learned latent space. Two distinct strategies are commonly used for this purpose in the literature. In the first strategy, the difference between the query image and its inverse image (reconstruction) is used as a novelty detector. Various distance measures ranging from mean squared error <ref type="bibr" target="#b20">[21]</ref> to discriminator output <ref type="bibr" target="#b19">[20]</ref> have been used in the literature for this purpose. In comparison, the second strategy explicitly models the learned latent space using a distribution <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14]</ref>. In this work, we consider the former strategy for novelty detection. We investigate limitations of existing representation learning techniques and propose learning a latent space that exclusively generates only in-class examples, to improve performance in novelty detection.</p><p>Existing work focuses on generating a latent representation that preserves details of the given class. In doing so, it is assumed that when an out-of-class object is presented to the network, it will do a poor job of describing the object, thereby reporting a relatively higher reconstruction error. However, this assumption does not hold at all times. For an example, experiments done on digits in the literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref> suggest that networks such as auto-encoders trained on digits with a simple shape such as 0 and 1 have high novelty detection accuracy. In contrast, digits with complex shapes, such as digit 8, have relatively weaker novelty detection accuracy. This is because a latent space learned for a class with complex shapes inherently learns to represent some of out-of-class objects as well. As an example, the latent space learned on digit 8 is also able to represent other digits such as 1,3,6,7 reasonably well -thereby producing very low distance error values for out-of-class examples as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (middle).</p><p>We note that the requirement in novelty detection is not only to ensure that in-class samples are well represented; it is also to ensure that out-of-class samples are poorly represented. To the best of our knowledge, none of the previous work has addressed the latter requirement. In this work, we propose One-Class GAN (OCGAN), a two-fold latent space learning process that considers both these requirements.</p><p>At a high-level, we learn a latent space that represents objects of a given class well. Secondly, we ensure that any example generated from the learned latent space is indeed from the known class. In other words, if the network is trained on a digits of 8, we ensure that any sample drawn from the latent space, when used to generate an image, corresponds to an image of digit 8. This ensures that outof-class samples are not well represented by the network. Shown in <ref type="figure" target="#fig_0">Figure 1</ref>(bottom) are the outputs generated by the proposed method for the inputs shown in <ref type="figure" target="#fig_0">Figure 1</ref>(top). Since the entire latent space corresponds to images from digit 8, all projections into the latent space in return produce images of digit 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>One-class Novelty Detection. One-class novelty detection is a well-defined research problem with standard evaluation metrics that has received considerable attention in the recent past. It has been traditionally treated as a representationlearning problem. The earliest methods in one-class novelty detection used Principal Component Analysis (PCA) <ref type="bibr" target="#b1">[2]</ref> and its kernel extension <ref type="bibr" target="#b4">[5]</ref> to find a subspace that best describes the given concept. With the advent of neural networks and deep learning, a similar mapping was sought using auto-encoder networks <ref type="bibr" target="#b3">[4]</ref>.</p><p>As discussed in the preceding section, once such a map-ping is learned, one-class novelty detection is carried out either based on reconstruction error or by explicitly modeling the normal behaviour of the known class in the latent space. In <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b20">[21]</ref> the former strategy has been used to perform novelty detection using mean squared error as the novelty function. In <ref type="bibr" target="#b19">[20]</ref>, a Generative Adversarial Network (GAN) <ref type="bibr" target="#b2">[3]</ref> is trained to de-noise noisy samples of the given class. There, the discriminator's prediction in the image space is used to quantify reconstruction error. Following a slightly different strategy, <ref type="bibr" target="#b22">[23]</ref> proposes to learn a mapping between a random distribution and the image manifold of the given class. In <ref type="bibr" target="#b22">[23]</ref>, the closest image to a query is sought through back-propagation, where novelty detection is performed based on the difference between the two images.</p><p>The latter strategy, where the behavior of the known class in the latent space is modeled, has also received considerable attention in recent works. Earlier work of this nature used one-class modeling tools such as One-class SVM <ref type="bibr" target="#b23">[24]</ref> and Support Vector Data Descriptor (SVDD) <ref type="bibr" target="#b24">[25]</ref> on top of an obtained latent representation. In <ref type="bibr" target="#b15">[16]</ref>, first, a GAN is used to obtain a latent representation. Then, the probability distribution of the latent space is modeled as a product of two marginal distributions where marginal distributions are learned empirically. In contrast, in <ref type="bibr" target="#b0">[1]</ref> the latent distribution is modeled using an auto-regressive network that is learned along with the parameters of the auto-encoder. Using a different approach, deep-SVDD <ref type="bibr" target="#b18">[19]</ref> tries to learn a latent space where intra-class variance is low. The method proposed by <ref type="bibr" target="#b18">[19]</ref> is conceptually similar to <ref type="bibr" target="#b14">[15]</ref> but does not use any external data in finding the solution as done in the latter work.</p><p>Anomaly Detection and One-class Classification. Both anomaly detection <ref type="bibr" target="#b28">[28]</ref> and one-class classification <ref type="bibr" target="#b23">[24]</ref> are problems related to one-class novelty detection. Both have similar objectives -to detect out-of-class samples given a set of in-class samples. A hard label is expected to be assigned to a given image in one-class classification; therefore its performance is measured using detection accuracy and F1 score. In contrast, novelty detection is only expected to associate a novelty score to a given image; therefore performance of novelty detection is measured using a Receiver Operating Characteristic (ROC) curve. However, boundarybased one-class classification methods such as One-class SVM, and SVDD can be adopted as novelty detection methods by considering distance to the decision boundary as the novelty score. In contrast, anomaly detection (also known as outlier detection) is an unsupervised learning task <ref type="bibr" target="#b28">[28]</ref>. Given a mixture of unlabeled in-class and out-of-class examples, goal of anomaly detection is to separate in-class examples from out-of class examples. Since anomaly detection and novelty detection follow different protocols, we note that these two tasks are not comparable. Therefore, tools designed for anomaly detection and novelty detection cannot be used interchangeably. Adversarial Learning. Given a set of images, Generative Adversarial Networks introduced in [3] play a two-player game between a generator network and a discriminator network. Here, the generator network tries to produce realistic images (fake images) from the given image distribution whereas the discriminator network tries to distinguish fake images from real images. At equilibrium, the generator network learns the distribution of the given image set. In order to achieve this state, GAN theory dictates that there should be a balance between the capacities of the two networks. In <ref type="bibr" target="#b8">[9]</ref>, GAN was extended to the conditional setting. Based on this extension, GANs have been used in many image-toimage translation applications since. It was shown in <ref type="bibr" target="#b16">[17]</ref> that GANs can be used to learn stable representations even with deep convolutional networks, provided that certain design choices are made. Inspired by the network architecture of <ref type="bibr" target="#b19">[20]</ref>, and following principles outlined in <ref type="bibr" target="#b16">[17]</ref>, we propose a deep convolutional GAN architecture as the backbone of our solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method: OCGAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>In the introduction, we presented an example where a network trained to represent a given class has ended up providing good representation for images of other classes. When images of a given class are sufficiently diverse, smoothly transitioning between the projection of one inclass image in the latent space to that of another can be done along infinitely many different paths -this is particularly the case for latent spaces with high dimensionality.</p><p>In training auto-encoders, we model projections of only observed examples into the latent space -not all possible paths between the corresponding latent points.</p><p>In <ref type="figure">Figure 2</ref> we visualize a path traced in the latent space between two points corresponding to two different images of the given class (class 8). This visualization reveals that as we transition from one point to the other in the latent space along the specified path, certain intermediate latent samples resemble the likeness of digit 1. When the network observes an instance of digit 1, it gets projected onto such samples. Since digit 1 is well represented by the network, its reconstruction error will be low, although it is out of class. The core idea of our proposal is based on this observation. We argue that if the entire latent space is constrained to represent images of the given class, the representation of outof-class samples will be minimal -thereby producing high reconstruction errors for them.</p><p>With this strategy in mind, we explicitly force the entirety of the latent space to represent only the given class. When applied to the example in <ref type="figure">Fig. 2</ref>, all latent samples <ref type="figure">Figure 2</ref>. This figure illustrates the latent space learned for digit 8 using a denoising-autoencoder network (left) and the proposed method (right). Visualization of a chosen path between two digit images in the latent space are shown in the figure. In the denoising auto-encoder, digit 1 is well represented in this path. Therefore, for a digit 1 image, the reconstruction error will be low. On the other hand, the proposed method produces in-class examples throughout the chosen path in the latent space between the two images. Therefore, when a digit 1 image that gets projected into this path is considered, we find that reconstruction error is high. along any path between the two 8's will reconstruct into a set of digit 8 images. Visualization of the path as shown in <ref type="figure">Figure 2</ref>(b) validates this claim. As a result, when an out-of-class digit 1 is presented to the model, there will be a high difference between the digit and the reconstruction of the digit (which will now look more like a digit 8). As a result, the proposed method is able to produce superior novelty detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed Strategy</head><p>The proposed solution, OCGAN, consists of four components: a denoising auto-encoder, two discriminators (latent and visual discriminator) and a classifier. The proposed network is trained using adversarial principles. We describe each of these components in detail below. Denoising auto-encoder: Following previous work, we use a denoising auto-encoder network to learn a representation for the given concept. The auto-encoder is an encoder (En) -decoder (De) structure that is trained with the objective of minimizing the distance between the input and the output of the network. It is the usual practice to have a bottleneck latent-space in between with a dimension smaller than the input. Due to this bottleneck, auto-encoder retains only essential information in the latent space that is required for reconstruction. In a denoising auto-encoder, noise is added to the input image and the network is expected to reconstruct the denoised version of the image. It is shown in the literature that denoising auto-encoders reduce over-fitting and improve generalizabilty of the network compared to regular auto-encoders. As a result, denoising auto-encoders open up the possibility of having a latent dimension larger than the input image dimension <ref type="bibr" target="#b27">[27]</ref>.</p><p>Further, our strategy revolves around densely sampling from the latent space. To facilitate this operation, with the intention of having a bounded support for the latent space, we introduce a tanh activation in the output layer of the encoder. Therefore, support of the latent space is (−1, 1) d , where d is the dimension of the latent space. In our implementation, we add zero mean Gaussian white noise with a variance of 0.2 to input images and train the auto-encoder using mean squared error loss as shown below:</p><formula xml:id="formula_0">l MSE = ||x − De(En(x + n))|| 2 2 ,</formula><p>where x is an input image and n ∼ N (0, 0.2). In addition, adversarial loss terms introduced in the following sections are also used to learn parameters of the auto-encoder. Since the decoder part of the auto-encoder also acts as the generator of images from latent space, we use the words decoder and generator interchangeably in the remainder of the text. Latent Discriminator: The motivation of our method is to obtain a latent space where each and every instance from the latent space represents an image from the given class. If representations of the given class are only confined to a sub-region of the latent space, this goal is not possible to achieve. Therefore, we explicitly force latent representations of in-class examples to be distributed uniformly across the latent space. We achieve this using a discriminator operating in the latent space that we call latent discriminator D l . The latent discriminator is trained to differentiate between latent representations of real images of the given class and samples drawn from a U(−1, 1) d distribution. We consider a loss of the form:</p><formula xml:id="formula_1">l latent = −(E s∼U(−1,1) [log D l (s)] + E x∼px [log(1 − D l (En(x + n)))])</formula><p>where, p x is the distribution of in-class examples. We train the latent discriminator along with the auto-encoder network using max En min D l l latent . Since the latent space is a hyper-cube with support (−1, 1) d , at equilibrium, the latent projections of examples from the given class are expected to be distributed evenly following a U(−1, 1) d distribution.</p><p>Visual Discriminator: In order for the network not to represent any out-of-class objects, we propose to sample exhaustively from the latent space and ensure corresponding images are not from out-of class. Since there are no negative classes present during training, this condition is difficult to enforce. Instead, we make sure that all images generated from latent samples are from the same image space distribution as the given class. In order to enforce this constraint, we use a second discriminator that we call visual discriminator (D v ).</p><p>(a) (b) <ref type="figure">Figure 3</ref>. Visualization of generated images from random latent samples when the network is trained (a) without informativenegative mining (b) with informative-negative mining, for digit 9. In the former case, obtained digits are of a different shape in certain instances. For example, the highlighted generated-image looks like a 0. In the latter case, all generated digits consistently look like a 9.</p><p>Visual discriminator is trained to differentiate between images of the given class and images generated from random latent samples using the decoder De(s), where s is a random latent sample. We refer to latter images as fake images for the remainder of the paper. When the visual discriminator is fooled, fake images chosen at random in general will look similar to examples from the given class. We evaluate adversarial loss l visual as follows.</p><formula xml:id="formula_2">l visual = −(E s∼U(−1,1) [log D v (De(s))] + E x∼p l [log(1 − D v (x))])</formula><p>We learn visual discriminator together with the autoencoder network using max De min Dv l visual . Informative-negative Mining: The components described thus far account for the core of the proposed network. Shown in <ref type="figure">Figure 3</ref>(a) is a visualization of fake images obtained by jointly training these three sub-networks using digit 9. <ref type="figure">Figure 3</ref>(a) suggests that the proposed network is able to generate plausible images of the given class for majority of the random latent samples. However, as indicated in the figure there are few cases where the produced output looks different from the given class. For example, the highlighted digit in <ref type="figure">Figure 3</ref>(a) looks more like a zero than a nine.</p><p>This result suggests that despite the proposed training procedure, there are latent space regions that do not produce images of the given class. This is because sampling from all regions in the latent space is impossible during trainingparticularly when the latent dimension is large. A naive solution to this problem is to reduce the dimensionality of the latent space. However, with a lower dimension, the amount of detail the network preserves goes down. As a result, although all latent samples produce an in-class image, a very low dimensionality would diminish performance in novelty detection.</p><p>As an alternative, we propose to actively seek regions in the latent space that produce images of poor quality.</p><p>For the remainder of the paper we refer to these images as informative-negative samples. We use informative-negative samples to train the generator so that it learns to produce good quality in-class images even for these latent samples. However, we continue to use samples chosen at random to train two discriminators, as feeding weaker samples would hinder training of discriminators. In order to find informative-negative samples, first we start with random latent-space samples and use a classifier to assess the quality of the image generated from the sample. The loss of the classifier is used to back-propagate and compute gradients in the latent space. We then take a small step in the direction of the gradient to move to a new point in the latent space where the classifier is confident that the generated image is out-of-class. Classifier: The role of the classifier is to determine how well the given image resembles content of the given class. Ideally such a classifier can be trained using positive and negative examples of a given class. However, since there are no negative training samples available, we train a weaker classifier instead. In the proposed mechanism, if the content belongs to the given class, the classifier deems it positive, and if the content bears no resemblances to the positive class, the classifier deems it negative.</p><p>We train the classifier using reconstructions of in-class samples as positives and fake images, those that are generated from random samples in the latent space, as negatives. This classifier is trained independent of other network elements using binary cross entropy loss l classifier . In other words, the classifier loss is not considered while learning generator and discriminator parameters. Initially, since the quality of fake samples is poor, the classifier is able to obtain very low loss value. As the quality of fake images improves with training, differentiation becomes harder and it forces the classifier to become smarter.</p><p>It should be noted that the classifier's prediction of a given image as a negative may or may not mean that the given image always corresponds to an informative-negative latent sample. Even if it does not, such images do not hinder the training process at all, and training proceeds as usual.</p><p>Since the informative-negative classifier does not participate in the GAN game, there is no requirement to balance the capacity of the classifier with the generator (whereas, this is the case for both other discriminators). Therefore, it is possible to make the classifier very strong to increase its confidence in in-class reconstructions. <ref type="figure">Figure 4</ref> shows the impact of the informative-negative mining procedure using a few illustrative examples. In the figure, image pairs before and after negative mining are displayed. We have shown cases where the original images are not largely changed in the bottom row. In the top row we have shown a few examples where the input images have been substantially altered as a result of informative-negative <ref type="figure">Figure 4</ref>. Informative-negative mining. Shown in the image are image pairs before and after mining process for different digits. In the top row, original images are subjected to substantial changes where they have been converted into a different digits altogether. These are the informative-negatives we are looking for. In the bottom row, the change is not substantial, which means the samples we mined are not informative. However, it still does not hurt our training process. mining. For example, the top left sample of digit 2 appears to be a digit 7 after the process. In <ref type="figure">Figure 3</ref>(b), we show the impact of this procedure by visualizing a few fake images generated from random latent samples for digit 9. It is evident from the figure that informative-negative mining has helped in generating digits of the desired class more consistently across the whole latent space. Full OCGAN Model: The full network of OCGAN and the breakdown of each individual component of the proposed network is shown in <ref type="figure">Figure 5</ref>. The network is trained in two iterative steps. In the first step, all other sub-networks except the classifier network are frozen. The classifier network is trained with reconstructed in-class examples and generated fake examples. Then, it is frozen and the autoencoder and two discriminators are trained adversarially. The latent discriminator is trained based on latent projections of in-class images and random samples drawn from U(−1, 1) distribution. The visual discriminator is trained using fake images generated from random latent samples and real images from the given class. Discriminators are trained by minimizing the loss l latent + l visual .</p><p>Prior to each generator step, informative-negative samples are sought in the latent space using a batch of random samples drawn from the U(−1, 1) distribution, and using gradient descent steps from the classifier's loss in the latent space. The auto-encoder is trained using informativenegative samples and latent projections of (noise-injected) real examples of the given class using 10×l MSE +l visual + l latent . A larger weight is given to the l MSE term to obtain good reconstructions. The coefficient was chosen empirically based on the quality of reconstruction. In our implementation, we started mining for informative-negative samples only after the network started producing fake images of reasonable quality. Steps of the training procedure is summarized in Algorithm 1.</p><p>Input : Set of training data x, iteration size N , parameter λ Output: Models: En, De, C, D l , D v for iteration 1 to → N do Classifier update: keep D l , D v , En, De fixed. n ←− N (0, I)</p><formula xml:id="formula_3">l 1 ←− En(x + n) l 2 ←− U(−1, 1) l classifier ←− C(De(l 2 ), 0) + C(De(l 1 ), 1) Back-propagatel classifier to change C Discriminator update: l latent ←− D l (l 1 , 0) + D l (l 2 , 1) l visual ←− D v (De(l 2 ), 0) + D v (x, 1) Back-propagatel latent + l visual and change D l , D v</formula><p>Informative-negative mining : Keep all networks fixed. for sub-iteration 1 to → 5 do l classifier ←− C(De(l 2 ), 1) Back-propagate l classifier to change l 2 end</p><formula xml:id="formula_4">Generator update: keep D l , D v ,C fixed. l latent ←− D l (l 1 , 1) + D l (l 2 , 0) l visual ←− D v (De(l 2 ), 1) + D v (x, 0) l mse ←− ||x − De(l 1 )|| 2</formula><p>Back-propagate l latent + l visual + λl mse to change En, De end Algorithm 1: Training methodology of the OCGAN model: D l , D v and C represent the outputs of the latent discriminator, visual discriminator and the classifier respectively. En and De are the encoder and the decoder/generator respectively. Real label and fake label are denoted by 1 and 0 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architecture and Hyper-parameter Selection:</head><p>The auto-encoder is a symmetric network with three 5 x 5 convolutions with stride 2 followed by three transposed convolutions. All convolutions and transposed-convolutions are followed by batch normalization and leaky ReLU (with slope 0.2) operations. A tanh activation was placed immediately after the last convolution layer to restrict support of the latent dimension. We used a base channel size of 64 for the auto-encoder and increased number of channels by a factor of 2 with every layer 1 .</p><p>The visual discriminator and classifier are networks with three 5 x 5 convolutions with stride 2. Base channel size of two networks were chosen to be 12 and 64 respectively. Latent discriminator is a fully connected network with layers At the end of training, we selected the model that resulted in minimum MSE on the validation set for evaluation. Model hyper-parameters such as learning rate, latent space size were chosen based on the MSE of validation set. The number of base channels in each network and coefficient of loss terms were decided based on the plot of training loss of each network component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Methodology</head><p>We test the effectiveness of the proposed method using four publicly available multi-class object recognition datasets. In order to simulate a one-class setting, each class at a time is considered as the known class, as proposed in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b18">[19]</ref>. The network is trained using only samples of the known class. During testing, we treat the union of remaining classes as out-of-class samples. Following previous work, we compare the performance of our method using Area Under the Curve (AUC) of Receiver Operating Characteristics (ROC) curve. Here, we note that there exist two protocols in the literature for one-class novelty detection. Protocol 1 : Training is carried out using 80% of in-class samples. The remaining 20% of in-class data is used for testing. Negative test samples are randomly selected so that they constitute half of the test set. Protocol 2 : Use the training-testing splits of the given dataset to conduct training. Training split of the known class is used for training / validation. Testing data of all classes are used for testing.</p><p>The work of <ref type="bibr" target="#b15">[16]</ref> used the 2 nd protocol to evaluate their performance in MNIST <ref type="bibr" target="#b7">[8]</ref>, FMNIST <ref type="bibr" target="#b29">[29]</ref> and COIL100 <ref type="bibr" target="#b10">[11]</ref> datasets, whereas the authors of <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b18">[19]</ref> chose the 1 st protocol on MNIST and CIFAR10 <ref type="bibr" target="#b6">[7]</ref> datasets. We compare our method on these baselines using the relevant protocol for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Experimental Results</head><p>In this section we briefly introduce each dataset used for evaluation and present experimental results for the proposed method. In <ref type="figure">Figure 6</ref>, a few representative examples from the considered datasets are shown. We tabulate results corresponding to Protocol 1 in <ref type="table" target="#tab_1">Table 1</ref> and results of protocol 2 in <ref type="table" target="#tab_2">Tables 2 and 3</ref>. COIL100 : COIL100 is a multi-class dataset where each object class is captured using multiple different poses. There are 100 image classes in the dataset with a few images per class (typically less than hundred). <ref type="figure">Figure 6</ref> suggests that the intra-class difference is very small for this <ref type="figure">Figure 5</ref>. Illustration of OCGAN architecture: the network consists of four sub-networks : an auto-encoder, two discriminators and a classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR10</head><p>COIL FMNIST MNIST <ref type="figure">Figure 6</ref>. Representative images from the datasets used for evaluation. Images in each column belong to the same class.</p><p>dataset. As a result, all considered method produces high AUC values for protocol 1 as shown in <ref type="table" target="#tab_1">Table 1</ref>. Our proposed method of OCGAN records 0.995 AUC, surpassing <ref type="bibr" target="#b15">[16]</ref> which reported AUC of 0.968. fMNIST : fMNIST is intended to be a replacement for MNIST, where the dataset comprises of 28×28 images of fashion apparels/accessories. As evident from <ref type="figure">Figure 6</ref>, fMNIST is a more challenging dataset compared to both COIL100 and MNIST, since there is considerable amount of intra-class variances. The proposed method improves novelty detection performance by over 2% compared to <ref type="bibr" target="#b15">[16]</ref> for this dataset, using protocol 1.</p><p>MNIST : MNIST dataset contains hand-written digits from 0-9 with a 28 × 28 resolution. This dataset has been widely used to benchmark one-class novelty detection results. In terms of complexity, it is an easier dataset compared to fMNIST, but more challenging than COIL100. We report performances of the proposed method on this dataset using both protocols. When protocol 1 was used, our OCGAN model yielded an improvement of about 3% compared to state-of-the-art <ref type="bibr" target="#b15">[16]</ref> method. As shown in <ref type="table" target="#tab_2">Table 2</ref>, when protocol 2 is used, our method has not only registered a better average AUC value, it has reported best AUC for individual classes in 9 out of 10 classes. CIFAR10 : CIFAR10 is an object recognition dataset that consists of images from 10 classes. Out of the considered datasets, CIFAR10 is the most challenging dataset due to it MNIST COIL fMNIST ALOCC DR <ref type="bibr" target="#b19">[20]</ref> 0.88 0.809 0.753 ALOCC D <ref type="bibr" target="#b19">[20]</ref> 0.82 0.686 0.601 DCAE <ref type="bibr" target="#b20">[21]</ref> 0.899 0.949 0.908 GPND <ref type="bibr" target="#b15">[16]</ref> 0.932 0.968 0.901 OCGAN 0.977 0.995 0.924 diverse content and complexity. Specifically, it should be noted that all other datasets are very well aligned, without a background. In comparison, CIFAR10 is not an aligned dataset and it contains objects of the given class across very different settings. As a result, one-class novelty detection results for this dataset are comparatively weaker for all methods. Out of the baseline methods, <ref type="bibr" target="#b18">[19]</ref> has done considerably better than other methods. Following their work, we carried out the same pre-processing in our experiments. In addition, we subtracted the class-mean image from all training and testing images. We obtained comparable results to deep-SVDD with the proposed method where we recorded average AUC of 0.6566.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In order to investigate the effectiveness of each additional component of the proposed work, we carried an ablation study using the MNIST dataset. Specifically, we consider four scenarios. In the first scenario we consider only the auto-encoder. In the second and third scenarios,  we use auto-encoder with the visual and latent discriminators respectively. In the final scenario, we consider the full proposed model, OCGAN. Mean AUC for each class of MNIST dataset is tabulated in <ref type="table" target="#tab_4">Table 4</ref>. We note that the AUC value obtained for the autoencoder is already high at 0.957. Therefore even slightest of improvement from this point is significant. When a latent discriminator is introduced, performance of the system improves marginally by 0.2%. When a visual discriminator is added on top, the performance improves further by 1%. When informative-negative mining as added, performance is further improved by a 0.4%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we dived deep into mechanics of reconstruction-error based novelty detection. We showed that a network trained on a single class is capable of rep-resenting some out-of-class examples, given that in-class objects are sufficiently diverse. In order to combat this issue we introduce a latent-space-sampling-based networklearning procedure. First we restricted the latent space to be bounded and forced latent projections of in-class population to be distributed evenly in the latent space using a latent discriminator. Then, we sampled from the latent space and ensured using a visual discriminator that any random latent sample generates an image from the same class. Finally, in an attempt to reduce false positives we introduced an informative-negative mining procedure. We showed that our OCGAN model outperforms many recently proposed one-class novelty detection methods on four publicly available datasets. Further, by performing an ablation study we showed that each component of the proposed method is important for the functionality of the system.</p><p>Experimental results suggest that the proposed method is effective especially when a single concept is present in images as is the case with COIL, MNIST and fMNIST datasets. In future work we aim to generalize OCGANs to natural image datasets with more complex structure. Further, we wish to investigate their applicability to video novelty detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Limitations of in-class representation based novelty detection. Top: Input images; Middle: Output of an auto-encoder network trained on digit 8. Bottom: Output produced by OC-GAN, the proposed method. Even though auto-encoder network is trained only on digits of 8, it provides good reconstruction for digits from classes 1,5,6 and 9. In contrast, OCGAN forces the latent representation of any example to reconstruct a digit 8. As a result, all out-of-class examples produce high Mean Squared Error (MSE). The intensity of red color in the bottom two rows is proportional to the MSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>sizes 128, 64, 32 and 16 respectively. Batch normalization and ReLu activations were used after each layer in all networks.</figDesc><table><row><cell>1 Source</cell><cell>code</cell><cell>in</cell><cell>MXNet</cell><cell>is</cell><cell>made</cell><cell>available</cell><cell>at</cell></row><row><cell cols="4">https://github.com/PramuPerera/OCGAN.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Mean One-class novelty detection using Protocol 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>One-class novelty detection results for MNIST dataset using Protocol 2.</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>MEAN</cell></row><row><cell>OCSVM [24]</cell><cell cols="11">0.988 0.999 0.902 0.950 0.955 0.968 0.978 0.965 0.853 0.955 0.9513</cell></row><row><cell>KDE [2]</cell><cell cols="11">0.885 0.996 0.710 0.693 0.844 0.776 0.861 0.884 0.669 0.825 0.8143</cell></row><row><cell>DAE [4]</cell><cell cols="11">0.894 0.999 0.792 0.851 0.888 0.819 0.944 0.922 0.740 0.917 0.8766</cell></row><row><cell>VAE [6]</cell><cell cols="11">0.997 0.999 0.936 0.959 0.973 0.964 0.993 0.976 0.923 0.976 0.9696</cell></row><row><cell cols="12">Pix CNN [26] 0.531 0.995 0.476 0.517 0.739 0.542 0.592 0.789 0.340 0.662 0.6183</cell></row><row><cell>GAN [23]</cell><cell cols="11">0.926 0.995 0.805 0.818 0.823 0.803 0.890 0.898 0.817 0.887 0.8662</cell></row><row><cell>AND [1]</cell><cell cols="11">0.984 0.995 0.947 0.952 0.960 0.971 0.991 0.970 0.922 0.979 0.9671</cell></row><row><cell cols="12">AnoGAN [23] 0.966 0.992 0.850 0.887 0.894 0.883 0.947 0.935 0.849 0.924 0.9127</cell></row><row><cell>DSVDD [19]</cell><cell cols="11">0.980 0.997 0.917 0.919 0.949 0.885 0.983 0.946 0.939 0.965 0.9480</cell></row><row><cell>OCGAN</cell><cell cols="11">0.998 0.999 0.942 0.963 0.975 0.980 0.991 0.981 0.939 0.981 0.9750</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>One-class novelty detection results for CIFAR10 dataset using Protocol 2. Plane and Car classes are annotated as Airplane and Automobile in CIFAR10.</figDesc><table><row><cell></cell><cell cols="6">PLANE CAR BIRD CAT DEER DOG FROG HORSE SHIP TRUCK MEAN</cell></row><row><cell>OCSVM [24]</cell><cell>0.630</cell><cell>0.440 0.649 0.487 0.735 0.500 0.725</cell><cell>0.533</cell><cell>0.649</cell><cell>0.508</cell><cell>0.5856</cell></row><row><cell>KDE [2]</cell><cell>0.658</cell><cell>0.520 0.657 0.497 0.727 0.496 0.758</cell><cell>0.564</cell><cell>0.680</cell><cell>0.540</cell><cell>0.6097</cell></row><row><cell>DAE [4]</cell><cell>0.411</cell><cell>0.478 0.616 0.562 0.728 0.513 0.688</cell><cell>0.497</cell><cell>0.487</cell><cell>0.378</cell><cell>0.5358</cell></row><row><cell>VAE [6]</cell><cell>0.700</cell><cell>0.386 0.679 0.535 0.748 0.523 0.687</cell><cell>0.493</cell><cell>0.696</cell><cell>0.386</cell><cell>0.5833</cell></row><row><cell>Pix CNN [26]</cell><cell>0.788</cell><cell>0.428 0.617 0.574 0.511 0.571 0.422</cell><cell>0.454</cell><cell>0.715</cell><cell>0.426</cell><cell>0.5506</cell></row><row><cell>GAN [23]</cell><cell>0.708</cell><cell>0.458 0.664 0.510 0.722 0.505 0.707</cell><cell>0.471</cell><cell>0.713</cell><cell>0.458</cell><cell>0.5916</cell></row><row><cell>AND [1]</cell><cell>0.717</cell><cell>0.494 0.662 0.527 0.736 0.504 0.726</cell><cell>0.560</cell><cell>0.680</cell><cell>0.566</cell><cell>0.6172</cell></row><row><cell>AnoGAN [23]</cell><cell>0.671</cell><cell>0.547 0.529 0.545 0.651 0.603 0.585</cell><cell>0.625</cell><cell>0.758</cell><cell>0.665</cell><cell>0.6179</cell></row><row><cell>DSVDD [19]</cell><cell>0.617</cell><cell>0.659 0.508 0.591 0.609 0.657 0.677</cell><cell>0.673</cell><cell>0.759</cell><cell>0.731</cell><cell>0.6481</cell></row><row><cell>OCGAN</cell><cell>0.757</cell><cell>0.531 0.640 0.620 0.723 0.620 0.723</cell><cell>0.575</cell><cell>0.820</cell><cell>0.554</cell><cell>0.6566</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study for OCGAN performed on MNIST.</figDesc><table><row><cell>Without any Discriminators</cell><cell>0.957</cell></row><row><cell>With latent Discriminator</cell><cell>0.959</cell></row><row><cell>With two Discriminators</cell><cell>0.971</cell></row><row><cell cols="2">Two Discriminators + Classifier 0.975</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">AND: Autoregressive Novelty Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition Conference (CVPR06</title>
		<meeting>Computer Vision and Pattern Recognition Conference (CVPR06</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernel pca for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="863" to="874" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cifar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Types of minority class examples and their influence on learning classifiers from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystyna</forename><surname>Napierala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerzy</forename><surname>Stefanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="563" to="597" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Columbia object image library (coil-20</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sameer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murase</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Active authentication using an autoencoder regularized cnn-based one-class classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poojan</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">One-class convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poojan</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="281" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual-minimax probability machines for one-class mobile active authentication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Biometrics: Theory, Applications, and Systems (BTAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning Deep Features for One-Class Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative Probabilistic Novelty Detection with Adversarial Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Almohsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Novelty detection using extreme value statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proceedings-Vision, Image and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="124" to="129" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anomaly detection using autoencoders with nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takehisa</forename><surname>Yairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MLSDA 2014 2Nd Workshop on Machine Learning for Sensory Data Analysis</title>
		<meeting>the MLSDA 2014 2Nd Workshop on Machine Learning for Sensory Data Analysis</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Objectcentric anomaly detection by attribute-based reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2013 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="787" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPMI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<editor>D. D.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning discriminative reconstructions for unsupervised outlier removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1511" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fashionmnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
