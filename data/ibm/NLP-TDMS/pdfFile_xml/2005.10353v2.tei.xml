<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WHENet: Real-time Fine-Grained Estimation for Wide Range Head Pose</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Zhou</surname></persName>
							<email>yijun.zhou1@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IC Lab</orgName>
								<orgName type="institution" key="instit2">Huawei Technologies</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Gregson</surname></persName>
							<email>james.gregson@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IC Lab</orgName>
								<orgName type="institution" key="instit2">Huawei Technologies</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WHENet: Real-time Fine-Grained Estimation for Wide Range Head Pose</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ZHOU, GREGSON: WHENET 1 available at: https://github.com/Ascend-Research/HeadPoseEstimation-WHENet</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an end-to-end head-pose estimation network designed to predict Euler angles through the full range head yaws from a single RGB image. Existing methods perform well for frontal views but few target head pose from all viewpoints. This has applications in autonomous driving and retail. Our network builds on multi-loss approaches with changes to loss functions and training strategies adapted to wide range estimation. Additionally, we extract ground truth labelling of anterior views from a current panoptic dataset for the first time. The resulting Wide Headpose Estimation Network (WHENet) is the first fine-grained modern method applicable to the full-range of head yaws (hence wide) yet also meets or beats state-of-the-art methods for frontal head pose estimation. Our network is compact and efficient for mobile devices and applications. Code will be available at: https://github.com/Ascend-Research/HeadPoseEstimation-WHENet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Head pose estimation (HPE) is the task of estimating the orientation of heads from images or video (see <ref type="figure" target="#fig_2">Figure 2</ref>(a)) and has seen considerable research. Applications of HPE are wide-ranging and include (but are not limited to) virtual &amp; augmented-reality <ref type="bibr" target="#b24">[25]</ref>, driver assistance, markerless motion capture <ref type="bibr" target="#b9">[10]</ref> or as an integral component of gaze-estimation <ref type="bibr" target="#b23">[24]</ref> since gaze and head pose are tightly linked <ref type="bibr" target="#b20">[21]</ref>.</p><p>The importance of HPE is well described in <ref type="bibr" target="#b23">[24]</ref>. They describe wide-ranging social interactions such as mutual gaze, driver-pedestrian &amp; driver-driver interaction. It is also important in providing visual cues for the targets of conversation, to indicate appropriate times for speaker/listener role switches as well as to indicate agreement <ref type="bibr" target="#b23">[24]</ref>. For systems to interact with people naturally, it is important to be sensitive to head pose.</p><p>Most HPE methods target frontal to profile poses since applications are plentiful, the face is feature-rich and training datasets are widely available. However, covering full range is useful in many areas including driver assistance <ref type="bibr" target="#b25">[26]</ref>, motion-capture and to generate attention maps for advertising and retail <ref type="bibr" target="#b36">[37]</ref>.</p><p>Furthermore, though methods such as <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref> perform well, they may be prohibitively large networks for mobile and embedded platforms. This is especially important in autonomous driving where many subsystems must operate concurrently. c 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.   <ref type="bibr" target="#b32">[33]</ref> by combining an EfficientNet <ref type="bibr" target="#b39">[40]</ref> backbone with classification and regression losses on each of pitch, yaw and roll. A new wrapped-loss stabilizes the network at large yaws. Training this network using anterior views repurposed from <ref type="bibr" target="#b17">[18]</ref> (lower-left) via a novel reprocessing procedure allows WHENet to predict head poses from anterior view as well as heads featuring heavy occlusions, fashion accessories and adverse lighting (right) with a mobile-friendly model. In spite of this, WHENet is as accurate or more accurate than existing state-of-the-art methods that are constrained to frontal or profile views. Some images are from <ref type="bibr" target="#b50">[51]</ref>&amp; <ref type="bibr" target="#b40">[41]</ref> Based on these criteria, we developed the Wide Headpose Estimation Network (WHENet) which extends head-pose estimation to the full range of yaws (hence wide) using a mobilefriendly architecture. In doing so, we make the following contributions:</p><p>• We introduce a wrapped loss that significantly improves yaw accuracy for anterior views in full-range HPE.</p><p>• We detail an automated labeling process for the CMU Panoptic Dataset <ref type="bibr" target="#b17">[18]</ref> allowing it to be used for training and validation data in full-range HPE.</p><p>• We extend our modified network (WHENet) to the full range of yaws where it achieves state-of-the-art performance for full-range HPE and within 1.8% of state-of-the-art for narrow range HPE despite being trained for another task.</p><p>• We demonstrate that simple modifications to HopeNet <ref type="bibr" target="#b32">[33]</ref> can achieve 29% improvement over the original network and 5 − 13% improvement over the current state-ofthe-art FSANet <ref type="bibr" target="#b47">[48]</ref> for narrow range HPE from RGB images in the AFLW2000 <ref type="bibr" target="#b50">[51]</ref> and BIWI <ref type="bibr" target="#b11">[12]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background &amp; Related Work</head><p>Head pose estimation has been actively researched over the past 25 years. Approaches up to 2008 are well surveyed in <ref type="bibr" target="#b23">[24]</ref> and much of our discussion on early methods follows their work. Due to the breadth of research activity and our target application of HPE from monocular RGB images, we exclude multi-view or active sensing methods from this review. Classical methods include template matching and cascaded detectors. Template matching compares input images with a set of labeled templates and assign a pose based on nearby matches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>. They can have difficulty differentiating between identity and pose similarity <ref type="bibr" target="#b23">[24]</ref>. Cascaded detectors train distinct detectors that also localize heads for each (discretized) pose. Challenges include prediction resolution and resolving when multiple detectors fire <ref type="bibr" target="#b23">[24]</ref>.</p><p>Geometric &amp; deformable models are similar methodologies. Geometric models use features, e.g. facial keypoints, from the input images and analytically determine the matching head pose using a static template <ref type="bibr" target="#b6">[7]</ref>. The majority of their complexity lies in detecting the features, which is itself a well studied problem, see e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref> and survey <ref type="bibr" target="#b41">[42]</ref>. Deformable models are similar but allow the template to deform to match subject-specific head features. Pose or other information is obtained from the deformed model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Regression &amp; classification methods serve as supersets or components of most other methods. Regression methods use or fit a mathematical model to directly predict pose based on labeled training data. The formulation of the regressor is wide-ranging and includes principle component analysis <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b51">52]</ref> and neural networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33]</ref> among others. In contrast, classification methods predict pose from a discretized set of poses. Prediction resolution tends to be lower, generally less than 10-12 discrete poses. Methods include decision trees &amp; random forests <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>, multi-task learning <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> and neural networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33]</ref>. Our networks are most similar to the multi-loss framework in <ref type="bibr" target="#b32">[33]</ref> which uses classification and regression objectives. Other works employ soft stage-wise regression <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> by training with both classification and regression objectives at multiple scales.</p><p>Multi-tasks methods combines the head pose estimation problem with other facial analysis problems. Studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">50]</ref> show that learning related tasks at the same time can achieve better performance than training tasks individually. For instance, in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> face detection and pose regression are trained jointly.</p><p>Full-range methods are much less common than narrow-range since most existing datasets for HPE focus on frontal to profile views. Recent methods include <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> which classify poses into coarsely-grained bins/classes to determine yaw. Unlike our method, pitch and roll are not predicted in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Datasets for facial pose include BIWI <ref type="bibr" target="#b11">[12]</ref>, AFLW2000 <ref type="bibr" target="#b50">[51]</ref> and 300W-LP <ref type="bibr" target="#b50">[51]</ref>. Both AFLW2000 and 300W-LP use a morphable model fit to faces under large pose variation and report Euler angles. 300W-LP generates additional synthetic views to enlarge the dataset. More recently, the UMD Faces <ref type="bibr" target="#b1">[2]</ref> and Pandora <ref type="bibr" target="#b3">[4]</ref> datasets provide a range of data labels, including head pose. A disadvantage for our application is that they do not cover the fullrange of head poses but only frontal-to-profile views.</p><p>Very important to our method is the CMU Panoptic Dataset <ref type="bibr" target="#b17">[18]</ref>. It captures subjects from an abundance of calibrated cameras covering a full-hemisphere and provides facial landmarks in 3D. Using this data, we are able to estimate head pose from near-frontal views and use this pose to label non-frontal viewpoints. By doing so, we cover the full range of camera-relative poses, allowing our method to be trained with anterior views. This is discussed further in Section 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>Our network design is derived from the multi-loss framework of Ruiz et al. <ref type="bibr" target="#b32">[33]</ref>. They combine a convolutional backbone with separate fully-connected networks that classify each of pitch, yaw and roll into 3 • bins using softmax with a cross-entropy loss. A mean squared error (MSE) regression loss is also applied between the ground-truth labels and the expected value of the softmax output. The two losses are weighted to produce the final training objective for each angle.</p><p>In <ref type="bibr" target="#b32">[33]</ref>, Ruiz et al. suggest that this combination of losses gives the resulting network robustness &amp; stability due to the softmax &amp; cross-entropy loss while still providing fine- where networks trained with MSE have errors approaching 180 • for yaw. Note the x (red) axis should align with the left ear of subjects. In (c), we compute virtual camera extrinsics oriented to provide a frontal view along with true extrinsics to extract Euler angles from the CMU Panoptic dataset <ref type="bibr" target="#b17">[18]</ref>. This allows us to automatically label tens of thousands of anterior view images for the full-range HPE task. We believe we are the first to do so.</p><p>grained supervision and output via the regression loss. We adopt this overall framework from <ref type="bibr" target="#b32">[33]</ref> but make substitutions for both loss functions in order to adapt the method to full-range. To our knowledge we are the first to do so. Yaw prediction is divided into 120 3 </p><formula xml:id="formula_0">L = αL reg + β L cls<label>(1)</label></formula><p>Here L cls is the classification loss and L reg is the regression loss while α and β trade off the influence of one with respect to the other. We tested different classification losses and chose a sigmoid activation with binary crossentropy for L cls . This differs from <ref type="bibr" target="#b32">[33]</ref> and shows marginally improved accuracy for the full-range task but is mentioned primarily for reproducibility.</p><p>As in <ref type="bibr" target="#b32">[33]</ref> we predict output angles from the bin logits by applying softmax to obtain bin probabilities and take an expectation of the result for each of yaw, pitch and roll:</p><formula xml:id="formula_1">θ pred = 3 N ∑ i=1 p i i − 1 + N 2 (2)</formula><p>Here p i is the probability of the i'th bin, 3 is the bin width in degrees and N is the bin-count of either 120 (yaw) or 66 (pitch and roll). The subtracted term shifts bin indices to bin centres. In <ref type="bibr" target="#b32">[33]</ref>, a mean squared error (MSE) loss was used for regression which was sufficient for the limited range of yaws that were targeted. However, on the full-range task, this loss leads to erratic behaviour for subjects with absolute yaws exceeding 150 • . This is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>(b) and is due to ±180 • having wildly different angles for the same pose.</p><p>To prevent this, we define a wrapped loss (see <ref type="figure" target="#fig_2">Figure 2</ref>(b)) that avoids this behavior. Rather than penalizing angle directly, it penalizes the minimal rotation angle that is needed to align each yaw prediction with its corresponding dataset annotation:</p><formula xml:id="formula_2">L wrap (θ pred , θ true ) = 1 N batch N batch ∑ i min[|θ (i) pred − θ (i) true | 2 , (360 − |θ (i) pred − θ (i) true |) 2 ]<label>(3)</label></formula><p>The loss function L wrap is plotted in <ref type="figure" target="#fig_2">Figure 2</ref>(b) for the target value θ true = 150 • and compared with an MSE loss. In the range (−30, 180] the two are identical but diverge for angles &lt; −30 • . As they diverge MSE increases rapidly but the wrapped loss decreases as poses become more similar. The wrapped loss is smooth &amp; differentiable everywhere except at a cusp 180 • from the ground-truth yaw. This does make training the network more difficult, we suspect this is due to the cusp occurring at maximal errors in yaw. The wrapped loss is key to the method's performance in anterior views, demonstrated empirically in <ref type="figure" target="#fig_2">Figure 2</ref>(b). A MSE trained network predicts entirely incorrect head poses while our predictions are consistent with the images. <ref type="figure" target="#fig_4">Figure 3</ref> plots errors as a function of angles and shows average errors at large yaw are decreased by more than 50%.</p><p>In <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref> AlexNet and ResNet50 were used as backbones. These are quite large networks for a highly specialized task such as HPE. Since a focus of our method is to be mobilefriendly, we instead opted for a lighter backbone: EfficientNet-B0 <ref type="bibr" target="#b39">[40]</ref>. EfficientNet-B0 is the baseline model of the EfficientNet family that incorporates Inverted Residual Blocks (from MobileNetV2 <ref type="bibr" target="#b33">[34]</ref>) to reduce the number of parameters while adding skip connections.</p><p>This model size reduction is important on low-power embedded devices where head pose estimation may be only one component of a larger system. We have successfully ported a preliminary implementation to a low-power embedded platform where we see inference speeds approaching 60fps</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets &amp; Training</head><p>Currently, major datasets for HPE are 300W-LP <ref type="bibr" target="#b50">[51]</ref>, AFLW2000 <ref type="bibr" target="#b50">[51]</ref> and BIWI <ref type="bibr" target="#b11">[12]</ref>. Both AFLW2000 &amp; 300W-LP use 3D Dense Face Alignment (3DDFA) to fit a morphable 3D model to 2D input images and provide accurate head poses as ground-truth. 300W-LP additionally generates synthetic views, greatly expanding the number of images.</p><p>The BIWI dataset is composed of video sequences collected by a Kinect. Subjects moved their heads trying to span all possible angles observed from a frontal position. Pose annotations were created from the depth information.</p><p>We follow the convention of <ref type="bibr" target="#b32">[33]</ref> by reserving AFLW2000 <ref type="bibr" target="#b50">[51]</ref> and BIWI <ref type="bibr" target="#b11">[12]</ref> for testing, while using 300W-LP <ref type="bibr" target="#b50">[51]</ref> for training.</p><p>Unfortunately, none of datasets mentioned above provide examples with (absolute) yaws larger than 100 • . To overcome this, we generated a new dataset combining 300W-LP with data from the CMU Panoptic Dataset <ref type="bibr" target="#b17">[18]</ref>. The CMU Panoptic Dataset captured video of subjects performing tasks in a dome from approximately 30 HD cameras.</p><p>The panoptic dataset includes 3D facial landmarks and calibrated camera extrinsics and intrinsics but does not include head pose information. We use the landmarks and camera calibrations to locate and crop images of subjects' heads and to compute the corresponding camera-relative head pose Euler angles. We believe we are the first to use this dataset in this context. <ref type="figure" target="#fig_1">Figure 1</ref> shows a frame from the dataset. The panoptic dataset has very little background variation and so cannot be used to train networks alone since networks do not learn features to differentiate subjects from general backgrounds. This is the motivation to combine it with 300W-LP, which is used for yaws in (−99 • , 99 • ) while the panoptic dataset provides data mostly outside this range.</p><p>Processing the CMU Panoptic Dataset. To compute camera-relative head pose Euler angles from the panoptic dataset we use the following procedure, depicted graphically in <ref type="figure" target="#fig_2">Figure 2</ref>(c). We first define a set of reference 3D facial landmarks (x re f ) matching the panoptic dataset facial keypoints annotations using a generic head model, except for the noisy jawline keypoints. A reference camera with intrinsics &amp; extrinsics (K re f ,E re f ) is then positioned to obtain a perfectly frontal view (yaw=pitch=roll=0) of x re f . For each subject in each frame, we estimate the rigid transformation R between x re f and the true keypoints x real provided by the panoptic dataset using <ref type="bibr" target="#b16">[17]</ref>. We then construct new extrinsics for a virtual camera E virt = E re f R −1 . This provides a (nominally) frontal view of the subject as they are positioned within the dome. Using the known real camera extrinsics E real from the panoptic dataset and E virt , we recover the rigid transformation T = E real E −1 virt between the virtual camera extrinsics and each real camera. Finally we extract Euler angles in pitch-yaw-roll (x-y-z) order from T , mirroring yaw and roll in order to match the rotation convention from the datasets. There is a translation component to the rigid transform but it is not needed to determine the head orientation with respect to the camera optical axes.</p><p>Using the synthetic reference and virtual camera in this process ensures that we have a consistent and automatic method for labeling. It also considerably reduces noise compared to manually selecting a frontal camera and annotating corresponding Euler angles.</p><p>To crop images, we define a spherical set of points around the subject's head as a helmet and project these into each view to determine a cropping region. Inspired by Ming et al. <ref type="bibr" target="#b34">[35]</ref>, we leave a margin from the head bounding box so the network can learn to distinguish foreground and background. We followed the method of <ref type="bibr" target="#b34">[35]</ref>, using K = 0.5 for data from 300W-LP and adjust the helmet radius to 21cm for CMU data.</p><p>Training of WHENet is devided into two stages. We first train a narrow range model, WHENet-V with prediction range between [−99 • , 99 • ] for yaw, pitch and roll. We train this narrow range network on the 300W-LP <ref type="bibr" target="#b50">[51]</ref> dataset using an ADAM optimizer with learning rate 1e-5. Full-range WHENet with 120 yaw bins is then trained starting from the WHENet-V weights using our full-rage data set combining 300W-LP and CMU Panoptic Dataset data. We use the same optimizer and learning rate for this step. This two-stage approach helps the full-range network converge better and learn more useful features since the CMU data has little background variation. During training, images are randomly downsampled by up to 15X to improve robustness.</p><p>Training data from 300W-LP and the panoptic dataset is used in nearly equal amounts, with the former providing narrow range samples and the latter primarily handling wide range. A small amount of panoptic data is also used to level a dip in the histogram of narrow range images near yaw=0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results &amp; Discussion</head><p>For wide range results in this section, we define the Absolute Wrapped Error (AWE) as AW E = min |θ pred − θ true |, 360 − |θ pred − θ true | which properly handles wrapping of yaws. We also define Mean AWE (MAWE) as the arithmetic mean of AWE. For results below, we are using α = 1, β = 1 for WHENet and α = 0.5, β = 2 for WHENet-V based on our ablations. Hyperparameter details can be found in our supplementary. <ref type="table" target="#tab_0">Table 1</ref> summarizes key results from WHENet and WHENet-V. We compare with eight narrow &amp; two full-range methods and report, where applicable/available, the output range, number of parameters, mean average errors on BIWI <ref type="bibr" target="#b11">[12]</ref>, AFLW-2000 <ref type="bibr" target="#b50">[51]</ref>, an average of both (to indicate generalization) and reported full-range MAE if applicable. WHENet &amp; WHENet-V are trained on 300W-LP <ref type="bibr" target="#b50">[51]</ref> or our combined dataset and are not trained on the AFLW2000 or BIWI datasets, as is standard practice. We begin with a discussion of the full-range network WHENet, as this is our primary application. Full-range WHENet results and comparisons are shown in <ref type="table" target="#tab_0">Table 1</ref>. Of the three full-range methods compared, WHENet is the only method to also predict pitch and roll. Comparisons are difficult to perform objectively since both Raza et al. <ref type="bibr" target="#b30">[31]</ref> and Rehder et al. <ref type="bibr" target="#b31">[32]</ref> predict yaws only (no pitch and roll) and do so on non-public datasets. For <ref type="bibr" target="#b30">[31]</ref>, MAE was not reported, so we report the lowest possible errors given a uniform distribution of yaws with their bin-widths (i.e. the result if their method performed perfectly). WHENet still shows a 31% improvements, although these results should be treated as qualitative given lack of consistent testing data. This latter point is something we attempt to address in this work by making our dataset processing code for the CMU Panoptic Dataset <ref type="bibr" target="#b17">[18]</ref> public.</p><p>Additionally, excluding WHENet-V, full-range WHENet has the lowest average errors on BIWI, second-lowest errors on AFLW2000 and has second lowest average overall errors, missing first place by 0.084 • (1.8%) to FSANet <ref type="bibr" target="#b47">[48]</ref>. This is significant since full-range WHENet was not trained specifically for the narrow range task, yet is still state-of-the-art or very competitve with FSANet <ref type="bibr" target="#b47">[48]</ref>, a significantly more complex network. In handling fullrange inputs, WHENet is significantly more capable than FSANet, yet sacrifices remarkably little accuracy. <ref type="figure" target="#fig_2">Figure 2</ref>(a) shows pose predictions generated using WHENet to track a subject rotating through a full revolution of yaw. WHENet produces coherent predictions throughout, even when the face is completely absent. A key to achieving this is our wrapped loss function, which significantly improves anterior views. Without the wrapped loss predictions are erratic and often nearly 180 • from the true pose as shown in <ref type="figure" target="#fig_2">Figure 2(b)</ref>. This improvement is enabled by our annotation process for the CMU panoptic dataset <ref type="bibr" target="#b17">[18]</ref> which allows us to automatically generate labelings for tens of thousands of anterior views to tackle this chal-  <ref type="bibr" target="#b18">[19]</ref>, KEPLER <ref type="bibr" target="#b19">[20]</ref> and FAN <ref type="bibr" target="#b5">[6]</ref> are all landmark based methods. 3DDFA <ref type="bibr" target="#b50">[51]</ref> uses CNN to fit a 3D dense model to a RGB image. Hopenet <ref type="bibr" target="#b32">[33]</ref> and FSANet <ref type="bibr" target="#b47">[48]</ref> are two landmark-free methods both of which explore the possbility of treating a continuous problem (head pose) into different classes/stages. As showed in Tables 2, WHENet-V achieves state-of-the-art accuracy on both BIWI <ref type="bibr" target="#b11">[12]</ref> and AFLW2000 <ref type="bibr" target="#b50">[51]</ref>, and outperforms the previous state-of-the-art FSANet <ref type="bibr" target="#b47">[48]</ref> by 0.52 • (13.1%) and 0.24 • (4.7%) respectively, leading to an aggregate improvement of 0.38 • (8.4%) on the two datasets overall. It also achieve first place in every metric. Interestingly, WHENet-V shows a 29% and 21% improvement over HopeNet <ref type="bibr" target="#b32">[33]</ref> on these datasets, despite of having a very similar network architecture. Ablation studies for the change of backbone and loss functions are shown in <ref type="figure" target="#fig_4">Figure 3</ref> and <ref type="table" target="#tab_2">Table 3</ref>. <ref type="figure" target="#fig_4">Figure 3</ref> plots the mean errors at different angle intervals between wrapped regression loss (WHENet) and MSE loss (WHENet MSE).The mean error in yaw for extreme poses (close to -180/180 degrees) reaches 35 degrees when using MSE loss. By introducing the wrapped loss, errors for these angles are reduced by more than 50% and are more consistent with those from medium or low yaws. High pitch and roll errors are typical in HPE methods as yaws approach ±90 • as seen in <ref type="bibr" target="#b34">[35]</ref>, we believe due to gimbal lock in the data labeling. Additionally, a quantitative result comparison can be found in <ref type="table" target="#tab_2">Table 3</ref>. WHENet CE uses cross-entropy loss for classification and wrapped regression loss for regression. WHENet MSE uses the binary cross-entropy loss for classification and MSE loss for regression. WHENet uses binary cross-entropy loss for classification and wrapped regression loss for regression. We can see in the wide-range dataset (combine), WHENet has superior performance in yaw which confirms our observations in <ref type="figure" target="#fig_4">Figure 3</ref>. Although WHENet sacrifices some performance in the narrow range testing compared with other two loss settings, our main focus is in the wide-range predictions where we see significantly improved errors at large yaws. We believe performance could be improved by adapting the template keypoints used in annotating the CMU dataset to each subject rather than using a fixed template but leave this to future work. The supplement provides additional ablation studies on the metaparameters α and β as well the effects of resolution and relative comparison to video and depth-based methods that are outside our target application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and future work</head><p>In this paper we have presented WHENet, a new method for HPE that can estimate head poses in the full 360 degree range of yaws. This is achieved by careful choice of our wrapped loss function as well as by developing an automated labeling method for the CMU Panoptic Dataset <ref type="bibr" target="#b17">[18]</ref>. We believe we are the first to adapt this dataset to the specific task of head-pose estimation. WHENet meets or exceeds the performance of state-of-the-art methods tuned for the specific task of frontal-to-profile HPE in spite of being trained for the full range of yaws. We are not aware of competing methods with similar capabilities and accuracy.</p><p>In the future, we would like to extend upon this work by reducing network size even further. Reducing input image resolution has the potential to lower memory usage as well as allow shallower networks with fewer features. This could yield even further improvements in speed and size of network.</p><p>Another interesting avenue is modifying the representation of head pose. Euler angles are minimal and interpretable but have the drawback of gimbal lock. The pitch-yaw-roll rotation ordering of existing datasets such as AFLW2000 and BIWI emphasize this effect at yaws near ±90 • where pitch and roll effectively counteract each other. We believe this leads to the relatively high pitch and roll errors near profile poses seen in <ref type="figure" target="#fig_4">Figure 3</ref> which can also be seen in methods such as Shao et al. <ref type="bibr" target="#b34">[35]</ref>. Relabeling data to yaw-pitch-roll order might reduce this. Similarly other rotation representations such as axis-angle, exponential map or quaternions may help, though adapting the architecture to alternative rotation representations is likely non-trivial due to the classification networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgment</head><p>We thank Shao Hua Chen and Tanmana Sadhu for their insightful discussions. We also thank Zhan Xu, Peng Deng, Rui Ma, Ruochen Wen, Qiang Wang and other colleagues in Huawei Technologies for their support in the project as well as the anonymous reviewers whose comments helped to improve the paper.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Robustness</head><p>A key objective of WHENet is to be robust to adverse imaging conditions as well as occlusions and accessories such as eyewear and hats. Much of the robustness of WHENet can be derived from using a similar network architecture as Hopenet <ref type="bibr" target="#b32">[33]</ref> which also performs well due to the CNN architecture. <ref type="figure" target="#fig_5">Figure 4</ref> shows a selection of occluded face images where the subject tried to maintain consistent head pose while blocking areas of their face. The angular predictions are quite stable with angles varying by only 7 • in spite of siginificant occlusions of features (some underlying variation of pose is expected due to subject motion). This suggests the method is learning high-level features rather than specific localized details. We also evaluted the effect of resolution. <ref type="figure" target="#fig_6">Figure 5</ref> illustrates qualitatively that prediction accuracy is not seriously degraded by aggressive downsampling of up to 16X. We carried out this test in aggregate on the AFLW2000 dataset. The results are shown in <ref type="figure">Figure 6</ref> and compared to Hopenet <ref type="bibr" target="#b32">[33]</ref> and FSANet <ref type="bibr" target="#b47">[48]</ref>. We list the smallest reported errors for Hopenet among the four training strategies in <ref type="bibr" target="#b32">[33]</ref> and thank the authors for providing this data. <ref type="figure">Figure 6</ref>: Effect of downsampling factor on MAE. WHENet (orange) shows consistent improvement over the already impressive Hopenet <ref type="bibr" target="#b32">[33]</ref> and FSANet <ref type="bibr" target="#b47">[48]</ref> performance (grey and black). For Hopenet we plot the minimum (best) value at each downsampling factor among all training strategies reported in <ref type="bibr" target="#b32">[33]</ref> In summation, full-range WHENet targets a task that is outside the scope of the existing state-of-the-art using a faster and significantly smaller network. In spite of this, it meets or beats state-of-the-art performance for the restricted case of HPE for frontal-to-profile views when evaluated on two datasets that were not used during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Applications</head><p>Here we show qualitative examples of WHENet applied to several applications that demonstrate how HPE can integrate with real-world systems and how our training strategy allows <ref type="figure">Figure 7</ref>: WHENet applied to head crops generated from keypoint predictions from <ref type="bibr" target="#b27">[28]</ref>, keypoints shown as dots, illustrating how HPE can be integrated with full-body pose estimation methods. Images from <ref type="bibr" target="#b42">[43]</ref> the method to generalize to low-resolution and low-quality data that was not present during training. <ref type="figure">Figure 7</ref> shows using a pose detector based on Lightweight OpenPose <ref type="bibr" target="#b27">[28]</ref> code to detect pose keypoints while using WHENet to predict head pose. Frequently pose-estimations do not estimate sufficient keypoints for accurate HPE but by incorporating a full-range HPE method such as WHENet, such limitations may be overcome. This could be used, for example, in sports broadcasting or by coaching staff to estimate participants fields of views and situational awareness when analyzing plays. <ref type="figure">Figure 8</ref> depicts a hypothetical driver-attention module where drivers are considered attentive with camera-relative yaws &lt; 30 • and inattentive otherwise. The extension to fullrange could extend this to predicting blind spots during other activities such as reversing without requiring additional hardware. <ref type="figure">Figure 8</ref>: Applications to autonomous driving and driver assistance. Left: Green boxes indicate yaws &lt; ±45 • and potential awareness of vehicle, red boxes indicate probable inattention. This example highlights the need for efficient and low-resolution approaches to HPE with 6 total low-resolution detections. Here low-quality pose-estimates yield poor cropping regions but WHENet successfully generalizes despite having no comparable training data. Images from <ref type="bibr" target="#b12">[13]</ref>. Right: WHENet is used to monitor driver attention, marking the driver as inattentive (red) when yaw exceeds 30 • and attentive (green) otherwise. Images from <ref type="bibr" target="#b0">[1]</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2005.10353v2 [cs.CV] 22 Sep 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The WHENet full-range head pose estimation network (center) refines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Head pose (a) is parameterized by pitch (red-axis), yaw (green-axis) and roll (blueaxis) angles in indicated directions. Our proposed wrapped loss function (b-left) avoids over-penalizing predictions &gt; 180 • from true where pose is similar but MSE produces extreme loss values. This improves predictions when subjects face away from camera (b-right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>• bins covering the full range of yaws (−180 • , 180 • ]. Pitch and roll predictions are each made from 66 3 • bins covering the range [−99 • , 99 • ], although only the bins from [−90, 90] are ultimately used. The regression and classification losses for each of pitch, yaw and roll are combined via:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Histogram of the errors on our hybrid dataset for WHENet and WHENet (MSE), errors at high yaw are signficantly reduced with our proposed wrapped loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Head pose estimation with occlusion. Subjects asked to remain still while covering different regions of their face. Predicted deviations are within 7 • of the unoccluded view (left). Some amount of deviation is expected due to slight subject motions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Downsampling factor vs. yaw, pitch &amp; roll. Ground-truth values were 47.6, 22.0, 18.8. Images were downsampled by indicated amount and then resized to their original size using nearest-neighbor interpolation before being supplied to WHENet. Head pose predictions remain relatively stable event when images are aggressively downsampled by up to 16X. Original image from<ref type="bibr" target="#b50">[51]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of results: 'Aggr. MAE': average of BIWI and AFLW2000 overall results. 'Full MWAE': full-range results. first/only, second, -: not reported</figDesc><table><row><cell>Method</cell><cell cols="5">Full Params BIWI AFLW2k Avg.</cell><cell>Full</cell></row><row><cell></cell><cell cols="6">Range (×10 6 ) MAE • MAE • MAE • MAWE •</cell></row><row><cell>KEPLER [20]</cell><cell>N</cell><cell>-</cell><cell>13.852</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Dlib (68 points) [19]</cell><cell>N</cell><cell>-</cell><cell cols="3">12.249 15.777 14.013</cell><cell>-</cell></row><row><cell>FAN (12 points) [6]</cell><cell>N</cell><cell cols="4">6-24 7.882 9.116 8.499</cell><cell>-</cell></row><row><cell>3DDFA [51]</cell><cell>N</cell><cell>-</cell><cell cols="3">19.07 7.393 13.231</cell><cell>-</cell></row><row><cell cols="2">Shao et al. (K=0.5) [35] N</cell><cell cols="4">24.6 5.999 5.478 5.875</cell><cell>-</cell></row><row><cell>Hopenet [33]</cell><cell>N</cell><cell cols="4">23.9 4.895 6.155 5.525</cell><cell>-</cell></row><row><cell>SSR-Net-MD [47]</cell><cell>N</cell><cell cols="4">0.2 4.650 6.010 5.330</cell><cell>-</cell></row><row><cell>FSA-Caps-Fusion [48]</cell><cell>N</cell><cell cols="4">1.2 4.000 5.070 4.535</cell><cell>-</cell></row><row><cell>Rehder et al. [32]</cell><cell>Yaw</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>19.0</cell></row><row><cell>Raza et al. [31]</cell><cell>Yaw</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>11.25</cell></row><row><cell>WHENet-V</cell><cell>N</cell><cell cols="4">4.4 3.475 4.834 4.155</cell><cell>-</cell></row><row><cell>WHENet</cell><cell>Y</cell><cell cols="5">4.4 3.814 5.424 4.619 7.655</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art on BIWI<ref type="bibr" target="#b11">[12]</ref>(left) and AFLW2000<ref type="bibr" target="#b50">[51]</ref>(right) dataset. Best (bold underlined) and second best (bold) results are highlighted.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">BIWI</cell><cell></cell><cell></cell><cell cols="2">AFLW2000</cell></row><row><cell>Method</cell><cell>Yaw</cell><cell cols="7">Pitch Roll MAE Yaw Pitch Roll MAE</cell></row><row><cell>KEPLER [20]</cell><cell>8.80</cell><cell>17.3</cell><cell cols="2">16.2 13.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Dlib (68 points) [19]</cell><cell>16.8</cell><cell>13.8</cell><cell cols="2">6.19 12.2</cell><cell cols="2">23.1 13.6</cell><cell cols="2">10.5 15.8</cell></row><row><cell>FAN (12 points) [6]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">8.53 7.48</cell><cell cols="2">7.63 7.88</cell></row><row><cell>3DDFA [51]</cell><cell cols="6">36.20 12.30 8.78 19.10 5.40 8.53</cell><cell cols="2">8.25 7.39</cell></row><row><cell>Hopenet(a = 1) [33]</cell><cell>4.81</cell><cell>6.61</cell><cell cols="2">3.27 4.90</cell><cell cols="2">6.92 6.64</cell><cell cols="2">5.67 6.41</cell></row><row><cell>Hopenet(a = 2) [33]</cell><cell>5.12</cell><cell>6.98</cell><cell cols="2">3.39 5.12</cell><cell cols="2">6.47 6.56</cell><cell cols="2">5.44 6.16</cell></row><row><cell cols="2">Shao et al. (K=0.5) [35] 4.59</cell><cell>7.25</cell><cell cols="2">6.15 6.00</cell><cell cols="2">5.07 6.37</cell><cell cols="2">4.99 5.48</cell></row><row><cell>SSR-Net-MD [47]</cell><cell>4.49</cell><cell>6.31</cell><cell cols="2">3.61 4.65</cell><cell cols="2">5.14 7.09</cell><cell cols="2">5.89 6.01</cell></row><row><cell>FSA-Caps-Fusion [48]</cell><cell>4.27</cell><cell>4.96</cell><cell cols="2">2.76 4.00</cell><cell cols="2">4.50 6.08</cell><cell cols="2">4.64 5.07</cell></row><row><cell>WHENet-V</cell><cell>3.60</cell><cell>4.10</cell><cell cols="2">2.73 3.48</cell><cell cols="2">4.44 5.75</cell><cell cols="2">4.31 4.83</cell></row><row><cell>WHENet</cell><cell>3.99</cell><cell>4.39</cell><cell cols="2">3.06 3.81</cell><cell cols="2">5.11 6.24</cell><cell cols="2">4.92 5.42</cell></row><row><cell>lenge.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Narrow range results and comparisons are listed in Tables 2. Dlib</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Result comparison of different loss settings. Combine indicates our combined dataset of CMU panoptic and 300W-LP</figDesc><table><row><cell></cell><cell cols="2">Combine</cell><cell cols="2">BIWI</cell><cell cols="2">AFLW2000</cell></row><row><cell></cell><cell cols="6">Yaw Pitch Roll MWAE Yaw Pitch Roll MAE Yaw Pitch Roll MAE</cell></row><row><cell>WHENet CE</cell><cell>8.75 7.65</cell><cell>6.74 7.71</cell><cell>3.51 4.13</cell><cell>3.04 3.56</cell><cell>5.50 6.36</cell><cell>4.94 5.60</cell></row><row><cell cols="2">WHENet MSE 9.69 7.15</cell><cell>6.19 7.68</cell><cell>3.79 4.82</cell><cell>3.21 3.94</cell><cell>5.03 6.41</cell><cell>5.16 5.53</cell></row><row><cell>WHENet</cell><cell>8.51 7.67</cell><cell>6.78 7.66</cell><cell>3.99 4.39</cell><cell>3.06 3.81</cell><cell>5.11 6.24</cell><cell>4.92 5.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>WHENet-V MAE vs. α and β on AFLW2000</figDesc><table><row><cell></cell><cell cols="2">α = 0.5 α = 1 α = 2</cell></row><row><cell>β = 0.5</cell><cell>4.984</cell><cell>4.966 5.113</cell></row><row><cell>β = 1</cell><cell>4.946</cell><cell>5.146 4.904</cell></row><row><cell>β = 2</cell><cell>4.834</cell><cell>4.953 5.189</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>WHENet-V MAE vs. α and β on BIWI</figDesc><table><row><cell></cell><cell cols="2">α = 0.5 α = 1 α = 2</cell></row><row><cell>β = 0.5</cell><cell>3.531</cell><cell>3.501 3.554</cell></row><row><cell>β = 1</cell><cell>3.551</cell><cell>3.676 3.626</cell></row><row><cell>β = 2</cell><cell>3.475</cell><cell>3.466 3.513</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>WHENet MAE vs. α and β on AFLW2000</figDesc><table><row><cell></cell><cell cols="2">α = 0.5 α = 1 α = 2</cell></row><row><cell>β = 0.5</cell><cell>5.822</cell><cell>5.624 5.620</cell></row><row><cell>β = 1</cell><cell>5.484</cell><cell>5.424 5.529</cell></row><row><cell>β = 2</cell><cell>5.658</cell><cell>5.414 5.509</cell></row><row><cell cols="3">Table 7: WHENet MAE vs. α and β on BIWI</cell></row><row><cell></cell><cell cols="2">α = 0.5 α = 1 α = 2</cell></row><row><cell>β = 0.5</cell><cell>3.823</cell><cell>3.855 3.880</cell></row><row><cell>β = 1</cell><cell>3.843</cell><cell>3.814 3.786</cell></row><row><cell>β = 2</cell><cell>3.710</cell><cell>4.064 3.935</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>WHENet MAE vs. α and β on our combined dataset α = 0.5 α = 1 α = 2</figDesc><table><row><cell>β = 0.5</cell><cell>8.009</cell><cell>8.287 7.394</cell></row><row><cell>β = 1</cell><cell>7.331</cell><cell>7.655 7.879</cell></row><row><cell>β = 2</cell><cell>7.878</cell><cell>7.457 7.694</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Comparison results on BIWI dataset with different modality methods. WHENet and WHENet-V are trained on 300W-LP and our combined dataset. The rest of the methods are trained on BIWI where they split the BIWI dataset into testing and training.</figDesc><table><row><cell></cell><cell cols="4">Yaw Pitch Roll MAE</cell></row><row><cell>RGB-based</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepHeadPose [23]</cell><cell cols="2">5.67 5.18</cell><cell>-</cell><cell>-</cell></row><row><cell>SSR-Net-MD [47]</cell><cell cols="2">4.24 4.35</cell><cell cols="2">4.19 4.26</cell></row><row><cell>VGG16 [14]</cell><cell cols="2">3.91 4.03</cell><cell cols="2">3.03 3.66</cell></row><row><cell cols="3">FSA-Caps-Fusion [48] 2.89 4.29</cell><cell cols="2">3.60 3.60</cell></row><row><cell>WHENet-V</cell><cell cols="2">3.60 4.10</cell><cell cols="2">2.73 3.47</cell></row><row><cell>WHENet</cell><cell cols="2">3.99 4.39</cell><cell cols="2">3.06 3.81</cell></row><row><cell>RGB+Depth</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepHeadPose [23]</cell><cell cols="2">5.32 4.76</cell><cell>-</cell><cell>-</cell></row><row><cell>Martin [22]</cell><cell>3.6</cell><cell>2.5</cell><cell>2.6</cell><cell>2.9</cell></row><row><cell>POSEidon+ [5]</cell><cell>1.7</cell><cell>1.6</cell><cell>1.7</cell><cell>1.6</cell></row><row><cell>RGB+Time</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VGG16+RNN [14]</cell><cell cols="2">3.14 3.48</cell><cell>2.6</cell><cell>3.07</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Hyperparameter Studies</head> <ref type="table">Tables 4, 5</ref><p>, 6, 7 and 8 show ablation studies of mean average error for the β and α metaparameters of WHENet-V and WHENet, tested on the AFLW2000, BIWI datasets and our combined dataset. From this, we selected the best overall performance as β = 2 and α = 0.5 for WHENet-V, β = 1 and α = 1 for WHENe, although performance is not overly sensitive to these choices.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Yawdd: A yawning detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shabnam</forename><surname>Abtahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Omidyeganeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Shirmohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnoosh</forename><surname>Hariri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM Multimedia Systems Conference</title>
		<meeting>the 5th ACM Multimedia Systems Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Umdfaces: An annotated face dataset for training deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Nanduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Joint Conference on Biometrics (IJCB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Colour invariant head pose classification in low resolution video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Benfold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Poseidon: Face-from-depth for driver pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Borghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Venturelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5494" to="5503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Face-from-depth for head pose estimation on depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Borghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-initializing head pose estimation with a 2d monocular usb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rothbucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Diepold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lehrstuhl für Datenverarbeitung</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d deformable face tracking with a commodity depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="229" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="109" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A robust real-time face tracking using head pose estimation for a markerless ar system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Márcio</forename><surname>Cerqueira De Farias Macedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antônio</forename><surname>Lopes Apolinário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio Carlos Dos Santos</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 XV Symposium on Virtual and Augmented Reality</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="224" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real time head pose estimation with random regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random forests for real time 3d face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Fossati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="437" to="458" />
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic facial analysis: From bayesian filtering to recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1548" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimation of pedestrian pose orientation using soft target training based on teacher-student framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyeong</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Yeal</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung Chul</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1147</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Edge and keypoint detection in facial regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Herpers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-H</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Sommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the Second International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="212" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Closed-form solution of absolute orientation using orthonormal matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><forename type="middle">M</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahriar</forename><surname>Hilden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Negahdaripour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1127" to="1135" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="190" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kepler: Keypoint and pose estimation of unconstrained faces by learning efficient h-cnn regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="258" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The influence of head contour and nose angle on the perception of eye-gaze direction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Langton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Honeyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="752" to="771" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real time head model creation and head pose estimation on consumer depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep head pose: Gaze-direction estimation in multimodal video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Head pose estimation in computer vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan Manubhai</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="607" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Head pose estimation and augmented reality tracking: An integrated system and evaluation for monitoring driver awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan Manubhai</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="311" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Head pose estimation for driver assistance systems: A robust algorithm and experimental evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan Manubhai</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Intelligent Transportation Systems Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="709" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Composite support vector machines for detection of faces across views and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="359" to="368" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Osokin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12004</idno>
		<title level="m">Real-time 2d multi-person pose estimation on cpu: Lightweight openpose</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="135" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Appearance based pedestriansâȂŹ head pose and body orientation estimation using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mudassar</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Saeed-Ur Rehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">272</biblScope>
			<biblScope unit="page" from="647" to="659" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Head detection and orientation estimation for pedestrian safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eike</forename><surname>Rehder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Kloeden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International IEEE Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2292" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fine-grained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2074" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving head pose estimation with a combined loss and bounding box margin adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhen</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08609</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding pose discrimination in similarity space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng-Jon</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Advance in head pose estimation from low resolution images: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teera</forename><surname>Siriteerakul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science Issues</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Head pose estimation using view based eigenspaces. In Object recognition supported by user interaction for service robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="302" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Context-aware cnns for person head detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2893" to="2901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Facial feature point detection: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="50" to="65" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ai challenger: a large-scale dataset for going deeper in image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06475</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">No matter where you are: Flexible graph-guided multi-task learning for multi-view head pose classification under target motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramanathan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A multi-task learning framework for head pose estimation under target motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramanathan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1070" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Model-based head pose tracking with stereovision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fifth IEEE International Conference on Automatic Face Gesture Recognition</title>
		<meeting>Fifth IEEE International Conference on Automatic Face Gesture Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="255" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ssr-net: A compact soft stagewise regression network for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Cheng</forename><surname>Hsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fsa-net: Learning fine-grained structure aggregation for head pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1087" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Posefree facial landmark fitting via optimized part mixtures and cascaded deformable shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1944" to="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Head pose estimation for driver monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youding</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kikuo</forename><surname>Fujimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="501" to="506" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
