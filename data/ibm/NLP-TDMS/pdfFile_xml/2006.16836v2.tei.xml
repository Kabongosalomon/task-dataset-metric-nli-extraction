<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Can Your Face Detector Do Anti-spoofing? Face Presentation Attack Detection with a Multi-Channel Face Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjith</forename><surname>George</surname></persName>
							<email>anjith.george@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<addrLine>Rue Marconi 19</addrLine>
									<postCode>CH -1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Marcel</surname></persName>
							<email>sebastien.marcel@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<addrLine>Rue Marconi 19</addrLine>
									<postCode>CH -1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Can Your Face Detector Do Anti-spoofing? Face Presentation Attack Detection with a Multi-Channel Face Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In a typical face recognition pipeline, the task of the face detector is to localize the face region. However, the face detector localizes regions that look like a face, irrespective of the liveliness of the face, which makes the entire system susceptible to presentation attacks. In this work, we try to reformulate the task of the face detector to detect real faces, thus eliminating the threat of presentation attacks. While this task could be challenging with visible spectrum images alone, we leverage the multi-channel information available from off the shelf devices (such as color, depth, and infrared channels) to design a multichannel face detector. The proposed system can be used as a live-face detector obviating the need for a separate presentation attack detection module, making the system reliable in practice without any additional computational overhead. The main idea is to leverage a single-stage object detection framework, with a joint representation obtained from different channels for the PAD task. We have evaluated our approach in the multi-channel WMCA dataset containing a wide variety of attacks to show the effectiveness of the proposed framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Face recognition technology has become ubiquitous these days, thanks to the advance in deep learning based methods <ref type="bibr" target="#b0">[1]</ref>. However, the reliability of face recognition systems (FRS) in the presence of attacks is poor in practical situations, mainly due to the vulnerability against presentation attacks (a.k.a spoofing attacks) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Presenting artifacts like a photograph or video in front of the camera could be enough to fool unprotected FR systems. The artifact used for this attack is known as a presentation attack instrument (PAI).</p><p>Presentation attack detection (PAD) tries to protect the FR systems against such attacks. Majority of the PAD methods proposed in the literature focuses on the detection of 2D attacks. Most of these methods use either feature-based methods or CNN based approaches using visible spectrum images. However, they do not emulate a realistic scenario of encountering realistic 3D attacks. Recently, there has been a lot of works focusing on the detection of a wide variety of attacks, including 2D, 3D, and partial attacks. Multi-channel methods have been proposed as a possible alternative to deal with real-world scenarios with a wide variety of attacks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Usage of multiple channels makes it harder to fool the PAD systems.</p><p>For reliable usage of face recognition systems, they must be equipped with a presentation attack detection module. The <ref type="figure">Fig. 1</ref>. Schematic of the proposed framework. The camera captures images in color, depth and infrared channels (all channel coming from a single commercially available device) and the RetinaNet based multi channel face detector performs simultaneous face detection and presentation attack detection using the composite image from these channels. The framework can be adapted to work with RGB-D combination just by stacking RGB and Depth channels at the input side (to make it work with OpenCV AI Kit (OAK-D) <ref type="bibr" target="#b5">[6]</ref> and Kinect <ref type="bibr" target="#b6">[7]</ref>). PAD module can act before, after, or together with the face recognition module.</p><p>Typically face recognition frameworks consists of a preprocessing stage including face detection and alignment, followed by the actual face recognition task. In the proposed PAD framework, we propose to use a multi-channel face detector as the PAD system. In this way, the preprocessing stage for the face recognition system itself can perform PAD, thanks to the multi-channel information. Furthermore, the face detectors used typically are also CNN based, so if we can replace this face detector with the proposed module, it should not increase the overall complexity while adding PAD capabilities, simplifying the overall face recognition pipeline by removing the redundancy.</p><p>Presentation attack detection is achieved by changing the task of the face detector from detecting faces to detecting bonafide faces. This is a hard challenge when only RGB channels are used and can result in a lot of detection errors. However, the proposed face detection based PAD framework ( <ref type="figure">Fig. 1)</ref> leverage the multi-channel information to discriminate between bonafide and presentation attacks.</p><p>Another advantage of the proposed face detection-based framework is the efficient use of the background. Most of the common PAD frameworks utilize only the facial region and completely ignores the information from the background. Notably, in a multi-channel framework, background could give beneficial negative samples to make the system robust. The proposed framework can be configured based on the availability of data in different ways. It can be trained as a purely one class model when no negative samples are available, i.e, when only bonafide samples are available. The bonafide face location can be used as positive samples, and the entire background can be used as negative samples. When attacks are available, the face detector can be trained to classify between bonafide and the attack classes. It is also possible to train the face detector as a multi-class classifier when we have the labels for different kinds of attack classes. While training the face detector, a lot of negative samples contribute to the loss function. To avoid this, we use the focal loss formulation, which focuses on the hard examples. If the face detection network is trained only on bonafide samples, most of the negative samples (patches) would be easy to classify. The attack class images can be used as hard negative samples. This introduces greater flexibility as the face detector can be trained with different configurations, for instance, as a oneclass classifier, either with only bonafide class, or with the addition of attack classes too in training providing the CNN based face detector with harder examples, or as multi class classifier. This could be useful in the cases where only a limited amount of attacks are available in training.</p><p>To the best knowledge of the authors, this is the first work using a multi-channel face detector for the task of face presentation attack detection. The main contributions of this work are listed below.</p><p>• Proposes a novel framework for PAD, using a multichannel face detector, performing simultaneous face localization and presentation attack detection. • The channels used in this work comes from an affordable consumer-grade camera, which makes it easy to deploy this system in real-world conditions. • The proposed algorithm can be used as a preprocessing stage in face recognition systems, thus reducing the redundancy of an additional PAD system in the face recognition pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Most of the prevailing literature in face presentation attack detection deals with the detection of 2D attacks using visible spectrum images. Majority of them depend on the quality degradation of the recaptured samples for PAD. Methods such as motion patterns <ref type="bibr" target="#b7">[8]</ref>, Local Binary Patterns (LBP) <ref type="bibr" target="#b8">[9]</ref>, image quality features <ref type="bibr" target="#b9">[10]</ref>, and image distortion analysis <ref type="bibr" target="#b10">[11]</ref> are examples of feature based PAD methods. Also, there are several CNN based methods achieving state of the art performance <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. While there has been a lot of work in the detection of 2D presentation attacks, the assumptions of quality degradation during recapture does not hold for attacks such as realistic silicone masks and partial attacks. Here we limit the discussion to recent and representative methods that handle a wide variety of 2D and 3D attacks.</p><p>With the ever-improving quality of attacks, visible spectrum images alone may not suffice for detecting presentation attacks. This problem becomes more severe when there is a wide variety of possible 2D and 3D presentation attacks possible. Multi-channel and multi-spectral methods have been proposed as a solution for this problem <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Raghavendra et al. <ref type="bibr" target="#b14">[15]</ref> presented an approach to use complementary information from different channels using a multi-spectral PAD framework. Their method used a fusion of wavelet-based features. Score level fusion achieved better performance as compared to feature fusion in detecting attacks prepared using different kinds of printers. Erdogmus and Marcel <ref type="bibr" target="#b16">[17]</ref> showed that 3D masks could fool the PAD systems easily. By combining the LBP features from color and depth channels, they could achieve good performance in the 3DMAD dataset.</p><p>Steiner et al. <ref type="bibr" target="#b15">[16]</ref> introduced multi-spectral SWIR imagebased PAD method, capturing four different wavelengths -935nm, 1060nm, 1300nm and 1550nm. Their method essentially consisted of a skin level classifier in a predefined Region Of Interest (ROI) where the skin was expected to be present. They trained a pixel-level SVM to classify each pixel as skin or not. The percentage of skin detections in the ROI was used as the PAD score. Their method achieved 99.28% accuracy in pixel-level skin classification.</p><p>In <ref type="bibr" target="#b17">[18]</ref>, the authors combined visible and thermal image patches for PAD. The patches used for the subsequent face recognition stage were selected by first classifying the patches as either bonafide or attacks.</p><p>Bhattacharjee et al. <ref type="bibr" target="#b18">[19]</ref> showed the vulnerability of CNN based face recognition systems against 3D masks. They also proposed simple thermal-based features for PAD. Further, in <ref type="bibr" target="#b19">[20]</ref>, they presented preliminary experiments on the use of additional channels for PAD.</p><p>In <ref type="bibr" target="#b3">[4]</ref>, George et al. presented a multi-channel face presentation attack detection framework. Their approach consisted of extending a pretrained face recognition network to accept multiple channels and to adapt a minimal number of layers to prevent overfitting. The proposed method achieved an error rate of 0.3 % in the challenging WMCA dataset using color, depth, infrared and thermal channels. Further several other works have reported improved performance with the use of multi-channel methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>Many multi-channel datasets have been made available for PAD recently. However, the variety of attacks is rather limited in most of the available databases.</p><p>Typical PAD frameworks perform a preprocessing stage involving face detection with RGB channel, followed by alignment and binary classification for PAD. The PAD classifier, for most of them, has the form of a binary classifier acting on the detected face regions which might result in poor performance in unseen attack scenarios. Such frameworks ignores the useful information coming from the background regions completely. Moreover, the PAD module adds additional complexity to the face recognition pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>In the proposed approach, we use an object detection framework for localizing face and classifying between bonafide and non-face (attacks) labels. Specifically, we formulate the problem of multi-channel presentation attack detection as a two-class object detection problem. We leverage a single-stage object detection framework for this purpose. The task for the proposed object detector is to detect the presence of 'bonafide' or 'non-face' classes. While training, face locations detected by the RGB face detector <ref type="bibr" target="#b22">[23]</ref> is used as the ground truth bounding box locations. The class labels of the bounding boxes is considered as 'bonafide' class for real samples and all other attack classes are grouped as 'non-face' class. In the evaluation phase, the confidence of the bonafide class is used for the scoring. If no object classes ('bonafide' or 'non-face') are detected, then the sample is considered as an attack which was not seen in training. We use the RetinaNet <ref type="bibr" target="#b23">[24]</ref> as the base architecture for the proposed multi-channel face detector, which obtained state of the art performance in object detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preprocessing</head><p>The data used in the network comprises of color, depth, and infrared channels. The color channel is converted to gray-scale. The raw depth and infrared channels available from the hardware are in 16-bit format. We first normalize these channels with Median Absolute Deviation (MAD) based normalization to convert them to 8 bit <ref type="bibr" target="#b4">[5]</ref>.</p><p>This stage is detailed as follows, let I be the image; first, a vector v containing non-zero elements of I is obtained to reduce the effect of background holes and dead pixels. The MAD value is computed as follows,</p><formula xml:id="formula_0">MAD = median(|v − median(v)|)<label>(1)</label></formula><p>Once the MAD is computed, the image can be normalized to 8 bit as:</p><formula xml:id="formula_1">I i,j = (I i,j − median(v) + σ · MAD) 2 · σ · MAD · (2 8 − 1),<label>(2)</label></formula><p>Values i and j denote the coordinates of the pixels in the image. The σ value used in our experiments was four.</p><p>Once all the channels are available in 8-bit format, they are concatenated to form the grayscale-depth-infrared composite image, which is used as the input in the subsequent CNN pipeline. An example of the composite bonafide image is shown in <ref type="figure" target="#fig_0">Fig. 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network architecture and loss function</head><p>The objective is to treat PAD problem as an object detection problem. In literature, there are several two-stage detectors that can do object detection by first generating the candidate locations and then classifying the candidate locations to get object labels. However, one stage object detectors are advantageous due to their simple design and faster inference. The RetinaNet <ref type="bibr" target="#b23">[24]</ref> architecture is a one-stage object detection architecture, which is simpler and quicker at the inference stage. One of the main issues with one stage detectors is the poor performance due to heavy class imbalance. Out of a large number of candidate locations, only a few of them contain objects. This could result in poor models as the loss includes a lot of contributions from the background. In RetinaNet <ref type="bibr" target="#b23">[24]</ref>, this issue is handled by introducing a new loss function called focal loss. This is done by down-weighting easy examples so that their contribution is less in the loss function. In summary, focal loss naturally favors the training on hard instances. 1) Architecture: We use the standard RetinaNet architecture built from a backbone network as the object detector. In our implementation, we used Feature Pyramid Network <ref type="bibr" target="#b24">[25]</ref> with ResNet-18 <ref type="bibr" target="#b25">[26]</ref> backbone. There are two subnetworks; one is tasked with regressing the bounding box and the other one is performing the classification. In our case, the classifier needs to classify the presence of 'bonafide' or 'non-face' categories. The task of the regression network is to predict the 'bonafide' (or 'non-face' ) bounding box. Instead of RGB images, we use the composite image created in the preprocessing stage as the input to the network.</p><p>2) Loss function: As shown earlier, the RetinaNet architecture consists of two subnetworks, one for landmark regression and one for classification. For the object classification, the typically used cross entropy loss (CE) has the following form:</p><formula xml:id="formula_2">CE(p, y) = − log(p) if y = 1 − log(1 − p) otherwise.<label>(3)</label></formula><p>In the above equation, y ∈ {±1} specifies the label, p ∈ [0, 1] is probability computed by the network for the class with label y = 1. We define p t as:</p><formula xml:id="formula_3">p t = p if y = 1 1 − p otherwise,<label>(4)</label></formula><p>so that we can rewrite the loss as CE(p, y) = CE(p t ) = − log(p t ).</p><p>After adding the modulation factor (1−pt) γ , and α-balancing the expression for focal loss (FL) becomes <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_4">FL(p t ) = −α t (1 − p t ) γ log(p t ).<label>(5)</label></formula><p>The classification head of the network is trained with this loss function which reduces the effect of easily classifiable background patches in the loss function.</p><p>3) Scoring method: The network described above is trained as an object detector to localize and classify face. Here the notation 'face' denotes the location of face irrespective of the class label. The possible classes are 'bonafide' and 'nonface'. The 'non-face' category might contain a wide variety of attacks, and its possible that in some cases, no objects are detected. However, for the PAD system, we need to get a score, which is defined as the probability of a bonafide class. The network outputs a classification label and class probability for the detected class, above a detection threshold. Once this information is available, the scoring is performed as follows. If the object class predicted is 'bonafide' then PAD score is computed as score = P bonaf ide where P bonaf ide is the probability of 'bonafide' class. If the object class returned is 'non-face' then score = 1 − P non−f ace where P non−f ace is the probability of the 'non-face' class. In the case where no objects are detected, as the case when an unseen attack is present, the sample is considered as an attack and given a predefined low score (i.e., considered as an attack).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Implementation details:</head><p>We used a standard RetinaNet architecture with ResNet-18 <ref type="bibr" target="#b25">[26]</ref> backbone for our experiments. Pretrained weights of a model trained on ImageNet <ref type="bibr" target="#b26">[27]</ref> was used to initialize the ResNet blocks. To train the face detector, train set of the PAD database was used for training, and the dev set was used for validation. The model was trained as a standard object detector with two classes ('bonafide' and 'non-face' classes). The face bounding box used in the training was obtained from MTCNN algorithm <ref type="bibr" target="#b22">[23]</ref> on color channel images. Adam Optimizer <ref type="bibr" target="#b27">[28]</ref> with a learning rate of 2 × 10 −5 was used in the training. The model was trained for 50 epochs with ten frames from each video on a GPU grid. All the layers were adapted during the network training. The model corresponding to minimum validation loss was selected for the evaluation. The implementation was done with PyTorch <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Databases used</head><p>The key requirement of the proposed framework is the use of 'multi-channel' information. We have conducted experiments in the publicly available Wide Multi-Channel presentation Attack (WMCA) <ref type="bibr" target="#b3">[4]</ref> database, which contains a wide variety of 2D and 3D presentation attacks. To the best of our knowledge, there are no other publically available PAD datasets that contain synchronized multi-channel data in a usable format for our task. One other notable multi-channel PAD dataset is CASIA-SURF [?]. However, they only provide cropped and preprocessed images in the public distribution which cannot be used in an object detector framework. The WMCA dataset contains a total of 1679 video samples from 72 individuals. Mainly, the database contains four different channels of information recorded simultaneously, namely, color, depth, infrared, and thermal channels, collected using two consumer-grade devices, Intel R RealSense TM SR300 (for color, depth and infrared), and Seek Thermal CompactPRO (for the thermal channel). The database contains different types of 2D and 3D attacks, such as print, replay, funny eyeglasses, fake head, rigid mask, flexible silicone mask, and paper masks. The statistics of attack types is shown in <ref type="table" target="#tab_0">Table I</ref>. More details of the database can be found in <ref type="bibr" target="#b3">[4]</ref>. Even though four different channels are available in the database, we use only three channels coming from the consumer-grade Intel RealSense camera (color, depth, and infrared). This makes the practical deployment more feasible as compared to the method in <ref type="bibr" target="#b3">[4]</ref>. The samples of RGB images for the attack categories are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Protocols</head><p>We use the same grandtest protocol defined in <ref type="bibr" target="#b3">[4]</ref> for our experiments. However, it is to be noted that we only use three channels out of the four channels available from the database. All the channels we used are coming from the same consumer grade device making it suitable for deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Metrics</head><p>For the evaluation of the algorithms, we have used the ISO/IEC 30107-3 metrics <ref type="bibr" target="#b2">[3]</ref>, Attack Presentation Classification Error Rate (APCER), and Bonafide Presentation Classification Error Rate (BPCER) along with the Average Classification Error Rate (ACER) in the eval set. We compute the threshold in the dev set for a BPCER value of 0.2%. EPC curves and APCER and BPCER are also calculated. Additionally, the APCER of individual attack types and the ACER-AP which is defined as the average of BPCER and the maximum APCER of the attack types are also reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Baseline methods</head><p>We have implemented feature-based and CNN based baselines to compare with the proposed method. We used the same channels, i.e., gray-scale, depth, and infrared channels in most of the baselines for a fair comparison (except for the use of color channel in some baselines). We compared the proposed approach with competing multi-channel baselines using the same channels. The description of the baseline methods is given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Haralick Fusion: Here we use an extension of RDWT-</head><p>Haralick-SVM method in <ref type="bibr" target="#b29">[30]</ref>. First, we perform a preprocessing stage, which consists of face detection. After that, the face region in all channels are normalized to the size of 128 × 128 pixels and rotated to make the eyeline horizontal. For each channel, Haralick <ref type="bibr" target="#b30">[31]</ref> features computed from 4 × 4 grid are concatenated to form the feature vector. This feature vector was used with logistic regression (LR) for individual channels for the PAD task.</p><p>A score fusion of all the channels was performed to get the final PAD score. • IQM-LBP fusion: In this baseline, we followed a similar preprocessing as the previous baseline. After the preprocessing stage, features are extracted from different channels, for example, Image Quality Measures (IQM) <ref type="bibr" target="#b9">[10]</ref> were extracted for the RGB channel, and variants of Local Binary Patterns (LBP) features for non-RGB channels. For each channel, a logistic regression model is trained, and score level fusion is performed to obtain the PAD scores. • FASNet (Color): This is an RGB only CNN <ref type="bibr" target="#b31">[32]</ref> baseline for the PAD task. Similar to the other baselines, this network also utilizes aligned images from the preprocessing stage. • MCCNN-GDI: Here we implemented <ref type="bibr" target="#b3">[4]</ref>, which achieved state of the art performance in the grandtest protocol in WMCA dataset, when used with gray-scale, depth, infrared and thermal channels. Here, to make it comparable with that of the proposed system, we retrained the system with the channels used in this work, i.e., grayscale, depth, and infrared channels.</p><p>Additionally, for the feature-based methods, the best performing method from each channel is also added in the evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Baseline results:</head><p>The results of different baselines systems with color, depth, and infrared channels, as well as the fusion methods, are shown in <ref type="table" target="#tab_0">Table II</ref>. For the individual channels, only the result from the best one is reported. Of all the channels present, the infrared channel performs the best with an ACER of 11.0%. Though fusion improves the overall ROC, the performance in low BPCER regions becomes slightly worse. Out of the baselines, the MCCNN-GDI method is clearly superior, achieving an ACER of 3%.</p><p>2) Results with the proposed framework: From <ref type="table" target="#tab_0">Table II</ref>, it can be seen that the proposed face detection based PAD framework achieves an ACER of 1.5% outperforming the state of the art baselines. Some of the successful detections are shown in <ref type="figure">Fig. 4</ref>.</p><p>To identify the limitations of the framework, some of the false detections are also shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. The instances of bonafide getting classified as the attack could be due to the missing values in the depth channel. Depth data from the device contains holes where depth is not estimated correctly. This can probably be avoided with a preprocessing stage with hole filling and smoothing of the depth channel.</p><p>The same could explain the misclassification of the transparent mask as bonafide. Some of the attacks with paper glasses are misclassified since it looks identical to bonafide with corrective glasses. <ref type="figure">Fig. 4</ref>.</p><p>Image showing the successful detections, green boxes indicate the bonafide detections and red boxes denote attack detections. First row shows the successful bonafide detections and second row shows the successful presentation attack detections.</p><p>3) Detailed analysis of the scores: In this section, we perform a detailed analysis of the scores to get insights into the misdetections and failure cases. For this analysis, we compute the decision threshold in the development set with BPCER 0.2% criterion, and we apply this threshold on the evaluation set. After that, we aggregate the APCER for individual attack categories. The results for different attack categories are tabulated in <ref type="table" target="#tab_0">Table III.</ref> From the <ref type="table" target="#tab_0">Table III</ref>, it can be seen that the proposed method achieves perfect performance in 2D attack classification. This is expected since the information from the depth channel   alone might suffice for this task. The performance in 3D masks are also satisfactory, considering the very low BPCER values. The glasses attacks are the most challenging ones, and they contribute to most of the misclassifications. The glasses category consists of 'Paper glass' <ref type="figure" target="#fig_1">(Fig. 3 (a)</ref>) and 'Funny eyes' <ref type="figure" target="#fig_1">(Fig. 3 (b)</ref>) sub categories. Moreover, these are the only partial attacks present in the database. Especially in the channels considered, the funny eyes and the paper glass appear very similar to bonafide samples with medical glass. This results in misclassification of these attacks. The ROC plots provide information about the performance in the evaluation set only. However, in realistic conditions, the decision threshold has to be set a priori. Expected performance curves (EPC) <ref type="bibr" target="#b32">[33]</ref> provide an unbiased estimate of the performance of (two-class) classifiers. </p><formula xml:id="formula_5">W ER = α × (F M R) + (1 − α) × (F N M R)<label>(6)</label></formula><p>where W ER (weighted error rate) is computed as a weighted combination of FMR and FNMR. For each value of α, the score-threshold which minimize the WER is selected based on the development group. The threshold chosen is used to compute the Half Total Error Rate (HTER) in the evaluation set. <ref type="figure" target="#fig_3">Fig. 6</ref> shows the EPC plots of the proposed approach and the baseline methods. From the EPC curves, it can be seen that, in the lower BPCER regions (α ≤ 0.5), the proposed approach outperforms the state of the art methods, indicating the robustness of the approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussions</head><p>From the experimental results, it can be seen that the proposed framework for PAD with multi-channel data works bet-ter than binary classifier based approaches in the low BPCER range. This framework is ideal for typical usage settings since face detection is a part of the face recognition pipeline. Instead of using conventional face detection methods which detect and pass both bonafide and attacks to the subsequent stages, the proposed method uses a multi-channel face detector, specifically trained to localize and distinguish bonafide and attack ('non-face') classes simultaneously. Also, it is worth noting that the hardware used in the proposed pipeline is a single off-the-shelf Intel RealSense SR300 camera. To sum it up, the proposed approach provides face PAD as a face detector itself. Currently, the multi channel presentation attack databases available are collected in controlled conditions. Availability of data collected 'in the wild' conditions could make it possible to train the face detector using just the bonafide samples. The harder negatives from the background might improve the performance of the detector greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>The proposed method shows a simple yet efficient method for PAD using multi-channel information. The method does not add additional complexity to a face recognition pipeline since it can be a drop-in replacement for the face detector in the preprocessing stage. Even though the method is multichannel, the channels used are readily available from cheap consumer devices which makes it feasible for deployment. The Intel RealSense family of devices, Kinect and the upcoming OpenCV AI Kit (OAK-D) <ref type="bibr" target="#b5">[6]</ref> are suitable candidates for such system as this model can be replaced with RGB-D channels as well.</p><p>Addition of a PAD module is essential to secure face recognition systems against spoofing attempts. Typical PAD modules adds another layer of computational complexity to the face recognition pipeline. In this work, we propose a simple yet effective solution to add PAD capabilities to a face recognition system without adding any additional computational complexity. This is done by swapping the face detector in typical FR pipelines with the proposed multichannel face detector. The (multi-channel) face detector is used as a presentation attack detector, thereby reducing the redundancy in the face recognition pipeline. To our best knowledge, this is the first method to use multi-channel face detection as a PAD system. Specifically, a multi-channel face detector is trained to localize and classify bonafide faces and presentation attacks. The proposed method utilizes color, depth, and infrared channels available from a commercially available sensors for PAD. The proposed method was compared with feature-based methods and state of the art CNN based methods for comparison in the WMCA dataset and was found to outperform the state of the art method while obviating the requirement of additional face detection stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Part of this research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. 2017-17020200005. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The composite image is formed from gray-scale, depth and infrared channels by stacking the normalized images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Attack categories in WMCA, (image taken from [4]) dataset (Color channel), (a): glasses (paper glasses), (b): glasses (funny eyes), (c): print (2D), (d): replay (2D), (e): fake head, (f): rigid mask (Obama mask), (g): rigid mask (transparent plastic mask), (h): rigid mask (custom made), (i): flexible mask (custom made), and (j): paper mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Image showing the failure case in detections, green boxes indicate the bonafide detections and red boxes denote attack detections. The first row shows the failure cases where bonafide are classified as attacks and the second row shows the failure cases where attacks are misclassified as bonafide.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>EPC plots for the reproducible baselines and proposed method in the eval set of grandtest protocol in WMCA dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I NUMBER</head><label>I</label><figDesc>OF SAMPLES IN EACH PRESENTATION ATTACK CATEGORY IN WMCA DATABASE.</figDesc><table><row><cell>Attack Type</cell><cell>Category</cell><cell>#Videos</cell></row><row><cell>Bonafide</cell><cell>-</cell><cell>347</cell></row><row><cell>Print</cell><cell>2D</cell><cell>200</cell></row><row><cell>Replay</cell><cell>2D</cell><cell>348</cell></row><row><cell>Fake head</cell><cell>3D</cell><cell>122</cell></row><row><cell>Rigid mask</cell><cell>3D</cell><cell>137</cell></row><row><cell>Flexible mask</cell><cell>3D</cell><cell>379</cell></row><row><cell>Paper mask</cell><cell>3D</cell><cell>71</cell></row><row><cell>Glasses</cell><cell>3D (Partial)</cell><cell>75</cell></row><row><cell>TOTAL</cell><cell></cell><cell>1679</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>OF THE BASELINE SYSTEMS AND THE COMPONENTS IN GRANDTEST PROTOCOL OF WMCA DATASET. THE VALUES REPORTED ARE OBTAINED WITH A THRESHOLD COMPUTED FOR BPCER 0.2% IN dev SET.</figDesc><table><row><cell>Method</cell><cell cols="2">dev (%)</cell><cell></cell><cell>test (%)</cell><cell></cell></row><row><cell></cell><cell cols="2">APCER ACER</cell><cell>APCER</cell><cell cols="2">BPCER ACER</cell></row><row><cell>Color (Haralick-LR)</cell><cell>41.1</cell><cell>20.6</cell><cell>42.0</cell><cell>1.0</cell><cell>21.5</cell></row><row><cell>Depth (Haralick-LR)</cell><cell>42.2</cell><cell>21.2</cell><cell>50.6</cell><cell>0.2</cell><cell>25.4</cell></row><row><cell>Infrared (Haralick-LR)</cell><cell>24.9</cell><cell>12.5</cell><cell>21.9</cell><cell>0.0</cell><cell>11.0</cell></row><row><cell>Haralick Fusion</cell><cell>29.0</cell><cell>14.6</cell><cell>27.0</cell><cell>0.3</cell><cell>13.6</cell></row><row><cell>IQM-LBP fusion</cell><cell>41.9</cell><cell>21.0</cell><cell>47.7</cell><cell>0.3</cell><cell>24.0</cell></row><row><cell>FASNet (Color)</cell><cell>29.3</cell><cell>14.7</cell><cell>24.9</cell><cell>3.8</cell><cell>14.3</cell></row><row><cell>MCCNN-GDI</cell><cell>2.3</cell><cell>1.2</cell><cell>6.1</cell><cell>0.0</cell><cell>3.0</cell></row><row><cell>Proposed</cell><cell>1.2</cell><cell>0.7</cell><cell>1.9</cell><cell>1.0</cell><cell>1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III DETAILED</head><label>III</label><figDesc>ANALYSIS OF THE PERFORMANCE FOR DIFFERENT PA'S, AT THRESHOLD COMPUTED FOR BPCER 0.2% IN dev SET.</figDesc><table><row><cell>Attack Type</cell><cell>Category</cell><cell cols="2">Dev Set Eval Set</cell></row><row><cell>Prints</cell><cell>2D</cell><cell>0.0%</cell><cell>0.0%</cell></row><row><cell>Replay</cell><cell>2D</cell><cell>0.0%</cell><cell>0.0%</cell></row><row><cell>Flexible mask</cell><cell>3D</cell><cell>2.0%</cell><cell>0.8%</cell></row><row><cell>Rigid mask</cell><cell>3D</cell><cell>3.7%</cell><cell>3.2%</cell></row><row><cell>Fake head</cell><cell>3D</cell><cell>0.0%</cell><cell>0.4%</cell></row><row><cell>Glasses</cell><cell>3D (Partial)</cell><cell>2.5%</cell><cell>11.9%</cell></row><row><cell>APCER-AP</cell><cell></cell><cell>3.7%</cell><cell>11.9%</cell></row><row><cell>BPCER</cell><cell></cell><cell>0.2%</cell><cell>1.0%</cell></row><row><cell>ACER-AP</cell><cell></cell><cell>1.9%</cell><cell>6.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>As opposed to the ROC curve, where the evaluation is done only on the test set, EPC combines both development and test group to obtain the expected performance. The EPC curve shows the performance changes as a trade-off between False Matching Rate (FMR) and False Non Matching Rate (FNMR). This trade-off is controlled by the parameter 0 ≤ α ≤ 1 as follows:</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Handbook of face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Handbook of biometric anti-spoofing : Presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<editor>Marcel, S., Nixon, M.S., Fierrez, J., Evans, N.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Information technology International Organization for Standardization</title>
		<idno>ISO/IEC JTC 1/SC 37 Biometrics</idno>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
		<respStmt>
			<orgName>ISO Standard</orgName>
		</respStmt>
	</monogr>
	<note>International Organization for Standardization</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Biometric face presentation attack detection with multichannel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mostaani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geissenbuhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nikisins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation in multichannel autoencoder based features for robust face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nikisins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Opencv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kit</surname></persName>
		</author>
		<ptr target="https://opencv.org/introducing-oak-spatial-ai-powered-by-opencv/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Counter-measures to photo attacks in face recognition: a public database and a baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (IJCB), 2011 international joint conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face anti-spoofing based on color texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2636" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image quality assessment for fake biometric detection: Application to iris, fingerprint, and face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="710" to="724" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face spoof detection with image distortion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="746" to="761" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep models for face antispoofing: Binary or auxiliary supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="389" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Pixel-wise Binary Supervision for Face Presentation Attack Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Face anti-spoofing using patch and depth-based cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Atoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Joint Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extended multispectral face presentation attack detection: An approach based on fusing information from individual spectral bands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Fusion (Fusion), 2017 20th International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reliable face anti-spoofing using multispectral swir imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spoofing face recognition with 3d masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information forensics and security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1084" to="1097" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disguise detection and face recognition in visible and thermal spectrums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Dhamecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (ICB), 2013 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spoofing deep face recognition with custom silicone masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>IEEE 9th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What you can&apos;t see can help youextended-range imaging for 3d-mask presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Biometrics Special Interest Group., no. EPFL-CONF-231840</title>
		<meeting>the 16th International Conference on Biometrics Special Interest Group., no. EPFL-CONF-231840</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Gesellschaft fuer Informatik eV (GI</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep models and shortwave infrared information to detect face presentation attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geissbühler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mostaani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Behavior, and Identity Science (T-BIOM)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning One Class Representations for Face Presentation Attack Detection using Multi-channel Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face presentation attack with latex masks in multispectral videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="275" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Statistical and structural approaches to texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="786" to="804" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transfer learning using convolutional neural networks for face antispoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lucena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Moia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lotufo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Image Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The expected performance curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mariéthoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML, Workshop on ROC Analysis in Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
