<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADAPTIVE INPUT REPRESENTATIONS FOR NEURAL LANGUAGE MODELING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADAPTIVE INPUT REPRESENTATIONS FOR NEURAL LANGUAGE MODELING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of <ref type="bibr" target="#b10">Grave et al. (2017)</ref> to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WIKITEXT-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the BILLION WORD benchmark, we achieve 23.02 perplexity. 1 1 Code and pre-trained models available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Language modeling is a basic task in natural language processing, with many applications such as speech recognition <ref type="bibr" target="#b1">(Arisoy et al., 2012)</ref> and statistical machine translation <ref type="bibr" target="#b28">(Schwenk et al., 2012;</ref><ref type="bibr" target="#b33">Vaswani et al., 2013;</ref><ref type="bibr" target="#b2">Baltescu &amp; Blunsom, 2015)</ref>. Recently, much progress has been made by neural methods <ref type="bibr" target="#b3">(Bengio et al., 2003;</ref><ref type="bibr" target="#b20">Mikolov et al., 2010)</ref> based on LSTMs <ref type="bibr" target="#b13">(JÃ³zefowicz et al., 2016)</ref>, gated convolutional networks <ref type="bibr" target="#b7">(Dauphin et al., 2017)</ref> and self-attentional networks <ref type="bibr" target="#b0">(Al-Rfou et al., 2018)</ref>.</p><p>There are different choices for the basic unit we wish to model, including full words <ref type="bibr" target="#b3">(Bengio et al., 2003)</ref>, characters for the input <ref type="bibr" target="#b15">(Kim et al., 2016)</ref>, or also the output <ref type="bibr" target="#b18">(Merity et al., 2018)</ref> as well as sub-words <ref type="bibr" target="#b4">(Buckman &amp; Neubig, 2018;</ref><ref type="bibr" target="#b19">Mielke &amp; Eisner, 2018)</ref>. Word-based models are particularly challenging since computing probabilities for all 800K words of the BILLION WORD benchmark is still a substantial part of the overall computation <ref type="bibr" target="#b6">(Chen et al., 2016)</ref>.</p><p>A popular approach to lower the computational burden is to structure the output vocabulary so that not all probabilities need to be computed. The hierarchical softmax does this by introducing latent variables or clusters to simplify normalization <ref type="bibr" target="#b8">(Goodman, 2001;</ref><ref type="bibr" target="#b22">Morin &amp; Bengio, 2005;</ref><ref type="bibr" target="#b21">Mikolov et al., 2011)</ref>. This has been further improved by the adaptive softmax which introduces a variable capacity scheme for output word embeddings, assigning more parameters to frequent words and fewer parameters to rare words <ref type="bibr" target="#b10">(Grave et al., 2017)</ref>.</p><p>In this paper, we introduce adaptive input embeddings which extend the adaptive softmax to input word representations. This factorization assigns more capacity to frequent words and reduces the capacity for less frequent words with the benefit of reducing overfitting to rare words. For a competitive setup on the BILLION WORD benchmark, adaptive input embeddings reduce the number of parameters in the input and output layers by 23% while achieving higher accuracy over fixed size embeddings. When the adaptive input representations are tied with an adaptive softmax in the output, then the number of parameters is reduced by a total of 61%.</p><p>Our experiments compare models based on word inputs, character inputs, as well as sub-word units using a self-attention architecture <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>. We show that models with adaptive word representations can outperform very strong character-based models while training more than twice as fast. We also substantially improve adaptive softmax by introducing additional dropout regularization in the tail projection. On the WIKITEXT-103 benchmark we achieve a perplexity of 18.7, a </p><formula xml:id="formula_0">i k o + N U M W y z W M T q M a A a B Z f Y N t w I f E w U 0 i g Q 2 A 0 m t 7 n f n a L S P J Y P Z p a g H 9 G R 5 C F n 1 F j J 7 0 f U j B k V W W c + 8 A b V m t t w F y D r x C t I D Q q 0 B t W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w H m l n 2 p M K J v Q E f Y s l T R C 7 W e L 0 H N y Y Z U h C W N l n z R k o f 7 e y G i k 9 S w K 7 G Q e U q 9 6 u f i f 1 0 t N e O N n X C a p Q c m W h 8 J U E B O T v A E y 5 A q Z E T N L K F P c Z i V s T B V l x v Z U s S V 4 q 1 9 e J 5 3 L h m f 5 / V W t W S / q K M M Z n E M d P L i G J t x B C 9 r A 4</formula><p>A m e 4 R X e n K n z 4 r w 7 H 8 v R k l P s n M I f O J 8 / t g + R 9 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q u R U N k T x I l J t y S 1 E 3 C q d 9 P h a 6 P U = " &gt; A A A B 9 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S x C T y U R Q Y 8 F L x 4 r 2 F Z o Q 9 l s X 9 q l m 0 3 c 3   </p><formula xml:id="formula_1">R R K 6 O / w 4 k E R r / 4 Y b / 4 b N 2 0 O 2 j q w M M y 8 x 5 u d I B F c G 9 f 9 d k o b m 1 v b O + X d y t 7 + w e F R 9 f i k o + N U M W y z W M T q M a A a B Z f Y N t w I f E w U 0 i g Q 2 A 0 m t 7 n f n a L S P J Y P Z p a g H 9 G R 5 C F n 1 F j J 7 0 f U j B k V W W c + 8 A b V m t t w F y D r x C t I D Q q 0 B t W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w H m l n 2 p M K J v Q E f Y s l T R C 7 W e L 0 H N y Y Z U h C W N l n z R k o f 7 e y G i k 9 S w K 7 G Q e U q 9 6 u f i f 1 0 t N e O N n X C a p Q c m W h 8 J U E B O T v A E y 5 A q Z E T N L K F P c Z i V s T B V l x v Z U s S V</formula><formula xml:id="formula_2">= " &gt; A A A B 9 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S x C T y U R Q Y 8 F L x 4 r 2 F Z o Q 9 l s N + 3 S z S b u v h R K 6 O / w 4 k E R r / 4 Y b / 4 b N 2 0 O 2 j q w M M y 8 x 5 u d I J H C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M X G q G W + z W M b 6 M a C G S 6 F 4 G w V K / p h o T q N A 8 m 4 w u c 3 9 7 p R r I 2 L 1 g L O E + x E d K R E K R t F K f j + i O G Z U Z p 3 5 Q A 2 q N b f h L k D W i V e Q G h R o D</formula><formula xml:id="formula_3">X i / F 8 M b P h E p S 5 I o t D 4 W p J B i T v A E y F J o z l D N L K N P C Z i V s T D V l a H u q 2 B K 8 1 S + v k 8 5 l w 7 P 8 / q r W r B d 1 l O E M z q E O H l x D E + 6 g B W 1 g 8 A T P 8 A p v z t R 5 c d 6 d j + V o y S l 2 T u E P n M 8 f E p K S M w = = &lt; / l a t e x i t &gt; d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 b 1 w P j S Z p r B D 3 Y A c H u y q D C z A G 2 g = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 6 K k k I u i x 4 M V j C 7 Y V 2 l A 2 m 0 m 7 d r M J u x u h h P 4 C L x 4 U 8 e p P 8 u a / c d v m o K 0 v L D y 8 M 8 P O v E E q u D a u + + 2 U N j a 3 t n f K u 5 W 9 / Y P D o + r x S V c n m W L Y Y Y l I 1 E N A N Q o u s W O 4 E f i Q K q R x I L A X T G 7 n 9 d 4 T K s 0 T e W + m K f o x H U k e c U a N t d r h s F p z G + 5 C Z B 2 8 A m p Q q D W s f g 3 C h G U x S s M E 1 b r v u a n x c 6 o M Z w J n l U G m M a V s Q k f Y t y h p j N r P F 4 v O y I V 1 Q h I l y j 5 p y M L 9 P Z H T W O t p H N j O m J q x X q 3 N z f 9 q / c x E N 3 7 O Z Z o Z l G z 5 U Z Q J Y h I y v 5 q E X C E z Y m q B M s X t r o S N q a L M 2 G w q N g R v 9 e R 1 6 F 4 2 P M v t q 1 q z X s R R h j M 4 h z p 4 c A 1 N u I M W d I A B w j O 8 w p v z 6 L w 4 7 8 7 H s r X k F D O n 8 E f O 5 w / A G Y z O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 b 1 w P j S Z p r B D 3 Y A c H u y q D C z A G 2 g = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 6 K k k I u i x 4 M V j C 7 Y V 2 l A 2 m 0 m 7 d r M J u x u h h P 4 C L x 4 U 8 e p P 8 u a / c d v m o K 0 v L D y 8 M 8 P O v E E q u D a u + + 2 U N j a 3 t n f K u 5 W 9 / Y P D o + r x S V c n m W L Y Y Y l I 1 E N A N Q o u s W O 4 E f i Q K q R x I L A X T G 7 n 9 d 4 T K s 0 T e W + m K f o x H U k e c U a N t d r h s F p z G + 5 C Z B 2 8 A m p Q q D W s f g 3 C h G U x S s M E 1 b r v u a n x c 6 o M Z w J n l U G m M a V s Q k f Y t y h p j N r P F 4 v O y I V 1 Q h I l y j 5 p y M L 9 P Z H T W O t p H N j O m J q x X q 3 N z f 9 q / c x E N 3 7 O Z Z o Z l G z 5 U Z Q J Y h I y v 5 q E X C E z Y m q B M s X t r o S N q a L M 2 G w q N g R v 9 e R 1 6 F 4 2 P M v t q 1 q z X s R R h j M 4 h z p 4 c A 1 N u I M W d I A B w j O 8 w p v z 6 L w 4 7 8 7 H s r X k F D O n 8 E f O 5 w / A G Y z O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 b 1 w P j S Z p r B D 3 Y A c H u y q D C z A G 2 g = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 6 K k k I u i x 4 M V j C 7 Y V 2 l A 2 m 0 m 7 d r M J u x u h h P 4 C L x 4 U 8 e p P 8 u a / c d v m o K 0 v L D y 8 M 8 P O v E E q u D a u + + 2 U N j a 3 t n f K u 5 W 9 / Y P D o + r x S V c n m W L Y Y Y l I 1 E N A N Q o u s W O 4 E f i Q K q R x I L A X T G 7 n 9 d 4 T K s 0 T e W + m K f o x H U k e c U a N t d r h s F p z G + 5 C Z B 2 8 A m p Q q D W s f g 3 C h G U x S s M E 1 b r v u a n x c 6 o M Z w J n l U G m M a V s Q k f Y t y h p j N r P F 4 v O y I V 1 Q h I l y j 5 p y M L 9 P Z H T W O t p H N j O m J q x X q 3 N z f 9 q / c x E N 3 7 O Z Z o Z l G z 5 U Z Q J Y h I y v 5 q E X C E z Y m q B M s X t r o S N q a L M 2 G w q N g R v 9 e R 1 6 F 4 2 P M v t q 1 q z X s R R h j M 4 h z p 4 c A 1 N u I M W d I A B w j O 8 w p v z 6 L w 4 7 8 7 H s r X k F D O n 8 E f O 5 w / A G Y z O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 b 1 w P j S Z p r B D 3 Y A c H u y q D C z A G 2 g = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 6 K k k I u i x 4 M V j C 7 Y V 2 l A 2 m 0 m 7 d r M J u x u h h P 4 C L x 4 U 8 e p P 8 u a / c d v m o K 0 v L D y 8 M 8 P O v E E q u D a u + + 2 U N j a 3 t n f K u 5 W 9 / Y P D o + r x S V c n m W L Y Y Y l I 1 E N A N Q o u s W O 4 E f i Q K q R x I L A X T G 7 n 9 d 4 T K s 0 T e W + m K f o x H U k e c U a N t d r h s F p z G + 5 C Z B 2 8 A m p Q q D W s f g 3 C h G U x S s M E 1 b r v u a n x c 6 o M Z w J n l U G m M a V s Q k f Y t y h p j N r P F 4 v O y I V 1 Q h I l y j 5 p y M L 9 P Z H T W O t p H N j O m J q x X q 3 N z f 9 q / c x E N 3 7 O Z Z o Z l G z 5 U Z Q J Y h I y v 5 q E X C E z Y m q B M s X t r o S N q a L M 2 G w q N g R v 9 e R 1 6 F 4 2 P M v t q 1 q z X s R R h j M 4 h z p 4 c A 1 N u I M W d I A B w j O 8 w p v z 6 L w 4 7 8 7 H s r X k F D O n 8 E f O 5 w / A G Y z O &lt; / l a t e x i t &gt; d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 b 1 w P j S Z p r B D 3 Y A c H u y q D C z A G 2 g = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 6 K k k I u i x 4 M V j C 7 Y V 2 l A 2 m 0 m 7 d r M J u x u h h P 4 C L x 4 U 8 e p P 8 u a / c d v m o K 0 v L D y 8 M 8 P O v E E q u D a u + + 2 U N j a 3 t n f K u 5 W 9 / Y P D o + r x S V c n m W L Y Y Y l I 1 E N A N Q o u s W O 4 E f i Q K q R x I L A X T G 7 n 9 d 4 T K s 0 T e W + m K f o x H U k e c U a N t d r h s F p z G + 5 C Z B 2 8 A m p Q q D W s f g 3 C h G U x S s M E 1 b r v u a n x c 6 o M Z w J n l U G m M a V s Q k f Y t y h p j N r P F 4 v O y I V 1 Q h I l y j 5 p y M L 9 P Z H T W O t p H N j O m J q x X q 3 N z f 9 q / c x E N 3 7 O Z Z o Z l G z 5 U Z Q J Y h I y v 5 q E X C E z Y m q B M s X t r o S N q a L M 2 G w q N g R v 9 e R 1 6 F 4 2 P M v t q 1 q z X s R R h j M 4 h z p 4 c A 1 N u I M W d I A B w j O 8 w p v z 6 L w 4 7 8 7 H s r X k F D O n 8 E f O 5 w / A G Y z O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 b 1 w P j S Z p r B D 3 Y A c H u y q D C z A G 2 g = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 6 K k k I u i x 4 M V j C 7 Y V 2 l A 2 m 0 m 7 d r M J u x u h h P 4 C L x 4 U 8 e p P 8 u a / c d v m o K 0 v L D y 8 M 8 P O v E E q u D a u + + 2 U N j a 3 t n f K u 5 W 9 / Y P D o + r x S V c n m W L Y Y Y l I 1 E N A N Q o u s W O 4 E f i Q K q R x I L A X T G 7 n 9 d 4 T K s 0 T e W + m K f o x H U k e c U a N t d r h s F p z G + 5 C Z B 2 8 A m p Q q D W s f g 3 C h G U x S s M E 1 b r v u a n x c 6 o M Z w J n l U G m M a V s Q k f Y t y h p j N r P F 4 v O y I V 1 Q h I l y j 5 p y M L 9 P Z H T W O t p H N j O m J q x X q 3 N z f 9 q / c x E N 3 7 O Z Z o Z l G z 5 U Z Q J Y h I y v 5 q E X C E z Y m q B M s X t r o S N q a L M 2 G w q N g R v 9 e R 1 6 F 4 2 P M v t q 1 q z X s R R h j M 4 h z p 4 c A 1 N u I M W d I A B w j O 8 w p v z 6 L w 4 7 8 7 H s r X k F D O n 8 E f O 5 w / A G Y z O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 b 1 w P j S Z p r B D 3 Y A c H u y q D C z A G 2 g = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 6 K k k I u i x 4 M V j C 7 Y V 2 l A 2 m 0 m 7 d r M J u x u h h P 4 C L x 4 U 8 e p P 8 u a / c d v m o K 0 v L D y 8 M 8 P O v E E q u D a u + + 2 U N j a 3 t n f K u 5 W 9 / Y P D o + r x S V c n m W L Y Y Y l I 1 E N A N Q o u s W O 4 E f i Q K q R x I L A X T G 7 n 9 d 4 T K s 0 T e W + m K f o x H U k e c U a N t d r h s F p z G + 5 C Z B 2 8 A m p Q q D W s f g 3 C h G U x S s M E 1 b r v u a n x c 6 o M Z w J n l U G m M a V s Q k f Y t y h p j N r P F 4 v O y I V 1 Q h I l y j 5 p y M L 9 P Z H T W O t p H N j O m J q x X q 3 N z f 9 q / c x E N 3 7 O Z Z o Z l G z 5 U Z Q J Y h I y v 5 q E X C E z Y m q B M s X t r o S N q a L M 2 G w q N g R v 9 e R 1 6 F 4 2 P M v t q 1 q z X s R R h j M 4 h z p 4 c A 1 N u I M W d I A B w j O 8 w p v z 6 L w 4 7 8 7 H s r X k F D O n 8 E f O 5 w / A G Y z O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 b 1 w P j S Z p r B D 3 Y A c H u y q D C z A G 2 g = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 6 K k k I u i x 4 M V j C 7 Y V 2 l A 2 m 0 m 7 d r M J u x u h h P 4 C L x 4 U 8 e p P 8 u a / c d v m o K 0 v L D y 8 M 8 P O v E E q u D a u + + 2 U N j a 3 t n f K u 5 W 9 / Y P D o + r x S V c n m W L Y Y Y l I 1 E N A N Q o u s W O 4 E f i Q K q R x I L A X T G 7 n 9 d 4 T K s 0 T e W + m K f o x H U k e c U a N t d r h s F p z G + 5 C Z B 2 8 A m p Q q D W s f g 3 C h G U x S s M E 1 b r v u a n x c 6 o M Z w J n l U G m M a V s Q k f Y t y h p j N r P F 4 v O y I V 1 Q h I l y j 5 p y M L 9 P Z H T W O t p H N j O m J q x X q 3 N z f 9 q / c x E N 3 7 O Z Z o Z l G z 5 U Z Q J Y h I y v 5 q E X C E z Y m q B M s X t r o S N q a L M 2 G w q N g R v 9 e R 1 6 F 4 2 P M v t q 1 q z X s R R h j M 4 h z p 4 c A 1 N u I M W d I A B w j O 8 w p v z 6 L w 4 7 8 7 H s r X k F D O n 8 E f O 5 w / A G Y z O &lt; / l a t e x i t &gt; d k n 1 &lt; l a t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Adaptive word representations are inspired by the adaptive softmax work <ref type="bibr" target="#b10">Grave et al. (2017)</ref> which first described a GPU friendly way to construct a hierarchical softmax and showed that it performs very competitively compared to a full softmax, while offering significantly faster speed and a lower memory footprint. <ref type="bibr" target="#b18">Merity et al. (2018)</ref> use a modified version of adaptive softmax which does not reduce the dimensionality of less frequent words in order to be able to share output embeddings with the input. This setup is akin to a hierarchical softmax with tied weights. We show that variable-sized input embeddings can perform better than fixed sized embeddings. Furthermore, this also enables weight sharing with an adaptive softmax output layer. <ref type="bibr" target="#b18">Merity et al. (2018)</ref> evaluates both character-based and word-based factorizations but does not directly compare them to each other. We perform a direct comparison of word-based and characterbased input vocabularies and also compare to a sub-word factorization for both the input and output. Recently, <ref type="bibr" target="#b0">Al-Rfou et al. (2018)</ref> demonstrated that self-attentional models can perform very well on language modeling tasks where the input and output is both characters. We also consider word-based benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ADAPTIVE INPUT REPRESENTATIONS</head><p>The adaptive softmax exploits the fact that the distribution of word types in natural language follows a Zipfian distribution in order to improve the computation of the output probabilities. We apply the same intuition for input word embeddings with the motivation to reduce the number of parameters which frees up capacity for other parts of the model.</p><p>We define a number of clusters that partitions the frequency ordered vocabulary V = V 1 âª V 2 , . . . , V nâ1 âª V n such that V i â© V j = â for âi, j, and i = j, where V 1 contains the most frequent words and V n the least frequent words. We will refer to V 1 as the head and to any subsequent clusters loosely as tail. We reduce the capacity for each cluster by a factor of k. That is, if words in V 1 have dimension d, then words in V n have dimension d k nâ1 . We typically set k = 4 following <ref type="bibr" target="#b10">Grave et al. (2017)</ref>.</p><p>Next, we add linear projections W 1 â R dÃd , . . . , W n â R d/k nâ1 Ãd to map the embeddings of each cluster to dimension d so that the concatenated output of the adaptive input embedding layer can be easily used by the subsequent model <ref type="figure">(Figure 1</ref>). We also project V 1 which already has dimension d.</p><p>When presented with a number of input words, the adaptive input embedding layer partitions the words into the various clusters, performs separate lookups in the embedding tables and then projects to dimension d, followed by concatenating the embeddings in the original order.</p><p>Weight sharing. When the output layer is an adaptive softmax with the same partition of V, d, and k as the adaptive input layer, then we can tie the weights <ref type="bibr" target="#b12">(Inan et al., 2016;</ref><ref type="bibr" target="#b26">Press &amp; Wolf, 2017)</ref>. This further reduces the number of parameters and can simultaneously improve performance ( Â§5). We can share both the parameters for the actual words as well as the projections W 1 , . . . , W n Sharing the word embeddings is straightforward except for the head where the adaptive softmax has n â 1 additional embeddings for the remaining clusters which are not shared with the input.</p><p>We share all projections, except for the head projection which is not available in the adaptive softmax since the model output is directly multiplied with the output word embeddings for the head band. Performance decreased when we added a head projection to the adaptive softmax in the output, regardless of when it was shared or not. Sharing both the word embeddings as well as the projections performed very well on WIKITEXT-103 but on BILLION WORD we only share the word embeddings as we found that this performed better on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MODEL</head><p>We follow most of the architectural choices described in <ref type="bibr" target="#b34">Vaswani et al. (2017)</ref> but use only a decoder network. We add sinusoidal position embeddings to the input layer and stack N = 16 blocks for both BILLION WORD and WIKITEXT-103. Each block contains two sub-blocks: the first is a multihead self-attention module with H = 16 heads. The second sub-block is a feed-forward module (FFN) of the form ReLU (W 1 X + b 1 )W 2 + b 2 where W 1 â R eÃe f f , W 1 â R e f f Ãe and e = 1024, e f f = 4096 unless otherwise stated. Different to <ref type="bibr" target="#b34">Vaswani et al. (2017)</ref> we apply layer normalization before the self-attention and FFN blocks instead of after, as we find it leads to more effective training. Sub-blocks are surrounded by a residual connection <ref type="bibr" target="#b11">(He et al., 2015)</ref>.</p><p>We use a dropout rate of 0.1 and attention dropout of 0.1 for BILLION WORD models, and increase regularization for WIKITEXT-103 by using dropout 0.3, and 0.1 ReLU dropout as well as attention dropout 0.1. We use the same hyperparameters for all models trained on the same dataset in order to enable a like for like comparison. When the dimensionality of the input or output layer differs from e, then we add a simple linear projection with no bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DATASETS</head><p>We experiment on the BILLION WORD benchmark and WIKITEXT-103. BILLION WORD contains 768M word tokens and has a vocabulary of about 800K word types, which corresponds to words with more than 3 occurrences in the training set <ref type="bibr" target="#b5">(Chelba et al., 2013)</ref>.</p><p>The training data of WIKITEXT-103 comprises about 100M tokens and a vocabulary of around 260K, corresponding to types with more than 3 occurrences in the training data <ref type="bibr" target="#b17">(Merity et al., 2016)</ref>. The dataset is composed of shuffled Wikipedia articles where the context carries across sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">BATCHING</head><p>For BILLION WORD we batch individual sentences since the corpus does not contain document structure. For  we partition the training data into blocks of 512 contiguous tokens ignoring document boundaries. Evaluation is the same except that we require blocks to contain complete sentences totaling up to 512 tokens. <ref type="bibr">2</ref> We limit the number of tokens per GPU to a maximum threshold B per GPU. That is, we add examples of similar length until we reach this threshold. When we train on multiple GPUs, each GPU processes B tokens using the same model parameters. This increases the effective batch size to the product of the number of GPUs and B. For BILLION WORD models we use B = 2048 and typically train on 32 GPUs, giving an effective batch size of 65K tokens. The smaller vocabulary of WIKITEXT-103 enables increasing B to 4096 and we train on 8 GPUs. We found that large batch training is beneficial for this dataset and we therefore accumulate gradient updates over two batches before committing a parameter update <ref type="bibr" target="#b23">(Ott et al., 2018a)</ref>. This gives an effective batch size of 65K tokens for  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">INPUT AND OUTPUT LAYER HYPERPARAMETERS</head><p>Embedding sizes. For fixed size word input layers and softmax output layers we generally use embeddings of size 512 for WIKITEXT-103. When we use an adaptive softmax in the output and fixed size word embeddings for the input, then we use dimension 256 for the input embeddings for BILLION WORD and 64 for  We tuned this choice on the validation set (Appendix A). BPE inputs and outputs have embeddings of size 1024.</p><p>Character CNN. We model character inputs by convolving the representations of all characters in a word following <ref type="bibr" target="#b14">Kim et al. (2015)</ref> which applies several filters, then max pooling, a number of highway layers and a projection. Character embeddings have size 128 and we apply seven filters of size 1x128, 2x256, 3x384, 4x512, 5x512, 6x512, 7x512, where 3x128 indicates a filter processing three characters that outputs 128 features. We use a single highway layer for WIKITEXT-103, and two for BILLION WORD. We do not add start of word and end of word markers as they did not improve validation accuracy. We train on the same pre-processed data as the other models, with unknown tokens in both the inputs and outputs.</p><p>Adaptive input representations and adaptive softmax. We use an adaptive softmax output layer to train models with large word-based vocabularies. For adaptive word inputs and adaptive softmax, we use embeddings of size d = 1024 for the head and reduce the size of subsequent clusters by a factor of k = 4. For WIKITEXT-103, we have three bands of size 20K (d=1024), 40K (d=256) and 200K (d=64). For BILLION WORD the bands are 60K (d=1024), 100K (d=256), and 640K (d=64).</p><p>Sub-word models. We learn a byte-pair encoding (BPE) of 32K codes on the training data of each benchmark <ref type="bibr" target="#b29">(Sennrich et al., 2016)</ref>. After applying the code to the training data we obtain a vocabulary of 33,337 tokens for WIKITEXT-103 and 32,347 tokens for BILLION WORD. BPE input/output embeddings have size 1024. The final evaluation is in terms word-level perplexity to be comparable to other models. The probability of a word is the product of the sub-word units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">OPTIMIZATION</head><p>Different to <ref type="bibr" target="#b34">Vaswani et al. (2017)</ref> we use Nesterov's accelerated gradient method <ref type="bibr" target="#b32">(Sutskever et al., 2013)</ref> with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 <ref type="bibr" target="#b25">(Pascanu et al., 2013)</ref>. The learning rate is linearly warmed up from 10 â7 to 1 for 16K steps and then annealed using a cosine learning rate schedule with C cycles <ref type="bibr" target="#b16">(Loshchilov &amp; Hutter, 2016)</ref>. Each cycle runs for twice the number of updates than the previous cycle and we lower the maximum and minimum learning rates by a rate M compared to the previous cycle. The initial minimum learning rate is 10 â5 and the maximum is 1.</p><p>BILLION WORD models train for a total of 975K updates over C = 3 cycles, the first cycle takes 137K steps, and we set M = 0.6. The WIKITEXT-103 models train for 286K steps over C = 4 cycles, the first cycle takes 18K setps and we set M = 0.75. We run experiments on DGX-1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband. We also use the NCCL2 library and the torch.distributed package for inter-GPU communication. We train models with 16-bit floating point precision, following <ref type="bibr" target="#b24">Ott et al. (2018b)</ref>. 10 LSTMs + SNM10-SKIP  23.7 --  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MAIN RESULTS</head><p>For the main results on BILLION WORD, we doubled the batch size by training on 64 GPUs instead of 32 GPUs. We also consider two larger setups, one where we added four more blocks (N = 20) and increased the FFN dimension to e f f = 6144 (large), and another where we add another four blocks (N = 24) with e f f = 8192 and e = 1536 (very large). All other settings follow Â§4.4 and all models were trained for the same number of steps. <ref type="table" target="#tab_1">Table 1</ref> compares our models to previous work on BILLION WORD. The adaptive input model outperforms the best previously reported result at an order of magnitude fewer parameters. Our large model performs nearly as well as an ensemble of over ten models and achieves a new state of the art of 24.14 perplexity. Our very large model performs as well as an ensemble of over ten models and achieves 23.02 perplexity. The Char-CNN model performs 0.6 PPL worse than the standard adaptive input model even though it trained for over 40% longer. <ref type="table" target="#tab_2">Table 2</ref> shows our result on  where adaptive inputs achieve 18.7 perplexity. For this result only, we partition the training data into blocks of 3072 contiguous tokens instead of 512 tokens as for other experiments. During evaluation we require blocks to contain complete sentences totaling up to 3072 tokens of which the first 2560 tokens serve as context to score the last 512 tokens; we take care to score all tokens in the test and validation sets. We motivate this choice in Â§5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">COMPARISON OF INPUT AND OUTPUT LAYER FACTORIZATIONS</head><p>Next, we perform a systematic comparison of different input and output layer factorizations. We consider a word-based setup with fixed size word input embeddings and a standard word softmax (SM) where embeddings have either dimension 512 (WIKITEXT-103) or 64 (BILLION WORD). We consider tying the input and output embeddings (SM-T). Instead of words, we try less sparse sub-   word units, both in the input and output, with embeddings of size 1024 (BPE) and shared weights (BPE-T). Next, we consider replacing the fixed size output representations by an adaptive softmax (ASM) and characters as input (CNN). Finally, we use both adaptive input word representations as well as an adaptive softmax (ADP) and a tied version (ADP-T). All models use the same selfattention architecture described in Â§4.1. <ref type="table" target="#tab_4">Table 3</ref> shows results when training all configurations for the same number of updates. Adaptive input representations with tied input and output layers (ADP-T) achieve the highest accuracy at the same speed as the BPE models which have a very small vocabulary (33K versus 260K). CNN is 1 perplexity worse than ADP-T and requires well over twice the training time. It is the slowest approach, even though it has a fast adaptive softmax in the output. Fixed word embeddings perform least well (SM). Sub-word units are fast to train and perform better than word models with fixed sized embeddings. ASM improves over SM and greatly speeds up training. For ASM, we found that reducing the dimension of the input word embeddings to 64 on WIKITEXT-103 results in better accuracy (Appendix A). <ref type="table" target="#tab_5">Table 4</ref> shows that adaptive input representations perform equally well on BILLION WORD compared to other factorizations. ADP-T is 34% faster than ADP because there are fewer parameters to update. Similar to before, ADP-T trains more than twice as fast as CNN at higher accuracy, however, the accuracy gap is narrower than for  Regularization is more important on WIKITEXT-103 while models for BILLION WORD benefit from additional capacity. Because of this we used input word embeddings of size 256 for ASM.</p><p>We also trained CNN without replacing input words outside the vocabulary by an unknown symbol, however, this only improved validation perplexity by 0.16.  <ref type="figure">Figure 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ANALYSIS</head><p>Next, we turn to the question of how well models perform on rare words compared to frequent words. We compute the average loss for each word in the test set and group words by frequency. <ref type="figure">Figure 2</ref> shows results on  Tying weights helps all models on rare words, likely because of regularization effects. Fixed size word embeddings with a word softmax (SM and SM-T) do not perform well on rare words. This is likely due to underfitting on common words and we use the largest possible embedding size we could fit on 16GB GPU cards given our batch size (more experimentation in Appendix A). BPE and BPE-T perform poorly on rare words because probabilities are a product of several sub-word units. ADP-T performs best across all frequency ranges. <ref type="figure">Figure 3</ref> bins the loss by the frequency of the previous word and shows that CNN does well when it has rare words in the context, however, ADP-T does best across all bins. <ref type="figure" target="#fig_5">Figure 4</ref> shows an equivalent analysis for BILLION WORD. The largest differences between models is on rare words. CNN performs best on very rare words but is outperformed by ADP in all other settings. Similar to WIKITEXT-103, BPE and BPE-T perform poorly on rare words. Further   Training block size is the number of consecutive tokens considered during training. Inference context is the number of tokens provided at evaluation before scoring tokens.</p><p>analysis (Appendix 5.3) binning the loss by the frequency of the previous word shows that weight sharing also helps for BILLION WORD and that CNN does very well on rare words for BILLION WORD compared to other models. <ref type="table" target="#tab_7">Table 5</ref> shows the importance of context size for  Training block size is the number of consecutive tokens that are considered at once during training. Inference context is the number of tokens that are provided at evaluation before any tokens are scored. Simply increasing the training block size from 512 to 3072 results in an improvement of nearly 1.2 perplexity with no inference context window. Increasing the context size at inference time results in an improvement of 0.6 perplexity for the largest training block size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">ADAPTIVE SOFTMAX VS. FULL SOFTMAX</head><p>We also found that adaptive softmax can benefit from additional regularization of rare words. Adaptive softmax first projects the model output to the dimension of a particular cluster and then computes a dot product with the respective word embeddings. We add dropout to the output of the first projection for all clusters, except for the head. This change enables the adaptive softmax to outperform a standard softmax over fixed size output word embeddings on   <ref type="table">(Table 6</ref>).</p><p>However, we found that adding dropout in this way is not helpful for larger datasets such as BILLION WORD. Unfortunately, a standard softmax over 800K words is not tractable and we were unable to make a comparison. It may be possible to achieve better results by tuning dropout for each band of the tail and we leave this for future work.  <ref type="table">Table 6</ref>: Perplexity on WIKITEXT-103 when regularizing rare words in adaptive softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Adaptive input embeddings vary the size of input word embeddings which can improve accuracy while drastically reducing the number of model parameters. When sharing parameters with an adaptive softmax, the number of parameters can be further reduced which improves training speed. We presented a comparison between different input and output layer factorizations including word inputs, character inputs and sub-word units in both the input and output.</p><p>Our experiments show that models with adaptive input embeddings train faster compared to character input CNNs while achieving higher accuracy. We achieve new state of the art results on WIKITEXT-103 and BILLION WORD. In future work, we will apply variable sized input embeddings to other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL A ADDITIONAL EXPERIMENTS ON WIKITEXT-103</head><p>This appendix shows various ablation. <ref type="table" target="#tab_10">Table 7</ref> shows that reducing the capacity of fixed size word input embddings is beneficial on WIKITEXT-103. The next set of results in <ref type="table" target="#tab_10">Table 7</ref> shows results for various settings of the SM and SM-T models. We also experimented with sharing the head projection but found this to perform less well than not sharing it. Finally, <ref type="table" target="#tab_11">Table 8</ref> shows various band sizes for adaptive input word embbedings.</p><p>We also show the performance of <ref type="bibr" target="#b18">Merity et al. (2018)</ref> who use an adaptive softmax with equally sized word representations and share the input and output embeddings (no dim reduction, tied).     <ref type="figure">Figure 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " a 9 V l t F J 2 B C w 1 u p p + i Y 3 U D m 9 s r D Q = " &gt; A A A B + H i c b Z D L S g M x F I b P 1 F u t l 4 6 6 d B M s Q l d l R g R d F t y 4 r G A v 0 A 4 l k 8 m 0 o Z l k S D J K H f o k b l w o 4 t Z H c e f b m L a z 0 N Y f A h / / O Y d z 8 o c p Z 9 p 4 3 r d T 2 t j c 2 t 4 p 7 1 b 2 9 g 8 O q + 7 R c U f L T B H a J p J L 1 Q u x p p w J 2 j b M c N p L F c V J y G k 3 n N z M 6 9 0 H q j S T 4 t 5 M U x o k e C R Y z A g 2 1 h q 6 1 Q g N F B u N D V Z K P q J o 6 N a 8 h r c Q W g e / g B o U a g 3 d r 0 E k S Z Z Q Y Q j H W v d 9 L z V B j p V h h N N Z Z Z B p m m I y w S P a t y h w Q n W Q L w 6 f o X P r R C i W y j 5 h 0 M L 9 P Z H j R O t p E t r O B J u x X q 3 N z f 9 q / c z E 1 0 H O R J o Z K s h y U Z x x Z C S a p 4 A i p i g x f G o B E 8 X s r Y i M s c L E 2 K w q N g R / 9 c v r 0 L l o + J b v L m v N e h F H G U 7 h D O r g w x U 0 4 R Z a 0 A Y C G T z D K 7 w 5 T 8 6 L 8 + 5 8 L F t L T j F z A n / k f P 4 A Z 1 u S 1 A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 9 V l t F J 2 B C w 1 u p p + i Y 3 U D m 9 s r DQ = " &gt; A A A B + H i c b Z D L S g M x F I b P 1 F u t l 4 6 6 d B M s Q l d l R g R d F t y 4 r G A v 0 A 4 l k 8 m 0 o Z l k S D J K H f o k b l w o 4 t Z H c e f b m L a z 0 N Y f A h / / O Y d z 8 o c p Z 9 p 4 3 r d T 2 t j c 2 t 4 p 7 1 b 2 9 g 8 O q + 7 R c U f L T B H a J p J L 1 Q u x p p w J 2 j b M c N p L F c V J y G k 3 n N z M 6 9 0 H q j S T 4 t 5 M U x o k e C R Y z A g 2 1 h q 6 1 Q g N F B u N D V Z K P q J o 6 N a 8 h r c Q W g e / g B o U a g 3 d r 0 E k S Z Z Q Y Q j H W v d 9 L z V B j p V h h N N Z Z Z B p m m I y w S P a t y h w Q n W Q L w 6 f o X P r R C i W y j 5 h 0 M L 9 P Z H j R O t p E t r O B J u x X q 3 N z f 9 q / c z E 1 0 H O R J o Z K s h y U Z x x Z C S a p 4 A i p i g x f G o B E 8 X s r Y i M s c L E 2 K w q N g R / 9 c v r 0 L l o + J b v L m v N e h FH G U 7 h D O r g w x U 0 4 R Z a 0 A Y C G T z D K 7 w 5 T 8 6 L 8 + 5 8 L F t L T j F z A n / k f P 4 A Z 1 u S 1 A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 9 V l t F J 2 B C w 1 u p p + i Y 3 U D m 9 s r D Q = " &gt; A A A B + H i c b Z D L S g M x F I b P 1 F u t l 4 6 6 d B M s Q l d l R g R d F t y 4 r G A v 0 A 4 l k 8 m 0 o Z l k S D J K H f o k b l w o 4 t Z H c e f b m L a z 0 N Y f A h / / O Y d z 8 o c p Z 9 p 4 3 r d T 2 t j c 2 t 4 p 7 1 b 2 9 g 8 O q + 7 R c U f L T B H a J p J L 1 Q u x p p w J 2 j b M c N p L F c V J y G k 3 n N z M 6 9 0 H q j S T 4 t 5 M U x o k e C R Y z A g 2 1 h q 6 1 Q g N F B u N D V Z K P q J o 6 N a 8 h r c Q W g e / g B o U a g 3 d r 0 E k S Z Z Q Y Q j H W v d 9 L z V B j p V h h N N Z Z Z B p m m I y w S P a t y h w Q n W Q L w 6 f o X P r R C i W y j 5 h 0 M L 9 P Z H j R O t p E t r O B J u x X q 3 N z f 9 q / c z E 1 0 H O R J o Z K s h y U Z x x Z C S a p 4 A i p i g x f G o B E 8 X s r Y i M s c L E 2 K w q N g R / 9 c v r 0 L l o + J b v L m v N e h F H G U 7 h D O r g w x U 0 4 R Z a 0 A Y C G T z D K 7 w 5 T 8 6 L 8 + 5 8 L F t L T j F z A n / k f P 4 A Z 1 u S 1 A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 9 V l t F J 2 B C w 1 u p p + i Y 3 U D m 9 s r D Q = " &gt; A A A B + H i c b Z D L S g M x F I b P 1 F u t l 4 6 6 d B M s Q l d l R g R d F t y 4 r G A v 0 A 4 l k 8 m 0 o Z l k S D J K H f o k b l w o 4 t Z H c e f b m L a z 0 N Y f A h / / O Y d z 8 o c p Z 9 p 4 3 r d T 2 t j c 2 t 4 p 7 1 b 2 9 g 8 O q + 7 R c U f L T B H a J p J L 1 Q u x p p w J 2 j b M c N p L F c V J y G k 3 n N z M 6 9 0 H q j S T 4 t 5 M U x o k e C R Y z A g 2 1 h q 6 1 Q g N F B u N D V Z K P q J o 6 N a 8 h r c Q W g e / g B o U a g 3 d r 0 E k S Z Z Q Y Q j H W v d 9 L z V B j p V h h N N Z Z Z B p m m I y w S P a t y h w Q n W Q L w 6 f o X P r R C i W y j 5 h 0 M L 9 P Z H j R O t p E t r O B J u x X q 3 N z f 9 q / c z E 1 0 H O R J o Z K s h y U Z x x Z C S a p 4 A i p i g x f G o B E 8 X s r Y i M s c L E 2 K w q N g R / 9 c v r 0 L l o + J b v L m v N e h F H G U 7 h D O r g w x U 0 4 R Z a 0 A Y C G T z D K 7 w 5 T 8 6 L 8 + 5 8 L F t L T j F z A n / k f P 4 A Z 1 u S 1 A = = &lt; / l a t e x i t &gt; V 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q u R U N k T x I l J t y S 1 E 3 C q d 9 P h a 6 P U = " &gt; A A A B 9 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S x C T y U R Q Y 8 F L x 4 r 2 F Z o Q 9 l s X 9 q l m 0 3 c 3 R R K 6 O / w 4 k E R r / 4 Y b / 4 b N 2 0 O 2 j q w M M y 8 x 5 u d I B F c G 9 f 9 d k o b m 1 v b O + X d y t 7 + w e F R 9 f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>4 q 1 9 e J 5 3 L h m f 5 / V W t W S / q K M M Z n E M d P L i G J t x B C 9 r A 4 A m e 4 R X e n K n z 4 r w 7 H 8 v R k l P s n M I f O J 8 / t g + R 9 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q u R U N k T x I l J t y S 1 E 3 C q d 9 P h a 6 P U = " &gt; A A A B 9 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S x C T y U R Q Y 8 F L x 4 r 2 F Z o Q 9 l s X 9 q l m 0 3 c 3 R R K 6 O / w 4 k E R r / 4 Y b / 4 b N 2 0 O 2 j q w M M y 8 x 5 u d I B F c G 9 f 9 d k o b m 1 v b O + X d y t 7 + w e F R 9 fi k o + N U M W y z W M T q M a A a B Z f Y N t w I f E w U 0 i g Q 2 A 0 m t 7 n f n a L S P J Y P Z p a g H 9 G R 5 C F n 1 F j J 7 0 f U j B k V W W c + 8 A b V m t t w F y D r x C t I D Q q 0 B t W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w H m l n 2 p M K J v Q E f Y s l T R C 7 W e L 0 H N y Y Z U h C W Nl n z R k o f 7 e y G i k 9 S w K 7 G Q e U q 9 6 u f i f 1 0 t N e O N n X C a p Q c m W h 8 J U E B O T v A E y 5 A q Z E T N L K F P c Z i V s T B V l x v Z U s S V 4 q 1 9 e J 5 3 L h m f 5 / V W t W S / q K M M Z n E M d P L i G J t x B C 9 r A 4 A m e 4 R X e n K n z 4 r w 7 H 8 v R k l P s n M I f O J 8 / t g + R 9 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q u R U N k T x I l J t y S 1 E 3 C q d 9 P h a 6 P U = " &gt; A A A B 9 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S x C T y U R Q Y 8 F L x 4 r 2 F Z o Q 9 l s X 9 q l m 0 3 c 3 R R K 6 O / w 4 k E R r / 4 Y b / 4 b N 2 0 O 2 j q w M M y 8 x 5 u d I B F c G 9 f 9 d k o b m 1v b O + X d y t 7 + w e F R 9 f i k o + N U M W y z W M T q M a A a B Z f Y N t w I f E w U 0 i g Q 2 A 0 m t 7 n f n a L S P J Y P Z p a g H 9 G R 5 C F n 1 F j J 7 0 f U j B k V W W c + 8 A b V m t t w F y D r x C t I D Q q 0 B t W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w H m l n 2 p M K J v Q E f Y s l T R C 7 W e L 0 H N y Y Z U h C W N l n z R k o f 7 e y G i k 9 S w K 7 G Q e U q 9 6 u f i f 1 0 t N e O N n X C a p Q c m W h 8 J U E B O T v A E y 5 A q Z E T N L K F P c Z i V s T B V l x v Z U s S V 4 q 1 9 e J 5 3 L h m f 5 / V W t W S / q K M M Z n E M d P L i G J t x B C 9r A 4 A m e 4 R X e n K n z 4 r w 7 H 8 v R k l P s n M I f O J 8 / t g + R 9 g = = &lt; / l a t e x i t &gt; V n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U 3 J G g 9 a h e b u v M B v T j m W 1 a J e 2 Y 8 4 = " &gt; A A A B 9 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S x C T y U R Q Y 8 F L x 4 r 2 F Z o Q 9 l s N + 3 S z S b u v h R K 6 O / w 4 k E R r / 4 Y b / 4 b N 2 0 O 2 j q w M M y 8 x 5 u d I J H C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M X G q G W + z W M b 6 M a C G S 6 F 4 G w V K / p h o T q N A 8 m 4 w u c 3 9 7 p R r I 2 L 1 g L O E + x E d K R E K R t F K f j + i O G Z U Z p 3 5 Q A 2 q N b f h L k D W i V e Q G h R o D a p f / W H M 0 o g r Z J I a 0 / P c B P 2 M a h R M 8 n m l n x q e U D a h I 9 6 z V N G I G z 9 b h J 6 T C 6 s M S R h r + x S S h f p 7 I 6 O R M b M o s J N 5 S L P q 5 e J / X i / F 8 M b P h E p S 5 I o t D 4 W p J B i T v A E y F J o z l D N L K N P C Z i V s T D V l a H u q 2 B K 8 1 S + v k 8 5 l w 7 P 8 / q r W r B d 1 l O E M z q E O H l x D E + 6 g B W 1 g 8 A T P 8 A p v z t R 5 c d 6 d j + V o y S l 2 T u E P n M 8 f E p K S M w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U 3 J G g 9 a h e b u v M B v T j m W 1 a J e 2 Y 8 4 = " &gt; A A A B 9 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S x C T y U R Q Y 8 F L x 4 r 2 F Z o Q 9 l s N + 3 S z S b u v h R K 6 O / w 4 k E R r / 4 Y b / 4 b N 2 0 O 2 j q w M M y 8 x 5 u d I J H C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M X G q G W + z W M b 6 M a C G S 6 F 4 G w V K / p h o T q N A 8 m 4 w u c 3 9 7 p R r I 2 L 1 g L O E + x E d K R E K R t F K f j + i O G Z U Z p 3 5 Q A 2 q N b f h L k D W i V e Q G h R o D a p f / W H M 0 o g r Z J I a 0 / P c B P 2 M a h R M 8 n m l n x q e U D a h I 9 6 z V N G I G z 9 b h J 6 T C 6 s M S R h r + x S S h f p 7 I 6 O R M b M o s J N 5 S L P q 5 e J / X i / F 8 M b P h E p S 5 I o t D 4 W p J B i T v A E y F J o z l D N L K N P C Z i V s T D V l a H u q 2 B K 8 1 S + v k 8 5 l w 7 P 8 / q r W r B d 1 l O E M z q E O H l x D E + 6 g B W 1 g 8 A T P 8 A p v z t R 5 c d 6 d j + V o y S l 2 T u E P n M 8 f E p K S M w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U 3 J G g 9 a h e b u v M B v T j m W 1 a J e 2 Y 8 4 = " &gt; A A A B 9 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S x C T y U R Q Y 8 F L x 4 r 2 F Z o Q 9 l s N + 3 S z S b u v h R K 6 O / w 4 k E R r / 4 Y b / 4 b N 2 0 O 2 j q w M M y 8 x 5 u d I J H C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M X G q G W + z W M b 6 M a C G S 6 F 4 G w V K / p h o T q N A 8 m 4 w u c 3 9 7 p R r I 2 L 1 g L O E + x E d K R E K R t F K f j + i O G Z U Z p 3 5 Q A 2 q N b f h L k D W i V e Q G h R o D a p f / W H M 0 o g r Z J I a 0 / P c B P 2 M a h R M 8 n m l n x q e U D a h I 9 6 z V N G I G z 9 b h J 6 T C 6 s M S R h r + x S S h f p 7 I 6 O R M b M o s J N 5 S L P q 5 e J / X i / F 8 M b P h E p S 5 I o t D 4 W p J B i T v A E y F J o z l D N L K N P C Z i V s T D V l a H u q 2 B K 8 1 S + v k 8 5 l w 7 P 8 / q r W r B d 1 l O E M z q E O H l x D E + 6 g B W 1 g 8 A T P 8 A p v z t R 5 c d 6 d j + V o y S l 2 T u E P n M 8 f E p K S M w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U 3 J G g 9 a h e b u v M B v T j m W 1 a J e 2 Y 8 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a p f / W H M 0 o g r Z J I a 0 / P c B P 2 M a h R M 8 n m l n x q e U D a h I 9 6 z V N G I G z 9 b h J 6 T C 6 s M S R h r + x S S h f p 7 I 6 O R M b M o s J N 5 S L P q 5 e J /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>e x i t s h a 1 _ b a s e 6 4 = " g 0 3 p v P N c / e d O r c X H B x K i B i K G C r M = " &gt; A A A B + n i c b Z D L S s N A F I Z P 6 q 3 W W 6 p L N 4 N F 6 M a S i K D L g h u X F e w F 2 l g m k 0 k 7 d D I J M x O l x D y K G x e K u P V J 3 P k 2 T t s s t P W H g Y / / n M M 5 8 / s J Z 0 o 7 z r d V W l v f 2 N w q b 1 d 2 d v f 2 D + z q Y U f F q S S 0 T W I e y 5 6 P F e V M 0 L Z m m t N e I i m O f E 6 7 / u R 6 V u 8 + U K l Y L O 7 0 N K F e h E e C h Y x g b a y h X R 2 E E p M s y L P J f S b O 3 D w f 2 j W n 4 c y F V s E t o A a F W k P 7 a x D E J I 2 o 0 I R j p f q u k 2 g v w 1 I z w m l e G a S K J p h M 8 I j 2 D Q o c U e V l 8 9 N z d G q c A I W x N E 9 o N H d / T 2 Q 4 U m o a + a Y z w n q s l m s z 8 7 9 a P 9 X h l Z c x k a S a C r JY F K Y c 6 R j N c k A B k 5 R o P j W A i W T m V k T G 2 G S h T V o V E 4 K 7 / O V V 6 J w 3 X M O 3 F 7 V m v Y i j D M d w A n V w 4 R K a c A M t a A O B R 3 i G V 3 iz n q w X 6 9 3 6 W L S W r G L m C P 7 I + v w B i O 6 U F A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 0 3 p v P N c / e d O r c X H B x K i B i K G C r M = " &gt; A A A B + n i c b Z D L S s N A F I Z P 6 q 3 W W 6 p L N 4 N F 6 M a S i K D L g h u X F e w F 2 l g m k 0 k 7 d D I J M x O l x D y K G x e K u P V J 3 P k 2 T t s s t P W H g Y / / n M M 5 8 / s J Z 0 o 7 z r d V W l v f 2 N w q b 1 d 2 d v f 2 D + z q Y U f F q S S 0 T W I e y 5 6 P F e V M 0 L Z m m t N e I i m O f E 6 7 / u R 6 V u 8 + U K l Y L O 7 0 N K F e h E e C h Y x g b a y h X R 2 E E p M s y L P J f S b O 3 D w f 2 j W n 4 c y F V s E t o A a F W k P 7 a x D E J I 2 o 0 I R j p f q u k 2 g v w 1 I z w m l e G a S K J p h M 8 I j 2 D Q o c U e V l 8 9 N z d G q c A I W x N E 9 o N H d / T 2 Q 4 U m o a + a Y z w n q s l m s z 8 7 9 a P 9 X h l Z c x k a S a C r J Y F K Y c 6 R j N c k A B k 5 R o P j W A i W T m V k T G 2 G S h T V o V E 4 K 7 / O V V 6 J w 3 X M O 3 F 7 V m v Y i j D M d w A n V w 4 R K a c A M t a A O B R 3 i G V 3 i z n q w X 6 9 3 6 W L S W r G L m C P 7 I + v w B i O 6 U F A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 0 3 p v P N c / e d O r c X H B x K i B i K G C r M = " &gt; A A A B + n i c b Z D L S s N A F I Z P 6 q 3 W W 6 p L N 4 N F 6 M a S i K D L g h u X F e w F 2 l g m k 0 k 7 d D I J M x O l x D y K G x e K u P V J 3 P k 2 T t s s t P W H g Y / / n M M 5 8 / s J Z 0 o 7 z r d V W l v f 2 N w q b 1 d 2 d v f 2 D + z q Y U f F q S S 0 T W I e y 5 6 P F e V M 0 L Z m m t N e I i m O f E 6 7 / u R 6 V u 8 + U K l Y L O 7 0 N K F e h E e C h Y x g b a y h X R 2 E E p M s y L P J f S b O 3 D w f 2 j W n 4 c y F V s E t o A a F W k P 7 a x D E J I 2 o 0 I R j p f q u k 2 g v w 1 I z w m l e G a S K J p h M 8 I j 2 D Q o c U e V l 8 9 N z d G q c A I W x N E 9 o N H d / T 2 Q 4 U m o a + a Y z w n q s l m s z 8 7 9 a P 9 Xh l Z c x k a S a C r J Y F K Y c 6 R j N c k A B k 5 R o P j W A i W T m V k T G 2 G S h T V o V E 4 K 7 / O V V 6 J w 3 X M O 3 F 7 V m v Y i j D M d w A n V w 4 R K a c A M t a A O B R 3 i G V 3 i z n q w X 6 9 3 6 W L S W r G L m C P 7 I + v w B i O 6 U F A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 0 3 p v P N c / e d O r c X H B x K i B i K G C r M = " &gt; A A A B + n i c b Z D L S s N A F I Z P 6 q 3 W W 6 p L N 4 N F 6 M a S i K D L g h u X F e w F 2 l g m k 0 k 7 d D I J M x O l x D y K G x e K u P V J 3 P k 2 T t s s t P W H g Y / / n M M 5 8 / s J Z 0 o 7 z r d V W l v f 2 N w q b 1 d 2 d v f 2 D + z q Y U f F q S S 0 T W I e y 5 6 P F e V M 0 L Z m m t N e I i m O f E 6 7 / u R 6 V u 8 + U K l Y L O 7 0 N K F e h E e C h Y x g b a y h X R 2 E E p M s y L P J f S b O 3 D w f 2 j W n 4 c y F V s E t o A a F Wk P 7 a x D E J I 2 o 0 I R j p f q u k 2 g v w 1 I z w m l e G a S K J p h M 8 I j 2 D Q o c U e V l 8 9 N z d G q c A I W x N E 9 o N H d / T 2 Q 4 U m o a + a Y z w n q s l m s z 8 7 9 a P 9 X h l Z c x k a S a C r J Y F K Y c 6 R j N c k A B k 5 R o P j W A i W T m V k T G 2 G S h T V o V E 4 K 7 / O V V 6 J w 3 X M O 3 F 7 V m v Y i j D M d w A n V w 4 R K a c A M t a A O B R 3 i G V 3 i z n q w X 6 9 3 6 W L S W r G L m C P 7 I + v w B i O 6 U F A = = &lt; / l a t e x i t &gt; d k n 1 ! d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C T j K L E x S l L d X f Q 2 c D K A X D c N s G O 8 = " &gt; A A A C C H i c b Z D L S s N A F I Y n 9 V b r L e r S h Y N V c G N J R N B l w Y 3 L C v Y C b S y T y a Q d O p m E m R O l h C z d + C p u X C j i 1 k d w 5 9 s 4 b b P Q 6 g 8 D H / 8 5 h z P n 9 x P B N T j O l 1 V a W F x a X i m v V t b W N z a 3 7 O 2 d l o 5 T R V m T x i J W H Z 9 o J r h k T e A g W C d R j E S + Y G 1 / d D m p t + + Y 0 j y W N z B O m B e R g e Q h p w S M 1 b f 3 e 6 E i N A v y b H S b y R M 3 z 3 F P 8 c E Q i F L x P Q 7 6 d t W p O V P h v + A W U E W F G n 3 7 s x f E N I 2 Y B C q I 1 l 3 X S c D L i A J O B c s r v V S z h N A R G b C u Q U k i p r 1 s e k i O j 4 w T 4 D B W 5 k n A U / f n R E Y i r c e R b z o j A k M 9 X 5 u Y / 9 W 6 K Y Q X X s Z l k g K T d L Y o T A W G G E 9 S w Q F X j I I Y G y B U c f N X T I f E J A M m u 4 o J w Z 0 / + S + 0 T m u u 4 e u z a v 2 w i K O M 9 t A B O k Y u O k d 1 d I U a q I k o e k B P 6 A W 9 W o / W s / V m v c 9 a S 1 Y x s 4 t + y f r 4 B h U f m e U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C T j K L E x S l L d X f Q 2 c D K A X D c N s G O 8 = " &gt; A A A C C H i c b Z D L S s N A F I Y n 9 V b r L e r S h Y N V c G N J R N B l w Y 3 L C v Y C b S y T y a Q d O p m E m R O l h C z d + C p u X C j i 1 k d w 5 9 s 4 b b P Q 6 g 8 D H / 8 5 h z P n 9 x P B N T j O l 1 V a W F x a X i m v V t b W N z a 3 7 O 2 d l o 5 T R V m T x i J W H Z 9 o J r h k T e A g W C d R j E S + Y G 1 / d D m p t + + Y 0 j y W N z B O m B e R g e Q h p w S M 1 b f 3 e 6 E i N A v y b H S b y R M 3 z 3 F P 8 c E Q i F L x P Q 7 6 d t W p O V P h v + A W U E W F G n 3 7 s x f E N I 2 Y B C q I 1 l 3 X S c D L i A J O B c s r v V S z h N A R G b C u Q U k i p r 1 s e k i O j 4 w T 4 D B W 5 k n A U / f n R E Y i r c e R b z o j A k M 9 X 5 u Y / 9 W 6 K Y Q X X s Z l k g K T d L Y o T A W G G E 9 S w Q F X j I I Y G y B U c f N X T I f E J A M m u 4 o J w Z 0 / + S + 0 T m u u 4 e u z a v 2 w i K O M 9 t A B O k Y u O k d 1 d I U a q I k o e k B P 6 A W 9 W o / W s / V m v c 9 a S 1 Y x s 4 t + y f r 4 B h U f m e U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C T j K L E x S l L d X f Q 2 c D K A X D c N s G O 8 = " &gt; A A A C C H i c b Z D L S s N A F I Y n 9 V b r L e r S h Y N V c G N J R N B l w Y 3 L C v Y C b S y T y a Q d O p m E m R O l h C z d + C p u X C j i 1 k d w 5 9 s 4 b b P Q 6 g 8 D H / 8 5 h z P n 9 x P B N T j O l 1 V a W F x a X i m v V t b W N z a 3 7 O 2 d l o 5 T R V m T x i J W H Z 9 o J r h k T e A g W C d R j E S + Y G 1 / d D m p t + + Y 0 j y W N z B O m B e R g e Q h p w S M 1 b f 3 e 6 E i N A v y b H S b y R M 3 z 3 F P 8 c E Q i F L x P Q 7 6 d t W p O V P h v + A W U E W F G n 3 7 s x f E N I 2 Y B C q I 1 l 3 X S c D L i A J O B c s r v V S z h N A R G b C u Q U k i p r 1 s e k i O j 4 w T 4 D B W 5 k n A U / f n R E Y i r c e R b z o j A k M 9 X 5 u Y / 9 W 6 K Y Q X X s Z l k g K T d L Y o T A W G G E 9 S w Q F X j I I Y G y B U c f N X T I f E J A M m u 4 o J w Z 0 / + S + 0 T m u u 4 e u z a v 2 w i K O M 9 t A B O k Y u O k d 1 d I U a q I k o e k B P 6 A W 9 W o / W s / V m v c 9 a S 1 Y x s 4 t + y f r 4 B h U f m e U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C T j K L E x S l L d X f Q 2 c D K A X D c N s G O 8 = " &gt; A A A C C H i c b Z D L S s N A F I Y n 9 V b r L e r S h Y N V c G N J R N B l w Y 3 L C v Y C b S y T y a Q d O p m E m R O l h C z d + C p u X C j i 1 k d w 5 9 s 4 b b P Q 6 g 8 D H / 8 5 h z P n 9 x P B N T j O l 1 V a W F x a X i m v V t b W N z a 3 7 O 2 d l o 5 T R V m T x i J W H Z 9 o J r h k T e A g W C d R j E S + Y G 1 / d D m p t + + Y 0 j y W N z B O m B e R g e Q h p w S M 1 b f 3 e 6 E i N A v y b H S b y R M 3 z 3 F P 8 c E Q i F L x P Q 7 6 d t W p O V P h v + A W U E W F G n 3 7 s x f E N I 2 Y B C q I 1 l 3 X S c D L i A J O B c s r v V S z h N A R G b C u Q U k i p r 1 s e k i O j 4 w T 4 D B W 5 k n A U / f n R E Y i r c e R b z o j A k M 9 X 5 u Y / 9 W 6 K Y Q X X s Z l k g K T d L Y o T A W G G E 9 S w Q F X j I I Y G y B U c f N X T I f E J A M m u 4 o J w Z 0 / + S + 0 T m u u 4 e u z a v 2 w i K O M 9 t A B O k Y u O k d 1 d I U a q I k o e k B P 6 A W 9 W o / W s / V m v c 9 a S 1 Y x s 4 t + y f r 4 B h U f m e U = &lt; / l a t e x i t &gt; ModelFigure 1: Illustration of adaptive input representations. Words are assigned to clusters V i based on their frequency which determines the size of the representations. Embeddings are projected to a common dimension d before being fed to the model. reduction of 10.5 perplexity over the previously best published result. On the larger BILLION WORD benchmark our best model with adaptive input embeddings achieves 23.02 perplexity, a reduction of nearly 5 perplexity over the next best previously published result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Loss of models binned by word frequency on the test set of WIKITEXT-103. Bins are not cumulative. Loss of models when binning by the frequency of the previous word measured on </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Loss of models when binning by word frequency on the test set of BILLION WORD. Bins are not cumulative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Word frequency bin Loss on the next word BPE BPE-T ASM CNN ADP ADP-T Figure 5: Loss of models when binning by the frequency of the previous word measured on BILLION WORD (cf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test perplexity on BILLION WORD. Adaptive inputs share parameters with an adaptive softmax. Training times of Char-CNN and Adaptive input models are measured when training with 64 GPUs. â  does not include embedding and softmax layers</figDesc><table><row><cell></cell><cell>Test</cell><cell>Train Time (hours)</cell><cell>Parameters</cell></row><row><cell>Grave et al. (2016)</cell><cell>40.8</cell><cell>-</cell><cell></cell></row><row><cell cols="2">Dauphin et al. (2017) 37.2</cell><cell>-</cell><cell>229M</cell></row><row><cell>Merity et al. (2018)</cell><cell>33.0</cell><cell>-</cell><cell>151M</cell></row><row><cell>Rae et al. (2018)</cell><cell>29.2</cell><cell>-</cell><cell></cell></row><row><cell>Adaptive inputs</cell><cell>18.7</cell><cell>67</cell><cell>247M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test perplexity on WIKITEXT-103 (cf. Table 1). Training time is based on 8 GPUs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="7">: Test perplexity on WIKITEXT-103 for various input and output layer factorizations. Train-</cell></row><row><cell cols="7">ing speed was measured on a single 8-GPU machine. (*) indicates a modified training regime</cell></row><row><cell cols="7">because of large memory requirements: the maximum number of tokens per GPU was lowered to</cell></row><row><cell cols="7">1024 from 4096 but the same number of updates were performed by processing four batches before</cell></row><row><cell cols="2">committing a weight update.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Input</cell><cell>Output</cell><cell>Valid</cell><cell>Test</cell><cell>Train time (hours)</cell><cell>Params</cell></row><row><cell cols="5">BPE-T BPE Embedding BPE Softmax (shared) 27.44 27.51</cell><cell cols="2">34 234.7M</cell></row><row><cell>BPE</cell><cell cols="2">BPE Embedding BPE Softmax</cell><cell cols="2">27.02 27.13</cell><cell cols="2">35 267.8M</cell></row><row><cell>ASM</cell><cell>Embedding</cell><cell>Adaptive</cell><cell cols="2">26.97 27.06</cell><cell cols="2">62 532.8M</cell></row><row><cell>CNN</cell><cell>Char-CNN</cell><cell>Adaptive</cell><cell cols="2">26.13 26.25</cell><cell cols="2">92 365.8M</cell></row><row><cell>ADP</cell><cell>Adaptive</cell><cell>Adaptive</cell><cell cols="2">26.38 26.49</cell><cell cols="2">65 458.4M</cell></row><row><cell cols="2">ADP-T Adaptive</cell><cell>Adaptive (shared)</cell><cell cols="2">25.51 25.58</cell><cell cols="2">43 330.8M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Test perplexity on BILLION WORD. Training speed measured on four 8-GPU machines.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Perplexity on WIKITEXT-103 with different context sizes during training and inference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Validation perplexity of our models on </figDesc><table><row><cell>Softmax cutoff</cell><cell>Valid PPL</cell></row><row><cell>20k/40k/200k</cell><cell>19.79</cell></row><row><cell>20k/140k/100k</cell><cell>20.26</cell></row><row><cell>20k/40k/60k/140k</cell><cell>20.53</cell></row><row><cell>60k/100k/100k</cell><cell>20.52</cell></row><row><cell>5k/155k/100k</cell><cell>20.06</cell></row><row><cell>20k/40k/200k</cell><cell>19.99</cell></row><row><cell>10k/60k/190k</cell><cell>19.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Validation perplexity on WIKITEXT-103 with tied adaptive inputs &amp; outputs. The bands signify the number of words belonging to each band. In every case, the first band has dimension 1024, the second band 256, the third band 64 and the fourth band (if it exists) 16.B ANALYSISThis appendix extends the analysis in Â§5.3 by showing a breakdown of the test loss when binning by the frequency of the previous word.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Respecting document boundaries may lead to better results and we leave this to future work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Tom Bosc for fruitful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Character-level language modeling with deeper self-attention. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<idno>abs/1808.04444</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Neural Network Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebru</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT Workshop on the Future of Language Modeling for HLT</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pragmatic neural language modelling in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RÃ©jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Neural Probabilistic Language Model. JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural lattice language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="529" to="541" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Google</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Strategies for training large vocabulary neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classes for Fast Maximum Entropy Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno>abs/1612.04426</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>CissÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">HervÃ©</forename><surname>JÃ©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno>abs/1611.01462</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>JÃ³zefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno>abs/1508.06615</idno>
		<title level="m">Character-aware neural language models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">SGDR: stochastic gradient descent with restarts. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An analysis of neural language modeling at multiple scales. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1803.08240</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Spell once, summon anywhere: A two-level open-vocabulary language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
		<idno>abs/1804.08205</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network based Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">TomÃ¡Å¡</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">KarafiÃ¡t</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LukÃ¡Å¡</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>CernockÃ½</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extensions of Recurrent Neural Network Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">TomÃ¡Å¡</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LukÃ¡Å¡</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>CernockÃ½</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical Probabilistic Neural Network Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing uncertainty in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fast parametric learning with activation memorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno>abs/1803.10049</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Attik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Large</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT Workshop on the Future of Language Modeling for HLT</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sparse non-negative matrix language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Pelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Decoding with Large-scale Neural Language Models improves Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
