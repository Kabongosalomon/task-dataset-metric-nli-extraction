<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Voxel and Coordinate Regression for Accurate 3D Facial Landmark Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
							<email>hongwen.zhang@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="department" key="dep3">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences (CASIA)</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences (UCAS)</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="department" key="dep3">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences (CASIA)</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences (UCAS)</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="department" key="dep3">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences (CASIA)</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences (UCAS)</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
							<email>znsun@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="department" key="dep3">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences (CASIA)</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences (UCAS)</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Voxel and Coordinate Regression for Accurate 3D Facial Landmark Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D face shape is more expressive and viewpointconsistent than its 2D counterpart. However, 3D facial landmark localization in a single image is challenging due to the ambiguous nature of landmarks under 3D perspective. Existing approaches typically adopt a suboptimal two-step strategy, performing 2D landmark localization followed by depth estimation. In this paper, we propose the Joint Voxel and Coordinate Regression (JVCR) method for 3D facial landmark localization, addressing it more effectively in an end-to-end fashion. First, a compact volumetric representation is proposed to encode the per-voxel likelihood of positions being the 3D landmarks. The dimensionality of such a representation is fixed regardless of the number of target landmarks, so that the curse of dimensionality could be avoided. Then, a stacked hourglass network is adopted to estimate the volumetric representation from coarse to fine, followed by a 3D convolution network that takes the estimated volume as input and regresses 3D coordinates of the face shape. In this way, the 3D structural constraints between landmarks could be learned by the neural network in a more efficient manner. Moreover, the proposed pipeline enables end-to-end training and improves the robustness and accuracy of 3D facial landmark localization. The effectiveness of our approach is validated on the 3DFAW and AFLW2000-3D datasets. Experimental results show that the proposed method achieves state-of-the-art performance in comparison with existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Facial landmark localization has been extensively studied in the last decades and significant progress has been made on solving this problem. Though impressive performance is achieved in 2D face alignment recently <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, 3D landmark localization in a single image remains challenging due to the ambiguous nature of landmarks under 3D perspective.</p><p>Currently, state-of-the-art approaches to facial landmark localization are dominated by regression based methods, demonstrating their effectiveness on addressing common issues such as occlusions, large variations of appearance on face images in the wild. Among them, cascaded regression methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> attempt to learn the mapping from shape-index features to the landmark coordinates. Though these methods could achieve highly accurate results for nearly frontal face images, their performances are barely satisfactory when it comes to novel view images. On the other hand, heatmap regression based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> estimate the heatmap for each individual landmark instead. Such heatmap representation encodes the likelihood of positions being a specific landmark. The heatmap regression strategy avoids the inefficient learning of the non-linear mapping from feature space to landmark positions, which has greatly facilitated landmark localization problems including face alignment <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and human pose estimation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Though these methods could work well when the facial parts are visible, they might produce blurred heatmaps when there are invisible landmarks due to occlusions, making it unstable and error-prone to estimate landmark positions from those multi-mode heatmaps.</p><p>For 3D landmark localization, popular approaches employ a two-step strategy lifting the 2D estimation to 3D shape. These methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> typically perform 2D landmark localization at first and then obtain the 3D face shape through depth estimation or 3D face model fitting. Though such strategy is effective, it is suboptimal and sensitive to the result of 2D landmark localization. In <ref type="bibr" target="#b10">[11]</ref>, Pavlakos et al. extend the 2D heatmap to 3D space and show that predicting the body joints in a discretized 3D space could be more effective for 3D human pose estimation. However, directly extending the 2D heatmap to its 3D version for each landmark is cumbersome and memory-demanding especially when the number of landmarks increases.</p><p>To cope with these limitations, we propose the Joint Voxel and Coordinate Regression (JVCR) method in this paper and make the following contributions towords accurate 3D facial landmark localization. Firstly, we propose the compact volumetric representation which encodes the voxel-wise likelihood of positions being the target landmarks in 3D space. The dimensionality of such a representation is fixed regardless of the number of landmarks, so that the required memory and computation could be reduced significantly. Secondly, we adopt a coarse-to-fine strategy to regress the volumetric representation so that the 3D structural constraints between landmarks could be learned by the neural network more efficiently. In addition, we employ 3D convolutions to regress the 3D coordinates of landmarks from the volumetric representation. In this way, the 3D convolution network takes as input the entire volumetric representation of all landmarks, so that the prediction for those invisible landmarks could be more robust. Finally, the proposed joint voxel and coor- dinate regression framework enables end-to-end training and shows promising results on 3D facial landmark localization.</p><p>Experimental results on 3DFAW <ref type="bibr" target="#b11">[12]</ref> and AFLW2000-3D <ref type="bibr" target="#b12">[13]</ref> datasets demonstrate the superiority of our approach. The remainder of this paper is organized as follows. Section II briefly reviews previous works related to ours. The technical details of the proposed method are presented in Section III. Experimental results are reported in Section IV. Finally, we conclude the paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>A significant amount of work has been introduced for landmark localization in the last decades. In this section, we briefly review previous works related to ours, including methods for 2D and 3D landmark localization.</p><p>2D landmark localization. Typical methods for 2D face alignment include Constrained Local Models (CLMs) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, Active Appearance Models (AAMs) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> and Cascaded Regression Methods (CRMs) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>. For CLMs and AAMs, they typically optimize the parametric representation of face shape iteratively according to the pretrained appearance and shape models. For CRMs, however, instead of representing the face shape parametricaly, they regress the facial landmark coordinates directly from shapeindex features. To avoid the inefficient learning of the pixelto-coordinate mapping in CRMs, a majority of recent approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> cast landmark localization as regressing the heatmaps of landmarks instead of the coordinate vector. Methods belonging to this type pursue regressing clear and accurate 2D heatmaps for the target landmarks. For example, Stacked Hourglass Networks <ref type="bibr" target="#b6">[7]</ref> uses the symmetric topology and intermediate supervision, which has been demonstrated to be effective in both applications of human pose estimation <ref type="bibr" target="#b6">[7]</ref> and face alignment <ref type="bibr" target="#b1">[2]</ref>. Several state-of-the-art works <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b19">[20]</ref> built upon this architecture achieve nearly saturate performances on 2D landmark localization. Despite their effectiveness, it is usually unstable to estimate the positions from multi-mode heatmaps for those invisible landmarks. Very recently, instead of adopting the maximum operation, Sun et al. <ref type="bibr" target="#b20">[21]</ref> propose to infer the landmark coordinate from its heatmap through the integral operation, which allows end-to-end training and shows its effectiveness on 2D human pose estimation.</p><p>3D landmark localization. One of the popular pipelines for 3D landmark localization adopts a two-stage strategy which performs the 2D landmark estimation at first and then predicts the depth information for these 2D landmarks. In <ref type="bibr" target="#b7">[8]</ref>, Zhao et al. propose a neural network to regress the 2D face shape firstly and then estimate the depth of the landmarks. In <ref type="bibr" target="#b8">[9]</ref>, Bulat et al. introduce a two-stage method which performs the 2D heatmap regression followed by depth prediction. Instead of estimating the depth information directly, Gou et al. <ref type="bibr" target="#b9">[10]</ref> propose to recover the 3D face shape by fitting the 3D morphable model to the 2D landmarks. Moreover, cascaded regression methods are also extended to 3D landmark localization. In <ref type="bibr" target="#b21">[22]</ref>, Tulyakov et al. propose the 3D shape invariant feature and estimate 3D face landmarks in a single step manner using the cascaded regressors. On the other hand, Pavlakos et al. <ref type="bibr" target="#b10">[11]</ref> introduce the volumetric representation for 3D body joints and show that predicting the joints in a discretized 3D space could be more effective for 3D pose estimation. The volumetric representation proposed in <ref type="bibr" target="#b10">[11]</ref> could be viewed as a natural extension of the 2D heatmap, which is highly demanding for memory and computation. Though regressing such a representation in a coarse-to-fine manner could alleviate this problem <ref type="bibr" target="#b10">[11]</ref>, it still cannot avoid the curse of dimensionality when the number of target landmarks increases. Hence it can not be easily generalized to other 3D object alignment problems. Alternatively, we propose to encode the positions of all landmarks in a single volume with the dimensionality fixed regardless of the number of landmarks, providing a much more efficient solution for general 3D landmark localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we present the Joint Voxel and Coordinate Regression (JVCR) method for 3D facial landmark localization in detail. The pipeline of the proposed method is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Our network architecture consists of two main blocks, a voxel regression subnetwork that estimates the compact volumetric representation from coarse to fine, followed by a 3D convolution network that takes the estimated volume as input and regresses the coordinate vector. In the following subsection, we begin with the description of the compact volumetric representation. Then we introduce the voxel regression subnetwork, the coordinate regression subnetwork as well as the training scheme for the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Compact Volumetric Representation for 3D Face Shape</head><p>Previous works <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b10">[11]</ref> have shown that encoding the landmark positions into the heatmap-like or volumetric representation could provide much more discriminative information than naively concatenating the coordinate vectors of 2D or 3D landmarks. Such forms of supervision make it easier for fully convolutional networks to learn the pixel to pixel mapping, and have been used in the context of both facial landmark localization <ref type="bibr" target="#b0">[1]</ref> and human pose estimation <ref type="bibr" target="#b22">[23]</ref>.</p><p>For 3D landmark localization, the volumetric representation proposed in <ref type="bibr" target="#b10">[11]</ref> encodes the position of a specific landmark in a volume with a 3D Gaussian centered around the groundtruth position. Though this idea extends the typically used 2D heatmap in a natural manner, it leads to a representation with large dimensionality. Instead of representing each landmark individually, we propose a volumetric representation encoding the positions of all target landmarks in a more compact manner. Specifically, coordinates of all the target landmarks are converted into a discretized 3D volume V with the size of w × h × d. Let v i,j,k denote the value of voxel (i, j, k). For the n-th landmark located at x n gt = (x, y, z), its contribution to v i,j,k can be written as:</p><formula xml:id="formula_0">v n i,j,k = 1 2πσ 2 e − (x−i) 2 +(y−j) 2 +(z−k) 2 2σ 2 (1)</formula><p>where the kernel size σ could be set empirically. For 3D face shape with N target landmarks, the overall contribution to v i,j,k takes as the maximum value in</p><formula xml:id="formula_1">{v n i,j,k } N n=1 : v i,j,k = max n v n i,j,k<label>(2)</label></formula><p>In this way, the dimensionality of the representation is fixed regardless of the number of target landmarks. <ref type="figure" target="#fig_1">Fig. 2</ref> visualizes the 3D face shape and the corresponding compact volumetric representation. It is worth noting that, in this paper, we also refer to the compact volumetric representation as volumetric representation or volume for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Joint Voxel and Coordinate Regression</head><p>Cascaded regression in a coarse-to-fine manner is wildly employed in 2D landmark localization. Such a strategy could make full use of the regressors and progressively refine the output of the networks. Our method follows this technique and decouples the 3D facial landmark localization problem into the following two sub-tasks. The first one aims to regress the ideal volumetric representation of 3D landmarks in a coarse-to-fine manner. The second one aims to regress the coordinates of landmarks from the volumetric representation.</p><p>1) Coarse-to-fine Voxel Regression: The voxel regression subnetwork G learns the mapping from pixels of the face image I to the volumetric representation V: G(I) → V. Inspired by previous works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref> on 2D and 3D human pose estimation, we also adopt the stacked hourglass networks <ref type="bibr" target="#b6">[7]</ref> with intermediate supervision and skip connection. Specifically, the voxel regression subnetwork consists of M stacked Hourglass modules <ref type="bibr" target="#b6">[7]</ref> of which supervisions are volumes denoted as {V m } M m=1 . Then, the voxel regression subnetwork is trained using the voxel-wise mean squared error loss:</p><formula xml:id="formula_2">L vox = M m i,j,k G m (I) i,j,k − V m i,j,k 2<label>(3)</label></formula><p>where G m (·) denotes the volume outputted by the m-th Hourglass module. Noted that G M (·) = G(·) is equivalent to the final output of the voxel regression subnetwork.</p><p>As pointed out in <ref type="bibr" target="#b10">[11]</ref>, the prediction along the z dimension is much more challenging than another two dimensions. Hence, coarse-to-fine regressing the volumes with the increasing resolution along z dimension could be more effective and robust. In practice, the resolution d of V m takes number from preset values and progressively increases along with m.</p><p>2) Coordinate Regression: Typical heatmap regression based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b6">[7]</ref> retrieve the coordinates of landmarks directly from the peak points of the corresponding heatmaps. Considering that the positions of all landmarks are encoded into a single volume, the conventional "tackingmaximum" operation is no longer applicable in our case since the order of landmarks is not preserved in our compact volumetric representation. Hence, it needs to infer the coordinates of landmarks from the corresponding volumetric representation.</p><p>To this end, we propose a coordinate regression subnetwork P to learn the mapping from the compact volumetric representation V to the corresponding coordinate vector x: P (V) → x. Inspired by the work on 3D object recognition <ref type="bibr" target="#b24">[25]</ref> and hand pose estimation <ref type="bibr" target="#b25">[26]</ref>, we adopt the 3D convolution kernel instead of 2D convolution in our coordinate regression subnetwork. The 3D convolution is typically used to extract features from both spatial and temporal dimensions for video analysis problems <ref type="bibr" target="#b26">[27]</ref>. Hence, the 3D convolution could be more naturally adopted to extract the 3D information from the volumetric representation. The proposed coordinate regression subnetwork consists of five 3D convolution layers, with batch normalization and Leaky ReLU activation added in between and a fully connected layer at the end. For training, we employ the L 2 regression loss on the predicted coordinate vector:</p><formula xml:id="formula_3">L coord = x gt − P (V) 2 2<label>(4)</label></formula><p>where x gt denotes the concatenated vector of the ground-truth 3D landmark coordinates.</p><p>3) Training: Instead of training the whole network from scratch, we adopt a two-stage training scheme which is more stable and effective in our experiments. The two subnetworks mentioned above are pre-trained separately for each subtask beforehand and fine-tuned as an integrated one finally. Specifically, at the pre-training stage, the voxel regression subnetwork is trained with the face images and the ground-truth volumes. Meanwhile, the coordinate regression subnetwork is trained with the ground-truth volumes and the corresponding coordinate vectors. At the fine-tuning stage, the coordinate regression subnetwork is attached to the voxel regression subnetwork, and the whole network is fine-tuned with the joint supervision of both the ground-truth volumes and coordinate vectors. Formally, the whole network is trained in an endto-end manner using the following loss function at the final stage:</p><formula xml:id="formula_4">L = L vox + λL coord = M m G m (I) − V m 2 2 + λ x gt − P (G M (I)) 2 2 (5)</formula><p>where λ is used to balance the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, the implementation detail of the proposed method are described firstly. Then, the datasets as well as evaluation metrics used in our experiments are introduced. Finally, we present the experimental results of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Detail</head><p>The proposed network takes as input a 256 × 256 face image and outputs predictions of the volumetric representation and the coordinate vector. Inspired by the setting of <ref type="bibr" target="#b6">[7]</ref>, four Hourglass modules are stacked together as the voxel regression subnetwork (i.e. M = 4) and output volumes with the size of 64 × 64 × d, where the resolution d of z-dimension is chosen from the set {1, 2, 4, 64} successively. The Gaussian kernel size in Eq. 1 is set to σ = 1 in our experiments. During training, data augmentation, such as rotation, scaling and fliping, was applied randomly to input images. The network was traind for 25 epochs in total, including 15 epochs for the pre-training stage and 10 epochs for the fine-tuning stage, respectively. We adopted the RMSprop <ref type="bibr" target="#b27">[28]</ref> optimization algorithm with an initial learning rate of 2.5 × 10 −4 , which was reduced by a factor of 10 every 10 epochs. Our approach was implemented using PyTorch. During testing, it takes about 50ms for our model to process an image on a TITAN Xp GPU. Code is made publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>3DFAW <ref type="bibr" target="#b11">[12]</ref>. The 3DFAW dataset is provided by the 3D Face Alignment in the Wild (3DFAW) Challenge <ref type="bibr" target="#b11">[12]</ref> organizers, containing more than 23000 face images from BU-4DFE <ref type="bibr" target="#b28">[29]</ref>, BP4D-Spontaneous <ref type="bibr" target="#b29">[30]</ref> and MultiPIE <ref type="bibr" target="#b30">[31]</ref>. 66 3D facial landmarks as well as the face bounding boxes are annotated for each face image. The 3D points are annotated consistently using a model-based structure-from-motion technique <ref type="bibr" target="#b31">[32]</ref>. The 3DFAW dataset is divided into three subsets: the training set, the validation set and the test set, containing 13969, 4725 and 4912 face images, respectively. Our method is trained on the training set and tested on both the validation and test set. It should be noted that the ground-truth 3D landmarks of the test set are not publicly available. Hence the evaluation results on the test set are provided by 3DFAW Challenge organizers via the CodaLab platform 2 .</p><p>300W-LP <ref type="bibr" target="#b12">[13]</ref>. The 300W-LP dataset contains 61225 synthesized face images across large poses ranging from −90 • to 90 • . Those images are synthesized from 300W <ref type="bibr" target="#b32">[33]</ref> using the 3D morphable model based profiling algorithm proposed in <ref type="bibr" target="#b12">[13]</ref>. For each face, 68 3D landmarks are retrieved from the parameters of the 3D morphable model, using the released code of <ref type="bibr" target="#b12">[13]</ref>. In our experiments, the depth values are normalized to have zero mean. We only use this dataset for training and test our method on the AFLW2000-3D dataset mentioned bellow.</p><p>AFLW2000-3D <ref type="bibr" target="#b12">[13]</ref>. The AFLW2000-3D dataset contains 2000 face samples selected from the AFLW <ref type="bibr" target="#b33">[34]</ref> dataset, introduced by Zhu et al. <ref type="bibr" target="#b12">[13]</ref> along with the 300W-LP dataset. The 68 3D landmarks annotated in AFLW2000-3D are consistent with those of 300W-LP. We use the AFLW2000-3D dataset only for testing in our experiments, following the common protocol in the literature <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>For fair comparison, the evaluation metrics are adopted in consistency with previous works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. For 3D landmark localization, the Ground Truth Error (GTE) and Cross View Ground Truth Consistency Error (CVGTCE) are used to measure the performance as recommenced in the 3DFAW Challenge <ref type="bibr" target="#b11">[12]</ref>. The GTE is defined as the average pointto-point Euclidean error normalized by the distance between the outer corners of the eyes. The CVGTCE is proposed in the 3DFAW Challenge and aims at evaluating the cross-view consistency of the predicted landmarks. For evaluating the 2D projection of the 3D landmarks, the metric is the Normalized Mean Error (NME), which is defined as the average 2D pointto-point Euclidean error normalized by the square root of the bounding box size.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results</head><p>In this subsection, we compare our approach with existing methods including top ranked methods on the 3DFAW Challenge, state-of-the-art 2D face alignment method FAN <ref type="bibr" target="#b1">[2]</ref> and 3D face model based methods such as 3DDFA <ref type="bibr" target="#b12">[13]</ref> and 3DSTN <ref type="bibr" target="#b34">[35]</ref>.</p><p>1) Evaluation on 3DFAW: The evaluation on 3DFAW consists of two parts. The first part is evaluated on the validation set. The second part is evaluated on the test set, of which performance is provided by the challenge organizers. <ref type="table" target="#tab_0">Table I</ref> shows the comparison results of GTE on the validation set. It can be observed that the proposed method outperforms others, all of which are based on the two-step strategy. For comparison with top ranked methods on the 3DFAW Challenge, we further evaluate our method on the test set. Comparisons of both the CVGTCE and GTE on the 3DFAW test set are reported in <ref type="table" target="#tab_0">Table II</ref>. Note that the ground truth 3D landmarks of the test set are not available to the participants, and the numbers for all methods are taken from the CodaLab leaderboard and the literature <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref>. As shown in <ref type="table" target="#tab_0">Table II</ref>, the proposed method achieves the best result in comparison with other methods, including the previously top-ranked method <ref type="bibr" target="#b8">[9]</ref> and Tulyakov et al. <ref type="bibr" target="#b21">[22]</ref> which is built upon a 3D variant of cascaded regression method. It demonstrates the effectiveness of the proposed JVCR framework for accurate 3D facial landmark localization. <ref type="figure" target="#fig_3">Fig. 4</ref> shows example results of the proposed method on the 3DFAW test set.</p><p>2) Evaluation on AFLW2000-3D: We further evaluate our method on AFLW2000-3D to demonstrate the effectiveness   of our method on face images with large pose and appearance variations. Our method is trained on 300W-LP and tested on AFLW2000-3D. Performance of both the 3D and 2D landmark localization is evaluated for thorough comparison.</p><p>Since there are a limited number of previous works on 3D facial landmark localization in the wild and the training code for most of them are not available, we consider the most recent state-of-the-art method FAN <ref type="bibr" target="#b1">[2]</ref>, with the code released by the authors, as the baseline for comparison. Specifically, for 3D facial landmark localization, the baseline method FAN+Depth performs 2D landmark localization using FAN <ref type="bibr" target="#b1">[2]</ref> at first and then estimates the depth using ResNet <ref type="bibr" target="#b37">[38]</ref>. Comparisons of GTE and CED curves with the baseline method are shown in <ref type="table" target="#tab_0">Table III</ref> and <ref type="figure" target="#fig_2">Fig. 3</ref> respectively. Benefiting from the endto-end pipeline, the proposed method outperforms the strong baseline method considerably.</p><p>We also compare our method with other methods on 2D facial landmark localization. In this case, only 2D coordinates are involved in the evaluation and the metric is the Normalized Mean Error (NME), where the normalized distance is square-root of the size of the bounding box enclosing all 2D landmarks. Comparisons of NME across poses are reported in <ref type="table" target="#tab_0">Table IV</ref>. Note that the results of RCPR <ref type="bibr" target="#b36">[37]</ref>, ESR <ref type="bibr" target="#b3">[4]</ref>, and SDM <ref type="bibr" target="#b2">[3]</ref> are obtained from <ref type="bibr" target="#b12">[13]</ref> and these methods have been retrained on 300W-LP for adaptation to large poses. As shown in <ref type="table" target="#tab_0">Table IV</ref>, the proposed method achieve a superior  performance especially for large poses. Example results of our method on AFLW2000-3D are depicted in <ref type="figure" target="#fig_4">Fig. 5</ref>. It can be seen that our method is robust to occlusions and large appearance variations occurred in face images in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we present the Joint Voxel and Coordinate Regression (JVCR) method for 3D facial landmark localization. First, we introduce the compact volumetric representation which encodes positions of all landmarks in a single volume. In this way, the dimensionality of the representation could be reduced greatly compared with the conventional volumetric representation. For robust and accurate 3D facial landmark localization, we perform the coarse-to-fine voxel regression and coordinate regression via the stacked hourglass network and 3D convolution network, respectively. Hence, the joint voxel and coordinate regression could combine the merits of both heatmap regression based methods and coordinate regression based methods. Moreover, the proposed method is able to produce satisfying results on face images with occlusions and large appearance variations. Experimental results on 3DFAW and AFLW2000-3D datasets demonstrate the superiority of our approaches in comparison with other state-of-the-art solutions. In future work, we will investigate the proposed method further in the context of general 3D object landmark localization such as 3D human pose estimation in the wild.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Pipeline of the Joint Voxel and Coordinate Regression (JVCR) method. The voxel regression subnetwork G consists of M Hourglass modules, which estimate the the compact volumetric representation from coarse to fine. The coordinate regression subnetwork P takes as input the estimated volume and regresses 3D coordinates of the face shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of the 3D face shape (a) and the compact volumetric representation (b). The voxel values are indicated by the density of the point cloud. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of cumulative errors distribution (CED) curves on AFLW2000-3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Example results of the proposed method on the 3DFAW dataset. The top row shows the face images as well as the 2D facial landmark localization results. The middle row shows the estimated volumetric representations. The last row shows the predicted 3D facial landmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Example results of the proposed method on the AFLW2000-3D dataset. Images are arranged as the same asFig. 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Comparison of GTE on the 3DFAW validation set.</figDesc><table><row><cell>Method</cell><cell>GTE (%)</cell></row><row><cell>SDM+3DMM [10]</cell><cell>6.34</cell></row><row><cell>Gou et al. [10]</cell><cell>5.90</cell></row><row><cell>Bulat et al. [9]</cell><cell>4.94</cell></row><row><cell>JVCR</cell><cell>4.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II :</head><label>II</label><figDesc>Comparisons of CVGTCE and GTE on the 3DFAW test set.</figDesc><table><row><cell>Method</cell><cell cols="2">CVGTCE (%) GTE (%)</cell></row><row><cell>Zavan et al. [36]</cell><cell>5.90</cell><cell>10.80</cell></row><row><cell>Gou et al. [10]</cell><cell>4.94</cell><cell>6.20</cell></row><row><cell>Zhao et al. [8]</cell><cell>3.97</cell><cell>5.88</cell></row><row><cell>Bulat et al. [9]</cell><cell>3.47</cell><cell>4.56</cell></row><row><cell>Tulyakov et al. [22]</cell><cell>3.80</cell><cell>5.10</cell></row><row><cell>JVCR</cell><cell>3.46</cell><cell>4.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III :</head><label>III</label><figDesc>Comparison of GTE on the AFLW2000-3D dataset.</figDesc><table><row><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GTE (%)</cell><cell></cell></row><row><cell>Method</cell><cell>GTE (%)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FAN+Depth [2]</cell><cell>7.45</cell><cell></cell><cell></cell><cell></cell></row><row><cell>JVCR</cell><cell>7.28</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table IV :</head><label>IV</label><figDesc>Comparison of NME (%) on the AFLW2000-3D dataset. Only 2D coordinates are involved in the evaluation.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">AFLW2000-3D (68 pts)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">[0 • ,30 • ] [30 • ,60 • ] [60 • ,90 • ] mean</cell><cell>std</cell></row><row><cell>RCPR [37]</cell><cell>4.26</cell><cell>5.96</cell><cell>13.18</cell><cell>7.80</cell><cell>4.74</cell></row><row><cell>ESR [4]</cell><cell>4.60</cell><cell>6.70</cell><cell>12.67</cell><cell>7.99</cell><cell>4.19</cell></row><row><cell>SDM [3]</cell><cell>3.67</cell><cell>4.94</cell><cell>9.76</cell><cell>6.12</cell><cell>3.21</cell></row><row><cell>3DDFA [13]</cell><cell>3.78</cell><cell>4.54</cell><cell>7.93</cell><cell>5.42</cell><cell>2.21</cell></row><row><cell>3DDFA+SDM [13]</cell><cell>3.43</cell><cell>4.24</cell><cell>7.17</cell><cell>4.94</cell><cell>1.97</cell></row><row><cell>3DSTN [35]</cell><cell>3.15</cell><cell>4.33</cell><cell>5.98</cell><cell>4.49</cell><cell>1.42</cell></row><row><cell>FAN [2]</cell><cell>2.77</cell><cell>3.48</cell><cell>4.60</cell><cell>3.62</cell><cell>0.92</cell></row><row><cell>JVCR</cell><cell>2.94</cell><cell>3.46</cell><cell>4.53</cell><cell>3.64</cell><cell>0.81</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/HongwenZhang/JVCR-3Dlandmark 2 https://competitions.codalab.org/competitions/10261</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional aggregation of local evidence for large pose face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="86" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and precise face alignment and 3d shape reconstruction from a single 2d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="590" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two-stage convolutional part heatmap regression for the 1st 3d face alignment in the wild (3dfaw) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shape augmented regression for 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y.</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="604" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The first 3d face alignment in the wild (3dfaw) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deformable model fitting by regularized landmark mean-shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="215" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pose-free facial landmark fitting via optimized part mixtures and cascaded deformable shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1944" to="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2025" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5669" to="5678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08229</idno>
		<title level="m">Integral human pose regression</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Viewpoint-consistent 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A high-resolution 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Worm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition, 2008. FG&apos;08. 8th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dense 3d face alignment from 2d video for real-time use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster than real-time facial alignment: A 3d spatial transformer network approach in unconstrained poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="4000" to="4009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d face alignment in the wild: A landmarkfree, nose-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>De Bittencourt Zavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">R</forename><surname>Bellon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="581" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
