<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
							<email>tao@asapp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models have become increasingly difficult to train because of the required computation time and cost. In this work, we present SRU++, a recurrent unit with optional built-in attention that exhibits state-of-the-art modeling capacity and training efficiency. On standard language modeling benchmarks such as ENWIK8, WIKI-103 and BILLION WORD datasets, our model obtains better perplexity and bits-per-character (bpc) while using 3x-10x less training time and cost compared to top-performing Transformer models. Our results reaffirm that attention is not all we need and can be complementary to other sequential modeling modules <ref type="bibr" target="#b20">(Merity, 2019;</ref> Gulati et al.,  2020). Moreover, fast recurrence with little attention can be a leading model architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many recent advances in language modeling have come from leveraging massive data and training models with significantly increased model size. Not surprisingly, the associated computation for training has grown enormously, requiring hundreds of GPU hours or days for an experiment. As a consequence, it has become imperative to build computationally efficient models that retain top modeling power with reduced or accelerated computation.</p><p>The Transformer architecture <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> was proposed to accelerate model training and has become the predominant architecture in NLP. Specifically, it is built entirely upon selfattention and avoids the use of recurrence to enable strong parallelization. While this change has led to many empirical success and improved computational efficiency, we are interested in revisiting the architectural question: Is attention all we need for modeling? The attention mechanism permits learning dependencies between any parts of the input, making it an extremely powerful neural component in many machine learning applications. We hypothesize that this advantage can still be complemented with other computation that is directly designed for sequential modeling. Indeed, several recent works have studied and confirmed the same hypothesis by leveraging recurrence in conjunction with attention. For example, Merity (2019) demonstrates that single-headed attention LSTMs can produce competitive results compared to Transformer models in language modeling. Other work have incorporated RNNs into Transformer, and obtain better results in machine translation <ref type="bibr" target="#b16">(Lei et al., 2018)</ref> and language understanding benchmarks <ref type="bibr">(Huang et al., 2020)</ref>. These results highlight one possibility -we could build more efficient models by combining attention and fast recurrent networks <ref type="bibr" target="#b3">(Bradbury et al., 2017;</ref><ref type="bibr" target="#b5">Campos et al., 2018;</ref><ref type="bibr" target="#b34">Zhang and Sennrich, 2019)</ref>.</p><p>In this work, we validate this idea and present a recurrent unit with built-in self-attention that achieves strong computation efficiency. Our work builds upon the SRU <ref type="bibr" target="#b16">(Lei et al., 2018)</ref>, a highly parallelizable RNN implementation that has been shown effective in NLP and speech applications <ref type="bibr" target="#b22">(Park et al., 2018;</ref><ref type="bibr" target="#b26">Shangguan et al., 2020;</ref><ref type="bibr" target="#b17">Lin et al., 2020)</ref>. We incorporate attention into the SRU by simply replacing the linear projection of input with a self-attention component. The proposed architecture, called SRU++, enjoys high parallelization and enhanced context modeling capacity. <ref type="figure">Figure 1</ref> compares its performance with the Transformer-XL model <ref type="bibr" target="#b7">(Dai et al., 2019)</ref> on the ENWIK8 dataset. SRU++ achieves better dev results while using a fraction of the training resources needed by the baseline. We evaluate SRU++ on several language modeling benchmarks, including the ENWIK8, WIKI-103 and BILLION WORD datasets. We use SRU++ as a replacement of the Transformer layer and stack multiple layers to construct our models. Our results demonstrate that SRU++ consistently outperforms various Transformer models, delivering on par or better results while using 2.5x-10x less computation. We open source our implementation to facilitate future research at https:// github.com/asappresearch/sru.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: SRU</head><p>We first describe the Simple Recurrent Unit (SRU) in this section. A single layer of SRU involves the following computation:</p><formula xml:id="formula_0">f [t] = σ (Wx[t] + v c[t − 1] + b) r[t] = σ W x[t] + v c[t − 1] + b c[t] = f [t] c[t − 1] + (1 − f [t]) (W x[t]) h[t] = r[t] c[t] + (1 − r[t]) x[t]</formula><p>where is the element-wise multiplication, Two code-level optimizations are performed to enhance the parallelism and therefore the speed of SRU. First, given the input sequence</p><formula xml:id="formula_1">X = {x[1], · · · , x[L]} where each x[t] ∈ R d is a d-dimensional</formula><p>vector, we group the three matrix multiplications across all time steps as a single multiplication. This significantly improves the computation intensity (e.g. GPU utilization). Specifically, the batched multiplication is a linear projection of the input tensor X ∈ R L×d :</p><formula xml:id="formula_2">U =   W W W   X ,<label>(1)</label></formula><p>where L is the sequence length, U ∈ R L×3×d is the output tensor and d is the hidden state size.</p><p>The second optimization performs all elementwise operations in an efficient way. This involves</p><formula xml:id="formula_3">f [t] = σ(U[t, 0] + v c[t − 1] + b) (2) r[t] = σ(U[t, 1] + v c[t − 1] + b ) (3) c[t] = f [t] c[t − 1] + (1 − f [t]) U[t, 2] (4) h[t] = r[t] c[t] + (1 − r[t]) x[t].<label>(5)</label></formula><p>Note that each dimension of the hidden vectors are independent once U is computed. As a result, these operations can be done in parallel across hidden dimension d (and batch size B in a mini-batch scenario). Similar to other operations such as attention and LSTM, this step is implemented as a CUDA kernel to accelerate computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SRU++</head><p>The key modification of SRU++ is to incorporate more expressive non-linear operations into the recurrent network. Note that the computation of U (Equation 1) is a linear transformation of the input sequence X. We can use other parameterized neural operators to replace this transformation. Specifically, SRU++ builds in self-attention to enhance its modeling capacity. Given the input sequence being represented as a tensor or matrix X ∈ R L×d , the attention component computes the query, key and value representations using the fol- lowing multiplications,</p><formula xml:id="formula_4">Q = W q X K = W k Q V = W v Q where W q ∈ R d ×d , W k ∈ R d ×d , W v ∈ R d</formula><p>×d are model parameters, and d is the attention dimension (typically smaller than d). Next, a weighted average output A ∈ R d ×L is computed using the scaled dot-product attention introduced in Vaswani et al. <ref type="formula" target="#formula_2">(2017)</ref>,</p><formula xml:id="formula_5">A = softmax Q K √ d V .</formula><p>Finally, the output U required by the element-wise kernel is obtained by another linear projection,</p><formula xml:id="formula_6">U = W o (Q + α · A) .</formula><p>where α ∈ R is a learned scalar and W o ∈ R 3d×d is the projection matrix applied to a residual connection (Q+α·A) which improves gradient propagation and stablizes training. α is initialized to zero and as a result</p><formula xml:id="formula_7">U = W o Q = (W o W q ) X</formula><p>initially falls back to a linear mapping of the input X skipping the attention transformation. Compared to Equation <ref type="formula" target="#formula_2">(1)</ref> Layer normalization We also experiment with adding layer normalization <ref type="bibr">(Ba et al., 2016)</ref> to each SRU++ layer. In our implementation, we apply normalization after the attention operation and before the matrix multiplication with W o ,</p><formula xml:id="formula_8">U = W o layernorm(Q + α · A).</formula><p>Another option is to apply normalization over the hidden states h[t] once they are produced. Applying either one or both normalizations all work well. Note that these variants are post-layer normalizations in which the normalization happens after the skip connection is added. In contrast, prelayer normalization <ref type="bibr" target="#b32">(Xiong et al., 2020)</ref> is applied within each non-linear layer. We use post normalization for better results following the empirical observations in <ref type="bibr" target="#b19">Liu et al. (2020b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>Datasets We evaluate our model on three language model benchmarks.</p><p>• ENWIK8 <ref type="bibr" target="#b13">(Hutter, 2012)</ref> is a character-level language modeling dataset consisting of 100 million tokens taken from Wikipedia. The vocabulary size of this dataset is slightly more than 200. We use the standard 90M/5M/5M splits as the training, dev and test sets, and report bits-per-character (bpc) as the evaluation metric.</p><p>• WIKI-103 <ref type="bibr" target="#b21">(Merity et al., 2016)</ref> is a wordlevel language modeling dataset. The training data contains 100 million tokens extracted from Wikipedia articles. Following prior work, we use a vocabulary that  Comparison between SRU++ and Transformer-XL on ENWIK8 dataset. We train SRU++ using the same setting as Transformer-XL base model. The metrics are smaller the better. † indicates training using automatic mixed precision and distributed data parallel in PyTorch.</p><p>has about 260K tokens, and adaptive embedding and softmax layers <ref type="bibr" target="#b9">(Grave et al., 2017;</ref><ref type="bibr" target="#b1">Baevski and Auli, 2019)</ref>.</p><p>• BILLION WORD <ref type="bibr" target="#b6">(Chelba et al., 2013</ref>) is a much larger dataset containing 768 million word tokens for training. Unlike WIKI-103 in which sentences in the same article are treated as consecutive inputs to model long context, the sentences in BILLION WORD are randomly shuffled. We follow <ref type="bibr" target="#b1">(Baevski and Auli, 2019)</ref> and use adaptive embedding and softmax layers for this dataset as well. The vocabulary size is about 800K, being the same as prior work.</p><p>Model All our language models are constructed with a word embedding layer, multiple layers of SRU++ and an output linear layer followed by softmax operation. We set the hidden dimensions d = 4 × d , the number of SRU++ layers to 10 and use single-head attention in each layer. We use the same dropout probability for all layers and tune this value according to the model size and the results on the dev set. For simplicity, we do not use additional techniques that are shown useful such as compressed memory <ref type="bibr" target="#b24">(Rae et al., 2020)</ref>, nearest-neighbor interpolation <ref type="bibr" target="#b14">(Khandelwal et al., 2020)</ref>, relative position <ref type="bibr" target="#b27">(Shaw et al., 2018;</ref><ref type="bibr" target="#b23">Press et al., 2020)</ref> and attention variants to handle very long context <ref type="bibr" target="#b29">(Sukhbaatar et al., 2019a;</ref><ref type="bibr" target="#b25">Roy et al., 2020)</ref>.  values and a fixed weight decay of 0.1 for all experiments. We use a cosine learning rate schedule following <ref type="bibr" target="#b7">(Dai et al., 2019)</ref> and an initial learning rate of 0.0003. We do not change the learning rate unless otherwise specified. See Appendix B for the detailed training configuration of each model. For ENWIK8 and WIKI-103 datasets, the training data is partitioned into B chunks by concatenating articles and ignoring the boundaries between articles. Each training batch contains M consecutive tokens from each chunk (i.e. the unroll size), which gives an effective size of B × M tokens per batch. Following previous work, the previous training batch is provided as additional context for attention, which results in a maximum attention length of 2M . For BILLION WORD dataset, we follow <ref type="bibr" target="#b7">(Dai et al., 2019</ref>) that concatenates sentences to create the training batches (instead of adding pad tokens). Sentences are randomly shuffled and separated by a special token &lt;s&gt; indicating sentence boundaries.  How much attention is needed? <ref type="bibr" target="#b20">Merity (2019)</ref> demonstrated that using a single attention layer with LSTM retains most of the modeling capacity compared to using multiple attention layers. We conduct a similar analysis to understand how much attention is needed in conjunction with the simple fast recurrence. To do so, we only enable the attention operation every k layers. The other layers without attention become the SRU variant with dimension projection illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> (b). Note that k = 1 means the default SRU++ model with attention in each layer, and k = 10 means only the last layer has attention in a 10layer SRU++ model. <ref type="table" target="#tab_6">Table 2</ref> presents the results by varying k. Our baseline model is a 10-layer model with 42M parameters. We see that using 50% less attention (k = 2) achieves almost no increase in test BPC. Using only one attention module (k = 10) leads to a marginal loss of 0.01 BPC, but reduces the training time by 40%. Our results still outperform Transformer-XL model and single-headed attention LSTM <ref type="bibr" target="#b20">(Merity, 2019)</ref> by 0.03 BPC. <ref type="figure">Figure 3</ref> showcases the training efficiency of our model. In comparison with Transformer-XL, SRU++ is 5x more efficient to reach a dev BPC of 1.09. Furthermore, using automatic mixed precision training and only one attention sub-layer (k = 10) achieves 16x reduction on training time and cost.</p><p>ENWIK8 <ref type="table" target="#tab_8">Table 3</ref> compares our model with other top-performing models on the ENWIK8 dataset. We train a base model with d = 3072 and a large model with d = 4096 using 400K training steps. The unroll size and attention context length are set to 1024 during training and 3072 during evaluation. To compare the computation efficiency we report the effective GPU days -the number of GPUs multiplied by the number of days needed to finish training. Our base model achieves an order of magnitude reduction on training time compared to previous reported numbers. Furthermore, our big model achieves a test BPC of 0.96, being on par with the state-of-the-art results. Interestingly, we found that turning off layer normaliza-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Parameters ↓ Test PPL ↓ GPU days ↓ All-attention network 36L <ref type="bibr" target="#b30">(Sukhbaatar et al., 2019b)</ref> 133M 20.6 -Feedback Transformer <ref type="bibr" target="#b8">(Fan et al., 2020</ref>   tion achieves better generalization on the evaluation sets. We present an additional analysis for layer normalization later in this section.   <ref type="bibr" target="#b23">Press et al. (2020)</ref>. We use a single V100 GPU, a batch size of 1 and maximum attention length 2560 for consistency. size d to 7616 and the batch size to 98K. In addition, we only use 2 attention layers (k = 5) for the large model. Both models use 10 layers and adopt the adaptive word embedding and softmax output layers as described in <ref type="bibr" target="#b1">Baevski and Auli (2019)</ref>. <ref type="table" target="#tab_11">Table 5</ref> presents the test perplexity and associated training cost (measured by GPU days needed). Our base and large model obtain a test perplexity of 25.1 and 23.7 respectively, outperforming the Transformer model of <ref type="bibr" target="#b1">Baevski and Auli (2019)</ref> given similar model size. Moreover, SRU++ achieves 3-4x training cost reduction and is trained using 8 GPUs. In comparison, the Transformer model uses 32 or 64 V100 GPUs for the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WIKI-103</head><p>Inference speed Recurrent networks enjoy a computational benefit because the associated computation is linear with respect to the input length, while a standard attention implementation scales quadratically.  WIKI-103 test set. We use a single V100 GPU for inference. Our large model runs at least 4.5x faster than all baseline models except Shortformer <ref type="bibr" target="#b23">(Press et al., 2020)</ref>. In addition, our model achieves 0.6-0.8 perplexity lower than Shortformer and runs 50% faster by using attention every 5 layers.</p><p>The effectiveness of layer normalization In our experiments, we have always observed that layer normalization stabilizes training. For large models however, layer normalization can lead to worse generalization on the evaluation sets, even if we tune dropout carefully. On the other hand, turning off layer normalization can achieve better dev and test results but makes training sensitive to learning rate and initialization. For example, we have to use a smaller learning rate of 0.00025 or lower to avoid sudden gradient explosion. <ref type="figure" target="#fig_1">Figure 4</ref> showcases our empirical observation on the ENWIK8 dataset. Similar results are observed on Wiki-103 dataset, and also in previous work <ref type="bibr" target="#b33">(Xu et al., 2019)</ref>. These results suggest possible future work by improving the normalization method <ref type="bibr" target="#b28">(Shen et al., 2020;</ref><ref type="bibr" target="#b4">Brock et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present a recurrent architecture with optional built-in attention and evaluate its effectiveness on various language modeling datasets. We demonstrate that highly expressive and efficient neural models can be designed using not just attention. In fact, by incorporating fast recurrent networks, very little attention computation is needed to achieve both top-performing modeling results as well as training speed. As future work, we believe the self-attentive recurrent models can be further improved using improved attention implementations, normalization and optimization techniques.</p><p>A Changes after the first version of this paper</p><p>The first version of this paper is submitted to arXiv in February 2021. The current version reports slightly improved perplexity results and reduced training time in the result section due to a few changes to our SRU++ implementation and the experiment code. The changes include:</p><p>• Using half precision for the elementwise kernel during mixed precision training. This improves the training speed by about 10% without hurting the final results. 4</p><p>• Fixing a precision issue that the loss tensor is half precision when using apdative softmax and mixed precision training. 5</p><p>• Fixing an implementation issue of data loading and batching in BILLION WORD experiments, which makes GPUs highly underutilized. This speeds up <ref type="bibr">BILLION WORD training significantly. 6</ref> In addition, we include more results on the BIL-LION WORD dataset by training a large SRU++ model with 467M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training details</head><p>We use the RAdam optimizer with the default hyperparameters β 1 = 0.9 and β 2 = 0.999 for all our experiments. We use a cosine learning rate schedule with only 1 cycle for simplicity. For faster training, we also leverage the native automatic mixed precision (AMP) training and distributed data parallel (DDP) of Pytorch in all experiments, except those in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure">Figure 1</ref> for a fair comparison with the Transformer-XL implementation. <ref type="table">Table 7</ref> shows the detailed training configuration of SRU++ models on ENWIK8 dataset. Most training options are kept the same for all models. We tune the dropout probability more carefully as we found training is more prone to over-fitting and under-fitting for this dataset. The large model is trained with 2x batch size. We increase the learning rate by a factor of √ 2 following the suggestion 4 https://github.com/asappresearch/ sru/pull/166 5 https://github.com/asappresearch/ sru/pull/168 6 https://github.com/asappresearch/ sru/pull/169 in <ref type="bibr" target="#b11">Hoffer et al. (2017)</ref>, which results in a rounded learning rate of 0.0004.</p><p>We additionally train a model without layer normalization to showcase that it can achieve stateof-the-art results, following our analysis in the result section. The model without layer normalization uses the smaller learning rate 0.0003, as large learning rates can lead to gradient explosion. We use an increased number of training steps of 600K for this experiment. <ref type="table">Table 8</ref> presents the detailed training configuration on WIKI-103 dataset. Similarly we use d = 3072 and d = 4096 for the base and large model respectively and fix d : d = 4 : 1. Following <ref type="bibr" target="#b1">(Baevski and Auli, 2019)</ref>, we use an adaptive word embedding layer and an adaptive softmax layer for our models, and we tie the weight matrices of the two layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of SRU and SRU++ networks: (a) the original SRU network, (b) the SRU variant using a projection trick to reduce the number of parameters, experimented in Lei et al. (2018) and (c) SRU++ proposed in this work. Numbers indicate the hidden size of intermediate inputs/outputs for d = 2048 and d = 512.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Understanding the empirical effect of layer normalization. We show the training and dev loss of the small and big SRU++ models on EN-WIK8 dataset. Models with layer normalization fit the training data better, but achieve worse generalization in over-parameterized cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Effective GPU</cell><cell>Transformer-XL</cell><cell>SRU++</cell><cell>SRU++ (single</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hour</cell><cell></cell><cell></cell><cell>attention)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>1.520</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>1.407</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>1.363</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>11</cell><cell>1.324</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>14</cell><cell>1.308</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>18</cell><cell>1.284</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>22</cell><cell>1.278</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>25</cell><cell>1.260</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>29</cell><cell>1.250</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>32</cell><cell>1.240</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>36</cell><cell>1.239</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40</cell><cell>1.226</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>43 47</cell><cell>1.222 1.214</cell><cell></cell><cell></cell><cell></cell><cell>1.5</cell><cell></cell><cell></cell></row><row><cell>50 54 58 61 65 68 72 76 79 83 86</cell><cell>1.209 1.201 1.196 1.197 1.194 1.190 1.188 1.181 1.181 1.175 1.170</cell><cell></cell><cell cols="2">Bits Per Character (BPC)</cell><cell>1.2 1.3</cell><cell></cell><cell></cell><cell>Transformer-XL SRU++ SRU++ (single attention)</cell></row><row><cell>90 94</cell><cell>1.174 1.167</cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell>97</cell><cell>1.166</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>90</cell><cell>180</cell><cell>270</cell><cell>360</cell></row><row><cell>101</cell><cell>1.162</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>104</cell><cell>1.158</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Effective training hours</cell></row><row><cell>108</cell><cell>1.157</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>112</cell><cell>1.155</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>115</cell><cell>1.151</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>119</cell><cell>1.150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>122</cell><cell>1.148</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>126</cell><cell>1.144</cell><cell></cell><cell></cell><cell cols="2">1.50</cell><cell></cell><cell></cell></row><row><cell>130</cell><cell>1.147</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>133 137 140 144 148 151 155 158 162 166</cell><cell>1.142 1.140 1.139 1.138 1.133 1.129 1.134 1.127 1.130 1.128</cell><cell></cell><cell>Bits Per Character (BPC)</cell><cell cols="2">1.17 1.33</cell><cell></cell><cell>5.1x efficiency</cell><cell>Transformer-XL SRU++ (single attention)</cell><cell>8.7x efficiency</cell></row><row><cell>169</cell><cell>1.122</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>173</cell><cell>1.121</cell><cell></cell><cell></cell><cell cols="2">1.00</cell><cell></cell><cell></cell></row><row><cell>176</cell><cell>1.125</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>90</cell><cell>180</cell><cell>270</cell><cell>360</cell></row><row><cell>180 184</cell><cell>1.121 1.120</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Effective training hours</cell></row><row><cell>187</cell><cell>1.120</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>191</cell><cell>1.118</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>194</cell><cell>1.114</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>198 202</cell><cell>1.112 1.114</cell><cell></cell><cell></cell><cell></cell><cell>1.50</cell><cell></cell><cell></cell></row><row><cell>205 209 212 216 220 223 227 230 234 237 241</cell><cell>1.113 1.110 1.111 1.107 1.109 1.108 1.108 1.106 1.104 1.102 1.104</cell><cell></cell><cell>Bits Per Character (BPC)</cell><cell></cell><cell>1.17 1.33</cell><cell></cell><cell>5.1x efficiency</cell><cell>Transformer-XL SRU++ (single attention) 8.7x efficiency</cell><cell>1.09 1.17</cell></row><row><cell>245 248</cell><cell>1.098 1.102</cell><cell></cell><cell></cell><cell></cell><cell>1.00</cell><cell>0</cell><cell>90</cell><cell>180</cell><cell>270</cell><cell>360</cell></row><row><cell>252</cell><cell>1.099</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>255</cell><cell>1.099</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Effective training hours</cell></row><row><cell>259</cell><cell>1.099</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>263</cell><cell>1.100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>266</cell><cell>1.098</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>270</cell><cell>1.100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>273</cell><cell>1.097</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>277</cell><cell>1.096</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>281</cell><cell>1.097</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>284</cell><cell>1.095</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>288</cell><cell>1.097</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>291</cell><cell>1.097</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>295</cell><cell>1.097</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>299</cell><cell>1.095</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>302</cell><cell>1.095</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>306</cell><cell>1.095</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>309</cell><cell>1.095</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>313</cell><cell>1.093</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>317</cell><cell>1.093</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>320</cell><cell>1.094</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>324</cell><cell>1.094</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>327</cell><cell>1.093</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1 Figure 1: Bits-per-character on ENWIK8 dev set vs. GPU hours used for training. SRU++ ob- tains better BPC by using 1/8 of the resources. We compare with Transformer-XL as it is one of the strongest models on the datasets tested and has the closest implementation to our model (e.g. local at- tention, state reuse etc.). Models are trained with fp32 precision and comparable training settings.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, the linear mapping here can be interpreted as applying a factorization trick W</figDesc><table /><note>o W q with a small inner dimension d &lt; d that can reduce the total number of parameters. Fig- ure 2 compares the differences of SRU, factorized SRU and SRU++.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Optimization We use RAdam<ref type="bibr" target="#b18">(Liu et al., 2020a)</ref> 1 , a variant of the Adam optimizer<ref type="bibr" target="#b15">(Kingma and Ba, 2014)</ref> for training.2  We use the default β</figDesc><table><row><cell>Model</cell><cell cols="3">Param BPC ↓ GPU hrs ↓</cell></row><row><cell>Trans-XL</cell><cell>41M</cell><cell>1.06</cell><cell>356</cell></row><row><cell>SHA-LSTM</cell><cell>54M</cell><cell>1.07</cell><cell>28  †</cell></row><row><cell>k = 1</cell><cell>42M</cell><cell>1.022</cell><cell>37  †</cell></row><row><cell>k = 2</cell><cell>41M</cell><cell>1.025</cell><cell>29  †</cell></row><row><cell>k = 5</cell><cell>41M</cell><cell>1.032</cell><cell>24  †</cell></row><row><cell>k = 10</cell><cell>42M</cell><cell>1.033</cell><cell>22  †</cell></row><row><cell>No attention</cell><cell>42M</cell><cell>1.190</cell><cell>20  †</cell></row></table><note>1 https://github.com/LiyuanLucasLiu/ RAdam2 RAdam is reported to be less sensitive to the choice of learning rate and warmup steps, while achieving similar re-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Analyzing SRU++ on ENWIK8 by enabling attention every k layers. We adjust the hidden size so the number of parameters are comparable. † indicates mixed precision training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Comparison between models on ENWIK8 dataset. We include the training cost (measured by the number of GPUs used × the number of days) if it is reported in the previous work. Our results are obtained using an AWS p3dn instance with 8 V100 GPUs. † indicates mixed precision training.</figDesc><table><row><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bits Per Character (BPC)</cell><cell>1.2 1.3</cell><cell></cell><cell cols="3">Transformer-XL SRU++ SRU++ (k=10, mixed precision)</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>90</cell><cell>180</cell><cell>270</cell><cell>360</cell></row><row><cell cols="6">Effective training hours Figure 3: Dev BPC vs. total GPU hours used on</cell></row><row><cell cols="6">ENWIK8 for each model. Using automatic mixed</cell></row><row><cell cols="6">precision (amp) and only one attention sub-layer</cell></row><row><cell cols="6">achieves 16x reduction. To compute the dev BPC,</cell></row><row><cell cols="6">the maximum attention length is the same as the</cell></row><row><cell cols="4">unroll size M during training.</cell><cell></cell></row><row><cell cols="6">hyperparameter setting including the batch size,</cell></row><row><cell cols="6">unroll size, attention context length, learning rate</cell></row><row><cell cols="6">and training iterations as the Transformer-XL base</cell></row><row><cell cols="6">model. Notably, our base model can be trained us-</cell></row><row><cell cols="6">ing 2 GPUs due to less memory usage. After train-</cell></row><row><cell cols="6">ing, we set the attention context length to 2048 for</cell></row><row><cell cols="6">testing, similarly to the Transformer-XL baseline.</cell></row><row><cell cols="6">Table 1 presents the results. Our model achieves</cell></row><row><cell cols="6">a test BPC of 1.03, outperforming the baseline by</cell></row><row><cell cols="6">0.03. By extending training attention context / un-</cell></row><row><cell cols="6">roll size from 512 to 768, we further obtain a test</cell></row><row><cell cols="2">BPC of 1.02.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Comparison between models on WIKI-103 dataset. We include the training cost (measured by the number of GPUs used × the number of days) if it is reported in the previous work. Our results are obtained using an AWS p3dn instance with 8 V100 GPUs. † indicates mixed precision training.</figDesc><table><row><cell>Model</cell><cell cols="3">Param PPL ↓ GPU days ↓</cell></row><row><cell>Transformer</cell><cell>331M</cell><cell>25.6 25.2</cell><cell>57  † 147  †</cell></row><row><cell></cell><cell>465M</cell><cell>23.9</cell><cell>192  †</cell></row><row><cell>SRU++</cell><cell>328M</cell><cell>25.1</cell><cell>36  †</cell></row><row><cell>SRU++</cell><cell>467M</cell><cell>23.5</cell><cell>63  †</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Comparison between SRU++ and the Transformer model of Baevski and Auli (2019) on BILLION WORD dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="2">Speed↑ PPL↓</cell></row><row><cell>kNN-LM (Khandelwal et al.)</cell><cell>145</cell><cell>15.8</cell></row><row><cell>Trans (Baevski and Auli)</cell><cell>2.5k</cell><cell>18.7</cell></row><row><cell>Trans-XL (Dai et al.)</cell><cell>3.2k</cell><cell>18.3</cell></row><row><cell>Shortformer (Press et al.)</cell><cell>15k</cell><cell>18.2</cell></row><row><cell>SRU++ Large</cell><cell>15k</cell><cell>17.4</cell></row><row><cell>SRU++ Large (k = 5)</cell><cell>22k</cell><cell>17.6</cell></row><row><cell>presents the result of SRU++</cell><cell></cell><cell></cell></row><row><cell>models and other top results on the WIKI-103</cell><cell></cell><cell></cell></row><row><cell>dataset. We train one base model with d = 3072</cell><cell></cell><cell></cell></row><row><cell>and 148M parameters, and a large model with d =</cell><cell></cell><cell></cell></row><row><cell>4096 and 232M parameters. As shown in the table,</cell><cell></cell><cell></cell></row><row><cell>our base model obtains a test perplexity of 18.3</cell><cell></cell><cell></cell></row><row><cell>using 8 GPU days of training, about 3x reduction</cell><cell></cell><cell></cell></row><row><cell>compared to the Transformer model in (Baevski</cell><cell></cell><cell></cell></row><row><cell>and Auli, 2019) and over 10x reduction compared</cell><cell></cell><cell></cell></row><row><cell>to Feedback Transformer (Fan et al., 2020). Our</cell><cell></cell><cell></cell></row><row><cell>big model achieves a test perplexity of 17.4 or 17.6</cell><cell></cell><cell></cell></row><row><cell>when trained with 2 attention layers. The required</cell><cell></cell><cell></cell></row><row><cell>training cost remains significantly lower.</cell><cell></cell><cell></cell></row><row><cell>BILLION WORD Finally, we evaluate SRU++</cell><cell></cell><cell></cell></row><row><cell>models on the BILLION WORD dataset. We dou-</cell><cell></cell><cell></cell></row><row><cell>ble our training iterations to 800K and use a learn-</cell><cell></cell><cell></cell></row><row><cell>ing rate of 0.0002 compared to the setting used for</cell><cell></cell><cell></cell></row><row><cell>WIKI-103 and ENWIK8 datasets. We train a base</cell><cell></cell><cell></cell></row><row><cell>model using d = 4096, d = 1024 and an effective</cell><cell></cell><cell></cell></row><row><cell>batch size of 65K tokens per gradient update. We</cell><cell></cell><cell></cell></row><row><cell>also train a large model by increasing the hidden</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Inference speed (tokens per second) on WIKI-103 test set. Results of baseline models are taken from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>compares the inference</cell></row><row><cell>speed of SRU++ with other strong models on</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/kimiyoung/ transformer-xl/tree/master/pytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Hugh Perkins, Joshua Shapiro, Sam Bowman and Danqi Chen for providing invaluable feedback for this work. In addition, we thank Jeremy Wohlwend, Jing Pan, Prashant Sridhar and Kyu Han for helpful discussions, and ASAPP Language Technology and Infra teams for the compute cluster setup for our research experiments. Finally, we'd like to thank Tao Ma, Will Robinson and Gustavo Sapoznik for their support of this work and all research work at ASAPP in general.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<title level="m">Quasi-Recurrent Neural Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Characterizing signal propagation to close the performance gap in unnormalized resnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Skip rnn: Learning to skip state updates in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giró I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Google</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09402</idno>
		<title level="m">Accessing higher-level representations in sequential transformers with feedback memory</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">2020. Conformer: Convolutionaugmented Transformer for Speech Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<title level="m">Ajay Mishra, and Bing Xiang. 2020. Trans-blstm: Transformer with bidirectional lstm for language understanding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The human knowledge compression contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://prize.hutter1.net" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalization through memorization: Nearest neighbor language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simple recurrent units for highly parallelizable recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autoregressive knowledge distillation through imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Wohlwend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Learning Representations</title>
		<meeting>the Eighth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single headed attention rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11423</idno>
	</analytic>
	<monogr>
		<title level="m">Stop thinking with your head</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully neural network based speech recognition on mobile and embedded devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhwan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Boo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iksoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonyong</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10620" to="10630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15832</idno>
		<title level="m">Shortformer: Better language modeling using shorter inputs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05997</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Optimizing speech recognition for the edge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raziel</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mcgraw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Powernorm: Rethinking batch normalization in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Augmenting self-attention with persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
		<meeting>the 37th International Conference on Machine Learning, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding and improving layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A lightweight recurrent network for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
