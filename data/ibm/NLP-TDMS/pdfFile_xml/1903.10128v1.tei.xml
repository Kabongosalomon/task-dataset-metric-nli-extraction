<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Back-Projection Network for Video Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
							<email>ukita@toyota-ti.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Back-Projection Network for Video Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We proposed a novel architecture for the problem of video super-resolution. We integrate spatial and temporal contexts from continuous video frames using a recurrent encoder-decoder module, that fuses multi-frame information with the more traditional, single frame super-resolution path for the target frame. In contrast to most prior work where frames are pooled together by stacking or warping, our model, the Recurrent Back-Projection Network (RBPN) treats each context frame as a separate source of information. These sources are combined in an iterative refinement framework inspired by the idea of back-projection in multiple-image super-resolution. This is aided by explicitly representing estimated inter-frame motion with respect to the target, rather than explicitly aligning frames. We propose a new video super-resolution benchmark, allowing evaluation at a larger scale and considering videos in different motion regimes. Experimental results demonstrate that our RBPN is superior to existing methods on several datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of super-resolution (SR) is to enhance a lowresolution (LR) image to higher resolution (HR) by filling missing fine details in the LR image. This field can be divided into Single-Image SR (SISR) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>, Multi-Image SR (MISR) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, and Video SR (VSR) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25]</ref>, the focus of this paper.</p><p>Consider a sequence of LR video frames I t−n , . . . , I t , . . . , I t+n , where we super-resolve a target frame, I t . While I t can be super-resolved independently of other frames as SISR, this is wasteful of missing details available from the other frames. In MISR, the missing details available from the other frames are fused for super-resolving I t . For extracting these missing details, all frames must be spatially aligned explicitly or implicitly. By separating differences between the aligned frames from missing details observed only in one or some of the frames, the missing details are extracted. This alignment is required to be very precise (e.g., sub-pixel accuracy) for SR. In MISR, however, the frames are aligned independently with no cue given by temporal smoothness, resulting in difficulty in the precise alignment. Yet another approach is to align the frames in temporal smooth order as VSR.</p><p>In recent VSR methods using convolutional networks, the frames are concatenated <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16]</ref> or fed into recurrent networks (RNNs) <ref type="bibr" target="#b12">[13]</ref>) in temporal order; no explicit alignment is performed. The frames can be also aligned explicitly, using motion cues between temporal frames with the alignment modules <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27]</ref>. These latter methods generally produce results superior to those with no explicit spatial alignment <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13]</ref>. Nonetheless, these VSR methods suffer from a number of problems. In the frameconcatenation approach <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>, many frames are processed simultaneously in the network, resulting in difficulty in training the network. In RNNs <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13]</ref>, it is not easy to jointly model subtle and significant changes (e.g., slow and quick motions of foreground objects) observed in all frames of a video even by those designed for maintaining long-term temporal dependencies such as LSTMs <ref type="bibr" target="#b6">[7]</ref>.</p><p>Our method proposed in this paper is inspired by "backprojection" originally introduced in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> for MISR. Back-projection iteratively calculates residual images as reconstruction error between a target image and a set of its corresponding images. The residuals are back-projected to the target image for improving its resolution. The multiple residuals can represent subtle and significant differences between the target frame and other frames independently. Recently, Deep Back-Projection Networks (DBPN) <ref type="bibr" target="#b7">[8]</ref> extended back-projection to Deep SISR under the assumption that only one LR image is given for the target image. In that scenario, DBPN produces a high-resolution feature map, iteratively refined through multiple up-and down-sampling layers. Our method, Recurrent Back-Projection Networks (RBPN), integrates the benefits of the original, MISR back projection and DBPN, for VSR. Here we use other video frames as corresponding LR images for the original MISR back-projection. In addition, we use the idea of iteratively refining HR feature maps representing missing details by up-and down-sampling processes to further improve the  <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref>. (b) Temporal aggregation improves (a) to preserve multiple motion regimes <ref type="bibr" target="#b24">[25]</ref>. (c) RNNs take a sequence of input frames to produce one SR image at a target frame, It <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27]</ref>. (d) Our recurrent back-projection network accepts It, which is enclosed by a blue dashed line, as well as a set of residual features computed from a pairing It with other frames (i.e., I t−k for k ∈ {1, · · · , n}), as enclosed by a red dotted line, while previous approaches using RNNs shown in (c) feed all temporal frames one by one along a single path. Residual features computed from the pairs of (It, I t−k ) (MISR path -the vertical red arrows) are fused with features extracted from variants of It (SISR path -the horizontal blue arrows) through RNN. quality of SR.</p><p>Our contributions include the following key innovations. Integrating SISR and MISR in a unified VSR framework: SISR and MISR extract missing details from different sources. Iterative SISR <ref type="bibr" target="#b7">[8]</ref> extracts various feature maps representing the details of a target frame. MISR provides multiple sets of feature maps from other frames. These different sources are iteratively updated in temporal order through RNN for VSR. Back-projection modules for RBPN: We develop a recurrent encoder-decoder mechanism for incorporating details extracted in SISR and MISR paths through the backprojection. While the SISR path accepts only I t , the MISR path also accepts I t−k where k ∈ [n]. A gap between I t and I t−k is larger than the one in other VSRs using RNN (i.e., gap only between I t and I t−1 ). Here, the network is able to understand this large gap since each context is calculated separately, rather than jointly as in previous work, this separate context plays an important role in RBPN. Extended evaluation protocol: We report extensive experiments to evaluate VSR. In addition to previously-standard datasets, Vid4 <ref type="bibr" target="#b23">[24]</ref> and SPMCS <ref type="bibr" target="#b29">[30]</ref>, that lack significant motions, a dataset containing various types of motion (Vimeo-90k <ref type="bibr" target="#b33">[34]</ref>) is used in our evaluation. This allows us to conduct a more detailed evaluation of strengths and weaknesses of VSR methods, depending on the type of the input video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>While SR has an extensive history, our discussion in this section focuses on deep SR -SR methods that involve deep neural network components, trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Image Super-resolution</head><p>Deep SISR is first introduced by SRCNN [4] that requires a predefined upsampling operator. Further im-provements include better up-sampling layers <ref type="bibr" target="#b27">[28]</ref>, residual learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>, back-projection <ref type="bibr" target="#b7">[8]</ref>, recursive layers <ref type="bibr" target="#b19">[20]</ref>, and progressive upsampling <ref type="bibr" target="#b20">[21]</ref>. See NTIRE2018 <ref type="bibr" target="#b30">[31]</ref> and PIRM2018 <ref type="bibr" target="#b0">[1]</ref> for comprehensive comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Recurrent Networks</head><p>Recurrent neural networks (RNNs) deal with sequential inputs and/or outputs, and have been employed for video captioning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref>, video summarization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>, and VSR <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref>. Two types of RNN have been used for VSR. A many-to-one architecture is used in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13]</ref> where a sequence of frames is mapped to a single target HR frame. A synchronous many-to-many RNN has recently been used in VSR by <ref type="bibr" target="#b26">[27]</ref>, to map a sequence of LR frames to a sequence of HR frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Deep Video Super-resolution</head><p>Deep VSR can be primarily divided into three types based on the approach to preserving temporal information as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> (a), (b), and (c). (a) Temporal Concatenation. The most popular approach to retain temporal information in VSR is by concatenating the frames as in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref>. This approach can be seen as an extension of SISR to accept multiple input images. VSR-DUF <ref type="bibr" target="#b15">[16]</ref> proposed a mechanism to construct up-sampling filters and residual images. However, this approach fails to represent the multiple motion regimes on a sequence because input frames are concatenated together. (b) Temporal Aggregation. To address the dynamic motion problem in VSR, <ref type="bibr" target="#b24">[25]</ref> proposed multiple SR inferences which work on different motion regimes. The final layer aggregates the outputs of all branches to construct SR frame. However, this approach basically still concatenates many input frames, resulting in difficulty in global optimization. (c) RNNs. This approach is first proposed by <ref type="bibr" target="#b12">[13]</ref> using bidirectional RNNs. However, the network has a small net- <ref type="figure">Figure 2</ref>. Overview of RBPN. The network has two approaches. The horizontal blue line enlarges It using SISR. The vertical red line is based on MISR to compute the residual features from a pair of It to neighbor frames (It−1, ..., It−n) and the precomputed dense motion flow maps (Ft−1, ..., Ft−n). Each step is connected to add the temporal connection. On each projection step, RBPN observes the missing details on It and extract the residual features from each neighbor frame to recover the missing details.</p><p>work capacity and has no frame alignment step. Further improvement is proposed by <ref type="bibr" target="#b29">[30]</ref> using a motion compensation module and a convLSTM layer <ref type="bibr" target="#b32">[33]</ref>. Recently, <ref type="bibr" target="#b26">[27]</ref> proposed an efficient many-to-many RNN that uses the previous HR estimate to super-resolve the next frames. While recurrent feedback connections utilize temporal smoothness between neighbor frames in a video for improving the performance, it is not easy to jointly model subtle and significant changes observed in all frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Recurrent Back-Projection Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>Our proposed network is illustrated in <ref type="figure">Fig. 2</ref>. Let I be LR frame with size of (M l × N l ). The input is sequence of n + 1 LR frames {I t−n , . . . , I t−1 , I t } where I t is the target frame. The goal of VSR is to output HR version of I t , denoted by SR t with size of (M h × N h ) where M l &lt; M h and N l &lt; N h . The operation of RBPN can be divided into three stages: initial feature extraction, multiple projections, and reconstruction. Note that we train the entire network jointly, end-to-end. Initial feature extraction. Before entering projection modules, I t is mapped to LR feature tensor L t . For each neighbor frame among I t−k , k ∈ [n], we concatenate the precomputed dense motion flow map F t−k (describing a 2D vector per pixel) between I t−k and I t with the target frame I t and I t−k . The motion flow map encourages the projection module to extract missing details between a pair of I t and I t−k . This stacked 8-channel "image" is mapped to a neighbor feature tensor M t−k . Multiple Projections. Here, we extract the missing details in the target frame by integrating SISR and MISR paths, then produce refined HR feature tensor. This stage receives L t−k−1 and M t−k , and outputs HR feature tensor H t−k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction.</head><p>The final SR output is obtained by feeding concatenated HR feature maps for all frames into a reconstruction module, similarly to <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_0">SR t = f rec ([H t−1 , H t−2 , ..., H t−n ]).</formula><p>In our experiments, f rec is a single convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multiple Projection</head><p>The multiple projection stage of RBPN uses a recurrent chain of encoder-decoder modules, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>The projection module, shared across time frames, takes two inputs:</p><formula xml:id="formula_1">L t−n−1 ∈ R M l ×N l ×c l and M t−n ∈ R M l ×N l ×c m , then produces two outputs: L t−n and H t−n ∈ R M h ×N h ×c h</formula><p>where c l , c m , c h are the number of channels for particular map accordingly.</p><p>The encoder produces a hidden state of estimated HR features from the projection to a particular neighbor frame. The decoder deciphers the respective hidden state as the next input for the encoder module as shown in <ref type="figure">Fig. 4</ref> which are defined as follows:</p><p>Encoder:</p><formula xml:id="formula_2">H t−n = Net E (L t−n−1 , M t−n ; θ E ) (1) Decoder: L t−n = Net D (H t−n ; θ D )<label>(2)</label></formula><p>The encoder module Net E is defined as follows:</p><formula xml:id="formula_3">SISR upscale: H l t−n−1 = Net sisr (L t−n−1 ; θ sisr ) (3) MISR upscale: H m t−n = Net misr (M t−n ; θ misr )<label>(4)</label></formula><p>Residual:</p><formula xml:id="formula_4">e t−n = Net res (H l t−n−1 − H m t−n ; θ res ) (5) Output: H t−n = H l t−n−1 + e t−n<label>(6)</label></formula><p>3.3. Interpretation <ref type="figure" target="#fig_3">Figure 5</ref> illustrates the RBPN pipeline, for a 3-frame video. In the encoder, we can see RBPN as the combination of SISR and MISR networks. First, target frame is enlarged by Net sisr to produce H l t−k−1 . Then, for each combination of concatenation from neighbor frames and target frame, Net misr performs implicit frame alignment and absorbs the motion from neighbor frames to produce warping  and H m t−k are fused back to H l t−k−1 to refine the HR features and produce hidden state H t−k . The decoder "deciphers" the hidden state H t−k to be the next input for the encoder L t−k . This process is repeated iteratively until the target frame is projected to all neighbor frames.</p><p>The optimal scenario for this architecture is when each frame can contribute to filling in some missing details in the target frame. Then H t−k generated in each step k produce unique features. In the generate case when n = 0 (no other frames) or the video is completely static (identical frames) RBPN will effectively ignore the Net misr module, and fall back to a recurrent SISR operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In all our experiments, we focus on 4× SR factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation and training details</head><p>We use DBPN <ref type="bibr" target="#b7">[8]</ref> for Net sisr , and Resnet <ref type="bibr" target="#b9">[10]</ref> for Net misr , Net res , and Net D . For Net sisr , we construct three stages using 8 × 8 kernel with stride = 4 and pad by 2 pixels. For Net misr , Net res , and Net D , we construct five blocks where each block consists of two convolutional layers with 3 × 3 kernel with stride = 1 and pad by 1 pixel. The up-sampling layer in Net misr and down-sampling layer in Net D use 8 × 8 kernel with stride = 4 and pad by 2 pixels. Our final network uses c l = 256, c m = 256, and c h = 64. We trained our networks using Vimeo-90k <ref type="bibr" target="#b33">[34]</ref>, with a training set of 64,612 7-frame sequences, with fixed resolution 448 × 256. Furthermore, we also apply augmentation, such as rotation, flipping, and random cropping. To produce LR images, we downscale the HR images 4× with bicubic interpolation.</p><p>All modules are trained end-to-end using per-pixel L1 loss per-pixel between the predicted frame and the ground truth HR frame. We use batch size of 8 with size 64 × 64 which is cropped randomly from 112 × 64 LR image. The learning rate is initialized to 1e − 4 for all layers and decrease by a factor of 10 for half of total 150 epochs. We initialize the weights based on <ref type="bibr" target="#b10">[11]</ref>. For optimization, we used Adam with momentum to 0.9. All experiments were conducted using Python 3.5.2 and PyTorch 1.0 on NVIDIA TITAN X GPUs. Following the evaluation from previous approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27]</ref>, we crop 8 pixels near image boundary and remove first six frames and last three frames. All measurements use only the luminance channel (Y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation studies</head><p>Baselines We consider three baselines, that retain some components of RBPN while removing others. First, we remove all components by Net sisr (DBPN); this ignores the video context. Second, we use DBPN with temporal concatenation (DBPN-MISR). Third, we remove the decoder, thus severing temporal connections, so that our model is reduced to applying back-projection Net misr with each neighboring frame, and concatenating the results; we call this baseline RBPN-MISR. The results are shown in Table 1. Our intuition suggests, and the results confirm, that such an approach would be weaker than RBPN, since it does not have the ability to separately handle changes of different magnitude that RBPN has. As expected, SISR suffers from ignoring extra information in other frames. RBPN-MISR and DBPN-MISR does manage to leverage multiple frames to improve performance, but the best results are obtained by the full RBPN model. Network setup. The modular design of our approach allows easy replacement of modules; in particular we consider choices of DBPN or ResNet for Net sisr , Net misr , or both. In <ref type="table">Table 2</ref>, we evaluate three combinations: RBPN  Context length We evaluated RBPN with different lengths of video context, i.e., different number of past frames n ∈ {2, . . . , 6}. <ref type="figure">Figure 6</ref> shows that performance (measured on (on SPMCS-32 test set) improves with longer context. The performance of RBPN/3 is even better than VSR-DUF as one of state-of-the-art VSR which uses six neighbor frames. It also shows that by adding more frames, the performance of RBPN increase by roughly 0.2 dB. <ref type="figure" target="#fig_4">Fig. 7</ref> provides an illustration of the underlying performance gains. Here, VSR-DUF fails to reconstruct the brick pattern, while RBPN/3 reconstructs it well, even with fewer frames in the context; increasing context length leads to further improvements. <ref type="bibr">RBPN</ref>  Temporal integration Once the initial feature extraction and the projection modules have produced a sequence of HR feature maps H t−k , k = 1, . . . , n, we can use these maps in multiple ways to reconstruct the HR target. The proposed DBPN concatenates the maps; We also consider an alternative where only the H t−n is fed to Net rec (referred to as Last). Furthermore, instead of concatenating the maps, we can feed them to a convolutional LSTM <ref type="bibr" target="#b32">[33]</ref>, the output of which is then fed to Net rec . The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Dropping the concatenation and only using last feature map harms the performance (albeit moderately). Replacing concatenation with an LSTM also reduces the performance (while increasing computational cost). We conclude that the RBPN design depicted in <ref type="figure">Fig. 2</ref> is better than the alternatives. Temporal order When selecting frames to serve as context for a target frame t, we have a choice of how to choose and order it: use only past frames (P; for instance, with n = 6, this means I t , I t−1 , . . . , I t−6 ), use both past and future (PF, I t−3 , . . . , I t , . . . , I t+3 ), or consider the past frames in random order (PR; we can do this since the motion flow is computed independently for each context frame w.r.t. the target). <ref type="table">Table 4</ref> shows that PF is better than P by 0.1 dB; presumably this is due to the increased, more symmetric representation of motion occurring in frame t. Interestingly, when the network is trained on PF, then tested on P (PF → P), the performance is decreased (-0.17dB), but when RBPN is trained on P then tested on PF (P → PF), the performance remains almost the same. The results of comparing order P to random ordering PR are shown in <ref type="table">Table 5</ref>. Interestingly, RBPN performance is  not significantly affected by the choice of order. We attribute this robustness to the decision to associate each context frame with the choice of order. Optical flow Finally, we can remove the optical flow component of M t−k , feeding the projection modules only the concatenated frame pairs. As <ref type="table" target="#tab_4">Table 6</ref> shows, explicit optical flow representation is somewhat, but not substantially, beneficial. We compute the flow using an implementation of <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the-state-of-the-arts</head><p>We compare our network with eight state-of-the-art SR algorithms: DBPN <ref type="bibr" target="#b7">[8]</ref>, BRCN <ref type="bibr" target="#b12">[13]</ref>, VESPCN <ref type="bibr" target="#b1">[2]</ref>, B 123 + T <ref type="bibr" target="#b24">[25]</ref>, VSR-TOFLOW <ref type="bibr" target="#b33">[34]</ref>, DRDVSR <ref type="bibr" target="#b29">[30]</ref>, FRVSR <ref type="bibr" target="#b26">[27]</ref>, and VSR-DUF <ref type="bibr" target="#b15">[16]</ref>. Note: only VSR-DUF and DBPN provide full testing code without restrictions, and most of the previous methods use different training sets. Other methods provide only the estimated SR frames. For RBPN, we use n = 6 with PF (past+future) order, which achieves the best results, denoted as RBPN/6-PF.</p><p>We carry out extensive experiments using three datasets: Vid4 <ref type="bibr" target="#b23">[24]</ref>, SPMCS <ref type="bibr" target="#b29">[30]</ref>, and Vimeo-90k <ref type="bibr" target="#b33">[34]</ref>. Each dataset has different characteristics. We found that evaluating on Vid4, commonly reported in literature, has limited ability to assess relative merits of competing approaches; the sequences in this set have visual artifacts, very little interframe variation, and fairly limited motion. Most notably, it only consists of four video sequences. SPMCS data exhibit more variation, but still lack significant motion. Therefore, in addition to the aforementioned data sets, we consider Vimeo-90k, a much larger and diverse data set, with high-quality frames, and a range of motion types. We stratify the Vimeo-90k sequences according to estimated motion velocities into slow, medium and fast "tiers", as shown in <ref type="figure">Fig. 8</ref>, and report results for these tiers separately. <ref type="table">Table 7</ref> shows the results on Vid4 test set. We also provide the average flow magnitude (pixel/frame) on Vid4. It shows that Vid4 does not contain significant motion. The results also show that RBPN/6-PF is better than the previous methods, except for VSR-DUF. <ref type="figure" target="#fig_0">Figure 12</ref> shows some qualitative results on Vid4. (on "Calendar"). The "MA-REE" text reconstructed with RBPN/6-PF has sharper images than previous methods. However, here we see that the ground truth (GT) itself suffers from artifacts and aliasing, perhaps due to JPEG compression. This apparently leads in some cases to penalizing sharper SR predictions, like those made by our network, as illustrated in <ref type="figure" target="#fig_0">Fig. 12</ref>. <ref type="table">Table 8</ref> shows the detailed results on SPMCS-11. RBPN/6-PF has better performance of 0.68 dB and 1.28 dB than VSR-DUF and DRDVSR, respectively. Even with fewer frames in the context, RBPN/4-P has better average performance than VSR-DUF and DRDVSR by 0.33 dB and 0.93 dB, respectively. Qualitative results on SPMCS are shown in <ref type="figure" target="#fig_0">Fig. 13</ref>. In the first row, we see that RBPN reproduces a well-defined pattern, especially on the stairs area. In the second row, RBPN recovers sharper details and produces better brown lines from the building pattern.</p><p>It is interesting to see that VSR-DUF tends to do better on SSIM than on PSNR. It has been suggested that PSNR is more sensitive to Gaussian noise, while SSIM is more sensitive to compression artifacts <ref type="bibr" target="#b11">[12]</ref>. VSR-DUF generates up-sampling filter to enlarge the target frame. The use of up-sampling filter can keep overall structure of target frame which tends to have higher SSIM. However, since the residual image produced by VSR-DUF fails to generate the missing details, PSNR tends to be lower. In contrast with VSR-DUF, our focus is to fuse the missing details to the target frame. However, if in some cases we generate sharper pattern than GT, this causes lower SSIM. This phenomenon mainly can be observed in the Vid4 test set. <ref type="table">Table 9</ref> shows the results on Vimeo-90k. RBPN/6-PF outperforms VSR-DUF by a large margin. RBPN/6-PF gets higher PSNR by 1.22 dB, 1.44 dB, and 2.54 dB than VSR-DUF on, respectively, slow, medium, and fast motion. It can be seen that RBPN is able to preserve different temporal scale. RBPN achieves the highest gap relative to prior work <ref type="figure">Figure 8</ref>. Examples from Vimeo-90k <ref type="bibr" target="#b33">[34]</ref>. Top row: fast camera motion; new object appears in the third frame. Middle row: medium motion, little camer movement but some scene movement (e.g., person's arm in the foreground). Bottom row: slow motion only.  on fast motion. Even with reduced amount of temporal context available, RBPN/3-P (using only 3 extra frames) does better than previous methods like VSR-DUF using the full 6-extra frame context. RBPN/3-P get higher PSNR by 0.77 dB, 0.82 dB, and 2 dB than VSR-DUF on slow, medium, and fast motion, respectively. <ref type="figure" target="#fig_0">Figure 14</ref> shows qualitative results on Vimeo-90k. RBPN/6-PF obtains reconstruction that appears most similar to the GT, more pleasing and sharper than reconstructions with other methods. We have highlighted regions in which this is particularly notable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a novel approach to video superresolution (VSR) called Recurrent Back-Projection Network (RBPN). It's a modular architecture, in which temporal and spatial information is collected from video frames surrounding the target frame, combining ideas from singleand multiple-frame super resolution. Temporal context is organized by a recurrent process using the idea of (back)projection, yielding gradual refinement of the high-"Calendar" (a) Bicubic (b) DBPN <ref type="bibr" target="#b7">[8]</ref> (c) VSR <ref type="bibr" target="#b17">[18]</ref> (d) VESPCN <ref type="bibr" target="#b1">[2]</ref> (e) B123 + T <ref type="bibr" target="#b24">[25]</ref> (f) DRDVSR <ref type="bibr" target="#b29">[30]</ref> (g) FRVSR <ref type="bibr" target="#b26">[27]</ref> (h) VSR-DUF <ref type="bibr" target="#b15">[16]</ref> (i) RBPN/6-PF (j) GT <ref type="figure">Figure 9</ref>. Visual results on Vid4 for 4× scaling factor. Zoom in to see better visualization.</p><p>(a) DBPN <ref type="bibr" target="#b7">[8]</ref> (b) DRDVSR <ref type="bibr" target="#b29">[30]</ref> (c) VSR-DUF <ref type="bibr" target="#b15">[16]</ref> (d) RBPN/6-PF (e) GT <ref type="figure" target="#fig_0">Figure 10</ref>. Visual results on SPMCS for 4× scaling factor. Zoom in to see better visualization. resolution features used, eventually, to reconstruct the highresolution target frame. In addition to our technical innovations, we propose a new evaluation protocol for video SR. This protocol allows to differentiate performance of video SR based on magnitude of motion in the input videos. In extensive experiments, we assess the role played by various design choices in the ultimate performance of our approach, and demonstrate that, on a vast majority of thousands of test video sequences, RBPN obtains significantly better performance than existing VSR methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Multiple scale factors</head><p>To enrich the evaluation of RBPN, we provide the results for multiple scaling factors (i.e., 2× and 8×) on Vimeo-90k <ref type="bibr" target="#b33">[34]</ref>, SPMCS-32 <ref type="bibr" target="#b29">[30]</ref>, and Vid4 <ref type="bibr" target="#b23">[24]</ref> as shown in <ref type="table" target="#tab_0">Table 10</ref>. Due to the limitation on other methods, the scores for other methods were copied from the respective publications. RBPN is superior to existing methods on all test sets except the SSIM score on Vid4 in scaling factor 2×. However, note that the difference between the best score (i.e., VSR-DUF) and our score is only 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Network size</head><p>We observe the performance of RBPN on different network sizes. The "original" RBPN use the same setup as in the main paper. We use DBPN <ref type="bibr" target="#b7">[8]</ref> for Net sisr , and Resnet <ref type="bibr" target="#b9">[10]</ref> for Net misr , Net res , and Net D . For Net sisr , we construct three stages using 8 × 8 kernel with stride = 4 and pad by 2 pixels. For Net misr , Net res , and Net D , we construct five blocks where each block consists of two convolutional layers with 3 × 3 kernel with stride = 1 and pad by 1 pixel. The up-sampling layer in Net misr and downsampling layer in Net D use 8 × 8 kernel with stride = 4 and pad by 2 pixels. It also uses c l = 256, c m = 256, and c h = 64.</p><p>RBPN-S uses Net misr , Net res , and Net D with three blocks, while RBPN-L uses deeper Net sisr with six stages. The other setup remain the same. <ref type="table" target="#tab_0">Table 11</ref> shows that RBPN/6 achieves the best performance. The performance of RBPN/6 is reported in detail in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Residual Learning</head><p>We also investigate the use of residual learning on RBPN. First, the target frame is interpolated using Bicubic, then RBPN only produces the residual image. Finally, the interpolated and residual images are combined to produce an SR image. Unfortunately, the current hyperparameters show that residual learning is not effective to improve RBPN as shown in <ref type="table" target="#tab_0">Table 12</ref>.</p><p>Vimeo-90k <ref type="bibr" target="#b33">[34]</ref> SPMCS-32 <ref type="bibr" target="#b29">[30]</ref> Vid4 <ref type="bibr">[</ref> (f) DRDVSR <ref type="bibr" target="#b29">[30]</ref> (g) FRVSR <ref type="bibr" target="#b26">[27]</ref> (h) VSR-DUF <ref type="bibr" target="#b15">[16]</ref> (i) RBPN/6-PF (j) GT "Calendar" (a) Bicubic (b) DBPN <ref type="bibr" target="#b7">[8]</ref> (c) VSR <ref type="bibr" target="#b17">[18]</ref> (d) VESPCN <ref type="bibr" target="#b1">[2]</ref> (e) B123 + T <ref type="bibr" target="#b24">[25]</ref> (f) DRDVSR <ref type="bibr" target="#b29">[30]</ref> (g) FRVSR <ref type="bibr" target="#b26">[27]</ref> (h) VSR-DUF <ref type="bibr" target="#b15">[16]</ref> (i) RBPN/6-PF (j) GT <ref type="figure" target="#fig_0">Figure 12</ref>. Visual results on Vid4 for 4× scaling factor.</p><p>(a) DBPN <ref type="bibr" target="#b7">[8]</ref> (b) DRDVSR <ref type="bibr" target="#b29">[30]</ref> (c) VSR-DUF <ref type="bibr" target="#b15">[16]</ref> (d) RBPN/6-PF (e) GT <ref type="figure" target="#fig_0">Figure 13</ref>. Visual results on SPMCS for 4× scaling factor.   <ref type="table" target="#tab_0">Table 13</ref>. Computational complexity on 4× SR with input size 120 × 160. *Counted manually from model definitions described in the papers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of Deep VSRs. (a) Input frames are concatenated to preserve temporal information</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The proposed projection module. The target features (Lt−n−1) is projected to neighbor features (Mt−n) to construct better HR features (Ht−n) and produce next LR features (Lt−n) for the next step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Encoder (the back-projection) (b) Decoder Figure 4. Detailed illustration of encoder and decoder. The encoder performs back-projection from Lt−n−1 to Mt−n to produce the residual et−n. features H m t−k which may capture missing details in the target frame. Finally, the residual features e t−k from H l t−k−1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The illustration of each operation in RBPN (n + 1 = 3). Zoom in to see better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Visual results on different frame length (SPMCS-32). Zoom in to see better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 .</head><label>11</label><figDesc>Visual results on Vimeo-90k for 4× scaling factor. Zoom in to see better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 14 .</head><label>14</label><figDesc>Visual results on Vimeo-90k for 4× scaling factor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Baseline comparison on SPMCS-32. Red here and in the other tables indicates the best performance (PSNR/SSIM).</figDesc><table><row><cell>Bicubic</cell><cell>DBPN</cell><cell>DBPN-MISR</cell><cell>RBPN-MISR</cell><cell>RBPN</cell></row><row><cell>1 Frame</cell><cell>1 Frame</cell><cell>5 Frames</cell><cell>5 Frames</cell><cell>5 Frames</cell></row><row><cell>27.13/0.749</cell><cell>29.85/0.837</cell><cell>30.64/0.859</cell><cell>30.89/0.866</cell><cell>31.40/0.877</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Comparison of temporal integration strategies on SPMCS-32.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Optical flow (OF) importance on SPMCS-32.</figDesc><table><row><cell></cell><cell>P</cell><cell>PF</cell><cell>P → PF</cell><cell>PF → P</cell></row><row><cell>PSNR/SSIM</cell><cell>31.64/0.883</cell><cell>31.74/0.884</cell><cell>31.66/0.884</cell><cell>31.57/0.881</cell></row><row><cell cols="5">Table 4. Effect of temporal order of context, RBPN/6 on SPMCS-</cell></row><row><cell>32.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>P</cell><cell>PR</cell><cell>P → PR</cell><cell>PR → P</cell></row><row><cell>PSNR/SSIM</cell><cell>31.40/0.877</cell><cell>31.39/0.876</cell><cell>31.39/0.877</cell><cell>31.35/0.875</cell></row><row><cell cols="5">Table 5. Effect of temporal order (RBPN/4) on SPMCS-32.</cell></row><row><cell></cell><cell></cell><cell cols="2">RBPN/5</cell><cell></cell></row><row><cell></cell><cell></cell><cell>w/</cell><cell>w/o</cell><cell></cell></row><row><cell cols="5">PSNR/SSIM 31.54/0.881 31.36/0.878</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Quantitative evaluation of state-of-the-art SR algorithms on Vid4 for 4×. Red indicates the best and blue indicates the second best performance (PSNR/SSIM). The calculation is computed without crop any pixels border and remove first and last two frames. For B123 + T and DRDVSR, we use results provided by the authors on their webpage. For BRCN, VESPCN, and FRVSR, the values taken from their publications. *The output is cropped 8-pixels near image boundary. Quantitative evaluation of state-of-the-art SR algorithms on SPMCS-11 for 4×. Red indicates the best and blue indicates the second best performance (PSNR/SSIM).</figDesc><table><row><cell></cell><cell></cell><cell>Flow</cell><cell>Bicubic</cell><cell>DBPN [8]</cell><cell>DRDVSR [30]</cell><cell>VSR-DUF [16]</cell><cell>RBPN/4-P</cell><cell>RBPN/6-P</cell><cell>RBPN/6-PF</cell></row><row><cell>Clip Name</cell><cell cols="2">Magnitude</cell><cell></cell><cell>(1 Frame)</cell><cell>(7 Frames)</cell><cell>(7 Frames)</cell><cell>(5 Frames)</cell><cell>(7 Frames)</cell><cell>(7 Frames)</cell></row><row><cell>car05 001</cell><cell></cell><cell>6.21</cell><cell>27.62</cell><cell>29.58</cell><cell>32.07</cell><cell>30.77</cell><cell>31.51</cell><cell>31.65</cell><cell>31.92</cell></row><row><cell>hdclub 003 001</cell><cell></cell><cell>0.70</cell><cell>19.38</cell><cell>20.22</cell><cell>21.03</cell><cell>22.07</cell><cell>21.62</cell><cell>21.91</cell><cell>21.88</cell></row><row><cell cols="2">hitachi isee5 001</cell><cell>3.01</cell><cell>19.59</cell><cell>23.47</cell><cell>23.83</cell><cell>25.73</cell><cell>25.80</cell><cell>26.14</cell><cell>26.40</cell></row><row><cell>hk004 001</cell><cell></cell><cell>0.49</cell><cell>28.46</cell><cell>31.59</cell><cell>32.14</cell><cell>32.96</cell><cell>32.99</cell><cell>33.25</cell><cell>33.31</cell></row><row><cell>HKVTG 004</cell><cell></cell><cell>0.11</cell><cell>27.37</cell><cell>28.67</cell><cell>28.71</cell><cell>29.15</cell><cell>29.28</cell><cell>29.39</cell><cell>29.43</cell></row><row><cell>jvc 009 001</cell><cell></cell><cell>1.24</cell><cell>25.31</cell><cell>27.89</cell><cell>28.15</cell><cell>29.26</cell><cell>29.81</cell><cell>30.17</cell><cell>30.26</cell></row><row><cell>NYVTG 006</cell><cell></cell><cell>0.10</cell><cell>28.46</cell><cell>30.13</cell><cell>31.46</cell><cell>32.29</cell><cell>32.83</cell><cell>33.09</cell><cell>33.25</cell></row><row><cell>PRVTG 012</cell><cell></cell><cell>0.12</cell><cell>25.54</cell><cell>26.36</cell><cell>26.95</cell><cell>27.47</cell><cell>27.33</cell><cell>27.52</cell><cell>27.60</cell></row><row><cell>RMVTG 011</cell><cell></cell><cell>0.18</cell><cell>24.00</cell><cell>25.77</cell><cell>26.49</cell><cell>27.63</cell><cell>27.33</cell><cell>27.64</cell><cell>27.69</cell></row><row><cell>veni3 011</cell><cell></cell><cell>0.36</cell><cell>29.32</cell><cell>34.54</cell><cell>34.66</cell><cell>34.51</cell><cell>36.28</cell><cell>36.14</cell><cell>36.53</cell></row><row><cell>veni5 015</cell><cell></cell><cell>0.36</cell><cell>27.30</cell><cell>30.89</cell><cell>31.51</cell><cell>31.75</cell><cell>32.45</cell><cell>32.66</cell><cell>32.82</cell></row><row><cell>Average</cell><cell></cell><cell>1.17</cell><cell>25.67/0.726</cell><cell>28.10/0.820</cell><cell>28.82/0.841</cell><cell>29.42/0.867</cell><cell>29.75/0.866</cell><cell>29.96/0.873</cell><cell>30.10/0.874</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Vimeo-90k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell cols="2">Slow</cell><cell>Medium</cell><cell>Fast</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bicubic</cell><cell cols="5">29.33/0.829 31.28/0.867 34.05/0.902</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DBPN [8]</cell><cell cols="5">32.98/0.901 35.39/0.925 37.46/0.944</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TOFLOW [34]</cell><cell cols="5">32.16/0.889 35.02/0.925 37.64/0.942</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">VSR-DUF/6 [16] 32.96/0.909 35.84/0.943 37.49/0.949</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RBPN/3-P</cell><cell cols="5">33.73/0.914 36.66/0.941 39.49/0.955</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RBPN/6-PF</cell><cell cols="5">34.18/0.920 37.28/0.947 40.03/0.960</cell><cell></cell><cell></cell><cell></cell></row><row><cell># of clips</cell><cell cols="2">1,616</cell><cell>4,983</cell><cell>1,225</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Avg. Flow Mag.</cell><cell cols="2">0.6</cell><cell>2.5</cell><cell>8.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Table 9. Quantitative evaluation of state-of-the-art SR algorithms</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">on Vimeo-90k [34] for 4×.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 .</head><label>10</label><figDesc>/0.939 36.96/0.956 40.18/0.969 32.39/0.919 28.43/0.866 DBPN [8] 39.69/0.973 42.33/0.979 45.12/0.984 37.58/0.966 32.30/0.934 Additional quantitative evaluation (PSNR/SSIM) of state-of-the-art SR algorithms. (*the values have been copied from the respective publications.)</figDesc><table><row><cell>24]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 .</head><label>11</label><figDesc>Network size analysis on SPMCS-32 (PSNR/SSIM). SSIM 31.57/0.882 31.64/0.883 Table 12. Residual analysis on SPMCS-32 (PSNR/SSIM).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">RBPN/6</cell><cell></cell></row><row><cell></cell><cell></cell><cell>w/</cell><cell>w/o</cell><cell></cell></row><row><cell cols="2">PSNR/RBPN/4-PF</cell><cell>RBPN/6-PF</cell><cell>VSR-DUF [16]</cell><cell>DRDVSR [30]</cell></row><row><cell>Time (s)</cell><cell>0.058</cell><cell>0.141</cell><cell>0.128</cell><cell>0.108</cell></row><row><cell># of param (M)</cell><cell>12.7</cell><cell>12.7</cell><cell>6.8*</cell><cell>0.7*</cell></row><row><cell># of FLOPS (G)</cell><cell>1650</cell><cell>2475</cell><cell>-</cell><cell>-</cell></row><row><cell>PSNR (dB)</cell><cell>29.75</cell><cell>30.10</cell><cell>29.42</cell><cell>28.82</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Complexity Analysis</head><p>We report computational time, no. of parameter, and no. of FLOPS of our proposal (and competition) in <ref type="table">Table 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Additional Qualitative Results</head><p>Here, we show additional results on several test sets and scaling factors. Figures 12,<ref type="bibr" target="#b12">13</ref>, and 14 show qualitative results for the 4× scaling factor on Vid4 <ref type="bibr" target="#b23">[24]</ref>, SPMCS-32 <ref type="bibr" target="#b29">[30]</ref>, and Vimeo-90k <ref type="bibr" target="#b33">[34]</ref>, respectively. RBPN/6-PF obtains reconstruction that appears most similar to the GT, more pleasing and sharper than reconstructions with other methods. We have highlighted regions in which this is particularly notable.</p><p>We also provide the results on a larger scaling factor (i.e., 8×) in <ref type="figure">Fig. 15</ref>. However, no results were provided by other methods on 8×, so we only compare ours with DBPN and Bicubic. It shows that RBPN/6 successfully generates the best results.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07517</idno>
		<title level="m">pirm challenge on perceptual image super-resolution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unified blind method for multi-image super-resolution and single/multi-image blur deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esmaeil</forename><surname>Faramarzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc P</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2101" to="2114" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Super resolution for multiview images using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camilo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><forename type="middle">L</forename><surname>Dorea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Queiroz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1249" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inception learning super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Rahmat</forename><surname>Widyanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Nobuhara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="6043" to="6048" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image quality metrics: Psnr vs. ssim</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Hore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djemel</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern recognition (icpr), 2010 20th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2366" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional networks for multi-frame superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving resolution by image registration. CVGIP: Graphical models and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="231" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motion analysis for image enhancement: Resolution, occlusion, and transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="324" to="335" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conferene on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video super-resolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="531" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Beyond pixels: exploring new representations and applications for motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A bayesian approach to adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image superresolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW)</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4729</idno>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09078</idno>
		<title level="m">Video enhancement with task-oriented flow</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4584" to="4593" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
