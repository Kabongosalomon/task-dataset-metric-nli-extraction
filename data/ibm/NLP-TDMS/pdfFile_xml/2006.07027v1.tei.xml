<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Toth</surname></persName>
							<email>toth@maths.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Mathematical Institute</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patric</forename><surname>Bonnier</surname></persName>
							<email>bonnier@maths.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Mathematical Institute</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Oberhauser</surname></persName>
							<email>oberhauser@maths.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Mathematical Institute</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sequential data such as time series, video, or text can be challenging to analyse as the ordered structure gives rise to complex dependencies. At the heart of this is non-commutativity, in the sense that reordering the elements of a sequence can completely change its meaning. We use a classical mathematical object -the tensor algebra -to capture such dependencies. To address the innate computational complexity of high degree tensors, we use compositions of low-rank tensor projections. This yields modular and scalable building blocks for neural networks that give state-of-the-art performance on standard benchmarks such as multivariate time series classification and generative models for video. Code available at github.com/tgcsaba/seq2tens.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We bring together three sets of ideas from different communities and use them to build neural networks (NN) for sequential data. None of them are particularly novel on their own but we show that when combined they lead to fast, scalable, and remarkably simple architectures that perform very well on benchmarks. They also enjoy the added benefit that their theoretical properties such as universality can be studied with well-developed tools from algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>Representing order by subsequences. The simplest kind of sequence is a string, that is a sequence of letters. They are determined by two things: (i) what letters appear in it, and (ii) in what order they appear. A classical way to produce a graded description of strings is by counting their non-contiguous sub-strings. These are the so-called k-mers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> which count all the non-contiguous sub-strings of length ≤ k. For example:</p><p>"aabc", 2-mers = {aa, ab, ac, ab, ac, bc}.</p><p>(</p><p>When dealing with sequences of vectors instead of letters, this type of categorization is impractical, but the second idea allows us to apply the same philosophy in this setting as well.</p><p>2. Non-commutative polynomials and the tensor algebra. A non-standard, more algebraic way, to compute the k-mers of strings is to map every letters to a non-commutative indeterminate variable and then parse the string by multiplication to generate a polynomial in non-commutative indeterminates. For example, by substituting a → (1 + X), b → (1 + Y ), c → (1 + Z) we map </p><p>One may now read off all the k-mers from the coefficients of the resulting polynomial, e.g. the term 2XY means that the 2-mer ab appears twice. Many other substitutions of letters to indeterminates are possible but a remarkable fact is that they all lead to the same feature space, namely the tensor algebra T(R d ). Importantly, this algebraic approach generalizes from strings to vector-sequences. We call this method of representing sequences by elements of the tensor algebra Seq2Tens.</p><p>3. Compositions and low-rank approximations. Matrices suffer from quadratic complexity in the dimension, and for tensors of higher degree this is generally much worse. For matrices, one may use low-rank (LR) approximations as a partial remedy, and motivated by this we construct LR approximation to elements of T(R d ). To enhance the efficiency of the LR approximations, we apply iterated compositions of Neural Low-rank Sequence-to-Tensor transformations (NLST). Combined with the above ideas and standard sequence processing techniques, this results in highly tractable neural networks for sequences.</p><p>Related work. Deep nets for sequential data are well-developed topic as evidenced by the success of LSTMs, RNNs, and convolution networks (CNN) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Similarly, LR approximations for matrices are a popular tool <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, and the extension to tensors of higher degree is an ongoing research effort <ref type="bibr" target="#b8">[9]</ref>. Various approaches to LR tensors have already been applied to deep nets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. However, classical LR approaches such as tensor trains, CP, and Tucker format <ref type="bibr" target="#b12">[13]</ref> approximate a tensor of fixed degree. In contrast, our NLST transformation approximates elements of the dual space of T(R d ) -that is, functionals of infinite sequences of tensors -and it accomplishes this by iterating sequence-to-sequence transformations that exploit the sequential structure of the data. More generally, the use of non-commutative algebras such as T(R d ) to describe sequences is a classical topic in mathematics <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Closely related to our approach are so-called path signatures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. They inject a path evolving in continuous time into a subset of T(R d ) and have been used in ML before <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. The main difference is that firstly, we decouple the algebraic sequential non-linearity from the state space non-linearity which allows us to optimize over both separately, and discretized signatures arise as a special case of this, see Appendix B.1. Secondly, even applied to this special case, our NLST layer never needs to compute a signature; instead it directly learns the functional which is much faster than other approaches.</p><p>Outline Section 2 formalizes the main ideas above to construct a feature map Φ that maps sequences in R d to the tensor algebra T(R d ). Section 3 shows how the results of Section 2 can be used to to build modular and scalable neural network layers. Section 4 demonstrates the flexibility and modularity of this approach on both discriminative and generative benchmarks. In the appendices we provide the mathematical background and full proofs for our theoretical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Capturing order via tensor multiplication</head><p>We denote the set of sequences of vectors in a vector space V 1 by</p><formula xml:id="formula_2">Seq(V ) = {x = (x i ) i=1,...,L : x i ∈ V, L ≥ 1}<label>(5)</label></formula><p>where L ≥ 1 is some arbitrary length. Note that Seq(V ) is not a linear space since there is no natural addition of two sequences of different length. Below we construct a feature map Φ : Seq(V ) → T(V ) that represents a sequence as an element of the linear space T(V ). This space is graded by the so-called tensor degree; informally, the higher the degree, the more complicated patterns are captured (analogously to how k-mers capture longer patterns for higher values of k). A function f (x) on sequences can then be described as a linear functional of Φ(x), that is f (x) ≈ , Φ(x) for some ∈ T(V ). For a background on tensors we refer to Appendix A.</p><p>Tensor algebras. Denote by T(V ) the set of sequences of tensors of all degrees,</p><formula xml:id="formula_3">T(V ) := {t = (t m ) m≥0 | t ∈ V ⊗m }<label>(6)</label></formula><p>where by convention V ⊗0 = R. For example, if V = R d and t = (t m ) m≥0 is some element of T(R d ), then its degree 1 component is a d-dimensional vector t 1 , its degree 2 component is a d × d matrix t 2 , and its degree 3 component is a degree 3 tensor t <ref type="bibr" target="#b2">3</ref> . By defining addition and scalar multiplication as s + t := (s m + t m ) m≥0 , c · t = (ct m ) m≥0 <ref type="bibr" target="#b6">(7)</ref> the set T(V ) becomes a linear space. Since each V ⊗m is a linear space, a finite sequence ( m ) M m=0 of tensors, m ∈ V ⊗m , yields a linear functional on T(V ). We use the notation </p><p>In particular, as idea 2 shows k-mers can be encoded on degree k-tensors.</p><p>Feature maps for sequences. We now produce a feature map Φ that represent a sequence x in V as an element Φ(x) of the linear space T(V ). Key to this is that T(V ) is not just a linear space but also carries a non-commutative product, namely the so-called tensor convolution product defined as</p><formula xml:id="formula_5">s · t := m i=0 s i ⊗ t m−i m≥0 = 1, s 1 + t 1 , s 2 + s 1 ⊗ t 1 + t 2 , . . . .<label>(10)</label></formula><p>We emphasize that is product is non-commutative, i.e. s · t = t · s and in a mathematically precise sense, this is the most general non-commutative product, see Appendix A. We now use this noncommutativity to capture the ordered structure in a sequence.</p><formula xml:id="formula_6">Definition 2.2. Define Φ : Seq(V ) → T(V ), x → L i=1 (1 + x i )<label>(11)</label></formula><p>where 1 = (1, 0, 0 ⊗2 , 0 ⊗3 , . . .) ∈ T(V ) and x i is identified as (0, x i , 0 ⊗2 , 0 ⊗3 , . . .) ∈ T(V ).</p><p>A direct calculation shows that for x = (x 1 , . . . , x L ), Φ(x) = (Φ m (x)) m≥0 is given explicitly as</p><formula xml:id="formula_7">Φ m (x) = 1≤i1&lt;···&lt;im≤L x i1 ⊗ · · · ⊗ x im ∈ V ⊗m .<label>(12)</label></formula><p>The sum is taken over all non-contiguous subsequences of x of length m in analogy to the k-mers of strings. However, instead of just counting occurrences of letters, we now represent a subsequence as an element of V ⊗m .</p><p>It is possible to replace the terms (1 + x i ) in the definition of Φ with other embeddings of x i into T(V ); we discuss this in detail in Appendix B but below we will use <ref type="bibr" target="#b10">(11)</ref> as it is the simplest and it also generalizes k-mers. The essential part of Definition 2.2 is that the product i is the non-commutative convolution tensor product as defined in <ref type="bibr" target="#b9">(10)</ref> . This methodology of representing sequences by elements of non-commutative tensor algebras is what we call Seq2Tens.</p><p>Universality. A function ϕ : V → W between two vector spaces is said to be universal if all continuous functions on V can be approximated as linear functions on the image of ϕ. One of the most powerful features of neural nets is their universality <ref type="bibr" target="#b24">[25]</ref>. A very attractive property of Φ is that if one has a universal map on the state space V , then by composing it with Φ one gets a universal map on sequences in V . The following statement and proof is made precise in Appendix B.1. Theorem 2.3. Let ϕ : V → W be a universal map that satisfies some mild constraints, then the map</p><formula xml:id="formula_8">Seq(V ) → T(W ), x → Φ(ϕ(x), . . . , ϕ(x n ))<label>(13)</label></formula><p>is universal.</p><p>Low-rank tensors. If V is d-dimensional, the d m real numbers required to describe and store an element of V ⊗m quickly become too costly. This is a problem even for degree 2 tensors, that is matrices. To address the d 2 complexity, low-rank approximations are widely used in practice <ref type="bibr" target="#b25">[26]</ref>. The definition below generalizes the rank of matrices to tensors of any degree m. Definition 2.4. The rank of a degree m tensor t ∈ V ⊗m is the smallest number r ≥ 0 such that one may write</p><formula xml:id="formula_9">t = r i=0 t 1 i ⊗ · · · ⊗ t m i , t 1 i , . . . , t m i ∈ V.<label>(14)</label></formula><p>Definition 2.4 is also known as the outer-product rank.</p><p>Low-rank tensor functionals. A simple but significant observation is that for a rank-1 tensor = z 1 ⊗ · · · ⊗ z m ∈ V ⊗m one may compute , Φ(x) very efficiently without computing all of</p><formula xml:id="formula_10">Φ(x) = (Φ m (x)) m≥0 ∈ T(V ) since a direct calculation (see Appendix B.1.1 and D.2) shows that , Φ(x) = 1≤i1&lt;···&lt;im≤L m k=1 z k , x i k .<label>(15)</label></formula><p>Theorem 3.1 below proves compositions of LR functionals approximate full rank functionals efficiently. This combined with <ref type="bibr" target="#b14">(15)</ref> explains the efficiency of the NLST layer that we introduce below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Building models with deep low-rank functionals</head><p>Computing the map Φ in its entirety is usually infeasible as it suffers tremendously from the curse of dimensionality. In fact, even if one only computes the tensor series up to degree M , one would need O(d M ) coordinates to store all tensors, which is too expensive for most applications. Thankfully Equation <ref type="bibr" target="#b14">(15)</ref> shows that low-rank functionals can be very quickly computed. We now use this to build the core layer of our Seq2Tens framework, the (stacked) NLST layer.</p><p>The NLST layer. As Φ applies to sequences of any length, we may use it to map the original sequence to another sequence in feature space</p><formula xml:id="formula_11">Seq(V ) → Seq(T(V )), (x 1 , x 2 , . . . , x L ) → Φ(x 1 ), Φ(x 1 , x 2 ), . . . , Φ(x 1 , . . . , x L ) .<label>(16)</label></formula><p>As pointed out above, this is very expensive computationally. If instead we are given a linear functional , then by using Equation <ref type="formula" target="#formula_2">(15)</ref>, computing the sequence-to-sequence transformation</p><formula xml:id="formula_12">x → , Φ(x 1 ) , . . . , , Φ(x 1 , . . . , x L )<label>(17)</label></formula><p>is no more expensive than computing , Φ(x) . We can also do this in parallel, that is, given n different low-rank functionals ( j ) n j=1 , the NLST layer is defined as the map</p><formula xml:id="formula_13">Φ θ : Seq(V ) → Seq(R n ), x → ( 1 , Φ(x 1 , . . . , x i ) , . . . , n , Φ(x 1 , . . . , x i ) ) L i=1 .<label>(18)</label></formula><p>We refer to θ = ( j ) n j=1 as the set of weights of the NLST layer. In analogy with NNs, n is called the width of the layerΦ θ and the maximal degree of 1 , . . . , n is its order. In Appendices D.2 D.3, we show thatΦ θ can be regarded as a certain RNN on an infinite dimensional space, but more efficient.</p><p>Stacked NLST layers. The universality of Φ, Theorem 2.3, guarantees that any real-valued function f (x) of the original sequence x can be approximated by a linear functional , Φ(x) of Φ(x). However, the linear functional might need to be of very high degree which makes it costly, unstable, and prone to overfitting. We now leverage idea 3, namely that compositions of simple objects can give efficient and robust approximations to complex objects. Theorem 3.1 makes this precise in our context of sequences and tensor algebra functionals; the proof requires delicate concepts from non-commutative algebra and is given in Appendix C where we also give more examples. Theorem 3.1. Denote by Φ 2 the map <ref type="bibr" target="#b15">(16)</ref>. If 1 is a degree 2 functional there exist some degree m ≥ 2 functional 2 , where m depends on 1 , such that Motivated by Theorem 3.1 and the empirical successes of stacked RNNs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, we can take a composition of several NLST layers, each parametrized by a different set of weights θ 1 , . . . , θ D and iterate equation (18) D times. In practice, this amounts to choosing n i different functionals i 1 , . . . , i ni at every stage i = 1, . . . , D and yields a sequence of maps</p><formula xml:id="formula_14">Seq(V ) → Seq(R n1 ) → Seq(R n2 ) → · · · → Seq(R n D ).<label>(20)</label></formula><p>Taking the last observation we are left with a map</p><formula xml:id="formula_15">ϕ θ1,...,θ D : Seq(V ) → R n D<label>(21)</label></formula><p>which, by Theorem 3.1 can cheaply approximate high degree functionals on Φ. We refer to ϕ θ1,...,θ D as the D-times stacked NLST layer.</p><p>Bidirectional NLST layer. The transformation in equation <ref type="formula" target="#formula_13">(18)</ref> is completely causal. That is, each step of the output sequence depends only on past information. For generative models it behoves us to make the output depend on both the past and the future information <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Similarly to bidirectional RNNs and LSTMs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, we may achieve this by setting</p><formula xml:id="formula_16">Φ bi (x) : Seq(V ) → Seq(R n+n ), x → (Φ θ1 (x 1 , . . . , x i ),Φ θ2 (x i , . . . , x L )) L i=1 .<label>(22)</label></formula><p>The sequential nature is kept intact by making the distinction between what classifies as past information (the first n coordinates), and what classifies as future information (the last n coordinates). This amounts to having a form of precognition in the model, and has been applied in e.g. dynamics generation <ref type="bibr" target="#b30">[31]</ref>, machine translation <ref type="bibr" target="#b33">[34]</ref>, and speech processing <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Sequence preprocessing layers. We combine the discussed layers with standard preprocessing techniques for sequences, such as:</p><formula xml:id="formula_17">(i) State-space non-linearities: (x i ) L i=1 → (ϕ(x i )) L i=1 , where ϕ : V → W is some non-linear function. (ii)</formula><p>Delay embeddings: For a given number of lags l ≥ 1,</p><formula xml:id="formula_18">x = (x i ) → (x l i ) L i=1 where x l i := (x i , x i−1 , . . . , x i−l )</formula><p>. That is, we consider the V l -valued process x along with its history up to time i − l. This is motivated by Takens' Theorem <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> and ensures that a high-dimensional dynamical system can be reconstructed from low-dimensional observations. (iii) Coordinate embeddings: x → (i, x i ) allows the subsequent layer to learn location specific behaviour <ref type="bibr" target="#b36">[37]</ref>. (iv) Taking differences:</p><formula xml:id="formula_19">x → (x i+1 − x i ) L i=1</formula><p>can increase the stationarity of the sequence <ref type="bibr" target="#b37">[38]</ref>, and the stability of cumulative algorithms such as <ref type="bibr" target="#b11">(12)</ref>. We give more intuition about the preprocessing layers and their usefulness in Appendix D.1. Theorem 2.3 guarantees that if ϕ is a universal feature map on the state-space then x → Φ(ϕ(x 1 ), . . . , ϕ(x L )) is universal. In the sequel we take ϕ to be a multilayer perceptron (MLP), that is, ϕ = ϕ D • · · · • ϕ 1 with ϕ j (x) = σ(W j x + b j ). By combining the dense layers ϕ 1 , . . . , ϕ D with delay embeddings (lags), we get a temporal convolution layer, i.e. ϕ j (x l i ) = σ l k=0 W j,k x i−k + b j , this motivates the use of convolutions in the preprocessing layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We demonstrate the modularity and flexibility of the above NLST layer and its variants by applying it to (i) multivariate time series classification, (ii) generative modelling of sequential data. In both cases, we take a strong baseline model (FCN and GP-VAE as detailed below) and perform a simple modification of the architecture by adding our layers. The additional computation time is negligible (in fact, for FCN it allows us to reduce the number of parameters significantly), but it can yield substantial improvements. This is remarkable, since the results of the original models are often already state-of-the-art on well-established and popular (frequentist and Bayesian) benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multivariate time series classification</head><p>We consider the supervised learning problem of multivariate time series classification (TSC). Many models have been proposed for this, and together they make up a semi-standardized benchmark dataset. A wide range of previous publications report test set results on the archive <ref type="bibr" target="#b38">[39]</ref>, which makes it possible to compare against several baseline methods without bias in parameter setting. We provide further details on the problem formulation, baselines and datasets in Appendix E.1.  <ref type="figure">Figure 2</ref>: Box-plot of classification accuracies (left) and critical difference diagram (right).</p><p>We introduce two very simple architectures that utilize NLST layers: (i) SNLST stacks 3 NLST layers, each of order 2 and width 64; (ii) FCN-SNLST precedes the stacked NLST block by a CNN block of 3 convolutional layers of width 64 and filter sizes of 8, 5, 3; a downsized version of FCN <ref type="bibr" target="#b39">[40]</ref> with a time-embedding preceding each convolutional layer <ref type="bibr" target="#b36">[37]</ref> and without a final global average pooling (GAP) layer, see <ref type="figure">Figure 1</ref>. See Appendix E.1 for implementation details.</p><p>Benchmark results. We trained the introduced models on each dataset 5 times while the baseline results were borrowed from their respective publications 2 . <ref type="figure">Figure 2</ref> depicts a comparison of the results as a box-plot of distributions and a critical difference diagram of mean ranks using the Nemenyi test <ref type="bibr" target="#b43">[44]</ref>. The only significant differences are between DTW↔MLSTMFCN and DTW↔FCN-SNLST based on the mean-ranks test. <ref type="table" target="#tab_7">Table 6</ref> in Appendix E.1 shows the full list of means and standard deviations. Since mean-ranks based tests raise some paradoxical issues <ref type="bibr" target="#b44">[45]</ref>, their validity is questionable and it is customary to conduct pairwise comparisons using frequentist <ref type="bibr" target="#b45">[46]</ref> or Bayesian <ref type="bibr" target="#b46">[47]</ref> hypothesis tests. We adopted 3 the Bayesian signed-rank test approach from <ref type="bibr" target="#b47">[48]</ref>, which is depicted in <ref type="table" target="#tab_1">Table 1</ref>, while visualizations of the Bayesian posteriors are on <ref type="figure" target="#fig_4">Figure 5</ref> in Appendix E.1. The results of the signed-rank test indicate that while SNLST is only better with moderate probability (p ≥ 0.6) than 4 of the baselines, FCN-SNLST performs better than most of them with high probability (p ≥ 0.75), except for MLSTMFCN with which they are close to being equivalent. Note however that as MLSTMFCN is effectively the concatenation of an FCN block with an LSTM layer <ref type="bibr" target="#b41">[42]</ref>, the computations can be bottlenecked for very long sequences, which is not the case for our method that can be implemented very efficiently, see Appendices D.3, D.4.</p><p>In conclusion, we have verified that SNLST already performs well on some classification tasks, and by preceding it with a CNN, its performance is elevated to being comparable or better than the stateof-the-art on the considered problems. Since the CNN block that precedes the FCN-SNLST layer is effectively a downsized version of the FCN model, FCN-SNLST can be thought of as a smaller  <ref type="bibr" target="#b48">[49]</ref> 0.369 0.000 0.631 0.005 0.000 0.995 LPS <ref type="bibr" target="#b49">[50]</ref> 0.477 0.002 0.520 0.001 0.000 0.999 mvARF <ref type="bibr" target="#b50">[51]</ref> 0.021 0.168 0.811 0.000 0.089 0.911 DTW <ref type="bibr" target="#b51">[52]</ref> 0.086 0.000 0.914 0.000 0.000 1.000 ARKernel <ref type="bibr" target="#b52">[53]</ref> 0 FCN, but with the final GAP layer replaced by our SNLST block. By this reasoning, we effectively upgraded the performance of an already high-performing model while simultaneously reducing its total number of parameters by about a factor of 3, see <ref type="table" target="#tab_6">Table 5</ref> in Appendix E.1. This strengthens our hypothesis that the NLST layers may very well find their best use as being part of larger models.</p><formula xml:id="formula_20">Model SNLST FCN-SNLST p(R &gt; C) p(R = C) p(R &lt; C) p(R &gt; C) p(R = C) p(R &lt; C) SNLST − − − 0.000 0.017 0.983 SMTS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generating sequential data</head><p>We now apply the Seq2Tens approach to build a Bayesian generative model and benchmark it on sequential data imputation for time series and video data.</p><p>The GP-VAE model. The base model is the recent GP-VAE <ref type="bibr" target="#b54">[55]</ref> that provides state-of-the-art results for probabilistic sequential data imputation. The GP-VAE is essentially based on the HI-VAE <ref type="bibr" target="#b55">[56]</ref> for handling missing data in variational autoencoders (VAEs) <ref type="bibr" target="#b56">[57]</ref> adapted to the handling of time series data by the use of a Gaussian process (GP) prior <ref type="bibr" target="#b57">[58]</ref> across time in the latent sequence space to capture temporal dynamics. The GP-VAE assumes that x ∈ Seq(R d ) is generated as</p><formula xml:id="formula_21">p θ (x i | z i ) = N (x i | g θ (z i ), σ 2 I d ),<label>(23)</label></formula><p>where z ∈ Seq(R d ) denotes a latent process and g θ : R d → R d is the time-point-wise decoder network parametrized by θ. The temporal interdependencies are modelled in the latent space by assigning independent GP priors to the coordinate processes of z, i.e. denoting z i = (z j i ) d j=1 ∈ R d , it is assumed that z j ∼ GP(m(·), k(·, ·)), where m and k are the mean and covariance functions. As usual in VAEs, exact Bayesian inference is intractable and free-form variational approximations are inefficient. Hence they apply amortized variational inference <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref> to fit a Gaussian approximation to the posterior time-marginals of z, the means and covariances of which are represented by the so-called encoder network. This allows one to efficiently fit a model to several examples jointly at training time, and make inference about unseen examples at testing time without any optimization overhead. Missing data is handled by imputing the missing values with 0 before feeding them into the encoder network, while the ELBO loss is only computed across the observed features during training similarly to the HI-VAE solution <ref type="bibr" target="#b55">[56]</ref>. See Appendix E.2 and <ref type="bibr" target="#b54">[55]</ref> for further details.</p><p>GP-VAE + B-NLST. We make one simple change to the GP-VAE architecture without changing any other hyperparameters: we introduce a single bidirectional NLST layer (B-NLST) into the encoder network and use it in the amortized representation of the means and structured covariances of the variational posterior. As before, the B-NLST layer is preceded by a time-embedding and differencing block, and succeeded by channel flattening and normalization 4 as depicted in <ref type="figure" target="#fig_2">Figure 3</ref>.  <ref type="bibr" target="#b56">[57]</ref> 0.599 ± 0.002 0.232 ± 0.000 0.922 ± 0.000 0.028 ± 0.000 0.677 ± 0.002 HI-VAE <ref type="bibr" target="#b55">[56]</ref> 0.372 ± 0.008 0.134 ± 0.003 0.962 ± 0.001 0.007 ± 0.000 0.686 ± 0.010 GP-VAE <ref type="bibr" target="#b54">[55]</ref> 0.350 ± 0.007 0.114 ± 0.002 0.960 ± 0.002 0.002 ± 0.000 0.730 ± 0.006 GP-VAE (B-NLST) 0.251 ± 0.008 0.092 ± 0.003 0.962 ± 0.001 0.002 ± 0.000 0.743 ± 0.007</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BRITS [61]</head><p>----0.742 ± 0.008 Benchmark results. To make the comparison, we ceteris paribus re-ran all experiments the authors originally included in their paper <ref type="bibr" target="#b54">[55]</ref>, which are imputation of Healing MNIST, Sprites, and Physionet 2012. The results are in <ref type="table" target="#tab_3">Table 2</ref>, which report the same metrics as used in <ref type="bibr" target="#b54">[55]</ref>, i.e. negative log-likelihood (NLL, lower is better), mean squared error (MSE, lower is better) on test sets, and downstream classification performance of a linear classifier (AUROC, higher is better). For all other models beside our GP-VAE (B-NLST), the results were borrowed from <ref type="bibr" target="#b54">[55]</ref>. We observe that simply adding the B-NLST layer improved the result in almost all cases, except for Sprites, where the GP-VAE already achieved a very low MSE score. Additionally, when comparing GP-VAE to BRITS on Physionet, the authors argue that although the BRITS achieves a higher AUROC score, the GP-VAE should not be disregarded as it fits a generative model to the data that enjoys the usual Bayesian benefits of predicting distributions instead of point predictions. Thus, we have shown that by simply adding our layer into the architecture, we managed to elevate the performance of GP-VAE to the same level while retaining these same benefits. We believe the reason for the improvement is a tighter amortization gap in the variational approximation <ref type="bibr" target="#b61">[62]</ref>, that is achieved by increasing the expressiveness of the encoder. For further details and discussion, see Appendix E.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>We used a classical non-commutative structure to construct a feature map for sequential data by translating temporal dependencies to algebraic relations. Compositions of low-rank functionals allowed us to build scalable "tensorized" versions of well-known models. We would like to emphasize two points: (i) Although our analysis in the appendices applies advanced ideas from algebra, this theoretical background is not needed for practitioners who want to apply the Seq2Tens approach in their models. (ii) Our experiments indicate that simple applications of such layers in popular architectures yields improvements on competitive baselines for both discriminative and generative models. Finally, as both theory and empirical evidence have shown strong results in favour of our approach, this suggests that when used as a modular building block, it can improve the performance of a wide array of models in a computationally efficient manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>At the heart of this work, there is a novel and theoretically motivated methodology for parsing sequential data in general, for which we believe a Broader Impact statement is not applicable. It could, however, lead to the better understanding of such data types and to the design of more efficient or expressive data analysis methodologies. As an application of the approach, we introduced a scalable building block that can be used efficiently e.g. in the context of deep learning models. The possible direct consequences of this are difficult to pinpoint, since the method is no more specific then variants of deep sequence models, which are applied to wide variety of tasks all over the world. The question then becomes, what are those tasks that the introduced method could be better suited to than alternatives? The experiments do suggest that it allows one to build more compactly parametrized and expressive models, which in turn could lead to the development of machine learning models being able to assimilate larger amounts of data for given amounts of computational resources.</p><p>The previous could benefit applications and industries working with big data, such as technology, finance, healthcare, governments, advertisement, etc. Obviously, every technological advancement carries with it both potential positive and negative aspects. For instance, positive impacts such as: (i) classification and analysis of medical data can help in analysing patients records and alleviate pressure from medical professionals, (ii) more informed and data assisted decision making on the part of governments, companies and individuals, (iii) an increased sense of security due to machine assisted surveillance systems. On the other hand, downsides may include negative aspects such as (iv) generative models for audio data can be used for nefarious actions in e.g. deepfakes, (v) loss of virtual anonymity due to the assimilation of individuals' virtual footprint, such as data collected by technological conglomerates, (vi) loss of privacy due to machine assisted surveillance systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How to use this appendix</head><p>For practitioners, we recommend a look at Section A for a refresher on tensor notation and Sections D and E for experiment and implementation details. In particular, we draw attention to D. <ref type="bibr" target="#b5">6</ref> that summarizes what we believe to be our main contribution in non-technical terms, namely how the expensive computation of Φ can be avoided by compositions of formula <ref type="bibr" target="#b14">(15)</ref>. This is all you need to apply Seq2Tens in practice! For theoreticians, we recommend Section B for a proof that Φ is universal and Section C where we use the formula <ref type="bibr" target="#b14">(15)</ref> to prove our main theoretical result, which is a formal proof of the effectiveness of the stacked NLST layer. We reemphasize that these theoretical sections are not needed for practitioners but we hope that they convince some readers that classic tools from non-commutative algebra can lead to insight and new models in ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Tensors and the Tensor Algebra</head><p>This section recalls some basics on the tensor product ⊗, the linear space T(V ), and the convolution product that turns T(V ) into an algebra -the so-called tensor algebra. We refer to <ref type="bibr" target="#b62">[63,</ref><ref type="bibr">Chapter 16]</ref> for more on tensors, to <ref type="bibr" target="#b14">[15]</ref> for a detailed study of tensor algebra, and to <ref type="bibr" target="#b63">[64]</ref> for applications of tensors in statistics.</p><p>Tensor products on R d . If x = (x 1 , . . . , x d ) ∈ R d and y = (y 1 , . . . , y e ) ∈ R e are two vectors, then their tensor product x ⊗ y is defined as the (d × e)-matrix, or degree 2 tensor, with entries (x ⊗ y) i,j = x i y j . This is also commonly called the outer product of the two vectors. The space</p><formula xml:id="formula_22">R d ⊗ R e is defined as the linear span of all degree 2 tensors x ⊗ y for x ∈ R d , y ∈ R e . If z ∈ R f is another vector, then one may form a degree 3 tensor x ⊗ y ⊗ z with shape (d × e × f ) defined to have entries (x ⊗ y ⊗ z) i,j,k = x i y j z k . The space R d ⊗ R e ⊗ R f is analogously defined as the linear span of all degree 3 tensors x ⊗ y ⊗ z for x ∈ R d , y ∈ R e , z ∈ R f .</formula><p>The tensor product of two general vector spaces V and W can be defined even if they are infinite dimensional, see <ref type="bibr" target="#b62">[63,</ref><ref type="bibr">Chapter 16</ref>], but we invite readers unfamiliar with general tensor spaces to think of V as R d below.</p><p>The tensor algebra T(V ). Ultimately we are not only interested in tensors of some fixed degree m -that is an element of V ⊗m -but sequences of tensors of increasing degree. Given some linear space V , the linear space T(V ) is defined as set of all tensors of any degree over V . Formally</p><formula xml:id="formula_23">T(V ) := m≥0 V ⊗m = (V ⊗0 , V, V ⊗2 , V ⊗3 , . . .)<label>(24)</label></formula><p>where we use the notation</p><formula xml:id="formula_24">V ⊗2 = V ⊗ V, V ⊗3 = V ⊗ V ⊗ V and so on, by convention we let V ⊗0 = R. We normally write elements of T(V ) as t = (t 0 , t 1 , t 2 , t 3 , . . .) such that t m ∈ V ⊗m ,</formula><p>that is,t 0 is a scalar, t 1 is a vector, t 2 is a matrix, t 3 is a 3-tensor and so on. Note that T(V ) is again a linear space if we define addition and scalar multiplication as</p><formula xml:id="formula_25">s + t = (s m + t m ) m≥0 ∈ T(V ) and c · t = (ct m ) m≥0 ∈ T(V ) (25) for s, t ∈ T(V ) and c ∈ R. Example A.1. Let V = R d . For x = (x i ) i=1,...,d ∈ R d consider t = (x ⊗m ) m≥0 ∈ T(R d ) where</formula><p>we denote for brevity t m := x ⊗m :=</p><p>x ⊗ · · · ⊗ x m many tensor products ⊗ ∈ (R d ) ⊗m and by convention we set</p><formula xml:id="formula_26">x ⊗0 := 1 ∈ (R d ) ⊗0 .</formula><p>That is,</p><formula xml:id="formula_27">t 1 = x ⊗1 is a d-dimensional vector, with the i coordinate equal to x i ; t 2 = x ⊗2 is d × d- matrix with the (i, j)-coordinate equal to x i x j ; t 3 = x ⊗3 is degree 3-tensor with the (i, j, k)- coordinate equal to x i x j x k .</formula><p>In this special case, the element t ∈ T(R d ) consists of entries t m = x ⊗m ∈ (R d ) ⊗m that are symmetric tensors, that is the (i 1 , . . . , i m )-th coordinate is the same as the (i σ(1) , . . . , i σ(d) ) coordinate if σ is a permutation of {1, . . . , d}. However, we emphasize that in general an element of T(R d ) does not need to be made up of symmetric tensors.</p><p>A non-commutative product on T(V ). Key to our approach is that T(V ) is not only a linear space, but what distinguishes it as a feature space for sequences is that it carries a non-commutative product. In other words, T(V ) is not just a vector space but a (non-commutative) algebra (an algebra is a vector space where one can multiply elements). This is the so-called tensor convolution product and defined as follows</p><formula xml:id="formula_28">s · t := m i=0 s i ⊗ t m−i m≥0 = 1, s 1 + t 1 , s 2 + s 1 ⊗ t 1 + t 2 , . . . .<label>(26)</label></formula><p>In a precise mathematical sense, T(V ) is the most general algebra containing V , see <ref type="bibr" target="#b62">[63,</ref><ref type="bibr">Chapter 16]</ref> Linear functionals of T(V ). In the main text, we construct a feature map Φ that injects a sequence x into a tensor algebra T(V ). Hence, to learn a real-valued function of a sequence f (x) ∈ R we learn a linear functional of Φ(x) to approximate</p><formula xml:id="formula_29">f (x) ≈ , Φ(x) .</formula><p>where is in the dual space of T(V ).</p><p>Example A.2. As a concrete example, applied to V = R d and denoting for brevity</p><formula xml:id="formula_30">t = Φ(x) ∈ T(R d ) we have , t = m≥0 m , t m .</formula><p>In coordinates,</p><formula xml:id="formula_31">0 , t 0 = 0 t 0 is the product of 0 , t 0 ∈ R, and 1 , t 1 = d i=1 i 1 t i 1 is the normal inner product of a vector 1 ∈ R d with the vector t 1 ∈ R d . Similarly, 2 , t 2 = d i,j=1 i,j 2 t i,j m is the coordinate-wise contraction of the (d×d)-matrix 2 with the (d×d)-matrix t 2 , etc. Equivalently, we can write , t = i1,...,im∈{1,...,d},m≥0 i1,...,im m t i1,...,im m B A universal feature map for sequences Seq2Tens in a nutshell. Given a sequence x = (x 1 , . . . , x L ) ∈ Seq(R d ) we build a feature map Φ : Seq(V ) → T(R d ) as follows (i) inject each sequence entry x i ∈ R d into T(R d ) by some map x i → ϕ(x i ),</formula><p>(ii) use the non-commutative product <ref type="bibr" target="#b25">(26)</ref> to form an element of T(R d ),</p><formula xml:id="formula_32">Φ(x) = ϕ(x 1 ) · · · ϕ(x L ) ∈ T(R d ).</formula><p>Why is this is a potentially good idea? Firstly, if ϕ is injective, then there is no entry-wise loss of information in step (i). Secondly, step (ii) uses "the most general product" to multiply ϕ(x 1 ) · · · ϕ(x L ) so it is intuitive that there is no loss of information in the map x → Φ(x) that acts on the whole sequence. In other words x → Φ(x) is an injection.</p><p>What have we gained? We have already seen in the main text, Idea 2 and Example 2.1, that a special case of this construction applied with ϕ(x i ) = 1 + x i are k-mers, and the number of k-mers can be read off the kth level (R d ) ⊗k of our feature space T(R d ). Hence, the grading of T(R d ) in terms of tensor degree provides us with a description of the global structure of the sequence that gets increasingly richer as the tensor degree k gets higher. This is analogous to how the k-mers reveal more about the global structure of a string as k increases. Below we make all this precise, but add one more aspect: besides constructing an injection Φ of our data (sequences of arbitrary length) into a linear space (the tensor algebra T(R d )), it is also desirable that the feature map Φ contains enough non-linearities that non-linear functions of our data x ∈ Seq(R d ) become linear functions of our features Φ(x). This property is called universality. The remainder of Section B make this statement mathematically rigorous, in the the sense that Theorem B.3 shows that Φ is a universal feature map for sequences in R d whenever ϕ is a universal feature map for vectors in R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 The universality of Φ.</head><p>Definition B.1. Let X be a topological space (the "data space") and W a linear space ("the feature space"). We say that a function f :</p><formula xml:id="formula_33">X → W is universal (to C b (X )) if the the set of functions {x → , f (x) : ∈ W } ⊆ C b (X ) (27) is dense in C b (X ). Example B.2. Classic examples of this in ML are • For X ⊂ R d bounded and W = T(R d ), the polynomial map p : R d → T(R d ), x → (1, x, x ⊗2 , x ⊗3 , x ⊗4 , . . .) is universal [65]. • The 1-layer neural net map x → θ N θ (x)</formula><p>where θ runs over all configurations of parameters is universal under some very mild conditions <ref type="bibr" target="#b24">[25]</ref>.</p><p>We now prove the main result of this section</p><formula xml:id="formula_34">Theorem B.3. Let ϕ : X → T(V ), x → (ϕ m (x)) m≥0 , ϕ m (x) ∈ V ⊗m be such that: 1. For any n ≥ 1 the support of (ϕ 0 , ϕ 1 , . . . , ϕ m1 ) ⊗n and ϕ m2 are disjoint if 1 ≤ m 1 &lt; m 2 .</formula><p>2. ϕ 0 = 1 and ϕ 1 : X → V is a bounded universal map with at least one constant term.</p><p>Then</p><formula xml:id="formula_35">Φ : Seq(X ) → T(V ), (x 1 , . . . , x L ) → L i=1 ϕ(x i )<label>(28)</label></formula><p>is universal. </p><formula xml:id="formula_36">(iv) TheoremB.3 implies that x → L i=1 (1, ϕ 1 (x), 0, . . .) is universal whenever ϕ 1 : X → V is universal.</formula><p>(v) By taking each ϕ m to be a trainable Neural Network one gets a trainable universal map Φ for sequences that includes all of the above,</p><p>We emphasize that the feature map <ref type="formula" target="#formula_35">(28)</ref> is very general since different choices of ϕ yield very different algebraic structures; e.g. for signature as in (ii) Φ takes values in the character group of the shuffle algebra <ref type="bibr" target="#b14">[15]</ref>; but even for (iii) the resulting structure is very different and was only recently studied, see <ref type="bibr" target="#b67">[68]</ref>. (iv) allows one to use a NN for ϕ 1 to learn the best structure implicitly from the data. Despite this generality, Theorem B.3 guarantees that the resulting map Φ will be a universal feature map for sequences.</p><formula xml:id="formula_37">B.1.1</formula><p>The algebra of linear functionals on Φ.</p><p>The proof of Theorem B.3 uses the fact that if ϕ is universal, then the space of linear functionals on Φ(x) forms a commutative algebra, that is for two linear functionals 1 , 2 there exists another linear functional such that By the assumption in Theorem B.3 we may assume without loss of generality that ϕ is of the form 1 + ϕ 1 ∈ (V ⊗0 , V, . . .), and for the rest of this subsection we will assume for convenience that ϕ = id since this does not change the algebraic structure in any way. That is, we consider the map</p><formula xml:id="formula_38">Seq(V ) → T(V ), Φ(x 1 , . . . , x L ) := L i=1 1 + x i .<label>(30)</label></formula><p>Since Φ takes values in the linear space T(V ), we may pair it with linear functions ∈ T(V ) . By expanding equation 30 we see that</p><formula xml:id="formula_39">Φ(x 1 , . . . , x L ) = 1 + L i=1 x i V + 1≤i1&lt;i2≤L x i1 ⊗ x i2 V ⊗2 + 1≤i1&lt;i2&lt;i3≤L x i1 ⊗ x i2 ⊗ x i3 V ⊗3 + · · · (31) In general, writing Φ m (x) for the projection of Φ(x) onto V ⊗m , we have Φ m (x) = 1≤i1&lt;···&lt;im≤L x i1 ⊗ · · · ⊗ x im .<label>(32)</label></formula><formula xml:id="formula_40">So if = 1 ⊗ · · · ⊗ m is a rank 1 functional acting on Φ, then , Φ m (x) = 1 ⊗ · · · ⊗ m , 1≤i1&lt;···&lt;im≤L x i1 ⊗ · · · ⊗ x im (33) = 1≤i1&lt;···&lt;im≤L 1 ⊗ · · · ⊗ m , x i1 ⊗ · · · ⊗ x im = 1≤i1&lt;···&lt;im≤L 1 , x i1 · · · m , x im . (34)</formula><p>Hence , Φ m (x) can be computed efficiently without computing Φ(x).</p><p>Non-linear functionals acting on Φ. We now investigate what happens when one applies nonlinear functions to Φ(x). To do this, we first note that since T(V ) is a vector space, we may form the tensor algebra over T(V ), denoted by T(T(V )), or T 2 (V ). It may be decomposed as the following multi-graded space</p><formula xml:id="formula_41">T 2 (V ) = n1,...,n k ≥0 V ⊗n1 · · · V ⊗n k<label>(35)</label></formula><p>where we use the notation ⊗ for the tensor product on V and the bar | for the tensor product on T(V ). See <ref type="bibr" target="#b68">[69]</ref> for more on T 2 (V ) and the bar notation. Definition B.5. If x ∈ V is a vector, we denote by x its extension</p><formula xml:id="formula_42">x := (x ⊗m ) m≥0 = (1, x, x ⊗2 , x ⊗3 , . . .)<label>(36)</label></formula><p>and if x = (x 1 , . . . , x L ) ∈ Seq(V ) is a sequence, then</p><formula xml:id="formula_43">x := (x 1 , . . . , x L ) ∈ Seq(T(V ))<label>(37)</label></formula><p>Since x is a sequence in T(V ), we may compute Φ(x ) which then takes values in T(T(V )) = T 2 (V ).</p><p>The reason for the above definition is that when products of linear functions in T(V ) act on Φ(x), they may be described as linear functions in T 2 (V ) acting on Φ(x ). That is, T(V ) is not big enough to capture all non-linear functions acting on Φ(x), but T 2 (V ) is.</p><p>Note that since x takes values in T(V ), rank 1 linear functionals on Φ(x ) can be written as e i1 | · · · |e in . Definition B.6. Assume that V has basis e 1 , . . . , e d . The quasi-shuffle product</p><formula xml:id="formula_44">: T 2 (V ) × T 2 (V ) → T 2 (V )<label>(38)</label></formula><p>is defined inductively on rank 1 tensors 1 = e i1 | · · · |e im , 2 = e j1 | · · · |e jn by</p><formula xml:id="formula_45">( 1 |e i ) ( 2 |e j ) = ( 1 |e i 2 )|e j + ( 1 2 |e j )|e i + ( 1 2 )|(e i ⊗ e j ).<label>(39)</label></formula><p>By linearity extends to a product on all of T(V ).</p><p>Example B.7.</p><p>e i e j = e i |e j + e j |e i + e i ⊗ e j <ref type="bibr" target="#b39">(40)</ref> e i (e j |e k ) = e i |e j |e k + e j |e i |e k + e j |e k |e i (41)</p><formula xml:id="formula_46">+ (e i ⊗ e j )|e k + e j |(e i ⊗ e k )<label>(42)</label></formula><p>(e i |e j ) (e k |e l ) = e i |e j |e k |e l + e i |e k |e j |e l + e i |e k |e l |e j (43) + e k |e i |e j |e l + e k |e i |e l |e j + e k |e l |e i |e j (44) + (e i ⊗ e k )|e j |e l + e i |(e j ⊗ e k )|e l + (e i ⊗ e k )|e l |e j (45) + e k |(e i ⊗ e l )|e j + (e i ⊗ e k )|(e j ⊗ e l )</p><p>Lemma B.8. The map Φ satisfies the following</p><formula xml:id="formula_48">1 , Φ(x) 2 , Φ(x) = 1 2 , Φ(x ) .<label>(47)</label></formula><p>Proof. By writing out Equation 32 in coordinates we get</p><formula xml:id="formula_49">e i1 | · · · |e im , Φ(x) = 1≤k1&lt;···&lt;km≤L e i1 , x k1 · · · e im , x km .<label>(48)</label></formula><p>In other words, Φ satisfies the recurrence equation</p><formula xml:id="formula_50">|e i , Φ(x) L = L−1 k=1 , Φ(x) k e i , x k+1<label>(49)</label></formula><p>where we use the notation Φ(x) k = Φ(x 1 , . . . , x k ). We now proceed by induction. Assume that the statement is true for 1 , 2 . Using the recurrence equation we may write</p><formula xml:id="formula_51">1 |e i , Φ(x) L 2 |e j , Φ(x) L = L−1 k=1 1 , Φ(x) k e i , x k+1 L−1 k=1 2 , Φ(x) k e j , x k+1 (50) = 1≤k1&lt;k2≤L−1 1 , Φ(x) k1 e i , x k1+1 2 , Φ(x) k2 e j , x k2+1 (51) + 1≤k1&lt;k2≤L−1 2 , Φ(x) k1 e j , x k1+1 1 , Φ(x) k2 e i , x k2+1<label>(52)</label></formula><formula xml:id="formula_52">+ L−1 k=1 1 , Φ(x) k e i , x k+1 2 , Φ(x) k e j , x k+1 .<label>(53)</label></formula><p>Using the recurrence equation again we may write this as</p><formula xml:id="formula_53">L−1 k=1 1 |e i , Φ(x) k 2 , Φ(x) k e j , x k+1 + L−1 k=1 2 |e j , Φ(x) k 1 , Φ(x) k e i , x k+1<label>(54)</label></formula><formula xml:id="formula_54">+ L−1 k=1 2 , Φ(x) k 1 , Φ(x) k e i ⊗ e j , x k+1 .<label>(55)</label></formula><p>By induction this simplifies to</p><formula xml:id="formula_55">L−1 k=1 1 |e i 2 , Φ(x) k e j , x k+1 + L−1 k=1 2 |e j 1 , Φ(x) k e i , x k+1<label>(56)</label></formula><formula xml:id="formula_56">+ L−1 k=1 2 1 , Φ(x) k e i ⊗ e j , x k+1<label>(57)</label></formula><p>which can be rewritten as</p><formula xml:id="formula_57">( 1 |e i 2 )|e j , Φ(x) L + ( 2 |e j 1 )|e i , Φ(x) L + ( 2 1 )|(e i ⊗ e j ), Φ(x) L (58) = 1 |e i 2 |e j , Φ(x) L<label>(59)</label></formula><p>Example B.9. For a simple example of this, note that</p><formula xml:id="formula_58">e i , Φ(x) e j , Φ(x) = L k=1 e i , x k L k=1 e j , x k = L k1,k2=1 e i , x k1 e j , x k2<label>(60)</label></formula><p>One may also write this with the quasi shuffle product since</p><formula xml:id="formula_59">e i , Φ(x) e j , Φ(x) = e i |e j + e j |e i + e i ⊗ e j , Φ(x) (61) = 1≤k1&lt;k2≤L e i , x k1 e j , x k2 + 1≤k1&lt;k2≤L e j , x k1 e i , x k2 + L k=1 e i , x k e j , x k (62) = L k1,k2=1 e i , x k1 e j , x k2<label>(63)</label></formula><p>We refer to <ref type="bibr" target="#b67">[68]</ref> for more on quasi-shuffle algebras and their use for time warping invariant features of time-series.</p><p>The space T 2 (V ) might seem very large and difficult to work with at first. The power of this representation comes from the fact that one may leverage this in proving strong statements about the original map Φ : Seq(V ) → T(V ), and we will use this in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Theorem B.3.</head><p>We prepare the proof of Theorem B.3 with the following lemma.</p><formula xml:id="formula_60">Lemma B.10. Let Seq 1 (V ) be the set of all x = (x 1 , . . . , x L ) ∈ Seq(V ) with the form x i = (1, x 1 i , . . . , x d i ).</formula><p>That is, all sequences where one of the terms is constant. Then the map</p><formula xml:id="formula_61">Seq 1 (V ) → T(V ), (x 1 , . . . , x L ) → L i=1 (1 + x i )<label>(64)</label></formula><p>is injective.</p><p>Proof. Follows from an induction argument over L. For L = 1 it is clear since</p><formula xml:id="formula_62">e i , Φ(x) = x i .<label>(65)</label></formula><p>Assume that it is true for L, let x = (x 1 , . . . , x L+1 ), y = (y 1 , . . . , y L+1 ), where we may assume that both have length L + 1 by taking any number of components to be 0 if necessary. Let 1 be some linear function that separates Φ(x 1 , . . . , x L ) and Φ(y 1 , . . . , y L ) and 2 some linear function that separates Φ(x 2 , . . . , x L+1 ) and Φ(y 2 , . . . , y L+1 ), then by fixing some γ ∈ R:</p><formula xml:id="formula_63">1 ⊗ e 0 + γe 0 ⊗ 2 , Φ(x) − Φ(y) (66) = 1 , Φ(x 1 , . . . , x L ) − Φ(y 1 , . . . , y L ) + γ 2 , Φ(x 2 , . . . , x L+1 ) − Φ(y 2 , . . . , y L+1 ) . (67) Since neither 1 , Φ(x 1 , . . . , x L ) − Φ(y 1 , . . . , y L ) nor 2 , Φ(x 2 , . . . , x L+1 ) − Φ(y 2 , . . . , y L+1 ) are 0 by assumption there exists some γ ∈ R such that 1 ⊗ e 0 + γe 0 ⊗ 2 , Φ(x) − Φ(y) = 0.</formula><p>This shows the assertion.</p><p>We now have everything to give a proof of Theorem B.3. Recall that Φ is the map Φ(x) = L i=1 ϕ(x i ) and that we may assume that ϕ has the form ϕ(</p><formula xml:id="formula_64">x) = 1 + ϕ 1 (x) ∈ (V ⊗0 , V ), where ϕ 1 : X → V is a bounded universal map.</formula><p>Proof of Theorem B.3. We will show that linear functionals on Φ are dense in the strict topology <ref type="bibr" target="#b69">[70]</ref>. By Theorem [70, Theorem 3.1] it is enough to show that linear functions on Φ form an algebra since by Lemma B.10 they separates the points of Seq(X ). Since they clearly form a vector space it is enough to show that they are closed under point-wise multiplication. Let 1 , 2 be two such, then by Lemma B.8</p><formula xml:id="formula_65">1 , Φ(x) 2 , Φ(x) = 1 2 , L i=1 (1 + ϕ 1 (x i ) )<label>(68)</label></formula><p>x</p><formula xml:id="formula_66">· · · Φ(x) · · · Φ 2 (x) · · · . . . . . . . . . . . . . . . . . . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>· · · <ref type="figure" target="#fig_3">Figure 4</ref>: The sequence-to-sequence transformation. The bottom row is the original sequence x and subsequent ones apply the map Φ in the sequence-to-sequence manner.</p><p>so it is enough to show that 1 2 also is a linear function on Φ(x). Note that inductively it is enough to show that if e i , e j are unit vectors, then e i ⊗ e j is a linear function on Φ(x). By assumption ϕ 1 is bounded and universal, so the continuous bounded function x → e i , ϕ 1 (x k ) e j , ϕ 1 (x k ) is approximately linear, and we may write</p><formula xml:id="formula_67">e i , ϕ 1 (x k ) e j , ϕ 1 (x k ) = h, ϕ 1 (x k ) + ε(x k )<label>(69)</label></formula><p>where ε(x k ) can be made arbitrarily small in the strict topology. The assertion now follows since</p><formula xml:id="formula_68">e i ⊗ e j , L i=1 (1 + ϕ 1 (x i ) ) = L k=1 e i , ϕ 1 (x k ) e j , ϕ 1 (x k ) = L k=1 h, ϕ 1 (x k ) + ε(x k ) (70) = e h , Φ(x) + n max 1≤k≤n ε(x k ).<label>(71)</label></formula><p>C NLST: Low Rank Functionals of Φ NLST in a nutshell. Even computing just the first M entries of Φ = (Φ m (x)) m≥0 ∈ T(R d ) is very costly since it involves O(d m ) coordinates. The key of our NLST layer is that we never compute Φ(x) (or the first m levels)! Instead, we use formula <ref type="bibr" target="#b14">(15)</ref> as derived in Section B.1.1, that shows that LR functionals , Φ(x) are extremely fast to evaluate. In the remainder of this Section C we formalize idea 3, in the sense that we prove that compositions of very simple objects (LR functionals) efficiently approximate complicated objects (full rank functionals of sequences), Proposition C.3.</p><p>Sequence to Sequence. As Φ applies to sequences of any length, we may use it to map the original sequence to another sequence in feature space,</p><formula xml:id="formula_69">Seq(V ) → Seq(T(V ))<label>(72)</label></formula><formula xml:id="formula_70">(x 1 , x 2 , x 3 , . . . , x L ) → Φ(x 1 ), Φ(x 1 , x 2 ), Φ(x 1 , x 2 , x 3 ), . . . , Φ(x 1 , . . . , x L ) .<label>(73)</label></formula><p>Since T(V ) is again a linear space, we can repeat this procedure to map Seq(T(V )) to Seq(T(T(V ))). By repeating this D times, we have constructed sequence-to-sequence transforms</p><formula xml:id="formula_71">Seq(V ) = Seq(T(V )) → Seq(T(T(V ))) → · · · → Seq(T (· · · T D times (V ) · · · )).<label>(74)</label></formula><p>See <ref type="figure" target="#fig_3">Figure 4</ref> for an illustration. We emphasize that in each step of the iteration, the newly created sequence evolves in a much richer space than in the previous step. To make this precise we now introduce the higher rank tensor algebras.</p><p>Higher rank tensor algebras. Just like in Appendix B we need to enlarge the ambient space T(V ). Recall that we defined T 2 (V ) := T(T(V )). This construction can be iterated indefinitely and leads to the higher rank tensor algebras, recursively defined as follows: Definition C.1. Define the spaces</p><formula xml:id="formula_72">T 0 (V ) = V, T D (V ) = m≥0 T D−1 (V ) ⊗m .<label>(75)</label></formula><p>We use the notation ⊗ (D) for the tensor product on T D−1 (V ) which makes T D (V ), +, ⊗ (D) into a multi-graded algebra over V . See <ref type="bibr" target="#b70">[71]</ref> for more on this iterated construction.</p><p>Half-shuffles. By iterating the sequence-to-sequence D times, one gets a map</p><formula xml:id="formula_74">Φ D : Seq(V ) → T D (V ).<label>(77)</label></formula><p>These are very large spaces, but as we will see in Proposition C.3 below, linear functionals on the full map Seq(V ) → T D (V ) can be de-constructed into so called half-quasi-shuffle on the original map Φ : Seq(V ) → T(V ).</p><p>Just like in Appendix B we consider the sequence x as its extension x taking values in T(V ). Hence linear functionals can be written as linear combinations of elements of the form e i1 ⊗ (2) · · · ⊗ (2) e in . Definition C.2. The half-quasi-shuffle product is defined on rank 1 tensors by</p><formula xml:id="formula_75">1 ≺ ( 2 ⊗ (2) e i ) = ( 1 2 ) ⊗ (2) e i<label>(78)</label></formula><p>and extends by bi-linearity to a map</p><formula xml:id="formula_76">T 2 (V ) × T 2 (V ) → T 2 (V ).</formula><p>The following Proposition shows that by composing Φ with itself, low degree tensors on the second level can be rewritten as higher degree tensors on the first level. This indicates that iterated compositions of Φ can be much more efficient than computing everything on the first level. We show this for the first level, but by iterating the statement it can be applied for any number D ≥ 2.</p><p>For the next statement, let be a linear functional on T 2 (V ), then the unit vector e is a linear functional on T 3 (V ). We again assume without loss of generality that ϕ(x) = 1 + x and use the notation Φ(x) k = Φ(x 1 , . . . , x k ). Proposition C.3. Let Φ be the sequence-to-sequence transformation:</p><p>Seq(V ) → Seq(T(V )), (x 1 , . . . , x L ) → (Φ(x 1 ), . . . , Φ(x, . . . , x L )) (79) and let ∆ : Seq(T(V )) → Seq(T(V )) be the discrete differentiation map ∆(x 0 , . . . , x L ) = (x 1 − x 0 , . . . , x L − x L−1 ). Then</p><formula xml:id="formula_77">e 1 ⊗ (3) e 2 , Φ(∆Φ(x)) = 1 ≺ 2 , Φ(x)<label>(80)</label></formula><p>Proof. By induction:</p><formula xml:id="formula_78">e 1 ⊗ (3) e 2⊗(2)ei , Φ(∆Φ(x)) (81) = 1≤k1&lt;k2≤L 1 , Φ(x) k1 − 1 , Φ(x) k1−1 2 ⊗ (2) e i , Φ(x) k2 − 2 ⊗ (2) e i , Φ(x) k2−1 (82) = L−1 k=1 1 , Φ(x) k 2 ⊗ (2) e i , Φ(x) k+1 − 2 ⊗ (2) e i , Φ(x) k (83) = L−1 k=1 1 , Φ(x) k 1≤l≤k 2 , Φ(x) l x i l+1 − 2 , Φ(x) l−1 x i l (84) = L−1 k=1 1 , Φ(x) k 2 , Φ(x) k x i k+1 = L−1 k=1 1 2 , Φ(x) k x i k+1 (85) = ( 1 2 ) ⊗ (2) e i , Φ(x) L .<label>(86)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details on algorithms</head><p>Here we give further information on the implementation of NLST layers detailed in the main text. From here onwards, we fix the state-space of sequences V = R d , which will be true for most practical time series, and even video can be represented as such by e.g. applying a preprocessing layer first and then flattening the coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Variations</head><p>Truncation degree. To reiterate from Section 2, the Seq2Tens feature map Φ :</p><formula xml:id="formula_79">Seq(R d ) → T(R d ) represents a sequence x = (x 1 , . . . , x L ) ∈ Seq(R d ) as a tensor in T (R d ), defined as Φ(x) = (Φ m (x)) m≥0 , Φ m (x) = 1≤i1&lt;···&lt;im≤L x i1 ⊗ · · · ⊗ x im ,<label>(87)</label></formula><p>where Φ m : Seq(V ) → V ⊗m is given by a summation over all noncontiguous length-m subsequences of x with non-repreating indicies. Therefore, for a sequence of length L ∈ N, Φ m can have potentially non-zero terms up to the Lth tensor level. An empirical observation is that for most practical datasets that we encountered, computing everything up to the Lth level is redundant in the sense that usually the first M ∈ N levels already contain most of the information a discriminative or a generative model picks up on where M L. It is thus better treated as a hyperparameter.</p><p>Separating functionals across levels. Now let us consider the low-rank Seq2Tens mapΦ θ , each output coordinate of which is given by a linear functional of Φ, i.e.Φ θ (x) = 1 , Φ(x) , . . . , n , Φ(x), for a sequence x ∈ Seq(R d ) and a collection of rank-1 weight tensors</p><formula xml:id="formula_80">θ = ( k ) n k=1 ⊂ T (R d ).</formula><p>Then, a single output coordinate ofΦ θ may be written for</p><formula xml:id="formula_81">1 ≤ k ≤ n as k , Φ(x) = m=0 k m , Φ m (x) ,<label>(88)</label></formula><p>for k = ( k m ) m≥0 , i.e. we take inner products of tensors that are on the same level of the tensor algebra, and then sum them all up. We found that rather than taking the summation across tensor levels, it is beneficial to treat the linear functional on each level as an independent output to havẽ</p><formula xml:id="formula_82">Φ m,θ (x) = ( 1 m , Φ m (x), . . . , n m , Φ m (x) ) andΦ θ (x) = (Φ m,θ (x)) m≥0 ,<label>(89)</label></formula><p>where nowΦ θ has output dimensionality (M × n) with M ∈ N the truncation degree of Φ as detailed in the previous paragraph. Hence this modification scales the output dimension by M , but it will be important for the next step we discuss. It is for this modification that in <ref type="figure">Figure 1</ref>, each output of an NLST layer has dimensionality 64 × 2, since we set n = 64 and M = 2, while in <ref type="figure" target="#fig_2">Figure 3</ref> the B-NLST layer has output dimensionality h × 4, since we set n = h and M = 4.</p><p>The need for normalization. Here we motivate the need to follow each NLST layer by some form of normalization. Let x = (x 1 , . . . , x L ) ∈ Seq(R d ) be a sequence. Let α ∈ R be a scalar and define y = αx ∈ Seq(R d ) a scaled version of x. Let us investigate how the features change:</p><formula xml:id="formula_83">Φ m (y) = 1≤i1&lt;···&lt;i M ≤L y i1 ⊗ · · · ⊗ y im = 1≤i1&lt;···&lt;i M ≤L (αx i1 ) ⊗ · · · ⊗ (αx im ) (90) = α m 1≤i1&lt;···&lt;i M ≤L x i1 ⊗ · · · ⊗ x im ,<label>(91)</label></formula><p>and therefore we have Φ(y) = (Φ m (y)) m≥0 = (α m Φ m (x)) m≥0 , which analogously translates into the low-rank Seq2Tens map sincẽ</p><formula xml:id="formula_84">Φ m,θ (y) = ( 1 m , Φ m (y) , . . . , n m , Φ m (y) ) = ( 1 m , α m Φ m (x) , . . . , n m , α m Φ m (x) ) (92) = α m ( 1 m , Φ m (x) , . . . , n m , Φ m (x) ).<label>(93)</label></formula><p>From this point alone, it is easy to see that Φ m , and thusΦ m,θ will move across wildly different scales for different values of m, which is inconvenient for the training of neural networks. To counterbalance this, we used a batch normalization layer after each NLST layer in Section 4.1 that computes mean and variance statistics across time and the batch itself, while in Section 4.2 we used a layer normalization that computes the statistics only across time.</p><p>Sequence differencing. In both Section 4.1 and Section 4.2, we precede each NLST layer by a differencing layer and a time-embedding layer, which we briefly motivated in Section 3. Here we aim to expand on the intuition.</p><formula xml:id="formula_85">Let ∆ : Seq(R d ) → Seq(R d ) be the discrete difference operator defined for a sequence x = (x 1 , . . . , x L ) ∈ Seq(R d ) as ∆x := (x 1 , x 2 − x 1 , . . . , x L − x L−1 ) ∈ Seq(R d ),<label>(94)</label></formula><p>where we made the simple identification that x 0 ≡ 0, i.e. for all sequences we first concatenate a 0 observation along the time axis. This step is beneficial for two reasons: (i) now ∆ preserves the length L of a sequence, (ii) now ∆ is one-to-one, since otherwise ∆ would be translation invariant, i.e. it would map all sequences which are translations of each other to the same output sequence.</p><p>To motivate differencing, first let us consider Φ m (x) for m = 1, and for brevity denote ∆x i = x i − x i−1 for i = 1, . . . , L and the convention x 0 = 0. Then, we may write</p><formula xml:id="formula_86">Φ 1 (x) = L i=1 ∆x i = x L ,<label>(95)</label></formula><p>which means that now the first level of the Seq2Tens map is simply point-wise evaluation at the last observation time, and when used as a sequence-to-sequence transformation over expanding windows (i.e. equation <ref type="formula" target="#formula_3">(16)</ref>), it is simply the identity map of the sequence.</p><p>Analogously, for the low-rank map we havẽ</p><formula xml:id="formula_87">Φ 1,θ (x) = L i=1 ( 1 1 , ∆x i , . . . , n 1 , ∆x i ) = ( 1 1 , x L , . . . , n 1 , x L ),<label>(96)</label></formula><p>which is simply a linear map applied to x in an observation-wise manner. The higher order terms, Φ m (x) andΦ m,θ (x) can generally be written as</p><formula xml:id="formula_88">Φ m (x) = 1≤i1&lt;...im≤L ∆x i1 ⊗ · · · ⊗ ∆x im ,<label>(97)</label></formula><formula xml:id="formula_89">andΦ m,θ = 1≤i1&lt;···&lt;im≤L ( z 1 m,1 , ∆x i1 · · · z 1 m,m , ∆x im , . . . , z n m,1 , ∆x i1 · · · z n m,m , ∆x im )<label>(98)</label></formula><p>for some rank-1 order-m tensors k m = z k m,1 ⊗ · · · ⊗ z k m,m for k = 1, . . . , n. We observed that this way the higher order terms are relatively stable across time as the length of a sequence increases, while without differencing they can become unstable, exhibit high oscillations, or simply blow-up.</p><p>An additional benefit of taking differences is that the maps Φ andΦ θ become padding invariant, that is, invariant to repetitions of elements. It is easy to see this by checking that if x i = x i−1 then ∆x i = 0 and all the corresponding terms in the summations (97) and (98) are zeros. This makes sequences of different length comparable without any explicit need for masking, by simply pre-or post-padding each sequence with repetitions of the first/last observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time-embeddings.</head><p>By time-embedding, we mean adding as an extra coordinate to an input sequence the observation times (t i , x i ) i=1,...,L ∈ Seq(R d+1 ). Some datasets already come with a pre-specified observation-grid, in which case we can use that as a time-coordinate at every use of a time-embedding layer. If there is no pre-specified observation grid, we can simply add a normalized and equispaced coordinate, i.e. t i = i/L. Time-embeddings can be beneficial preceding both convolutional layers and NLST layers. For convolutions, it allows to learn features that are not translation invariant <ref type="bibr" target="#b36">[37]</ref>. For the NLST layer, the interpretation is slightly different. Note that in both Sections 4.1 and 4.2, we employ the timeembedding before the differencing block. This can be equivalently reformulated as after differencing adding an additional constant coordinate to the sequence, i.e.</p><formula xml:id="formula_90">(t i − t i−1 , x i − x i−1 ) i=1,...,L ∈ Seq(R d+1 ),</formula><p>where t i − t i−1 = 1/L is simply a constant. This is motivated by Lemma B.10, which states that the map Φ : Seq(R d ) → T (V ) is injective for sequences with a constant coordinate. Thus, the time-embedding before the differencing block is equivalent to adding a constant coordinate after the differencing block, and its purpose is to guarantee injectivity of the map Φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Recursive computations</head><p>Next, we show how the computation of the maps Φ m and Φ m,θ can be formulated as a joint recursion over the values of m and over time.</p><p>Since Φ m is given by a summation over all noncontiguous length-m subsequences with nonrepetitions of a sequence x ∈ Seq(R d ), simple reasoning shows that Φ m obeys the recursion across m and time for 2 ≤ l ≤ L and 1 ≤ m</p><formula xml:id="formula_91">Φ m (x 1 , . . . x l ) = Φ m (x 1 , . . . x l−1 ) + Φ m−1 (x 1 , . . . , x l−1 ) ⊗ x l ,<label>(99)</label></formula><p>with the initial conditions Φ 0 ≡ 1, Φ 1 (x 1 ) = x 1 and Φ m (x 1 ) = 0 for m ≥ 2.</p><formula xml:id="formula_92">Let = ( m ) m≥0 ∈ T (R d )</formula><p>be a sequence of rank-1 tensors with m = z m,1 ⊗· · ·⊗z m,m ∈ (R d ) ⊗m a rank-1 tensor of order-m. Then, m , Φ m (x) may be computed analogously to (99) using the recursion</p><formula xml:id="formula_93">for 2 ≤ l ≤ L, 1 ≤ m m , Φ m (x 1 , . . . , x l ) = z m,1 ⊗ · · · ⊗ z m,m , Φ m (x 1 , . . . , x l ) (100) = z m,1 ⊗ · · · ⊗ z m,m , Φ m (x 1 , . . . , x l−1 ) (101) + z m,1 ⊗ · · · ⊗ z m,m−1 , Φ m−1 (x 1 , . . . , x l−1 ) z m,m , x l (102) = m , Φ m (x 1 , . . . x l−1 ) (103) + z m,1 ⊗ · · · ⊗ z m,m−1 , Φ m−1 (x 1 , . . . , x l−1 ) z m,m , x l<label>(104)</label></formula><p>and the initial conditions can be rewritten as the identities z 0,0 , Φ 0 = 1, z m,1 , Φ 1 (x) = z m,1 , x and z m,1 ⊗ · · · ⊗ z m,m , Φ m (x 1 ) = 0 for 2 ≤ m.</p><p>A slight inefficiency of the previous recursion given in Lines (103), (104) is that one generally cannot substitute m−1 , Φ m (x 1 , . . . , x l−1 ) for the z m,1 ⊗ · · · ⊗ z m,m−1 , Φ m (x 1 , . . . , x l−1 ) term in Line (104), since m−1 = z m,1 ⊗ · · · ⊗ z m,m−1 generally. This means that to construct the order-m linear functional m , Φ m (x 1 , . . . , x l ) , one has to start from scratch by first constructing the order-1 term z m,1 , Φ 1 first, then the order-2 term z m,1 ⊗ z m,2 , Φ 2 , and so forth. This further means in terms of complexities that while Formula (99) has linear complexity in the largest value of m, henceforth denoted by M ∈ N, Formula (103), (104) has a quadratic complexity in M due to the non-recursiveness of the rank-1 tensors ( m ) m = (z m,1 ⊗ · · · ⊗ z m,m ) m .</p><p>The previous observation indicates that an even more memory and time efficient recursion can be devised by parametrizing the rank-1 tensors ( m ) m in a recursive way as follows:</p><formula xml:id="formula_94">let 1 = z 1 ∈ R d and define m = m−1 ⊗ z m ∈ (R d ) ⊗m for 2 ≤ m, i.e. m = z 1 ⊗ · · · ⊗ z m for {z 1 , . . . z m } ⊂ R d .</formula><p>This parametrization indeed allows to substitute m−1 in Line <ref type="formula" target="#formula_1">(104)</ref>, which now becomes <ref type="figure">.</ref> . . , x l−1 ) z m , x l , (105) and hence, due to the recursion across m for both m and Φ m , it is now linear in the maximal value of m, denoted by M ∈ N. This results in a less flexible, but more efficient variant of the NLST, due to the additional added recursivity constraint on the rank-1 weight tensors. We refer to this version as the recursive variant, while to the non-recursive construction as the independent variant.</p><formula xml:id="formula_95">m , Φ m (x 1 , . . . , x l ) = m , Φ m (x 1 , . . . , x l−1 + m−1 , Φ m−1 (x 1 ,</formula><p>Next, we show how the previous computations can be rewritten as a simple RNN-like discrete dynamical system. For simplicity, we consider the recursive formulation, but the independent variant can also be formulated as such with a larger latent state size. Let ( j ) j=1,...,n be n ∈ N different rank-1 recursive weight tensors, i.e. j = ( j m ) m≥0 , j m = z j 1 ⊗ . . . z j m for z j m ∈ R d , m ≥ 0 and j = 1, . . . , n. Also, denote h j m,i := j m , Φ m (x 1 , . . . , x i ) ∈ R, a scalar corresponding to the output Algorithm 1 Computing the NLST layer with independent tensors across levels 1: Input: Sequences (x j ) j=1,...,nx = (x j 1 , . . . , x j L ) j=1,...,nx ⊂ Seq(R d ), rank-1 tensors ( k ) k=1,...,n = (z k m,1 ⊗ · · · ⊗ z k m,m ) k=1,...,n m=1,..., Save Y m ← ·R[:, :, ] 9: end for 10: Output: Sequences (Y 1 , . . . , Y M ) each of shape (n x × L × n ) Algorithm 2 Computing the NLST layer with recursive tensors across levels Save Y m ← R[:, :, ] 8: end for 9: Output:</p><formula xml:id="formula_96">M ⊂ T (R d ), truncation degree M ∈ N 2: Compute M [m, i, j, l, k] ← z j m,k , x i l for m ∈ {1,</formula><formula xml:id="formula_97">1: Input: Sequences (x j ) j=1,...,nx = (x j 1 , . . . , x j L ) j=1,...,nx ⊂ Seq(R d ), rank-1 tensors ( k ) k=1,...,n = (z k 1 ⊗ · · · ⊗ z k m ) k=1,...,n m=1,...,M ⊂ T (R d ), truncation degree M ∈ N 2: Compute M [m, i, j, l] ← z j m , x i l for m ∈ {1, . . . , M }, i ∈ {1, . . . , n x }, j ∈ {1,</formula><formula xml:id="formula_98">Sequences (Y 1 , . . . , Y M ) each of shape (n x × L × n )</formula><p>of the jth linear functional on the mth tensor level for the sequence (x 1 , . . . , x i ). We collect all such functionals for given m and i into h m,i := (h 1 m,i , . . . h n m,i ) ∈ R n , i.e. h m,i =Φ m,θ (x 1 , . . . , x i ). Additionally, we collect all weight vectors z j m ∈ R d for a given m ∈ N into the matrix Z m := (z 1 m , . . . , z n m ) ∈ R n×d . Then, we may write the following vectorized version of equation <ref type="formula" target="#formula_2">(105)</ref>:</p><formula xml:id="formula_99">h 1,i = h 1,i−1 + Z 1 x i , (106) h m,i = h m,i−1 + h m−1,i−1 Z m x i for m ≥ 2,<label>(107)</label></formula><p>with the initial conditions h m,0 = 0 ∈ R n for all m ≥ 1, and denoting the Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Algorithms</head><p>We have shown previously that one may compute Φ θ (x 1 , . . . , x i ) = (Φ m,θ (x 1 , . . . , x i )) m≥0 recursively in a vectorized way for a given sequence (x 1 , . . . , x i ) ∈ Seq(R d ). Now, in Algorithms 1 and 2, we additionally show how to further vectorize the previous computations across time and the batch. For this purpose, let (x j ) j=1,...,n X ⊂ Seq(R d ) be n X ∈ N sequences in R d and ( k ) k=1,...,n ⊂ T (R d ) be n be rank-1 tensors in T (R d ).</p><p>Additionally, we adopted the notation for describing algorithms from <ref type="bibr" target="#b71">[72]</ref>. For arrays, 1-based indexing is used. Let A and B be k-dimensional arrays with size (n 1 × · · · × n k ), and let i j ∈ {1, . . . , n j } for j ∈ {1, . . . , k}. Then, the following operations are defined:   (iii) The shift along axis j by +m for m ∈ N:</p><formula xml:id="formula_100">A[. . . , :, +m, :, . . . ][. . . , i j−1 , i j , i j+1 , . . . ] := A[. . . , i j−1 , i j − m, i j+1 , . . . ], if i j &gt; m, 0, if i j ≤ m.</formula><p>(iv) The Hadamard product of arrays A and B:</p><formula xml:id="formula_101">(A B)[i 1 , . . . , i k ] := A[i 1 , . . . , i k ] · B[i 1 , . . . , i k ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Complexity analysis</head><p>We give a complexity analysis of Algorithms 1 and 2. Inspection of Algorithm 1 says that it has O(M 2 · n x · L · n ) complexity in both time and memory with an additional memory cost of storing the O(M 2 · n · d) number of parameters, the rank-1 weight tensors ( k m ) m , which are stored in terms of their components z k m,j ∈ R d . In contrast, Algorithm 1 has a time and memory cost of O(M · n x · L · n ), thus linear in M , and the recursive rank-1 weight tensors are now only an additional O(M · n · d) number of parameters.</p><p>Additionally to the big-O bounds on complexities, another important question is how well the computations can be parallelized, which can have a larger impact on computations when e.g. running on GPUs. Observing the algorithms, we can see that they are not completely parallelizable due to the cumsum ( ) operations in Lines 6, 8 (Algorithm 1) and Lines 4, 6 (Algorithm 2). The cumulative sum operates recursively on the whole time axis, therefore it is not parallelizable, but can be computed very efficiently on modern architectures.</p><p>To gain further intuition about what kind of performance one can expect for our NLST layers, we benchmarked the computation time of a forward pass for varying sequence lengths and varying hyperparameters of the model. For comparison, we ran the same experiment with an LSTM layer and a Conv1D layer with a filter size of 32. The input is a batch of sequences of shape (n X × L × d), while the output has shape (n X × L × h), where d ∈ N is the state-space dimension of the input sequences, while h ∈ N is simply the number of channels or hidden units in the layer. For our layers, we used our own implementation in Tensorflow, while for LSTM and Conv1D, we used the Keras implementation using the Tensorflow backend.</p><p>In <ref type="table" target="#tab_4">Table 3</ref>, we report the average computation time of a forward pass over 100 trials, for fixed batch size n X = 32, state-space dimension d = 64, output dimension h = 64 and varying sequence lengths L ∈ {32, 64, 128, 256, 512, 1024}. NLST and NLST-R respectively refer to the independent and recursive variants, and M ∈ N denotes the truncation degree. We can observe that while the LSTM practically scales linearly in L, the scaling of NLST is sublinear for all practical purposes, exhibiting a growth rate that is more close to that of the Conv1D layer, that is fully parallelizable. Specifically, while the LSTM takes 3.7 seconds to make a forward pass for L = 1024, all variants of the NLST layer take less time than that by a factor of at least a 100. This suggests that its computations are highly parallelizable across time. Additionally, we observe that NLST exhibits a more aggressive growth rate with respect to the parameter M due to the quadratic complexity in M (although the numbers show only linear growth), while NLST-R scales very favourably in M as well due to the linear complexity (the results indicate a sublinear growth rate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Initialization</head><p>Below we detail the initialization procedure used by our models for the parameters θ of the NLST layer, where θ = ( k ) n k=1 ⊂ T (R d ). As before, each k = ( k m ) m≥0 ∈ T (R d ) is given as a sequence of rank-1 tensors, such that k m = z k m,1 ⊗ · · · ⊗ z k m,m with z k m,j ∈ R d for the independent variant, while k m = z k 1 ⊗ · · · ⊗ z k m with z m ∈ R d for the recursive variant. Hence, by initialization, we mean the initialization of the components z k m,j or z k m . To find a satisfactory initialization scheme, we took as starting point the Glorot <ref type="bibr" target="#b72">[73]</ref> initialization, which specifies that for a layer with input dimension n in and output dimension n out , the weights should be independently drawn from a centered distribution with variance 2/(n in + n out ), where the distribution that is used is usually a uniform or a Gaussian. due to the independence of the corresponding terms in the product. Therefore, to have E[ k m,i ] 2 = 2/(d m + n ), where we made the substitutions n in = d m and n out = n , we can simply set</p><formula xml:id="formula_102">σ 2 m = m 2 d m + n .<label>(112)</label></formula><p>Recursive variant. In the recursive variant, the weights themselves are constructed recursively as k m = z k 1 ⊗ · · · ⊗ z k m for k = 1, . . . , n and m ≥ 0.</p><p>Thus, for i = (i 1 , . . . , i m ) ∈ {1, . . . , d} m , the ith component of k m is given as</p><formula xml:id="formula_104">k m,i = z k 1,i1 · · · z k m,im ,<label>(114)</label></formula><p>and if we assume that for a given m ∈ N, z k m,im is drawn from a centered distribution with variance σ 2 m , then we have</p><formula xml:id="formula_105">E[ k m,i ] = 0 and E[ k m,i ] 2 = σ 2 1 · · · σ 2 m ,<label>(115)</label></formula><p>which means that now our goal is to have σ 2 1 · · · σ 2 m = 2/(d m + n ) for all m ≥ 1, which is achievable inductively by</p><formula xml:id="formula_106">σ 2 1 = 2 d + n and σ 2 m+1 = d m + n d m+1 + n for m ≥ 1.<label>(116)</label></formula><p>For both variants of the algorithms, we used the above described initialization schemes, where the tensor component z's were drawn from a centered uniform or a Gaussian distribution with the specified variances. Although the resulting weight tensors k m were of neither distribution, they had the pre-specified first two moments, that seemed sufficient to successfully train models with NLST layers when combined with succeeding normalization layers as described in Appendix D.1.</p><p>However, we remark that when Glorot <ref type="bibr" target="#b72">[73]</ref> derived his weight initialization scheme, he considered a fully linear regime across layers, while for x = (x 1 , . . . , x L ) ∈ Seq(R d ), the features Φ m (x 1 , . . . x L ) on which the weight tensors k m act will be highly nonlinear for any given m ≥ 2 with increasing levels of nonlinearity for larger values of m. Therefore, it is highly likely that better initialization schemes can be derived by studying the distribution of Φ m (x), that might also make it possible to abandon the normalization layers succeeding the NLST layers and still retain the layers' ability to learn relevant features of the data. Alternatively, data dependent initializations could also prove useful here, such as the LSUV initialization in <ref type="bibr" target="#b73">[74]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 Breaking the curse of dimensionality</head><p>We would like to re-emphasize a central, but potentially elusive point, that we believe is a key contribution of this work.</p><p>The problem. The feature map</p><formula xml:id="formula_107">x → Φ(x) = (Φ m (x)) m≥0 Φ m (x) = 1≤i1&lt;···&lt;im≤L x i1 ⊗ · · · ⊗ x im ∈ (R d ) ⊗m .<label>(117)</label></formula><p>injects a sequence x into the linear space T(R d ). The solution: formula <ref type="bibr" target="#b14">(15)</ref> and being serious about Idea 3. Instead, formula <ref type="bibr" target="#b14">(15)</ref> shows that for rank-1 functionals, m = z 1 ⊗ · · · ⊗ z m , the computation of m , Φ m (x) is computationally very cheap; in particular it does not to compute the tensor Φ m (x); see Appendix B.1.1 and D.2. A priori, the restriction to rank-1 functionals would severely limit the class of functions f (x) that can be approximated by such linear functionals. But this is resolved by our (stacked) NLST layer: first note, that by applying formula <ref type="bibr" target="#b14">(15)</ref> we can quickly compute the sequence-to-sequence transform</p><formula xml:id="formula_108">Seq(R d ) → Seq(R d ), x → ( 1 , Φ(x 1 , . . . , x i ) , . . . , d , Φ(x 1 , . . . , x i ) ) L i=1<label>(118)</label></formula><p>for any set of d rank-1 weight tensors k = (z k m,1 ⊗ · · · ⊗ z k m,m ) M m=1 , k = 1 . . . , d. Secondly, recall that Idea 3 from the introduction says that compositions (here, sequence-to-sequence transforms) of simple objects (here, rank 1 functionals) can approximate complex objects (here, full rank functionals) This suggests that by stacking the NLST layer (118) one can recover the full expressiveness of full rank functionals in a computationally very efficient way. We have validated this (i) theoretically, in the form of Theorem 3.1, (ii) empirically, in our experiments with the stacked NLST layer.</p><p>A special case: signatures In Appendix D we have focused on the feature map (117) since it is the natural generalizaton of m-mers but as discussed in Section B.1 this one special case of a general construction. Another special case are discretized signatures (Remark B.4) which have been used in ML <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76]</ref>. All these works firstly compute the (signature) feature map Φ(x) ∈ T(R d ) and subsequently a linear functional of it , Φ(x) . Combining this with neural nets, sequence transforms, and pre/postprocessing can lead to competitive models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76]</ref> and specialized software is available to compute Φ(x). But these are computationally costly since the bottleneck is still the evaluation of the signature map Φ(x). So far, two approaches have been explored to deal with the curse of dimensionality of (signature) tensors: (i) kernelization, (ii) preprocessing with (random) projections of</p><formula xml:id="formula_109">x i ∈ R d to Ax i ∈ R d d d.</formula><p>The former can be very competitive but suffers from the usual disadvantages of kernel learning; the latter only ameliorates the curse of dimensionality since the cost is still O((d ) m ). We emphasize that our NLST approach immediately applies to signatures but is very different to any of the above since it allows us to efficiently learn functionals of signatures without ever computing signatures!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Details on experiments</head><p>In the following, we give details on the time series classification (Appendix E.1) and sequential data imputation (Appendix E.2) experiments. For running all experiments, we used GPU-based computations on a set of computing nodes, that were equipped with 11 NVIDIA GPUs in total: 4 Tesla K40Ms, 5 Geforce 2080 TIs and 2 Quadro GP100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Time series classification</head><p>Problem formulation. Classification is a traditional task in discriminative supervised machine learning: let X be the data space and Y = {1, . . . , c} the discrete output space that consists of only categorical values with c ∈ N the total number of classes. The problem is then to predict the corresponding labels of a set of unlabelled examples X = (</p><formula xml:id="formula_110">x i ) n X i=1 given a set of labelled examples (X, y) = (x i , y i ) n X i=1 ⊆ X × Y.</formula><p>In the context of time series classification (TSC), the data space X = Seq(R d ) is the space of multivariate sequences, i.e. x i = (x i,j ) lx i j=1 where l xi ∈ N is the length of the sequence x i that can change from instance from instance. Datasets details. We use a collection of multivariate TSC datasets available from <ref type="bibr" target="#b38">[39]</ref> to benchmark our model as several previous publications report test set accuracies on this archive that we can use as baselines, e.g. DTW i <ref type="bibr" target="#b51">[52]</ref>, ARKernel <ref type="bibr" target="#b52">[53]</ref>, SMTS <ref type="bibr" target="#b48">[49]</ref>, LPS <ref type="bibr" target="#b49">[50]</ref>, gRSF <ref type="bibr" target="#b53">[54]</ref>, mvARF <ref type="bibr" target="#b50">[51]</ref>, MUSE <ref type="bibr" target="#b42">[43]</ref>, MLSTMFCM. <ref type="bibr" target="#b41">[42]</ref>. We also include another two deep learning baselines from <ref type="bibr" target="#b39">[40]</ref>, FCN and ResNet, since they were found to be the two best performing architectures for TSC in <ref type="bibr" target="#b40">[41]</ref> among a variety of architectures. <ref type="table" target="#tab_5">Table 4</ref> details the datasets from <ref type="bibr" target="#b38">[39]</ref> that were used for the TSC experiment. The columns are defined as follows: n c denotes the number of classes, d the dimension of the state space, l x the range of sequence lengths, n X and n X respectively denote the number of examples in the prespecified training the testing sets. As preprocessing, the state space dimensions were normalized to zero mean and unit variance.</p><p>Implementation details. The two models, SNLST and FCN-SNSLST, are structured as detailed on <ref type="figure">Figure 1</ref>, where SNLST only consists of the stacked NLST block, while FCN-SNLST consists of both the CNN and NLST block. In the CNN block, each convolutional layer is preceded by a timeembedding layer and succeeded by a batch normalization layer and a ReLU activation. In the SNLST block, each NLST layer is preceded by a time embedding and a differencing layer and succeeded by a flattening 5 and a batch normalization layer, as motivated in Appendix D.1. Note that in the SNLST architecture, the first time difference layer has width d + 1 instead of the 65 in FCN-SNLST.</p><p>We have additionally reimplemented FCN and ResNet using exactly the same settings and training methodology as in <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. FCN is a convolutional network similar to the CNN block in our model, but with larger layer widths of <ref type="bibr">[128,</ref><ref type="bibr">256,</ref><ref type="bibr">128]</ref>, no time-embeddings and a final final GAP layer. The ResNet model consists of 3 such CNN blocks, each of 3 convolutional layers with varying widths, and residual connections in-between. Hence, it is the largest model among the considered architectures. We implemented all models in Keras using the Tensorflow blackend, while for our NLST layers we used our own Tensorflow implementation. Code is available at GITHUBAUTHOR. Parameter comparison. <ref type="table" target="#tab_6">Table 5</ref> depicts the median number of parameters for the deep learning models and their absolute deviation. We can see that while SNLST has about half as many parameters as FCN-SNLST due to the added CNN block in the latter, FCN-SNLST still has less parameters than FCN by a factor of 3 as it is a much thinner network. ResNet has the largest number of parameters with about twice as many as FCN. The absolute deviation is relatively small for all as the only source variation is the size of the kernel in the first layer due to varying state space dimensionality. Furthermore, although we did not include MLSTMFCN in the parameter comparison, another deep learning architecture among the baselines, it is straightforward to see that it has at least as many parameters as FCN, since it is given by the concatenation of the FCN model with an LSTM layer.</p><p>Training details. Here we give details on the training methodology used for our models. The main model, FCN-SNLST is overall a smaller, but deeper and thinner network than the other deep learning baselines. It is an empirical observation that such models are more difficult to train than shallow and wide architectures. In order to be able to use the same architectures on all datasets without running the risk of overfitting on the smaller datasets, a minibatch size of n = 4 was used as an implicit regularizer. As optimizer, we employed the SWATS optimizer 6 <ref type="bibr" target="#b76">[77]</ref>, which starts with Adam <ref type="bibr" target="#b77">[78]</ref> and switches to vanilla SGD at an automatically determined switchover point, that in our experience provided better results than using Adam just by itself and converged faster and more reliably than SGD. The initial learning rate was set to α = 1 × 10 −3 , and we employed a learning rate decay of β = 1/ √ 2 after 50 epochs of no improvement in the training loss, and stopping early after no improvement over 300 epochs, after which the lowest training loss parameter set was used. The baseline models, FCN and ResNet, were trained with the same methodology as in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Results details. The full table of results are visible in <ref type="table" target="#tab_7">Table 6</ref>, where for the models that we have run ourselves (FCN, ResNet, SNLST, FCN-SNLST), we report the mean and standard deviation over 5 model trains with different seeds. In Section 4, <ref type="figure">Figure 2</ref> depicts the box-plot distributions of classification accuracies and the corresponding critical difference (CD) diagram. The CD diagram depicts the mean ranks of each method averaged over datasets with a calculated CD region using the Nemenyi test <ref type="bibr" target="#b43">[44]</ref>. For ranking, an average ranking method was used, as opposed to a dense ranking, which in a group of ties assigns the average rank to all methods. Specifically, such a ranking method underemphasizes those datasets where most methods achieve a 1.0 classification accuracy.</p><p>For the Bayesian signed-rank test <ref type="bibr" target="#b47">[48]</ref>, the resulting posterior probabilities are compared in <ref type="table" target="#tab_1">Table 1</ref>, Section 4. The posterior distributions themselves are visualized on <ref type="figure" target="#fig_4">Figure 5</ref> The region of practical equivalence (rope) was set to rope = 1 × 10 −3 , that is, two accuracies were practically equivalent if they are at most the given distance from each other. For the visualizations and computation of probabilities, the posteriors were evaluated using n = 10 5 Monte Carlo samples. </p><formula xml:id="formula_111">= (x i ) n X i=1 ⊂ Seq(R d ) with x i = (x i,tj ) Li j=1 .</formula><p>However, now it is not guaranteed that all coordinates if x i,tj are observed for a given time-point t j . Therefore, we are also given for every <ref type="figure">1} d )</ref>, that specifies whether a given coordinate of x i was observed at time t j or not. The task is then to model the distribution of the unobserved coordinates given the observed coordinates potentially at different time-points.</p><formula xml:id="formula_112">x i ∈ X an additional observation mask m i = (m i,tj ) Li j=1 ∈ Seq({0,</formula><p>Model details. We expand on the GP-VAE [55] model in details. Let x = (x i ) i=1,...,L ∈ Seq(R d ) be a sequence of length L ∈ N. The model assumes that x is noisily generated time-point-wise conditioned on discrete-time realizations of a latent process denoted by z = (z i ) i=1,...,L ∈ Seq(R d ),</p><formula xml:id="formula_113">p θ (x i |z i ) = N (x i | g θ (z i ), σ 2 I d ),<label>(119)</label></formula><p>where g θ : R d → R d is the time-point-wise decoder network parametrized by θ, while σ 2 ∈ R is the observation noise variance. The temporal interdependencies are modelled in the latent space by assigning independent Gaussian process (GP) priors <ref type="bibr" target="#b57">[58]</ref> to the coordinate processes of z, i.e. denoting z i = (z j i ) j=1,...,d ∈ R d , it is assumed that z j ∼ GP(m(·), k(·, ·)), where m : R → R and k : R × R → R are the mean and covariance functions. The authors propose to use the Cauchy kernel as covariance function, defined as</p><formula xml:id="formula_114">k(τ, τ ) =σ 2 1 + (τ − τ ) 2 l 2 −1 ,<label>(120)</label></formula><p>which can be seen as an infinite mixture of RBF kernels, allowing one to model temporal dynamics on multiple length scales. For the variational approximation <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref>, an amortized Gaussian <ref type="bibr" target="#b58">[59]</ref> is used that factorizes across the latent space dimensions, but not across the observation times:</p><formula xml:id="formula_115">q ψ (z 1 , . . . , z L | x 1 , . . . x L ) = q ψ (z 1 1 , . . . z 1 L | x 1 , . . . , x L ) · · · , q ψ (z d 1 , . . . z d L | x 1 , . . . x L ) (121) = N (z 1 1 , . . . , z 1 L | m 1 , A 1 ) · · · N (z d 1 , . . . , z d L | m d , A d ),<label>(122)</label></formula><p>where m j ∈ R L are the posterior means and A j ∈ R L×L are the posterior covariance matrices for j = 1, . . . , d . In general, estimating the full covariance matrices A j ∈ R L×L from a single data example x = (x 1 , . . . , x L ) ∈ Seq(R d ) is an ill-posed problem. To circumvent the curse of dimensionality in the matrix estimation while allowing for long-range correlations in time, a structured precision matrix representation is used, such that A −1 j = B j B j with B j ∈ R L×L a lower bidiagonal matrix, such as in <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b82">83]</ref>, which results in a tridiagonal precision matrix and a potentially dense covariance matrix.</p><p>Training across the whole dataset X = (x i ) n X i=1 ⊂ Seq(R d ) is coupled through the decoder and encoder parameters θ and ψ, and the ELBO is computed as</p><formula xml:id="formula_116">1 n X n X i=1 log p(x i ) ≥ 1 n X n X i=1 Li j=1 E q ψ (zi,j | xi) [log p θ (x i,j | z i,j )] (123) − βD KL [q ψ (z i | x i ) p(z i )]) ,<label>(124)</label></formula><p>where the log-likelihood term is only computed across observed features as was done in <ref type="bibr" target="#b55">[56]</ref>. Similarly to β-VAEs <ref type="bibr" target="#b83">[84]</ref>, β is used to rebalance the KL term, now to account for the missingness rate.  <ref type="table" target="#tab_8">Table 7</ref> details the datasets used, which are the same ones that were considered in <ref type="bibr" target="#b54">[55]</ref>. The columns are defined as: n c ∈ N denotes the number of classes if the dataset is labelled, m ∈ (0, 1) denotes ratio of missing data, d ∈ N denotes the state space dimension of input sequences, L X ∈ N denotes the length of sequences, n X , n X ∈ N denote the number of examples in the respective training and testing sets. As is visible, for Sprites no labels are available, while for Physionet all examples are included in the training set. For HMNIST, the MNAR version was used, which was found to be the most difficult missingness mechanism in <ref type="bibr" target="#b54">[55]</ref>.</p><p>Experiments details. As depicted in <ref type="figure" target="#fig_2">Figure 3</ref>, the difference between the original GP-VAE model and ours is that is that we additionally employ a single bidirectional NLST block (B-NLST) in the encoder network following the convolutional layer, but preceding the time-distributed dense layers. The motivation for this is that the original encoder only takes local sequential structure into account using the convolutional layer. Hence, it does not exploit global information about the sequence, which might limit flexibility and expressiveness of the encoder network. This limitation can lead to suboptimal inference, due to the fact that the encoder is not able to represent a rich enough subset of the variational family of distributions. This is called the amortization gap in the literature <ref type="bibr" target="#b61">[62]</ref>.</p><p>We have thus hypothesized that by incorporating a bidirectional NLST layer into the model that takes sequential structure into account not only locally, but globally, we can improve the expressiveness of the encoder network, that can in turn improve on the variational approximation. However, it should be noted that according to the findings of <ref type="bibr" target="#b61">[62]</ref>, a larger encoder network can potentially result in the variational parameters being overfitted to the training data, and can degrade the generalization on unseen data examples. Therefore, the main question is whether increasing the expressiveness of the encoder will improve the quality of the variational approximation on both seen and unseen examples, or will it lead to overfitting to the seen examples?</p><p>Another interesting question that we have not considered experimentally, but could lead to improvements is the following. The time-point-wise decoder function assumes that d ∈ N is large enough, so that z ∈ Seq(R d ) is able to fully represent x ∈ Seq(R d ) in a time-point-wise manner including its dynamics. Although in theory the GP prior should be able to learn the temporal dynamics in the latent space, this might again only be possible for a large enough latent state size d . In practice, it could turn out to be more efficient to use some of the contextual information in the decoder network as well, either locally, using e.g. a CNN, or globally, using e.g. NLST layers or RNNs/LSTMs. Implementation details. For the implementation of the GP-VAE, we used the same one as in <ref type="bibr" target="#b54">[55]</ref>, which implements it using Keras and Tensorflow. The B-NLST layer used our own implementation based on the same frameworks. The hyperparameters of the models, which are depicted in Appendix A in <ref type="bibr" target="#b54">[55]</ref>, were left unchanged. The only change we concocted is the B-NLST layer in the encoder network as depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. The width of the B-NLST layer was set to be the same as the convolutional layer, and M = 4 tensor levels were used. The parametrization of the low-rank weight tensors in B-NLST used the independent formulation as discussed in Appendix D.2.</p><p>We also made a simple change to how the data is fed into the encoder. In the original model, the missing values were imputed with 0, while we instead used the forward imputed values. This was necessary due to the difference operation preceding the B-NLST layer in <ref type="figure" target="#fig_2">Figure 3</ref>. With the zero imputation, the coordinates with missing values exhibited higher oscillations after differencing, while with forward imputation the missing values were more well-behaved. A simple way to see this is that, if there were no preprocessing and convolutional layers in <ref type="figure" target="#fig_2">Figure 3</ref> preceding the difference block, then this step would be equivalent to imputing the missing values with zero after differencing.</p><p>Results details. <ref type="table" target="#tab_3">Table 2</ref> shows the achieved performance on the datasets with our enhanced model GP-VAE (B-NLST) compared against the original GP-VAE <ref type="bibr" target="#b54">[55]</ref> and the same baselines. We can observe that just simply increasing the expressiveness of the encoder did manage to improve on the results in almost all cases. The only case where no improvement was observable is Sprites, where the GP-VAE already achieved a very low MSE score of M SE = 2 × 10 −3 .</p><p>To gain some intuition whether the lack of improvement on Sprites was due to the GP-VAE's performance already being maxed out, or there was some other pathology in the model, we further investigated the Sprites dataset and found a bottleneck in both the original and enhanced GP-VAE models. Due to the high dimensionality of the state space of input sequences, d = 12288, the width of the first convolutional layer in the encoder network was set to h = 32 in order to keep the number of parameters in the layer manageable and be able to train the model with a batch size of n = 64, while all subsequent layers had a width of h = 256. Thus, to see if this was indeed an information bottleneck, we increased the convolution width to h = 256 and decreased the batch size to n = 16, with all other hyperparameters unchanged. Then, we trained using this modification both the baseline GP-VAE and our GP-VAE (B-NLST) five times on the Sprites datasets. The achieved MSE scores were (i) GP-VAE (base): M SE = 1.4 × 10 −3 ± 4.1 × 10 −5 , (ii) GP-VAE (B-NLST): M SE = 1.3×10 −3 ±4.9×10 −5 . Therefore, the smaller convolutional layer was indeed causing an information bottleneck, and by increasing its width to be on par with the other layers in the encoder, we managed to improve the performance of both models. The improvement on the GP-VAE (B-NLST) was larger, which can be explained by the observation that lifting the information bottleneck additionally allowed the benefits of the B-NLST layer to kick in, as detailed previously.</p><p>To sum up, we have empirically validated the hypothesis that capturing global information in the encoder was indeed beneficial, and managed to improve on the results even on unseen examples in all cases. The experiment as a whole supports that our introduced NLST layers can serve as useful building blocks in a wide range of models, not only discriminative, but also generative ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>"</head><label></label><figDesc>aabc" →(1 + X)(1 + X)(1 + Y )(1 + Z) (2) = 1 + 2X + Y + Z + XX + 2XY + 2XZ + Y Z (3) + XXY + XXZ + 2XY Z + XXY Z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>,Example 2 . 1 .</head><label>21</label><figDesc>The non-commutative polynomial P (x 1 , x 2 , x 3 ) = 3+x 1 +2x 1 x 3 +5x 2 2 corresponds to the linear functional on T(R 3 ),x → , (x ⊗m ) m≥0 where := ( 0 , 1 , 2 ) :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Reconstruction examples from Sprites (left), and encoder network used to represent the means and covariances of the variational posterior with the original in green (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Remark B. 4 .</head><label>4</label><figDesc>(i) By taking ϕ(x) = (1, x, 0, 0, . . .) one generalizes k-mers, Idea 2 and Example 2.1, (ii) By taking ϕ : R d → T(R d ), x → exp(x) one recovers Chen's signature [66, 16, 67] as used in rough paths.(iii) By taking ϕ : R d → T(V ), ϕ 1 (x) the polynomial map and ϕ m (x) = 0 for m ≥ 2 one recovers the iterated sums of<ref type="bibr" target="#b67">[68]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 :</head><label>5</label><figDesc>. . . , M }, i ∈ {1, . . . , n x }, j ∈ {1, . . . , n }, l ∈ {1, . . . , L} and k ∈ {1, . . . , m} 3: for m = 1 to M do 4: Assign R ← M [m, :, :, :, 1] for k = 2 to m do 6: Iterate R ← M [m, :, :, :, k] R[:, :, + 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. . . , n } and l ∈ {1, . . . , L} 3: Assign R ← M [1, :, :, :] 4: Save Y 1 ← R[:, :, ] 5: for m = 2 to M do 6: Update R ← M [m, :, :, :] R[:, :, + 1] 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(i) The cumulative sum along axis j: A[. . . , :, , :, . . . ][. . . , i j−1 , i j , , i j+1 . . . ] := ij κ=1 A[. . . , i j−1 , κ, i j+1 , . . . ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 × 8 . 5 × 1 . 3 × 10 −3 1 . 8 × 10 0 3 . 2 × 3 (</head><label>185131018323</label><figDesc>10 −4 1.2 × 10 −1 1.7 × 10 −3 4.5 × 10 −3 9.9 × 10 −3 1.7 × 10 −3 2.5 × 10 −3 3.4 × 10 −3 64 10 −4 2.3 × 10 −1 1.8 × 10 −3 4.5 × 10 −3 9.9 × 10 −3 1.8 × 10 −3 2.6 × 10 −3 3.4 × 10 −3 128 9.7 × 10 −4 4.6 × 10 −1 2.1 × 10 −3 4.9 × 10 −3 1.0 × 10 −2 2.1 × 10 −3 2.9 × 10 −3 3.8 × 10 −3 256 1.1 × 10 −3 9.3 × 10 −1 2.4 × 10 −3 5.2 × 10 −3 1.1 × 10 −2 2.4 × 10 −3 3.2 × 10 −3 4.0 × 10 −3 512 10 −3 6.0 × 10 −3 1.1 × 10 −2 3.0 × 10 −3 4.0 × 10 −3 4.8 × 10 −3 1024 1.9 × 10 −3 3.7 × 10 0 4.4 × 10 −3 7.1 × 10 −3 1.2 × 10 −2 4.4 × 10 −3 5.1 × 10 −3 6.0 × 10 −ii) The slice-wise sum along axis j: A[. . . , :, Σ, :, . . . ][. . . , i j−1 , i j+1 , . . . ] := nj κ=1 A[. . . , i j−1 , κ, i j+1 , . . . ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Independent variant. We first consider the independent variant of the algorithm. The weights are given as the rank-1 tensorsk m = z k m,1 ⊗ · · · ⊗ z k m,m ∈ (R d ) ⊗m for k = 1, . . . , n and m ≥ 0. (108) Denote z k m,j = (z k m,j,1 , . . . , z k m,j,d ) ∈ R d ,and assume that for a given m ∈ N that each of z k m,j,p are drawn independently from some distribution with E[z k m,j,p ] = 0 and E[z k m,j,p ] 2 = σ 2 m for j = 1, . . . , m and p = 1, . . . , d. (109) Then, for a given multi-index i = (i 1 , . . . , i m ) ∈ {1, . . . , d} m , the ith coordinate of k m is given as k m,i = z k m,1,i1 · · · z k m,m,im (110) and has as its first two moments E[ k m,i ] = 0 and E[ k m,i ] 2 = σ 2m m (111)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>But if we evaluate a linear functional = ( m ) m (x) , see Example A.2, by first computing each Φ m (x) and subsequently evaluating m , Φ m (x) then this is very costly since both m and Φ m (x) are degree m tensors which have d m entries and thus suffer from the curse of dimensionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Posterior probabilities given by a Bayesian signed-rank test comparison of the proposed methods against the baselines. {R &gt; C}, {R &lt; C}, {R = C} refer to the respective events that the row method is better, the column method is better, or that they are equivalent.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of GP-VAE (B-NLST) with the baseline methods</figDesc><table><row><cell>Method</cell><cell></cell><cell>HMNIST</cell><cell></cell><cell>Sprites</cell><cell>Physionet</cell></row><row><cell></cell><cell>NLL</cell><cell>MSE</cell><cell>AUROC</cell><cell>MSE</cell><cell>AUROC</cell></row><row><cell>Mean imputation</cell><cell>-</cell><cell>0.168 ± 0.000</cell><cell>0.938 ± 0.000</cell><cell>0.013 ± 0.000</cell><cell>0.703 ± 0.000</cell></row><row><cell>Forward imputation</cell><cell>-</cell><cell>0.177 ± 0.000</cell><cell>0.935 ± 0.000</cell><cell>0.028 ± 0.000</cell><cell>0.710 ± 0.000</cell></row><row><cell>VAE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Forward pass computation time in seconds on a Gefore 2080Ti GPU for varying sequence length L, fixed batch size N = 32, state-space dimension d = 64 and output dimension h = 64.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Specification of datasets used for benchmarking</figDesc><table><row><cell>Dataset</cell><cell>nc</cell><cell>d</cell><cell>lx</cell><cell>n X</cell><cell>n X</cell></row><row><cell>Arabic Digits</cell><cell>10</cell><cell>13</cell><cell cols="2">4-93 6600</cell><cell>2200</cell></row><row><cell>AUSLAN</cell><cell>95</cell><cell>22</cell><cell cols="2">45-136 1140</cell><cell>1425</cell></row><row><cell>Char. Traj.</cell><cell>20</cell><cell>3</cell><cell>109-205</cell><cell>300</cell><cell>2558</cell></row><row><cell>CMUsubject16</cell><cell>2</cell><cell>62</cell><cell>127-580</cell><cell>29</cell><cell>29</cell></row><row><cell>DigitShapes</cell><cell>4</cell><cell>2</cell><cell>30-98</cell><cell>24</cell><cell>16</cell></row><row><cell>ECG</cell><cell>2</cell><cell>2</cell><cell>39-152</cell><cell>100</cell><cell>100</cell></row><row><cell>Jap. Vowels</cell><cell>9</cell><cell>12</cell><cell>7-29</cell><cell>270</cell><cell>370</cell></row><row><cell>Kick vs Punch</cell><cell>2</cell><cell>62</cell><cell>274-841</cell><cell>16</cell><cell>10</cell></row><row><cell>LIBRAS</cell><cell>15</cell><cell>2</cell><cell>45</cell><cell>180</cell><cell>180</cell></row><row><cell>NetFlow</cell><cell>2</cell><cell>4</cell><cell>50-997</cell><cell>803</cell><cell>534</cell></row><row><cell>PEMS</cell><cell cols="2">7 963</cell><cell>144</cell><cell>267</cell><cell>173</cell></row><row><cell>PenDigits</cell><cell>10</cell><cell>2</cell><cell>8</cell><cell cols="2">300 10692</cell></row><row><cell>Shapes</cell><cell>3</cell><cell>2</cell><cell>52-98</cell><cell>18</cell><cell>12</cell></row><row><cell>UWave</cell><cell>8</cell><cell>3</cell><cell>315</cell><cell>896</cell><cell>3582</cell></row><row><cell>Wafer</cell><cell>2</cell><cell>6</cell><cell>104-198</cell><cell>298</cell><cell>896</cell></row><row><cell>Walk vs Run</cell><cell>2</cell><cell cols="2">62 128-1918</cell><cell>28</cell><cell>16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Parameter comparison</cell></row><row><cell>Model</cell><cell cols="2">Trainable parameters</cell></row><row><cell></cell><cell>Median</cell><cell>Abs. Dev.</cell></row><row><cell>SNLST</cell><cell cols="2">3.6 × 10 4 1.6 × 10 3</cell></row><row><cell cols="3">FCN-SNLST 7.9 × 10 4 2.4 × 10 3</cell></row><row><cell>FCN</cell><cell cols="2">2.7 × 10 5 3.7 × 10 3</cell></row><row><cell>ResNet</cell><cell cols="2">5.1 × 10 5 2.5 × 10 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Classifier accuracies on multivariate time series classification datasets with the highest for each row highlighted in bold. Posterior distribution plots of pairwise Bayesian signed-rank test comparisons E.2 Generative sequential data imputation Problem formulation. Imputation of sequential data can be formulated as a problem of generative unsupervised learning. The input space is given as X = Seq(R d ) and we are given a number of examples X</figDesc><table><row><cell>Dataset</cell><cell>SMTS</cell><cell>LPS</cell><cell cols="6">mvARF DTW ARKernel gRSF MLSTMFCN MUSE</cell><cell>FCN</cell><cell>ResNet</cell><cell>SNLST</cell><cell>FCN-SNLST</cell></row><row><cell>ArabicDigits</cell><cell>0.964</cell><cell>0.971</cell><cell>0.952</cell><cell>0.908</cell><cell>0.988</cell><cell>0.975</cell><cell>0.990</cell><cell>0.992</cell><cell cols="3">0.994(0.001) 0.996(0.001) 0.968(0.003)</cell><cell>0.993(0.001)</cell></row><row><cell>AUSLAN</cell><cell>0.947</cell><cell>0.754</cell><cell>0.934</cell><cell>0.727</cell><cell>0.918</cell><cell>0.955</cell><cell>0.950</cell><cell>0.970</cell><cell>0.976(0.003)</cell><cell>0.977(0.002)</cell><cell cols="2">0.969(0.004) 0.993(0.001)</cell></row><row><cell>Char. Traj.</cell><cell>0.992</cell><cell>0.965</cell><cell>0.928</cell><cell>0.948</cell><cell>0.900</cell><cell>0.994</cell><cell>0.990</cell><cell>0.937</cell><cell cols="2">0.991(0.001) 0.990(0.002)</cell><cell cols="2">0.957(0.005) 0.994(0.001)</cell></row><row><cell cols="3">CMUsubject16 0.997 1.000</cell><cell>1.000</cell><cell>0.930</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell cols="5">1.000 1.000(0.000) 1.000(0.000) 1.000(0.000) 1.000(0.000)</cell></row><row><cell>DigitShapes</cell><cell cols="2">1.000 1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell cols="5">1.000 1.000(0.000) 1.000(0.000) 1.000(0.000) 1.000(0.000)</cell></row><row><cell>ECG</cell><cell>0.818</cell><cell>0.820</cell><cell>0.785</cell><cell>0.790</cell><cell>0.820</cell><cell>0.880</cell><cell>0.870</cell><cell cols="2">0.880 0.852(0.023)</cell><cell cols="2">0.858(0.015) 0.842(0.004)</cell><cell>0.860(0.021)</cell></row><row><cell>Jap. Vowels</cell><cell>0.969</cell><cell>0.951</cell><cell>0.959</cell><cell>0.962</cell><cell>0.984</cell><cell>0.800</cell><cell>1.000</cell><cell>0.976</cell><cell>0.986(0.003)</cell><cell>0.990(0.004)</cell><cell cols="2">0.979(0.001) 0.980(0.002)</cell></row><row><cell>Kick vs Punch</cell><cell>0.820</cell><cell>0.900</cell><cell>0.976</cell><cell>0.600</cell><cell>0.927</cell><cell>1.000</cell><cell>0.900</cell><cell cols="5">1.000 0.960(0.049) 0.780(0.271) 1.000(0.000) 1.000(0.000)</cell></row><row><cell>LIBRAS</cell><cell>0.909</cell><cell>0.903</cell><cell>0.945</cell><cell>0.888</cell><cell>0.952</cell><cell>0.911</cell><cell>0.970</cell><cell>0.894</cell><cell>0.967(0.005)</cell><cell>0.964(0.007)</cell><cell cols="2">0.773(0.017) 0.957(0.007)</cell></row><row><cell>NetFlow</cell><cell cols="2">0.977 0.968</cell><cell>nan</cell><cell>0.976</cell><cell>nan</cell><cell>0.914</cell><cell>0.950</cell><cell>0.961</cell><cell cols="2">0.958(0.005) 0.863(0.043)</cell><cell>0.793(0.135)</cell><cell>0.960(0.004)</cell></row><row><cell>PEMS</cell><cell>0.896</cell><cell>0.844</cell><cell>nan</cell><cell>0.832</cell><cell>0.750</cell><cell>1.000</cell><cell>nan</cell><cell>nan</cell><cell cols="2">0.770(0.021) 0.793(0.024)</cell><cell>0.747(0.015)</cell><cell>0.857(0.013)</cell></row><row><cell>PenDigits</cell><cell>0.917</cell><cell>0.908</cell><cell>0.923</cell><cell>0.927</cell><cell>0.952</cell><cell>0.932</cell><cell>0.970</cell><cell>0.912</cell><cell>0.968(0.001)</cell><cell cols="2">0.964(0.003) 0.954(0.003)</cell><cell>0.953(0.004)</cell></row><row><cell>Shapes</cell><cell cols="2">1.000 1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell cols="2">1.000 0.617(0.100)</cell><cell cols="3">0.917(0.091) 1.000(0.000) 1.000(0.000)</cell></row><row><cell>UWave</cell><cell cols="2">0.941 0.980</cell><cell>0.952</cell><cell>0.916</cell><cell>0.904</cell><cell>0.929</cell><cell>0.970</cell><cell>0.916</cell><cell>0.979(0.001)</cell><cell cols="2">0.979(0.001) 0.938(0.003)</cell><cell>0.969(0.002)</cell></row><row><cell>Wafer</cell><cell>0.965</cell><cell>0.962</cell><cell>0.931</cell><cell>0.974</cell><cell>0.968</cell><cell>0.992</cell><cell>0.990</cell><cell>0.997</cell><cell>0.989(0.002)</cell><cell>0.986(0.004)</cell><cell cols="2">0.981(0.002) 0.989(0.001)</cell></row><row><cell>Walk vs Run</cell><cell cols="2">1.000 1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell cols="5">1.000 1.000(0.000) 1.000(0.000) 1.000(0.000) 1.000(0.000)</cell></row><row><cell>Avg. acc.</cell><cell>0.945</cell><cell>0.933</cell><cell>0.949</cell><cell>0.899</cell><cell>0.938</cell><cell>0.955</cell><cell>0.970</cell><cell>0.962</cell><cell>0.938</cell><cell>0.941</cell><cell>0.931</cell><cell>0.969</cell></row><row><cell>Med. acc.</cell><cell>0.964</cell><cell>0.964</cell><cell>0.952</cell><cell>0.929</cell><cell>0.952</cell><cell>0.984</cell><cell>0.990</cell><cell>0.976</cell><cell>0.978</cell><cell>0.978</cell><cell>0.969</cell><cell>0.991</cell></row><row><cell>Sd. acc.</cell><cell>0.059</cell><cell>0.073</cell><cell>0.055</cell><cell>0.111</cell><cell>0.073</cell><cell>0.058</cell><cell>0.039</cell><cell>0.043</cell><cell>0.106</cell><cell>0.075</cell><cell>0.089</cell><cell>0.046</cell></row><row><cell>Avg. rank</cell><cell>7.281</cell><cell>7.375</cell><cell>8.214</cell><cell>8.750</cell><cell>7.433</cell><cell>5.500</cell><cell>4.700</cell><cell>5.967</cell><cell>4.906</cell><cell>5.281</cell><cell>6.969</cell><cell>4.281</cell></row><row><cell>Med. rank</cell><cell>8.000</cell><cell>7.500</cell><cell>7.750</cell><cell>9.000</cell><cell>6.500</cell><cell>6.250</cell><cell>5.500</cell><cell>5.500</cell><cell>4.750</cell><cell>5.250</cell><cell>6.750</cell><cell>5.000</cell></row><row><cell>Sd. rank</cell><cell>3.082</cell><cell>3.122</cell><cell>2.622</cell><cell>3.022</cell><cell>2.441</cell><cell>3.136</cell><cell>2.419</cell><cell>3.259</cell><cell>2.603</cell><cell>3.104</cell><cell>2.356</cell><cell>1.591</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Specification of datasets used for imputation</figDesc><table><row><cell>Datasets details.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>nc</cell><cell>m</cell><cell>d</cell><cell>Lx</cell><cell>n X</cell><cell>n X</cell></row><row><cell cols="3">HMNIST 10 0.45</cell><cell>28 × 28</cell><cell cols="3">10 60000 10000</cell></row><row><cell>Sprites</cell><cell>-</cell><cell>0.6</cell><cell>64 × 64 × 3</cell><cell>8</cell><cell>9000</cell><cell>2664</cell></row><row><cell>Physionet</cell><cell>2</cell><cell>0.82</cell><cell>35</cell><cell>48</cell><cell>3997</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The reader is invited to think of V as R d , but we prefer the more general setting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">, Φ 2 (x) = 2 , Φ(x) .(19)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Except for FCN and ResNet, which we ran 5 times using the same settings as<ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. For MLSTM-FCN<ref type="bibr" target="#b41">[42]</ref>, the results are the same as those reported in<ref type="bibr" target="#b42">[43]</ref> 3 Using the implementation available at https://github.com/janezd/baycomp.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In this case layer normalization, since the additional minibatch noise injected by a batch normalization layer can be detrimental while training VAEs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">, Φ(x) 2 , Φ(x) = , Φ(x) .(29)This new functional is constructed in explicit way from 1 and 2 , with a so-called quasi-shuffle product. In the remainder of this section B.1, we prepare and give the proof of Theorem B.3: subsection B.1.1 introduces the quasi-shuffle product, and subsection B.1.1 uses this to prove Theorem B.3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">By flattening we mean merging all dimensions except the batch and time axes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We used the Keras implementation available at https://github.com/cainmagi/MDNT.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast string kernels using inexact matching for protein sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1078</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1409.3215</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1609.03499</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tensor rank and the ill-posedness of the best low-rank approximation problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1084" to="1127" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the tensor svd and the optimal low rank orthogonal approximation of tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1709" to="1734" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A literature survey of low-rank tensor approximation techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grasedyck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kressner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tobler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GAMM-Mitteilungen</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="78" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tensorizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="442" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised learning with tensor networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stoudenmire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4799" to="4807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
		<title level="m">Tensor spaces and numerical tensor calculus</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lothaire</surname></persName>
		</author>
		<title level="m">Combinatorics on Words. Cambridge Mathematical Library</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reutenauer</surname></persName>
		</author>
		<title level="m">Free Lie Algebras. Clarendon press -Oxford</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integration of paths, geometric invariants and a generalized Baker-Hausdorff formula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. of Math</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="178" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Un outil algebrique: Les series formelles non commutatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fliess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Systems Theory</title>
		<editor>G. Marchesini and S. K. Mitter</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1976" />
			<biblScope unit="page" from="122" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Uniqueness for the signature of a path of bounded variation and the reduced path group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Hambly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="167" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rough paths, signatures and the modelling of functions on streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4537</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A primer on the signature method in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chevyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kormilitzin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03788</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The iisignature library: efficient calculation of iterated-integral signatures and log signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08252</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sparse arrays of signatures for online character recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0371</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep signature transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Arribas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Variational gaussian processes with signature covariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Why are big data matrices approximately low rank?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Udell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Townsend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematics of Data Science</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE workshop on automatic speech recognition and understanding</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting the past and the future in protein secondary structure prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brunak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pollastri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="937" to="946" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Disentangled sequential autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Translation modeling with bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting strange attractors in turbulence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Takens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamical systems and turbulence</title>
		<meeting><address><addrLine>Warwick</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1980" />
			<biblScope unit="page" from="366" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Yorke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Casdagli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Embedology</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical Physics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="579" to="616" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9628" to="9639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Introduction to time series and forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Brockwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multivariate time series classification datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baydogan</surname></persName>
		</author>
		<ptr target="http://mustafabaydogan.com" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2020" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Time series classification from scratch with deep neural networks: A strong baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning for time series classification: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="917" to="963" />
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multivariate lstm-fcns for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="237" to="245" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multivariate time series classification with weasel+muse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Leser</surname></persName>
		</author>
		<idno>abs/1711.11343</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Distribution-free Multiple Comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nemenyi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963" />
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Should we really use post-hoc tests based on mean-ranks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benavoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mangili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="152" to="161" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Time for a change: a tutorial for comparing multiple classifiers through bayesian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benavoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaffalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2653" to="2688" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A bayesian wilcoxon signed-rank test based on the dirichlet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benavoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mangili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaffalon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ruggeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning a symbolic representation for multivariate time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="400" to="422" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Time series representation and similarity based on local autopatterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="476" to="509" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Autoregressive forests for multivariate time series modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tuncel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Baydogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="202" to="215" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithm optimization for spoken word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Autoregressive Kernels For Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.0673</idno>
		<imprint>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generalized random shapelet forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Papapetrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Boström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1053" to="1085" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Gp-vae: Deep probabilistic time series imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fortuin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Handling incomplete heterogeneous data using vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nazabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Olmos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03653</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<title level="m">Gaussian processes for machine learning</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Amortized inference in probabilistic reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>E. P. Xing and T. Jebara</editor>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="22" to="24" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Brits: Bidirectional recurrent imputation for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="6775" to="6785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Inference suboptimality in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>J. Dy and A. Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Algebra</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<title level="m">Tensor Methods in Statistics</title>
		<imprint>
			<publisher>Dover</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rudin</surname></persName>
		</author>
		<title level="m">Principles of Mathematical Analysis</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Iterated integrals and exponential homomorphisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. London Math. Soc</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="502" to="512" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Integration of paths -a faithful representation of paths by non-commutative formal power series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="395" to="407" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Time-warping invariants of multidimensional time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ebrahimi-Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tapia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05823</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cumulants, free cumulants and half-shuffles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ebrahimi-Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A generalization of the strict topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions</title>
		<imprint>
			<date type="published" when="1971" />
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Adapted topologies and higher rank signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08897</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Kernels for sequentially ordered data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Király</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">All you need is a good init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06422</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Signatory: differentiable computations of the signature and logsignature transforms, on both cpu and gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">The esig python package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Datasig</surname></persName>
		</author>
		<ptr target="https://github.com/datasig-ac-uk/esig" />
		<imprint>
			<biblScope unit="page" from="2020" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Improving generalization performance by switching from adam to sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Advances in variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bütepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2008" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Structured uncertainty prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dorta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5477" to="5485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Dynamic word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bamler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="380" to="389" />
		</imprint>
		<respStmt>
			<orgName>JMLR. org</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">betavae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iclr</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
