<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
							<email>zhaojian90@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
							<email>jianshu@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<email>chengyu996@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Sim</surname></persName>
							<email>tsim@comp.nus.edu.sgeleyans</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Qihoo 360 AI Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database "Multi-Human Parsing (MHP)" for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the primary goals of intelligent human-computer interaction is understanding the humans in visual scenes. It involves several perceptual tasks including detection, i.e. localizing different persons at a coarse, bounding box level ( <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>), instance segmentation, i.e. labelling each pixel * indicates equal contributions. human-centric analysis have been devoted to (a) detection (localizing different persons at a coarse, bounding box level), (b) instance segmentation (labelling each pixel of each person uniquely) or (c) human parsing (decomposing persons into their semantic categories), we focus on (d) multi-human parsing (parsing body parts and fashion items at the instance level), which aligns better with many real-world applications. We introduce a new lagescale, richly-annotated Multi-Human Parsing (MHP) dataset consisting of images with various viewpoints, poses, occlusion, human interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for solving the challenging multi-human parsing problem effectively and efficiently. Best viewed in color.</p><p>of each person uniquely ( <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>), and human parsing, i.e. decomposing persons into their semantic categories ( <ref type="figure" target="#fig_0">Fig. 1  (c)</ref>). Recently, deep learning based methods have achieved remarkable sucess in these perceptual tasks thanks to the availability of plentiful annotated images for training and evaluation purposes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17]</ref>. Though exciting, current progress is still far from the  <ref type="bibr" target="#b38">[39]</ref> 748 452 -296 13 Fashionista <ref type="bibr" target="#b42">[43]</ref> 685 456 -229 56 PASCAL-Person-Part <ref type="bibr" target="#b3">[4]</ref> 3,533 1,716 -1,817 7 ATR <ref type="bibr" target="#b27">[28]</ref> 17,700 <ref type="bibr" target="#b15">16</ref>,000 700 1,000 18 LIP <ref type="bibr" target="#b16">[17]</ref> 50,462 30,462 10,000 10,000 20 MHP v1.0 <ref type="bibr" target="#b24">[25]</ref> 4,980 3,000 1,000 980 19 MHP v2.0 25,403 15,403 5,000 5,000 59 utimate goal of visually understanding humans. As <ref type="figure" target="#fig_0">Fig. 1</ref> shows, previous efforts on understanding humans in visual scenes either only consider coarse information or are agnostic to different instances. In the real-world scenarios, it is more likely that there simutaneously exist multiple persons, with various human interactions, poses and occlusion. Thus, it is more practically demanded to parse human body parts and fashion items at the instance level, which is recently defined as the multi-human parsing task <ref type="bibr" target="#b24">[25]</ref>. Multihuman parsing enables more detailed understanding of humans in crowded scenes and aligns better with many realworld applications, such as group behavior analysis <ref type="bibr" target="#b14">[15]</ref>, person re-identification <ref type="bibr" target="#b46">[47]</ref>, e-commerce <ref type="bibr" target="#b37">[38]</ref>, image editing <ref type="bibr" target="#b41">[42]</ref>, video surveillance <ref type="bibr" target="#b5">[6]</ref>, autonomous driving <ref type="bibr" target="#b6">[7]</ref> and virtual reality <ref type="bibr" target="#b28">[29]</ref>. However, the existing benchmark datasets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17]</ref> are not suitable for such a new task. Even though Li et al. <ref type="bibr" target="#b24">[25]</ref> proposed a preliminary Multi-Human Parsing (MHP v1.0) dataset, it only contains 4,980 images annotated with 18 semantic labels. In this work, we propose a new large-scale benchmark "Multi-Human Parsing (MHP v2.0)", aiming to push the frontiers of multihuman parsing research towards holistically understanding humans in crowded scenes. The data in MHP v2.0 cover wide variability and complexity w.r.t. viewpoints, poses, occlusion, human interactions and background. It in total includes 25,403 human images with pixel-wise annotations of 58 semantic categories.</p><p>We further propose a novel deep Nested Adversarial Network (NAN) model for solving the challenging multihuman parsing problem. Unlike most existing methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref> which rely on separate stages of instance localization, human parsing and result refinement, the proposed NAN parses semantic categories and differentiates different person instances simultaneously in an effective and time-efficient manner. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. Each sub-task is simpler than the original multi-human parsing task, and is more easily addressed by the corresponding sub-net. Unlike many multi-task learning applications, in our method the sub-nets depend on each other, forming a causal nest by dynamically boosting each other through an adversarial strategy (See <ref type="figure">Fig. 5</ref>), which is hence called a "nested adversarial learning" structure. Such a structure enables effortless gradient BackproPagation (BP) in NAN such that it can be trained in a holistic, end-to-end way, which is favorable to both accuracy and speed. We conduct qualitative and quantitative experiments on the MHP v2.0 dataset proposed in this work, as well as the MHP v1.0 <ref type="bibr" target="#b24">[25]</ref>, PASCAL-Person-Part <ref type="bibr" target="#b3">[4]</ref> and Buffy <ref type="bibr" target="#b38">[39]</ref> benchmark datasets. The results demonstrate the superiority of NAN on multi-human parsing over the state-of-the-arts.</p><p>Our contributions are summarized as follows 1 .</p><p>• We propose a new large-scale benchmark and evaluation server to advance understanding of humans in crowded scenes, which contains 25,403 images annotated pixel-wisely with 58 semantic category labels.</p><p>• We propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing, which serves as a strong baseline to inspire more future research efforts on this task.</p><p>• Comprehensive evaluations on the MHP v2.0 dataset proposed in this work, as well as the MHP v1.0 <ref type="bibr" target="#b24">[25]</ref>, PASCAL-Person-Part <ref type="bibr" target="#b3">[4]</ref> and Buffy <ref type="bibr" target="#b38">[39]</ref> benchmark datasets verify the superiority of NAN on understanding humans in crowded scenes over the state-of-thearts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human Parsing Datasets The statistics of popular publicly available datasets for human parsing are summarized in Tab. 1. The Buffy <ref type="bibr" target="#b38">[39]</ref> dataset was released in 2011 for human parsing and instance segmentation. It contains only 748 images annotated with 13 semantic categories. The Fashionista <ref type="bibr" target="#b42">[43]</ref> dataset was released in 2012 for human parsing, containing limited images annotated with 56 fashion categories. The PASCAL-Person-Part <ref type="bibr" target="#b3">[4]</ref> dataset was initially annotated by Chen et al. <ref type="bibr" target="#b3">[4]</ref> from the PASCAL-VOC-2010 <ref type="bibr" target="#b11">[12]</ref> dataset. Chen et al. <ref type="bibr" target="#b2">[3]</ref> extended it for human parsing with 7 coarse body part labels. The ATR <ref type="bibr" target="#b27">[28]</ref> dataset was released in 2015 for human parsing with a large number of images annotated with 18 semantic categories. The LIP <ref type="bibr" target="#b16">[17]</ref> dataset further extended ATR <ref type="bibr" target="#b27">[28]</ref> by cropping person instances from Microsoft COCO <ref type="bibr" target="#b29">[30]</ref>. It  is a large-scale human parsing dataset with densely pixelwise annotations of 20 semantic categories. But it has two limitations. 1) Despite the large data size, it contains limited semantic category annotations, which restricts the finegrained understanding of humans in visual scenes. 2) In LIP <ref type="bibr" target="#b16">[17]</ref>, only a small proportion of images involve multiple persons with interactions. Such an instance-agnostic setting severely deviates from reality. Even in the MHP v1.0 dataset proposed by Li et al. <ref type="bibr" target="#b24">[25]</ref> for multi-human parsing, only 4,980 images are included and annotated with 18 semantic labels. Comparatively, our MHP v2.0 dataset contains 25,403 elaborately annotated images with 58 finegrained semantic part labels. It is the largest and most comprehensive multi-human parsing dataset to date, to our best knowledge. Visual comparisons between LIP <ref type="bibr" target="#b16">[17]</ref>, MHP v1.0 <ref type="bibr" target="#b24">[25]</ref> and our MHP v2.0 are provided in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background Hat Hair Gloves Sunglasses Upper-clothes Dress Coat Socks Pants Jumpsuits Scarf Skirt Face Left-arm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cap/Hat Helmet Face Hair Left-arm Right-arm Left-hand Right-hand Protector Bikini/Bra Jacket/Windbreaker/Hoodie T-shirt Polo-shirt Sweater Singlet Torso-skin Pants Shorts/Swim-shorts Skirt Stockings Socks Left-boot Right-boot Left-shoe Right-shoe Left-highheel Right-highheel Left-sandal Right-sandal Left-leg Right-leg Left-foot Right-foot Coat Dress Robe Jumpsuits Other-full-body-clothes Headware Backpack Ball Bats Belt Bottle Carrybag Cases Sunglasses Eyeware Gloves Scarf Umbrella Wallet/Purse Watch Wristband Tie Other-accessaries Other-Upper-Body-Clothes Other-Lower-Body-Clothes</head><p>Human Parsing Approaches Recently, many research efforts have been devoted to human parsing <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref> due to its wide range of potential applications. For example, Liang et al. <ref type="bibr" target="#b26">[27]</ref> proposed a proposalfree network for instance segmentation by directly predicting the instance numbers of different categories and the pixel-level information. Gong et al. <ref type="bibr" target="#b16">[17]</ref> proposed a selfsupervised structure-sensitive learning approach, which imposes human pose structures to parsing results without resorting to extra supervision. Liu et al. <ref type="bibr" target="#b30">[31]</ref> proposed a single frame video parsing method which integrates frame parsing, optical flow estimation and temporal fusion into a unified network. Zhao et al. <ref type="bibr" target="#b45">[46]</ref> proposed a self-supervised neural aggregation network, which learns to aggregate the multiscale features and incorporates a self-supervised joint loss to ensure the consistency between parsing and pose. He et al. <ref type="bibr" target="#b18">[19]</ref> proposed the Mask R-CNN, which is extended from Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Brabandere et al. <ref type="bibr" target="#b8">[9]</ref> proposed to tackle instance segmentation with a discriminative loss function, operating at the pixel level, which encourages a convolutional network to produce a representation of the image that can be easily clustered into instances with a simple post-processing step. However, these methods either only consider coarse semantic information or are agnostic to different instances. To enable more detailed human-centric analysis, Li et al. <ref type="bibr" target="#b24">[25]</ref> initially proposed the multi-human parsing task, which aligns better with the realistic scenarios. They also proposed a novel MH-Parser model as a reference method which generates parsing maps and instance masks simutaneously in a bottom-up fashion. Jiang et al. <ref type="bibr" target="#b20">[21]</ref> proposed a new approach to segment human instances and label their body parts using region assembly. Li et al. <ref type="bibr" target="#b25">[26]</ref> proposed a framework with a human detector and a categorylevel segmentation module to segment the parts of objects at the instance level. These methods involve mutiple separate stages for instance localization, human parsing and result refinement. In comparison, the proposed NAN produces accurate multi-human parsing results through a single forward-pass in a time-efficient manner without tedious pre-or post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Human Parsing Benchmark</head><p>In this section, we introduce the "Multi-Human Parsing (MHP v2.0)", a new large-scale dataset focusing on semantic understanding of humans in crowded scenes with several appealing properties. 1) It contains 25,403 elaborately annotated images with 58 fine-grained labels on body parts, fashion items and one background label, which is larger and more comprehensive than previous similar attempts <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b24">25]</ref>. 2) The images within MHP v2.0 are collected from real-world scenarios, involving humans with various viewpoints, poses, occlusion, interactions and resolution.</p><p>3) The background of images in MHP v2.0 is more complex and diverse than previous datasets. Some examples are showed in <ref type="figure" target="#fig_1">Fig. 2</ref>. The MHP v2.0 dataset is expected to provide a new benchmark suitable for multi-human parsing together with a standard evaluation server where the test set will be kept secret to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Collection and Annotation</head><p>We manually specify some underlying relationships (such as family, couple, team, etc.) and possible scenes (such as sports, conferences, banquets, etc.) to ensure the diversity of returned results. Based on any one of these specifications, corresponding multi-human images are located by performing Internet searches over Creative Commons licensed imagery. For each identified image, the contained human number and the corresponding URL are stored in a spreadsheet. Automated scrapping software is used to download the multi-human imagery and stores all relevant information in a relational database. Moreover, a pool of images containing clearly visible persons with interactions and rich fashion items is also constructed from the existing human-centric datasets <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b21">22]</ref> 2 to augment and complement Internet scraping results.</p><p>After curating the imagery, manual annotation is conducted by professional data annotators, which includes two distinct tasks. The first task is manually counting the number of foreground persons and duplicating each image to several copies according to the count number. Each duplicated image is marked with the image ID, the contained person number and a self-index. The second is assigning the fine-grained pixel-wise label to each semantic category for each person instance. We implement an annotation tool and generate multi-scale superpixels of images based on <ref type="bibr" target="#b1">[2]</ref> to speed up the annotation. See <ref type="figure" target="#fig_4">Fig. 4</ref> for an example. Each multi-human image contains at least two instances. The annotation for each instance is done in a left-to-right order, corresponding to the duplicated image with the self-index from beginning to end. For each instance, 58 semantic categories are defined and annotated, including cap/hat, helmet, face, hair, left-arm, right-arm, left-hand, right-hand, protector, bikini/bra, jacket/windbreaker/hoodie, t-shirt, polo-shirt, sweater, singlet, torso-skin, pants, shorts/swimshorts, skirt, stockings, socks, left-boot, right-boot, leftshoe, right-shoe, left-highheel, right-highheel, left-sandal, right-sandal, left-leg, right-leg, left-foot, right-foot, coat, dress, robe, jumpsuits, other-full-body-clothes, headwear, backpack, ball, bats, belt, bottle, carrybag, cases, sunglasses, eyewear, gloves, scarf, umbrella, wallet/purse, watch, wristband, tie, other-accessaries, other-upper-bodyclothes and other-lower-body-clothes. Each instance has a complete set of annotations whenever the corresponding category appears in the current image. When annotating one instance, others are regarded as background. Thus, the resulting annotation set for each image consists of N instance-level parsing masks, where N is the number of persons in the image.</p><p>After annotation, manual inspection is performed on all images and corresponding annotations to verify the correctness. In cases where annotations are erroneous, the information is manually rectified by 5 well informed analysts. The whole work took around three months to accomplish by 25 professional data annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset Splits and Statistics</head><p>In total, there are 25,403 images in the MHP v2.0 dataset. Each image contains 2-26 person instances, with 3 on average. The resolution of the images ranges from 85×100 to 4,511×6,919, with 644×718 on average. We spit the images into training, validation and testing sets. Following random selection, we arrive at a unique split consisting of 15,403 training and 5,000 validation images with publicly available annotations, as well as 5,000 testing images with annotations withheld for benchmarking purpose.</p><p>The statistics w.r.t. data distribution on 59 semantic categories, the average semantic category number per image and the average instance number per image in the MHP v2.0 dataset are illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref> (a), (b) and (c), respectively. In general, face, arms and legs are the most remarkable parts of a human body. However, understanding humans in crowded scenes needs to analyze fine-grained details of each person of interest, including different body parts, clothes and accessaries. We therefore define 11 body parts, and 47 clothes and accessaries. Among these 11 body parts, we divide arms, hands, legs and feet into left and right side for more precise analysis, which also increases    the difficulty of the task. We define hair, face and torsoskin as the remaining three body parts, which can be used as auxiliary guidance for more comprehensive instancelevel analysis. As for clothing categories, we have common clothes like coat, jacket/windbreaker/hoodie, sweater, singlet, pants, shorts/swim-shorts and shoes, confusing categories such as t-shirt v.s. polo-shirt, stockings v.s. socks, skirt v.s. dress and robe, and boots v.s. sandals and highheels, and infrequent categories such as cap/hat, helmet, protector, bikini/bra, jumpsuits, gloves and scarf. Furthermore, accessaries like sunglasses, belt, tie, watch and bags are also taken into account, which are common but hard to predict, especially for the small-scale ones.</p><p>To summarize, the pre-defined semantic categories of MHP v2.0 involve most body parts, clothes and accessaries of different styles for men, women and children in all seasons. The images in the MHP v2.0 dataset contain diverse instance numbers, viewpoints, poses, occlusion, interactions and background complexities. MHP v2.0 aligns better with real-world scenarios and serves as a more real-istic benchmark for human-centric analysis, which pushes the frontiers of fine-grained multi-human parsing research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep Nested Adversarial Networks</head><p>As shown in <ref type="figure">Fig. 5</ref>, the proposed deep Nested Adversarial Network (NAN) model consists of three GANlike sub-nets that jointly perform semantic saliency prediction, instance-agnostic parsing and instance-aware clustering end-to-end. NAN produces accurate multi-human parsing results through a single forward-pass in a time-efficient manner without tedious pre-or post-processing. We now present each component in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semantic Saliency Prediction</head><p>Large modality and interaction variations are the main challenge to multi-human parsing and also the key obstacle to learning a well-performing human-centric analysis model. To address this problem, we propose to decompose the original task into three granularities and adaptively impose a prior on the specific process, each with the aid of a GAN-based sub-net. This reduces the training complexity and leads to better empirical performance with limited data.</p><p>The first sub-net estimates semantic saliency maps to locate the most noticeable and eye-attracting human regions in images, which serves as a basic prior to facilitate further processing on humans, as illustrated in <ref type="figure">Fig. 5</ref> left. We formulate semantic saliency prediction as a binary pixel-wise labelling problem to segment out foreground v.s. background. Inspired by the recent success of Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b31">[32]</ref> based methods on image-to-image applications <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19]</ref>, we leverage an FCN backbone (FCN-8s <ref type="bibr" target="#b31">[32]</ref>) as the generator G1 θ1 : R H×W ×C → R H×W ×C of NAN for semantic saliency prediction, where θ 1 denotes the network parameters, and H, W , C and C denote the image height, width, channel number and semantic category (i.e., foreground plus background) number, repectively.</p><p>Formally, let the input RGB image be denoted by x and</p><formula xml:id="formula_0">ℒ "#$% G1 RGB D1 ℒ &amp;'( Pred. G.T. Pred. G.T. ℒ ') 1:1 1:2 1:4 1:8 Multi-Scale Fusion Unit G2 Pred. G.T. ℒ *&amp; ℒ "#$+ D2 Real Fake G3 ℒ "#$,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D3</head><p>Concat Fake Real Pred.</p><p>G.T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ℒ --</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat</head><p>Real Fake <ref type="figure">Figure 5</ref>: Deep Nested Adversarial Networks (NAN) for multi-human parsing. NAN consists of three GAN-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. Each sub-task is simpler than the original multi-human parsing task, and is more easily addressed by the corresponding sub-net. The sub-nets depend on each other, forming a causal nest by dynamically boosting each other via an adversarial strategy. Such a structure enables effortless gradient BackPropagation (BP) of NAN such that it can be trained in a holistic, end-to-end way. NAN produces accurate multi-human parsing results through a single forward-pass in a time-efficient manner without tedious pre-or post-processing. Best viewed in color.</p><p>the semantic saliency map be denoted by x , then</p><formula xml:id="formula_1">x := G1 θ 1 (x).<label>(1)</label></formula><p>The key requirements for G1 are that the semantic saliency map x should present indistinguishable properities compared with a real one (i.e., ground truth) in appearance while preserving the intrinsic contextually remarkable information.</p><p>To this end, we propose to learn θ 1 by minimizing a combination of two losses:</p><formula xml:id="formula_2">LG1 θ 1 = −λ1L adv 1 + λ2Lss,<label>(2)</label></formula><p>where L adv1 is the adversarial loss for refining realism and alleviating artifacts, L ss is the semantic saliency loss for pixel-wise image labelling, λ are weighting parameters among different losses. L ss is a pixel-wise cross-entropy loss calculated based on the binary pixel-wise annotations to learn θ 1 :</p><formula xml:id="formula_3">Lss = Lss(X (θ1)|X).<label>(3)</label></formula><p>L adv1 is proposed to narrow the gap between the distributions of generated and real results. To facilitate this process, we leverage a Convolutional Neural Network (CNN) backbone as the discriminator D1 φ1 : R H×W ×C → R 1 to be as simple as possible to avoid typical GAN tricks. We alternatively optimize G1 θ1 and D1 φ1 to learn θ 1 and φ 1 :</p><formula xml:id="formula_4">L G1 adv 1 = L adv 1 (K(θ1)|X (θ1), X GT), L D1 adv 1 = L adv 1 (K(φ1)|X (θ1), X GT),<label>(4)</label></formula><p>where K denotes the binary real v.s. fake indicator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Instance-Agnostic Parsing</head><p>The second sub-net concatenates the information from the original RGB image with semantic saliency prior as input and estimates a fine-grained instance-agnostic parsing map, which further serves as stronger semantic guidance from the global perspective to facilitate instance-aware clustering, as illustrated in <ref type="figure">Fig. 5</ref> middle. We formulate instance-agnostic parsing as a multi-class dense classification problem to mask semantically consistent regions of body parts and fashion items. Inspired by the leading performance of the skip-net on recognition tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b19">20]</ref>, we modify a skip-net (WS-ResNet <ref type="bibr" target="#b39">[40]</ref>) into an FCN-based architecture as the generator G 2θ 2 : R H×W ×(C+C ) → R H/8×W/8×C of NAN to learn a highly non-linear transformation for instance-agnostic parsing, where θ 2 denotes the network parameters for the generator and C denotes the semantic category number. The prediction is downsampled by 8 for accuracy v.s. speed trade-off. Contextual information from global and local regions compensates each other and naturally benefits human parsing. The hierarchical features within a skip-net are multi-scale in nature due to the increasing receptive field sizes, which are combined together via skip connections. Such a combined representation comprehensively maintains the contextual information, which is crucial for generating smooth and accurate parsing results.</p><p>Formally, let the instance-agnostic parsing map be denoted by x , then</p><formula xml:id="formula_5">x := G2 θ 2 (x ∪ x ).<label>(5)</label></formula><p>Similar to the first sub-net, we propose to learn θ 2 by minimizing:</p><p>LG2 θ 2 = −λ3L adv 2 + λ4Lgp, <ref type="bibr" target="#b5">(6)</ref> where L gp is the global parsing loss for semantic part labelling. L gp is a standard pixel-wise cross-entropy loss calculated based on the multi-class pixel-wise annotations to learn θ 2 . θ 1 is also slightly finetuned due to the hinged gradient backpropagation route within the nested structure:</p><formula xml:id="formula_6">Lgp = Lgp(X (θ2, θ1)|X ∪ X (θ1)).<label>(7)</label></formula><p>L adv2 is proposed to ensure the correctness and realism of the current phase and also the previous one for information flow consistency. To facilitate this process, we leverage a same CNN backbone with D1 φ1 as the discriminator D2 φ2 : R H×W ×(C +C ) → R 1 , which are learned separately. We alternatively optimize G2 θ2 and D2 φ2 to learn θ 2 , φ 2 and slightly finetune θ 1 :</p><formula xml:id="formula_7">L G2 adv 2 = L adv 2 (K(θ2)|X (θ1) ∪ X (θ2, θ1), X GT ∪ X GT), L D2 adv 2 = L adv 2 (K(φ2)|X (θ1) ∪ X (θ2, θ1), X GT ∪ X GT).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Instance-Aware Clustering</head><p>The third sub-net concatenates the information from the original RGB image with semantic saliency and instanceagnostic parsing priors as input and estimates an instanceaware clustering map by associating each semantic parsing mask to one of the person instances in the scene, as illustrated in <ref type="figure">Fig. 5</ref> right. Inspired by the observation that a human glances at an image and instantly knows how many and where the objects are in the image, we formulate instance-aware clustering by parallelly inferring the instance number and pixel-wise instance location, discarding the requirement of time-consuming region proposal generation. We modify a same backbone architecture G2 θ2 to incorporate two sibling branches as the generator G 3θ 3 :</p><p>R H/8×W/8×(C+C +C ) → R H/8×W/8×M ∪R 1 of NAN for location-sensitive learning, where θ 3 denotes the network parameters for the generator and M denotes the pre-defined instance location coordinate number. As multi-scale features integrating both global and local contextual information are crucial for increasing location prediction accuracy, we further augment the pixel-wise instance location prediction branch with a Multi-Scale Fusion Unit (MSFU) to fuse shallow-, middle-and deep-level features, while using the feature maps downsampled by 8 concatenated with feature maps from the first branch for instance number regression.</p><p>Formally, let the pixel-wise instance location map be denoted byx and the instance number be denoted by n, theñ</p><formula xml:id="formula_8">x ∪ n := G3 θ 3 (x ∪ x ∪ x ).<label>(9)</label></formula><p>We propose to learn θ 3 by minimizing:</p><p>LG3 θ 3 = −λ5L adv 3 + λ6L pil + λ7Lin, <ref type="bibr" target="#b9">(10)</ref> where L pil is the pixel-wise instance location loss for pixelwise instance location regression and L in is the instance number loss for instance number regression. L pil is a standard smooth-1 loss <ref type="bibr" target="#b15">[16]</ref> calculated based on the foreground pixel-wise instance location annotations to learn θ 3 . Since a person instance can be identified by its top-left corner (x l , y l ) and bottom-right corner (x r , y r ) of the surrounding bounding box, for each pixel belonging to the person instance, the pixel-wise instance location vector is defined as [x l /w, y l /h, x r /w, y r /h], where w and h are the width and height of the person instance for normalization, respectively. L in is a standard 2 loss calculated based on the instance number annotations to learn θ 3 . θ 2 and θ 1 are also slightly finetuned due to the chained schema within the nest:</p><formula xml:id="formula_9">L pil = L pil (X(θ3, θ2, θ1)|X ∪ X (θ1) ∪ X (θ2, θ1)), Lin = Lin(N (θ3, θ2, θ1)|X ∪ X (θ1) ∪ X (θ2, θ1)).<label>(11)</label></formula><p>Given these information, instance-aware clustering maps can be effortlessly generated with little computational overhead, which are denoted byX ∈ R M . Similar to L adv2 , L adv3 is proposed to ensure the correctness and realism of all phases for the information flow consistency. To facilitate this process, we leverage a same CNN backbone with D2 φ2 as the discriminator D3 φ3 : R H×W ×(C +C +M ) → R 1 , which are learned separately. We alternatively optimize G3 θ3 and D3 φ3 to learn θ 3 , φ 3 and slightly finetune θ 2 and θ 1 :</p><formula xml:id="formula_10">   L G3 adv 3 = L adv 3 (K(θ 3 )|X (θ 1 ) ∪ X (θ 2 , θ 1 ) ∪X(θ 3 , θ 2 , θ 1 ), X GT ∪ X GT ∪X GT ), L D3 adv 3 = L adv 3 (K(φ 3 )|X (θ 1 ) ∪ X (θ 2 , θ 1 ) ∪X(θ 3 , θ 2 , θ 1 ), X GT ∪ X GT ∪X GT ).<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training and Inference</head><p>The goal of NAN is to use sets of real targets to learn three GAN-like sub-nets that mutually boost and jointly accomplish multi-human parsing. Each separate loss serves as a deep supervision within the nested structure benefitting network convergence. The overall objective function for NAN is</p><formula xml:id="formula_11">LNAN = − 3 i=1 λiL adv i + λ4Lss + λ5Lgp + λ6L pil + λ7Lin.<label>(13)</label></formula><p>Clearly, the NAN is end-to-end trainable and can be optimized with the proposed nested adversarial learning strategy and BP algorithm.</p><p>During testing, we simply feed the input image X into NAN to get the instance-agnostic parsing map X from G2 θ2 , pixel-wise instance location mapX and instance number N from G3 θ3 . Then we employ an off-the-shelf clustering method <ref type="bibr" target="#b32">[33]</ref> to obtain the instance-aware clustering mapX. Example results are visualized in <ref type="figure" target="#fig_7">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate NAN qualitatively and quantitatively under various settings and granularities for understanding humans in crowded scenes. In particular, we evaluate multihuman parsing performance on the MHP v2.0 dataset proposed in this work, as well as the MHP v1.0 <ref type="bibr" target="#b24">[25]</ref> and PASCAL-Person-Part <ref type="bibr" target="#b3">[4]</ref> benchmark datasets. We also evaluate instance-agnostic parsing and instance-aware clustering results on the Buffy <ref type="bibr" target="#b38">[39]</ref> benchmark dataset, which are byproducts of NAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Implementation Details</head><p>Throughout the experiments, the sizes of the RGB image X, the semantic saliency prediction X , inputs to the discriminator D1 φ1 and inputs to the generator G2 θ2 are fixed as 512×512; the sizes of the instance-agnostic parsing prediction X , instance-aware clustering predictionX, inputs to the discriminator D2 φ2 , inputs to the generator G3 θ3 , inputs to the discriminator D3 φ3 and instance location mapX are fixed as 64×64; the channel number of the pixel-wise instance location map is fixed as 4, incorporating two corner points of the associated bounding box; the constraint factors λ i , i ∈ {1, 2, 3, 4, 5, 6, 7} are empirically fixed as 0.01, 0.01, 0.01, 1.00, 1.00, 10.00 and 1.00, respectively; the generator G1 θ1 is initialized with FCN-8s <ref type="bibr" target="#b31">[32]</ref> by replacing the last layer with a new convolutional layer with kernel size 1 × 1 × 2, pretrained on PASCAL-VOC-2011 <ref type="bibr" target="#b12">[13]</ref> and finetuned on the target dataset; the generator G2 θ2 is initialized with WS-ResNet <ref type="bibr" target="#b39">[40]</ref> by eliminating the spatial pooling layers, increasing the strides of the first convolutional layers up to 2 in B i , i ∈ {2, 3, 4}, eliminating the top-most global pooling layer and the linear classifier, and adding two new convolutional layers with kernel sizes 3 × 3 × 512 and 1 × 1 × C , pretrained on Ima-geNet <ref type="bibr" target="#b34">[35]</ref> and PASCAL-VOC-2012 <ref type="bibr" target="#b10">[11]</ref>, and finetuned on the target dataset; the generator G3 θ3 is initialized with the same backbone architecture and pre-trained weights with G2 θ2 (which are learned separately), by further augmenting it with two sibling branches for pixel-wise instance location map prediction and instance number prediction, where the first branch utilizes a MSFU (three convolutional layers with kernal sizes 3 × 3 × i, i ∈ {128, 128, 4} for specific scale adaption) ended with a convolutional layer with kernel size 1 × 1 × 4 for multi-scale feature aggregation and a final convolutional layer with kernel size 1 × 1 × 4 for location regression and the second branch utilizes the feature maps downsampled by 8 concatenated with the feature maps from the first branch ended with a global pool-ing layer, a hidden 512-way fully-connected layer and a final 1-way fully-connected layer for instance number regression; the three discriminators Di φi , i ∈ {1, 2, 3} (which are learned separately) are all initialized with a VGG-16 <ref type="bibr" target="#b36">[37]</ref> by adding a new convolutional layer at the very begining with kernel size 1 × 1 × 3 for input adaption, and replacing the last layer with a new 1-way fully-connected layer activated by sigmoid, pre-trained on ImageNet <ref type="bibr" target="#b34">[35]</ref> and finetuned on the target dataset; the newly added layers are randomly initialized by drawing weights from a zeromean Gaussian distribution with standard deviation 0.01; we employ an off-the-shelf clustering method <ref type="bibr" target="#b32">[33]</ref> to obtain the instance-aware clustering mapX; the dropout ratio is empirically fixed as 0.7; the weight decay and batch size are fixed as 5×10 −3 and 4, respectively; We use an initial learning rate of 1×10 −6 for pre-trained layers, and 1×10 −4 for newly added layers in all our experiments; we decrease the learning rate to 1/10 of the previous one after 20 epochs and train the network for roughly 60 epochs one after the other; the proposed network is implemented based on the publicly available TensorFlow <ref type="bibr" target="#b0">[1]</ref> platform, which is trained using Adam (β 1 =0.5) on four NVIDIA GeForce GTX TITAN X GPUs with 12G memory; the same training setting is utilized for all our compared network variants; we evaluate the testing time by averaging the running time for images on the target set on NVIDIA GeForce GTX TITAN X GPU and Intel Core i7-4930K CPU@3.40GHZ; our NAN can rapidly process one 512 × 512 image in about 1 second, which compares much favorably to other stateof-the-art approaches, as the current state-of-the-art methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref> rely on region proposal preprocessing and complex processing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Evaluation Metrics</head><p>Following <ref type="bibr" target="#b24">[25]</ref>, we use the Average Precision based on part (AP p ) and Percentage of Correctly parsed semantic Parts (PCP) metrics for multi-human parsing evaluation. Different from the Average Precision based on region (AP r ) used in instance segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18]</ref>, AP p uses part-level pixel Intersection over Union (IoU) of different semantic part categories within a person instance to determine if one instance is a true positive. We prefer AP p over AP r as we focus on human-centric analysis and we aim to investigate to how well a person instance as a whole is parsed. Additionally, we also report the AP p vol , which is the mean of the AP p at IoU thresholds ranging from 0.1 to 0.9, in increments of 0.1. As AP p averages the IoU of each semantic part category, it fails to reflect how many semantic parts are correctly parsed. We further incorporate the PCP, originally used in human pose estimation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4]</ref>, to evaluate the parsing quality within person instances. For each true-positive person instance, we find all the semantic categories (excluding background) with pixel IoU larger than a threshold, which are regarded as correctly parsed. The PCP of one person instance is the ratio between the correctly parsed semantic category number and the total semantic category number of that person. Missed person instances are assigned with 0 PCP. The overall PCP is the average PCP for all person instances. Note that PCP is also a human-centric evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluations on the MHP v2.0 Benchmark</head><p>The MHP v2.0 dataset proposed in this paper is the largest and most comprehensive multi-human parsing benchmark to date, which extends MHP v1.0 <ref type="bibr" target="#b24">[25]</ref> to push the frontiers of understanding humans in crowded scenes by containing 25,403 elaborately annotated images with 58 fine-grained semantic category labels. Annotation examples are visualized in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>. The data are randomly organized into 3 splits, consisting of 15,403 training and 5,000 validation images with publicly available annotations, as well as 5,000 testing images with annotations withheld for benchmarking purpose. Evaluation systems report the AP p and PCP over the validation and testing sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Component Analysis</head><p>We first investigate different architectures and loss function combinations of NAN to see their respective roles in multi-human parsing. We compare 16 variants from four aspects, i.e., different baselines (Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>  proposed NAN, and upperbounds (X GT : use the ground truth semantic saliency maps instead of G1 prediction while keeping other settings the same; X GT : use the ground truth instance-agnostic parsing maps instead of G2 prediction while keeping other settings the same; N GT : use the ground truth instance number instead of G3 prediction while keeping other settings the same;X GT : use the ground truth pixel-wise instance location maps instead of G3 prediction while keeping other settings the same).</p><p>The performance comparison in terms of AP p @IoU=0.5, AP p vol and PCP@IoU=0.5 on the MHP v2.0 validation set is reported in Tab. 2. By comaring the results from the 1 st v.s. 3 rd panels, we observe that our proposed NAN consistently outperforms the baselines Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> and MH-Parser <ref type="bibr" target="#b24">[25]</ref> by a large margin, i.e., 10.33% and 6.78% in terms of AP p , 9.26% and 6.90% in terms of AP p vol , and 9.25% and 7.46% in terms of PCP. Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> suffers difficulties to differentiate <ref type="bibr" target="#b2">3</ref> As existing instance segmentation methods only offer silhouettes of different person instances, for comparison, we combine them with our instance-agnostic parsing prediction to generate the final multi-human parsing results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Quantitative Comparison</head><p>The performance comparison of the proposed NAN with two state-of-the-art methods in terms of AP p @IoU=0.5, AP p vol and PCP@IoU=0.5 on the MHP v2.0 testing set is reported in Tab. 3. Following <ref type="bibr" target="#b24">[25]</ref>, we conduct experiments under three settings: All reports the evaluation over the whole testing set; Inter 20% reports the evaluation over the sub-set containing the images with top 20% interaction intensity <ref type="bibr" target="#b3">4</ref> ; Inter 10% reports the evaluation over the sub-set containing the images with top 10% interaction intensity. Our NAN is significantly superior over other state-of-thearts on setting-1. In particular, NAN improves the 2 nd -best by 7.15%, 5.70% and 5.27% in terms of all metrics. For the more challenging scenarios with intensive interactions (setting-2, 3), NAN also consistently achieves the best performance. In particular, for Inter 20% and Inter 10% , NAN improves the 2 nd -best by 5.23%, 4.65% and 5.62%; 1.63%, 3.20% and 3.33% in terms of all metrics. This verifies the effectiveness of our NAN for multi-human parsing and understanding humans in crowded scenes. Moreover, NAN can rapidly process one 512×512 image in about 1 second with acceptable resource consumption, which is attractive to real applications. This compares much favorably to MH-Parser <ref type="bibr" target="#b24">[25]</ref> (14.94 img/s), which relies on separate and complex post-processing (including CRF <ref type="bibr" target="#b22">[23]</ref>) steps. <ref type="figure" target="#fig_7">Fig. 6</ref> visualizes the qualitative comparison of the proposed NAN with two state-of-the-art methods and corresponding ground truths on the MHP v2.0 dataset. Note that Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> only offers silhouettes of different person instances, we only compare our instance-aware clustering results with it while comparing our holistic results with MH-Parser <ref type="bibr" target="#b24">[25]</ref>. It can be observed that the proposed NAN performs well in multi-human parsing with a wide range of viewpoints, poses, occlusion, interactions and background complexity. The instance-agnostic parsing and instanceaware clustering predictions of NAN present high consistency with corresponding ground truths, thanks to the novel network structure and effective training strategy. In contrast, Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> suffers difficulties to differentiate entangled humans, while MH-Parser <ref type="bibr" target="#b24">[25]</ref> struggles to generate fine-grained parsing results and clearly segmented instance masks. This further desmonstrates the effectiveness of the proposed NAN. We also show some failure cases of our NAN in <ref type="figure" target="#fig_9">Fig. 7</ref>. As can be observed, humans in crowded scenes with heavy occlusion, extreme poses and intensive interactions are difficult to identify and segment. Some small-scale semantic categories within person instances are difficult to parse. This confirms that MHP v2.0 aligns with real-world situations and deserves more furture attention and research efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Qualitative Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluations on the MHP v1.0 Benchmark</head><p>The MHP v1.0 5 dataset is the first multi-human parsing benchmark, originally proposed by Li et al. <ref type="bibr" target="#b24">[25]</ref>, which contains 4,980 images annotated with 18 semantic labels. Annotation examples are visualized in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>. The data are randomly organized into 3 splits, consisting of 3,000 training, 1,000 validation and 1,000 testing images with publicly available annotations. Evaluation systems report the AP p and PCP over the testing set. Refer to <ref type="bibr" target="#b24">[25]</ref> for more details.</p><p>The performance comparison of the proposed NAN with three state-of-the-art methods in terms of AP p @IoU=0.5, AP p vol and PCP@IoU=0.5 on the MHP v1.0 <ref type="bibr" target="#b24">[25]</ref> testing set is reported in Tab. 4. With the nested adversarial learning of semantic saliency prediction, instance-agnostic parsing and instance-aware clustering, our method outperforms the 2 ndbest by 4.41% for AP p 0.5 , 6.95% for AP p vol and 8.04% for PCP 0.5 . Visual comparison of multi-human parsing results by NAN and three state-of-the-art methods is provided in <ref type="figure" target="#fig_10">Fig. 8</ref>, which further validates the advantages of our NAN over existing solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluations on the PASCAL-Person-Part Benchmark</head><p>The PASCAL-Person-Part 6 <ref type="bibr" target="#b3">[4]</ref> dataset is a set of additional annotations for PASCAL-VOC-2010 <ref type="bibr" target="#b11">[12]</ref>. It goes beyond the original PASCAL object detection task by providing pixel-wise labels for six human body parts, i.e., head, torso, upper-/lower-arms, and upper-/lower-legs. The rest of each image is considered as background. There are 3,535 images in the PASCAL-Person-Part <ref type="bibr" target="#b3">[4]</ref> dataset, which is split into separate training set containing 1,717 images and testing set containing 1,818 images. For fair comparison, we report the AP r over the testing set for multi-human parsing. Refer to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41]</ref> for more details.</p><p>The performance comparison of the proposed NAN with two state-of-the-art methods in terms of AP r @IoU=k k= 0.7 0.5 <ref type="bibr" target="#b4">5</ref> The dataset is available at http://lv-mhp.github.io/ <ref type="bibr" target="#b5">6</ref> The dataset is available at http://www.stat.ucla.edu/ xianjie.chen/pascal_part_dataset/pascal_part. html      and AP r vol on the PASCAL-Person-Part <ref type="bibr" target="#b3">[4]</ref> testing set is reported in Tab. 5. Our method dramatically surpasses the 2 nd -best by 18.90% for AP r 0.7 and 13.80% for AP r vol .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cap/Hat Helmet Face Hair Left-arm Right-arm Left-hand Right-hand Protector Bikini/Bra Jacket/Windbreaker/Hoodie T-shirt Polo-shirt Sweater Singlet Torso-skin Pants Shorts/Swim-shorts Skirt Stockings Socks Left-boot Right-boot Left-shoe Right-shoe Left-highheel Right-highheel Left-sandal Right-sandal Left-leg Right-leg Left-foot Right-foot Coat Dress Robe Jumpsuits Other-full-body-clothes Headware Backpack Ball Bats Belt Bottle Carrybag Cases Sunglasses Eyeware Gloves Scarf Umbrella Wallet/Purse Watch Wristband Tie Other-accessaries Other-Upper-Body-Clothes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background Hat Hair Sunglasses Upper-clothes Dress Belt Pants</head><p>Right Qualitative multi-human parsing results by NAN are visualized in <ref type="figure" target="#fig_11">Fig. 9</ref>, which possess a high concordance with corresponding ground truths. This again verifies the effectiveness of our method for human-centric analysis.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Evaluations on the Buffy Benchmark</head><p>The Buffy 7 <ref type="bibr" target="#b38">[39]</ref> dataset was released in 2011 for human parsing and instance segmentation, which contains 748 images annotated with 12 semantic labels. The data are randomly organized into 2 splits, consisting of 452 training and 296 testing images with publicly available annotations. For fair comparison, we report the Forward (F) and Backward (B) scores <ref type="bibr" target="#b20">[21]</ref> over the episode 4, 5 and 6 for instance seg- <ref type="bibr" target="#b6">7</ref> The dataset is available at https://www.inf.ethz.ch/ personal/ladickyl/Buffy.zip mentation evaluation. Refer to <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b20">21]</ref> for more details.</p><p>The performance comparison of the proposed NAN with three state-of-the-art methods in terms of F and B scores on the Buffy <ref type="bibr" target="#b38">[39]</ref> dataset episode 4, 5 and 6 is reported in Tab. 6. Our NAN consistently achieves the best performance for all metrics. In particualr, NAN significantly improves the 2 nd -best by 6.13% for F score and 7.98% for B score, with an average boost of 7.05%. Qualitative instance-agnostic parsing and instance-aware clustering results by NAN are visualized in <ref type="figure" target="#fig_0">Fig. 10</ref>, which well shows the promising potential of our method for fine-grained understanding humans in crowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we presented "Multi-Human Parsing (MHP v2.0)", a large-scale multi-human parsing dataset and a carefully designed benchmark to spark progress in understanding humans in crowded scenes. MHP v2.0 contains 25,403 images, which are richly labelled with 59 semantic categories. We also proposed a novel deep Nested Adversarial Network (NAN) model to address this challenging problem and performed detailed evaluations of the proposed method with current state-of-the-arts on MHP v2.0 and several other datasets. We envision the proposed MHP v2.0 dataset and the baseline method would drive the human parsing research towards real-world application scenario with simultaneous presence of multiple persons and complex interactions among them. In future, we will continue to take efforts to construct a more comprehensive multi-human parsing benchmark dataset with more images and more detailed semantic category annotations to further push the frontiers of multi-human parsing research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of motivation. While existing efforts on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Annotation examples for our "Multi-Human Parsing (MHP v2.0)" dataset and existing datasets. (a) Examples in LIP [17]. LIP is restricted to an instance-agnostic setting and has limited semantic category annotations. (b) Examples in MHP v1.0 [25]. MHP v1.0 has lower scalability, variability and complexity, and only contains coarse labels. (c) Examples in our MHP v2.0. MHP v2.0 contains fine-grained semantic category labels with various viewpoints, poses, occlusion, interactions and background, aligned better with reality. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The data distribution on semantic category labels of our MHP v2.0 and the MHP v1.0 datasets. (b) The statistic comparison on the average semantic category number per image between our MHP v2.0 and the MHP v1.0 datasets. (c) The statistic comparison on the average instance number per image between our MHP v2.0 and the MHP v1.0 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Dataset statistics. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Annotation tool for multi-human parsing. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>3 and MH-Parser [25]), different network structures (w/o G1, G2 w/o concatenated input (RGB only), G3 w/o concatenated input (RGB only), w/o D1, w/o D2, D2 w/o concatenated input, w/o D3, D3 w/o concatenated input, w/o MSFU), our</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Multi-human parsing qualitative comparison on the MHP v2.0 dataset. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Failure cases of multi-human parsing results by our NAN on the proposed MHP v2.0 dataset. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Multi-human parsing qualitative comparison on the MHP v1.0 [25] dataset. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Multi-human parsing qualitative comparison on the PASCAL-Person-Part [4] dataset. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Background</head><label></label><figDesc>Head Right-torso Left-upper-arm Right-upper-arm Left-upper-leg Right-upper-leg Left-torso Left-lower-arm Right-lower-arm Left-lower-leg Right-lower-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative instance-agnostic parsing (upper panel) and instance-aware clustering (lower panel) results by NAN on the Buffy [39] dataset. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics for publicly available human parsing datasets.</figDesc><table><row><cell>Datasets</cell><cell>Instance Aware?</cell><cell># Total</cell><cell># Training</cell><cell># Validation</cell><cell># Testing</cell><cell># Category</cell></row><row><cell>Buffy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Component analysis on the MHP v2.0 validation set. Di, i ∈ {1, 2, 3} with NAN, i.e., 1.02%, 8.82% and 6.78%; 5.81%, 13.11% and 11.48%; 4.21%, 9.94% and 8.15% decrease in terms of all metrics. Nested adversarial learning strategy ensures the correctness and realism of all phases for information flow consistency, the superiority of which is verified by comparing Di, i ∈ {2, 3} w/o concatenated input with NAN, i.e., 3.28%, 10.83% and 9.47%; 3.03%, 8.23% and 7.07% decline in terms of all metrics. MSFU dynamically fuses multi-scale features for enhancing instance-aware clustering accuracy, the superiority of which is verified by comparing w/o MSFU with NAN, i.e., 6.07%, 16.15% and 9.43% drop in terms of all metrics. Finally, we also evaluate the limitations of our current algorithm. By comparing X GT with NAN, only 1.34%, 0.82% and 3.74% improvement in term of all metrics are obtained, which shows that the errors from semantic saliency prediction are already small and have only little effect on the final results. A large gap between 28.98%, 48.55% and 38.03% of X GT and 24.83%, 42.77% and 34.37% of NAN shows that a better instance-agnostic parsing network architecture can definitely help improve the performance of multi-human parsing under our NAN framework. By comparing N GT andX GT with NAN, 3.56%, 4.99% and 4.88%; 5.35%, 8.67% and 6.81% improvement in term of all metrics are obtained, which shows that accurate instance-aware clustering results are critical for superior multi-human parsing.</figDesc><table><row><cell>Setting</cell><cell>Method</cell><cell>AP</cell><cell>p 0.5 (%)</cell><cell>AP</cell><cell>p vol</cell><cell>(%)</cell><cell>PCP 0.5 (%)</cell></row><row><cell></cell><cell>Mask R-CNN [19]</cell><cell cols="2">14.50</cell><cell></cell><cell cols="2">33.51</cell><cell>25.12</cell></row><row><cell>Baseline</cell><cell>MH-Parser [25]</cell><cell cols="2">18.05</cell><cell></cell><cell cols="2">35.87</cell><cell>26.91</cell></row><row><cell></cell><cell>w/o G1</cell><cell cols="2">22.67</cell><cell></cell><cell cols="2">38.11</cell><cell>31.95</cell></row><row><cell></cell><cell>G2 w/o concatenated input</cell><cell cols="2">21.88</cell><cell></cell><cell cols="2">36.79</cell><cell>29.02</cell></row><row><cell></cell><cell>G3 w/o concatenated input</cell><cell cols="2">22.36</cell><cell></cell><cell cols="2">35.92</cell><cell>25.48</cell></row><row><cell></cell><cell>w/o D1</cell><cell cols="2">23.81</cell><cell></cell><cell cols="2">33.95</cell><cell>27.59</cell></row><row><cell>Network Structure</cell><cell>w/o D2</cell><cell cols="2">19.02</cell><cell></cell><cell cols="2">29.66</cell><cell>22.89</cell></row><row><cell></cell><cell>D2 w/o concatenated input</cell><cell cols="2">21.55</cell><cell></cell><cell cols="2">31.94</cell><cell>24.90</cell></row><row><cell></cell><cell>w/o D3</cell><cell cols="2">20.62</cell><cell></cell><cell cols="2">32.83</cell><cell>26.22</cell></row><row><cell></cell><cell>D3 w/o concatenated input</cell><cell cols="2">21.80</cell><cell></cell><cell cols="2">34.54</cell><cell>27.30</cell></row><row><cell></cell><cell>w/o MSFU</cell><cell cols="2">18.76</cell><cell></cell><cell cols="2">26.62</cell><cell>24.94</cell></row><row><cell>Ours</cell><cell>NAN</cell><cell cols="2">24.83</cell><cell></cell><cell cols="2">42.77</cell><cell>34.37</cell></row><row><cell></cell><cell>X GT</cell><cell cols="2">26.17</cell><cell></cell><cell cols="2">43.59</cell><cell>38.11</cell></row><row><cell>Upperbound</cell><cell>X GT N GT</cell><cell cols="2">28.98 28.39</cell><cell></cell><cell cols="2">48.55 47.76</cell><cell>38.03 39.25</cell></row><row><cell></cell><cell>X GT</cell><cell cols="2">30.18</cell><cell></cell><cell cols="2">51.44</cell><cell>41.18</cell></row><row><cell cols="8">entangled humans. MH-Parser [25] involves multiple</cell></row><row><cell cols="8">stages for instance localization, human parsing and result</cell></row><row><cell cols="8">refinement with high complexity, yielding sub-optimal</cell></row><row><cell cols="8">results, whereas NAN parses semantic categories, dif-</cell></row><row><cell cols="8">ferentiates different person instances and refines results</cell></row><row><cell cols="8">simultaneously through deep nested adversarial learning</cell></row><row><cell cols="8">in an effective yet time-efficient manner. By comaring the</cell></row><row><cell cols="8">results from the 2 nd v.s. 3 rd panels, we observe that NAN</cell></row><row><cell cols="8">consistently outperforms the 9 variants in terms of network</cell></row><row><cell cols="8">structure. In particular, w/o G1 refers to truncating the</cell></row><row><cell cols="8">semantic saliency prediction sub-net from NAN, leading</cell></row><row><cell cols="8">to 2.16%, 4.66% and 2.42% performance drop in terms of</cell></row><row><cell cols="8">all metrics. This verifies the necessity of semantic saliency</cell></row><row><cell cols="8">prediction that locates the most noticeable human regions</cell></row><row><cell cols="8">in images to serve as a basic prior to facilitate further</cell></row><row><cell cols="8">human-centic processing. The superiority of incorporating</cell></row><row><cell cols="8">adaptive prior information to specific process can be</cell></row><row><cell cols="8">verified by comparing Gi, i ∈ {2, 3} w/o concatenated</cell></row><row><cell cols="8">input with NAN, i.e., 2.95%, 5.98% and 5.35%; 2.47%,</cell></row><row><cell cols="8">6.85% and 8.89% differences in terms of all metrics. The</cell></row><row><cell cols="8">superiority of incorporating adversarial learning to specific</cell></row><row><cell cols="5">process can be verified by comparing w/o</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :Helmet Face Hair Left-arm Right-arm Left-hand Right-hand Protector Bikini/Bra Jacket/Windbreaker/Hoodie T-shirt Polo-shirt Sweater Singlet Left-boot Right-boot Left-shoe Right-shoe Left-highheel Right-highheel Left-sandal Right-sandal Left-leg Belt Bottle Carrybag Cases Sunglasses Eyeware Gloves Scarf Umbrella Wallet/Purse Watch Wristband Tie Other-accessaries Other-Upper-Body-Clothes Other-Lower-Body-Clothes Background Torso-skin Pants Shorts/Swim-shorts Skirt Stockings Right-leg Left-foot Right-foot Coat Dress Robe Jumpsuits Other-full-body-clothes Headware Backpack Ball Bats Belt</head><label>3</label><figDesc>Multi-human parsing quantitative comparison on the MHP v2.0 testing set.</figDesc><table><row><cell>Method</cell><cell cols="9">All 0.5 (%) AP p AP p vol (%) PCP0.5(%) AP p 0.5 (%) AP p Inter 20% vol (%) PCP0.5(%) AP p 0.5 (%) AP p Inter 10% vol (%) PCP0.5(%)</cell><cell>Speed (img/s)</cell></row><row><cell>Mask R-CNN [19]</cell><cell>14.90</cell><cell>33.88</cell><cell>25.11</cell><cell>4.77</cell><cell>24.28</cell><cell>12.75</cell><cell>2.23</cell><cell>20.73</cell><cell>8.38</cell><cell>-</cell></row><row><cell>MH-Parser [25]</cell><cell>17.99</cell><cell>36.08</cell><cell>26.98</cell><cell>13.38</cell><cell>34.25</cell><cell>22.31</cell><cell>13.25</cell><cell>34.29</cell><cell>21.28</cell><cell>14.94</cell></row><row><cell>NAN</cell><cell>25.14</cell><cell>41.78</cell><cell>32.25</cell><cell>18.61</cell><cell>38.90</cell><cell>27.93</cell><cell>14.88</cell><cell>37.49</cell><cell>24.61</cell><cell>1.08</cell></row><row><cell cols="2">Cap/Hat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Multi-human parsing quantitative comparison on the MHP v1.0 [25] testing set.</figDesc><table><row><cell>Method</cell><cell>AP p 0.5 (%)</cell><cell>AP p vol (%)</cell><cell>PCP0.5(%)</cell></row><row><cell>DL [9]</cell><cell>47.76</cell><cell>47.73</cell><cell>49.21</cell></row><row><cell>MH-Parser [25]</cell><cell>50.10</cell><cell>48.96</cell><cell>50.70</cell></row><row><cell>Mask R-CNN [19]</cell><cell>52.68</cell><cell>49.81</cell><cell>51.87</cell></row><row><cell>NAN</cell><cell>57.09</cell><cell>56.76</cell><cell>59.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Multi-human parsing quantitative comparison on the PASCAL-Person-Part<ref type="bibr" target="#b3">[4]</ref> testing set.</figDesc><table><row><cell>Method</cell><cell>AP r 0.5 (%)</cell><cell>AP r 0.6 (%)</cell><cell>AP r 0.7 (%)</cell><cell>AP r vol (%)</cell></row><row><cell>MNC [8]</cell><cell>38.80</cell><cell>28.10</cell><cell>19.30</cell><cell>36.70</cell></row><row><cell>Li et al. [26]</cell><cell>40.60</cell><cell>30.40</cell><cell>19.10</cell><cell>38.40</cell></row><row><cell>NAN</cell><cell>59.70</cell><cell>51.40</cell><cell>38.00</cell><cell>52.20</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NAN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>G.T.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Background Head Torso Upper-arm Lower-arm Upper-leg Lower-leg</cell><cell></cell><cell></cell></row><row><cell>NAN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>G.T.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Instance segmentation quantitative comparison on the Buffy [39] dataset episode 4, 5 and 6.</figDesc><table><row><cell>Method</cell><cell>F (%)</cell><cell>B (%)</cell><cell>Ave. (%)</cell></row><row><cell>Vineet et al. [39]</cell><cell>-</cell><cell>-</cell><cell>62.40</cell></row><row><cell>Jiang et al. [21]</cell><cell>68.22</cell><cell>69.66</cell><cell>68.94</cell></row><row><cell>MH-Parser [25]</cell><cell>71.11</cell><cell>71.94</cell><cell>71.53</cell></row><row><cell>NAN</cell><cell>77.24</cell><cell>79.92</cell><cell>78.58</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The dataset, annotation tools, and source codes for NAN and evaluation metrics are available at https://github.com/ZhaoJ9014/ Multi-Human-Parsing_MHP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">PASCAL-VOC-2012<ref type="bibr" target="#b10">[11]</ref> and Microsoft COCO<ref type="bibr" target="#b29">[30]</ref> are not included due to limited percent of crowd-scene images with fine details of persons.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For each testing image, we calculate the pair-wise instance bounding box IoU and use the mean value as the interaction intensity for each image.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The work of Jian Zhao was partially supported by China Scholarship Council (CSC) grant 201503170248.</p><p>The work of Jiashi Feng was partially supported by NUS startup R-263-000-C08-133, MOE Tier-I R-263-000-C21-112, NUS IDS R-263-000-C67-646 and ECRA R-263-000-C87-133.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<title level="m">Tensorflow: A system for large-scale machine learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-task recurrent neural network for immediacy prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3352" to="3360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A system for video surveillance and monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duggins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tolliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Enomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VSAM final report</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">TPAMI</biblScope>
			<biblScope unit="page" from="743" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2011/workshop/index.html.8" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressive search space reduction for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Concepts not alone: Exploring pairwise relationships for zero-shot video activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3487</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Fast r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05446</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Detangling people: Individuating multiple close people and their body parts via region assembly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03880</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1931" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Instance-level salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07206</idno>
		<title level="m">Multi-human parsing in the wild</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03612</idno>
		<title level="m">Holistic, instance-level human parsing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human parsing with contextualized convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1386" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A virtual reality platform for dynamic human-scene interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Surveillance video parsing with single frame supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3674" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Electronic commerce: A managerial perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Viehland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Prentice Hall</publisher>
			<biblScope unit="volume">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human instance segmentation from video using detector-based conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="648" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3570" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond frontal faces: Improving person recognition using multiple cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4804" to="4813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">From facial expression recognition to interpersonal relation prediction. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-supervised neural aggregation networks for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
