<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<email>vedaldi@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we present a framework for the recognition of natural scene text. Our framework does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past. The deep neural network models at the centre of this framework are trained solely on data produced by a synthetic text generation engine -synthetic data that is highly realistic and sufficient to replace real data, giving us infinite amounts of training data. This excess of data exposes new possibilities for word recognition models, and here we consider three models, each one "reading" words in a different way: via 90k-way dictionary encoding, character sequence encoding, and bag-of-N-grams encoding. In the scenarios of language based and completely unconstrained text recognition we greatly improve upon state-of-the-art performance on standard datasets, using our fast, simple machinery and requiring zero data-acquisition costs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text recognition in natural images, scene text recognition, is a challenging but wildly useful task. Text is one of the basic tools for preserving and communicating information, and a large part of the modern world is designed to be interpreted through the use of labels and other textual cues. This makes scene text recognition imperative for many areas in information retrieval, in addition to being crucial for human-machine interaction.</p><p>While the recognition of text within scanned documents is well studied and there are many document OCR systems that perform very well, these methods do not translate to the highly variable domain of scene text recognition. When applied to natural scene images, traditional OCR techniques fail as they are tuned to the largely black-and-white, line-based environment of printed documents, while text occurring in natural scene images suffers from inconsistent lighting conditions, variable fonts, orientations, background noise, and imaging distortions.</p><p>To effectively recognise scene text, there are generally two stages: word detection and word recognition. The detection stage generates a large set of word bounding box candidates, and is tuned for speed and high recall. Previous work uses sliding window methods <ref type="bibr" target="#b25">[26]</ref> or region grouping methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref> very successfully for this. Subsequently, these candidate detections are recognised, and this recognition process allows for filtering of false positive word detections. Recognition is therefore a far more challenging problem and it is the focus of this paper.</p><p>While most approaches recognize individual characters by pooling evidence locally, Goodfellow et al. <ref type="bibr" target="#b7">[8]</ref> do so from the image of the whole character string using a convolutional neural network (CNN) <ref type="bibr" target="#b13">[14]</ref>. They apply this to street numbers and synthetic CAPTCHA recognition obtaining excellent results. Inspired by this approach, we move further in the direction of holistic word classification for scene text, and make two important contributions. Firstly, we propose a state-of-the-art CNN text recogniser that also pools evidence from images of entire words. Crucially, however, we regress all the characters simultaneously, formulating this as a classification problem in a large lexicon of 90k possible words (Sect. 3.1). In order to do so, we show how CNNs can be efficiently (a) (b) <ref type="figure">Figure 1</ref>: (a) The text generation process after font rendering, creating and coloring the imagelayers, applying projective distortions, and after image blending. (b) Some randomly sampled data created by the synthetic text engine.</p><p>trained to recognise a very large number of words using incremental training. While our lexicon is restricted, it is so large that this hardly constitutes a practical limitation. Secondly, we show that this state-of-the-art recogniser can be trained purely from synthetic data. This result is highly non-trivial as, differently from CAPTCHA, the classifier is then applied to real images. While synthetic data was used previously for OCR, it is remarkable that this can be done for scene text, which is significantly less constrained. This allows our framework to be seamlessly extended to larger vocabularies and other languages without any human-labelling cost. In addition to these two key contributions, we study two alternative models -a character sequence encoding model with a modified formulation to that of <ref type="bibr" target="#b7">[8]</ref> (Sect. 3.2), and a novel bag-of-N-grams encoding model which predicts the unordered set of N-grams contained in the word image (Sect. 3.3).</p><p>A discussion of related work follows immediately and our data generation system described after in Sect. 2. Our deep learning word recognition architectures are presented in Sect. 3, evaluated in Sect. 4, and conclusions are drawn in Sect. 5.</p><p>Related work. Traditional text recognition methods are based on sequential character classification by either sliding windows <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> or connected components <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, after which a word prediction is made by grouping character classifier predictions in a left-to-right manner. The sliding window classifiers include random ferns <ref type="bibr" target="#b21">[22]</ref> in Wang et al. <ref type="bibr" target="#b25">[26]</ref>, and CNNs in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref>. Both <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref> use a small fixed lexicon as a language model to constrain word recognition.</p><p>More recent works such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20]</ref> make use of over-segmentation methods, guided by a supervised classifier, to generate candidate proposals which are subsequently classified as characters or false positives. For example, PhotoOCR <ref type="bibr" target="#b2">[3]</ref> uses binarization and a sliding window classifier to generate candidate character regions, with words recognised through a beam search driven by classifier scores followed by a re-ranking using a dictionary of 100k words. <ref type="bibr" target="#b10">[11]</ref> uses the convolutional nature of CNNs to generate response maps for characters and bigrams which are integrated to score lexicon words.</p><p>In contrast to these approaches based on character classification, the work by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> instead uses the notion of holistic word recognition. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref> still rely on explicit character classifiers, but construct a graph to infer the word, pooling together the full word evidence. Rodriguez et al. <ref type="bibr" target="#b23">[24]</ref> use aggregated Fisher Vectors <ref type="bibr" target="#b22">[23]</ref> and a Structured SVM framework to create a joint word-image and text embedding. <ref type="bibr" target="#b6">[7]</ref> use whole word-image features to recognize words by comparing to simple black-and-white font-renderings of lexicon words.</p><p>Goodfellow et al. <ref type="bibr" target="#b7">[8]</ref> had great success using a CNN with multiple position-sensitive character classifier outputs (closely related to the character sequence model in Sect. 3.2) to perform street number recognition. This model was extended to CAPTCHA sequences (up to 8 characters long) where they demonstrated impressive performance using synthetic training data for a synthetic problem (where the generative model is known), but we show that synthetic training data can be used for a real-world data problem (where the generative model is unknown).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Synthetic Data Engine</head><p>This section describes our scene text rendering algorithm. As our CNN models take whole word images as input instead of individual character images, it is essential to have access to a training dataset of cropped word images that covers the whole language or at least a target lexicon. While there are some publicly available datasets from ICDAR <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>, the Street View Text (SVT) dataset <ref type="bibr" target="#b25">[26]</ref> and others, the number of full word image samples is only in the thousands, and the vocabulary is very limited. These limitations have been mitigated before by mining for data or having access to large proprietary datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>, but neither of these approaches are wholly accessible or scalable.</p><p>Here we follow the success of some synthetic character datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref> and create a synthetic word data generator, capable of emulating the distribution of scene text images. This is a reasonable goal, considering that much of the text found in natural scenes is computer-generated and only the physical rendering process (e.g. printing, painting) and the imaging process (e.g. camera, viewpoint, illumination, clutter) are not controlled by a computer algorithm. The word samples are generated with a fixed height of 32 pixels, but with a variable width. Since the input to our CNNs is a fixed-size image, the generated word images are rescaled so that the width equals 100 pixels. Although this does not preserve the aspect ratio, the horizontal frequency distortion of image features most likely provides the word-length cues. We also experimented with different padding regimes to preserve the aspect ratio, but found that the results are not quite as good as with resizing.</p><p>The synthetic data is used in place of real-world data, and the labels are generated from a corpus or dictionary as desired. By creating training datasets much larger than what has been used before, we are able to use data-hungry deep learning algorithms to train richer, whole-word-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>In this section we describe three models for visual recognition of scene text words. All use the same framework of generating synthetic text data (Sect. 2) to train deep convolutional networks on whole-word image samples, but with different objectives, which correspond to different methods of reading. Sect. 3.1 describes a model performing pure word classification to a large dictionary, explicitly modelling the entire known language. Sect. 3.2 describes a model that encodes the character at each position in the word, making no language assumptions to naively predict the sequence of characters in an image. Sect. 3.3 describes a model that encodes a word as a bag-of-N-grams, giving a compositional model of words as not only a collection of characters, but of 2-grams, 3-grams, and more generally, N-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding Words</head><p>This section describes our first model for word recognition, where words w are constrained to be selected in a pre-defined dictionary W. We formulate this as multi-class classification problem, with one class per word. While the dictionary W of a natural language may seem too large for this approach to be feasible, in practice an advanced English vocabulary, including different word forms, contains only around 90k words, which is large but manageable.</p><p>In detail, we propose to use a CNN classifier where each word w ∈ W in the lexicon corresponds to an output neuron. We use a CNN with four convolutional layers and two fully connected layers.</p><p>Rectified linear units are used throughout after each weight layer except for the last one. In forward order, the convolutional layers have 64, 128, 256, and 512 square filters with an edge size of 5, 5, 3, and 3. Convolutions are performed with stride 1 and there is input feature map padding to preserve spatial dimensionality. 2 × 2 max-pooling follows the first, second and third convolutional layers. The fully connected layer has 4096 units, and feeds data to the final fully connected layer which performs classification, so has the same number of units as the size of the dictionary we wish to recognize. The predicted word recognition result w * out of the set of all dictionary words W in a language L for a given input image x is given by w * = arg max w∈W P (w|x, L). Since P (w|x, L) = P (w|x)P (w|L)P (x)</p><formula xml:id="formula_0">P (x|L)P (w)</formula><p>and with the assumptions that x is independent of L and that prior to any knowledge of our language all words are equally probable, our scoring function reduces to w * = arg max w∈W P (w|x)P (w|L). The per-word output probability P (w|x) is modelled by the softmax scaling of the final fully connected layer, and the language based word prior P (w|L) can be modelled by a lexicon or frequency counts. A schematic of the network is shown in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>.</p><p>Training. We train the network by back-propagating the standard multinomial logistic regression loss with dropout <ref type="bibr" target="#b9">[10]</ref>, which improves generalization. Optimization uses stochastic gradient descent (SGD), dynamically lowering the learning rate as training progresses. With uniform sampling of classes in training data, we found the SGD batch size must be at least a fifth of the total number of classes in order for the network to train.</p><p>For very large numbers of classes (i.e. over 5k classes), the SGD batch size required to train effectively becomes large, slowing down training a lot. Therefore, for large dictionaries, we perform incremental training to avoid requiring a prohibitively large batch size. This involves initially training the network with 5k classes until partial convergence, after which an extra 5k classes are added. The original weights are copied for the original 5k classes, with the new classification layer weights being randomly initialized. The network is then allowed to continue training, with the extra randomly initialized weights and classes causing a spike in training error, which is quickly trained away. This process of allowing partial convergence on a subset of the classes, before adding in more classes, is repeated until the full number of desired classes is reached. In practice for this network, the CNN trained well with initial increments of 5k classes, and after 20k classes is reached the number of classes added at each increment is increased to 10k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoding Sequences of Characters</head><p>This section describes a different model for word recognition. Rather than having a single large dictionary classifier as in Sect. 3.1, this model uses a single CNN with multiple independent classifiers, each one predicting the character at each position in the word. This character sequence encoding model is a complete departure from the dictionary-constrained model, as this allows entirely unconstrained recognition of words.</p><p>A word w of length N is modelled as a sequence of characters such that w = (c 1 , c 2 , . . . , c N ) where each c i ∈ C = {1, 2, . . . , 36} represents a character at position i in the word, from the set of 10 digits and 26 letters. Each c i can be predicted with a single classifier, one for each character in the word. However, since words have variable length N which is unknown at test time, we fix the number of characters to 23, the maximum length of a word in the training set, and introduce a null character class. Therefore a word is represented by a string w = (C ∪ {φ}) <ref type="bibr" target="#b22">23</ref> . Then for a given image x, each character is predicted as c * i = arg max ci∈C∪{φ} P (c i |Φ(x)). P (c i |Φ(x)) is given by the i-th classifier acting on a single set of shared CNN features Φ(x).</p><p>The base CNN has the same structure as the first five layers of Sect. 3.1: four convolutional layers followed by a fully connected layer, giving Φ(x). The output of the fully connected layer is then fed to 23 separate fully connected layers with 37 neurons each, one for each character class. These fully connected layers are independently softmax normalized and can be interpreted as the probabilities P (c i |Φ(x)) of the width-resized input image x. <ref type="figure" target="#fig_1">Fig. 2 (b)</ref> illustrates this model. The model is trained as in Sect. 3.1 on purely synthetic data by SGD with dropout regularisation, back-propagating gradients from each 23 softmax classifier to the base net.</p><p>Discussion. This sequential character encoding model is similar to the model used by Goodfellow et al. in <ref type="bibr" target="#b7">[8]</ref>. Although the model of <ref type="bibr" target="#b7">[8]</ref> is not applied to scene text (only street numbers and CAPTCHA puzzles), it uses a separate character classifier for each letter in the word, able to recognise numbers up to 5 digits long and CAPTCHAs up to 8 characters long. However, rather than incorporating a nocharacter class in each character positions's classifier, a further length classifier is trained to output the predicted length of the word. This requires a final post-processing stage to find the optimal word prediction given the character classifier outputs and the length classifier output. We achieve a similar effect but without requiring any post processing -the word can be read directly from the CNN output, stripping the no-character class predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoding Bags of N-grams</head><p>This section describes our last word recognition model, which exploits compositionality to represent words. In contrast to the sequential character encoding of Sect. 3.2, words can be seen as a composition of an unordered set of character N-grams, a bag-of-N-grams. In the following, if s ∈ C N and w ∈ C M are two strings, the symbol s ⊂ w indicates that s is a substring of w. An N -gram of word w is a substring s ⊂ w of length |w| = N . We will denote with G N (w) = {s : s ⊂ w ∧ |s| ≤ N } the set of all grams of word w of length up to N and with G N = ∪ w∈W G N (w) the set of all such grams in the language. For example, G 3 (spires) = {s, p, i, r, e, s, sp, pi, ir, re, es, spi, pir, ire, res}. This method of encoding variable length sequences is similar to Wickelphone phoneme-encoding methods <ref type="bibr" target="#b27">[28]</ref>.</p><p>Even for small values of N , G N (w) encodes each word w ∈ W nearly uniquely. For example, with N = 4, this map has only 7 collisions out of a dictionary of 90k words. The encoding G N (w) can be represented as a |G N |-dimensional binary vector of gram occurrences. This vector is very sparse, as on average |G N (w)| ≈ 22 whereas |G N | = 10k. Given w, we predict this vector using the same base CNN as in Sect. 3.1 and Sect. 3.2, but now have a final fully connected layer with |G N | neurons to represent the encoding vector. The scores from the fully connected layer can be interpreted as probabilities of an N-gram being present in the image by applying the logistic function to each neuron. The CNN is therefore learning to recognise the presence of each N-gram somewhere within the input image.</p><p>Training. With a logistic function, the training problem becomes that of |G N | separate binary classification tasks, and so we back-propagate the logistic regression loss with respect to each Ngram class independently. To jointly train a whole range of N-grams, some of which occur very frequently and some barely at all, we have to scale the gradients for each N-gram class by the inverse frequency of their appearance in the training word corpus. We also experimented with hinge loss and simple regression to train but found frequency weighted binary logistic regression was superior. As with the other models, we use dropout and SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>This section evaluates our three text recognition models. Sect. 4.1 describes the benchmark data, Sect. 4.2 the implementation details, and Sect. 4.3 the results of our methods, that improve on the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>A number of standard datasets are used for the evaluation of our systems -ICDAR 2003, ICDAR 2013, Street View Text, and IIIT5k. ICDAR 2003 <ref type="bibr" target="#b15">[16]</ref> is a scene text recognition dataset, with the test set containing 251 full scene images and 860 groundtruth cropped images of the words contained with the full images. We follow the standard evaluation protocol by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> and perform recognition on only the words containing only alphanumeric characters and at least three characters. The test set of 860 cropped word images is referred to as IC03. The lexicon of all test words is IC03-Full, and the per-image 50 word lexicons defined by <ref type="bibr" target="#b25">[26]</ref> and used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> are referred to as IC03-50. There is also the lexicon of all groundtruth test words -IC03-Full which contains 563 words. ICDAR 2013 <ref type="bibr" target="#b12">[13]</ref> test dataset contains 1015 groundtruth cropped word images from scene text. Much of the data is inherited from the ICDAR 2003 datasets. We refer to the 1015 groundtruth cropped words as IC13. Street View Text <ref type="bibr" target="#b25">[26]</ref> is a more challenging scene text dataset than the ICDAR datasets. It contains 250 full scene test images downloaded from Google Street View. The test set of 647 groundtruth cropped word images is referred to as SVT. The lexicon of all test words is SVT-Full (4282 words), and the smaller per-image 50 word lexicons defined by <ref type="bibr" target="#b25">[26]</ref> and used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> are referred to as SVT-50. IIIT 5k-word <ref type="bibr" target="#b16">[17]</ref> test dataset contains 3000 cropped word images of scene text downloaded from Google image search. Each image has an associated 50 word lexicon (IIIT5k-50) and 1k word lexicon (IIIT5k-1k).</p><p>For training, validation and large-lexicon testing we generate datasets using the synthetic text engine from Sect. 2. 4 million word samples are generated for the IC03-Full and SVT-Full lexicons each, referred to as Synth-IC03 and Synth-SVT respectively. In addition, we use the dictionary from Hunspell, a popular open source spell checking system, combined with the ICDAR and SVT test words as a 50k word lexicon. The 50k Hunspell dictionary can also be expanded to include different word endings and combinations to give a 90k lexicon. We generate 9 million images for the 50k word lexicon and 9 million images for the 90k word lexicon. The 9 million image synthetic dataset covering 90k words, Synth, is available for download at http://www.robots.ox.ac.uk/˜vgg/ data/text/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We perform experiments on all three encoding models described in Sect. 3. We will refer to the three models as DICT, CHAR, and NGRAM for the dictionary encoding model, character sequence encoding model, and N-gram encoding model respectively. The input images to the CNNs are greyscale and resized to 32 × 100 without aspect ratio preservation. The only preprocessing, performed on each sample individually, is the sample mean subtraction and standard deviation normalization (after resizing), as this was found to slightly improve performance. Learning uses a custom version of Caffe <ref type="bibr" target="#b11">[12]</ref>.</p><p>All CNN training is performed solely on the Synth training datasets, with model validation performed on a 10% held out portion. The number of character classifiers in the CHAR character sequence encoding models is set to 23 (the length of the largest word in our 90k dictionary). In the NGRAM models, the number of N-grams in the N-gram classification dictionary is set to 10k. The N-grams themselves are selected as the N-grams with at least 10 appearances in the 90k word corpus -this equates to 36 1-grams (the characters), 522 2-grams, 3965 3-grams, and 5477 4-grams, totalling 10k.</p><p>In addition to the CNN model defined in Sect. 3, we also define larger CNN, referred to as DICT+2, CHAR+2, and NGRAM+2. The larger CNN has an extra 3 × 3 convolutional layer with 512 filters before the final pooling layer, and an extra 4096 unit fully connected layer after the original 4096 unit fully connected layer. Both extra layers use rectified linear non-linearities. Therefore, the total structure for the DICT+2 model is conv-pool-conv-pool-conv-conv-pool-conv-fc-fc-fc, where conv is a convolutional layer, pool is a max-pooling layer and fc is a fully connected layer. We train these larger models to investigate the effect of additional model capacity, as the lack of over-fitting experienced on the basic models is suspected to indicate under-capacity of the models.  <ref type="table">Table 1</ref>: Left: The word recognition accuracy for the different proposed models with different trained lexicons.</p><p>Where a lexicon is not specified for a dataset, the only language constraints are those imposed by the model itself. The fixed lexicon CHAR model results (IC03-50 and SVT-50) are obtained by selecting the lexicon word with the minimum edit distance to the predicted character sequence. Right: Some random example results from the SVT and ICDAR 2013 dataset. D denotes DICT+2-90k with no lexicon, D-50 the DICT+2-90k model constrained to the image's 50 word lexicon, C denotes the CHAR+2 model with completely unconstrained recognition, and C-50 gives the result of the closest edit distance 50-lexicon word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments</head><p>We evaluate each of our three models on challenging text recognition benchmarks. First, we measure the accuracy on a large dataset, containing the images of words from the full lexicon (up to 90k words depending on the model). Due to the lack of human-annotated natural image datasets of such scale, we use the test split of our Synth dataset (Sect. 4.1). This allows us to assess how well our models can discriminate between a large number of words. Second, we consider the standard benchmarks IC03 <ref type="bibr" target="#b15">[16]</ref>, SVT <ref type="bibr" target="#b25">[26]</ref>, and IC13 <ref type="bibr" target="#b12">[13]</ref>, which contain natural scene images, but cover smaller word lexicons. The evaluation on these datasets allows for a fair comparison against the state of the art. The results are shown in <ref type="table">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Dictionary Encoding. For the DICT model, we train a model with only the words from the IC03-Full lexicon (DICT-IC03-Full), a model with only the words from the SVT-Full lexicon (DICT-SVT-Full), as well as models for the 50k and 90k lexicons -DICT-50k, DICT-90k, and DICT+2-90k. When a small lexicon is provided, we set the language prior P (w|L) to be equal probability for lexicon words, otherwise zero. In the absence of a small lexicon, P (w|L) is simply the frequency of word w in a corpus (we use the opensubtitles.org English corpus) normalized according to the power law.</p><p>The results in <ref type="table">Table 1</ref> show exceptional performance for the dictionary based models. When the model is trained purely for a dataset's corpus of words (DICT-IC03-Full and DICT-SVT-Full), the 50-lexicon recognition problem is largely solved for both ICDAR 2003 and SVT, achieving 99.2% and 96.1% word recognition accuracy respectively, that is 7 mistakes out of 860 in the ICDAR 2003 test set, of which most are completely illegible. The Synth dataset performs very closely to that of the ICDAR 2003 dataset, confirming that the synthetic data is close to the real world data.</p><p>Drastically increasing the size of the dictionary to 50k and 90k words gives very little degradation in 50-lexicon accuracy. However without the 50-lexicon constraint, as expected the 50k and 90k dictionary models perform significantly worse than when the dictionary is constrained to only the groundtruth words -on SVT, the word classification from only the 4282 groundtruth word set yields 87% accuracy, whereas increasing the dictionary to 50k reduces the accuracy to 78.5%, and the accuracy is further reduced to 73.0% with 90k word classes. Incorporating the extra layers in to the network with DICT+2-90k increases the accuracy a lot, giving 80.7% on SVT for full 90k-way classification, almost identical to a dictionary of 50k with the basic CNN architecture.</p><p>We also investigate the contribution that the various stages of the synthetic data generation engine make to real-world recognition accuracy. <ref type="figure" target="#fig_2">Figure 3</ref> (left) shows DICT-IC03-Full and DICT-SVT-Full accuracy when trained identically but with different levels of sophistication of synthetic training data. As more sophisticated training data is used, the recognition accuracy increases -the addition of random image-layer colouring causing a significant increase in performance (+44% on IC03 and +40% on SVT), as does the addition of natural image blending (+1% on IC03 and +6% on SVT).</p><p>Character Sequence Encoding. The CHAR models are trained for character sequence encoding. The models are trained on image samples of words uniformly sampled from the 90k dictionary.</p><p>The output of the model are character predictions for a possible 23 characters of the test image's word. We take the predicted word as the MAP-optimal sequence of characters, stripping any no-  character classifications. The constrained lexicon results for IC03-50, IC03-Full, and SVT-50, are obtained by finding the lexicon word with the minimum edit distance to a raw predicted character sequence. Given this is a completely unconstrained recognition, with no language model at all, the results are surprisingly good. The 50-lexicon results are very competitive compared to the other encoding methods. However, we can see the lack of language constraints cause the out-of-lexicon results to be lacklustre, achieving an accuracy of only 79.5% with the CHAR+2 model on ICDAR 2013 as opposed to 90.8% with the DICT+2-90k model. As with the DICT models, increasing the number of layers in the network increases the word recognition accuracy by between 6-8%.</p><p>Some example word recognition results with dictionary and character sequence encodings are shown to the right of <ref type="table">Table 1</ref>.</p><p>Bag-of-N-grams Encoding. The NGRAM model's output is thresholded to result in a binary activation vector of the presence of any of 10k N-grams in a test word. Decoding the N-gram activations into a word could take advantage of a statistical model of the language. Instead, we simply search for the word in the lexicon with the nearest (in terms of the Euclidean distance) N-gram encoding, denoted as NGRAM-NN and NGRAM+2-NN models. This extremely naive method still gives competitive performance, illustrating the discriminative nature of N-grams for word recognition. Instead, one could learn a linear SVM mapping from N-gram encoding to dictionary words, allowing for scalable word recognition through an inverted index of these mappings. We experimented briefly with this on the IC03-Full lexicon -training an SVM for each lexicon word from a training set of Synth data, denoted as NGRAM+2-SVM -and achieve 97% accuracy on IC03-50 and 94% accuracy on IC03-Full. <ref type="figure" target="#fig_2">Figure 3</ref> (right) shows the N-gram recognition results for the NGRAM+2 model, thresholded at 0.99 probability.</p><p>Comparison &amp; Discussion. <ref type="table" target="#tab_2">Table 2</ref> compares our models to previous work, showing that all three models achieve state-of-the-art results in different lexicon scenarios. With tightly constrained language models such as in DICT-IC03-Full and DICT-SVT-Full, we improve accuracy by +6%. However, even when the models are expanded to be mostly unconstrained, such as with DICT+2-90k, CHAR+2 and NGRAM+2-SVM, our models still outperform previous methods. Considering a complete absence of a language model, the no-lexicon recognition results for the CHAR+2 model on SVT and IC13 are competitive with the system of <ref type="bibr" target="#b2">[3]</ref>, and as soon as a language model is introduced in the form of a lexicon for SVT-50, the simple CHAR+2 model gives +2.2% accuracy over <ref type="bibr" target="#b2">[3]</ref>. Performance could be further improved by techniques such as model averaging and testsample augmentation, albeit at a significantly increased computational cost. Our largest model, the DICT+2-90k model comprised of over 490 million parameters, can process a word in 2.2ms on a single commodity GPU.</p><p>Our models set a new benchmark for scene text recognition. In a real-world system, the large DICT+2-90k model should be used for the majority of recognition scenarios unless completely unconstrained recognition is required where the CHAR+2 model can be used. However, when looking at the average edit distance of erroneous recognitions, the CHAR+2 model greatly outperforms the DICT+2-90k model, with an average error edit distance of 1.9 compared to 2.5 on IC13, suggesting the CHAR+2 model may be more suitable for a retrieval style application in conjunction with a fuzzy search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we introduced a new framework for scalable, state-of-the-art word recognition -synthetic data generation followed by whole word input CNNs. We considered three models within this framework, each with a different method for recognising text, and demonstrated the vastly superior performance of these systems on standard datasets. In addition, we introduced a new synthetic word dataset, orders of magnitude larger than any released before.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 1 . 4 . 5 .</head><label>1145</label><figDesc>illustrates the generative process and some resulting synthetic data samples. These samples are composed of three separate image-layers -a background image-layer, foreground image-layer, and optional border/shadow image-layer -which are in the form of an image with an alpha channel. The synthetic data generation process is as follows: Font rendering -a font is randomly selected from a catalogue of over 1400 fonts downloaded from Google Fonts. The kerning, weight, underline, and other properties are varied randomly from arbitrarily defined distributions. The word is rendered on to the foreground image-layer's alpha channel with either a horizontal bottom text line or following a random curve. 2. Border/shadow rendering -an inset border, outset border or shadow with a random width may be rendered from the foreground. 3. Base coloring -each of the three image-layers are filled with a different uniform color sampled from clusters over natural images. The clusters are formed by k-means clustering the three color components of each image of the training datasets of<ref type="bibr" target="#b15">[16]</ref> into three clusters. Projective distortion -the foreground and border/shadow image-layers are distorted with a random, full-projective transformation, simulating the 3D world. Natural data blending -each of the image-layers are blended with a randomly-sampled crop of an image from the training datasets of ICDAR 2003 and SVT. The amount of blend and alpha blend mode (e.g. normal, add, multiply, burn, max, etc.) is dictated by a random process, and this creates an eclectic range of textures and compositions. The three image-layers are also blended together in a random manner, to give a single output image. 6. Noise -Gaussian noise, blur, and JPEG compression artefacts are introduced to the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A schematics of the CNNs used showing the dimensions of the featuremaps at each stage for (a) dictionary encoding, (b) character sequence encoding, and (c) bag-of-N-gram encoding. The same five-layer, base CNN architecture is used for all three models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Left: The recognition accuracies of the DICT-IC03-Full and DICT-SVT-Full models evaluated on IC03 and SVT respectfully. The models (a-f) are trained on purely synthetic data with increasing levels of sophistication of the synthetic data. (a) Black text rendered on a white background with a single font, Droid Sans. (b) Incorporating all of Google fonts. (c) Adding background, foreground, and border colouring. (d) Adding perspective distortions. (e) Adding noise, blur and elastic distortions. (f) Adding natural image blending -this gives an additional 6.2% accuracy on SVT. Right: The N-gram recognition results with probability over 0.99 from the NGRAM+2 model on random test images from SVT and ICDAR 2013.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison to previous methods. The ICDAR 2013 results given are case-insensitive. Bolded results outperform previous state-of-the-art methods. The baseline method is from a commercially available OCR system.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the EPSRC and ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Word spotting and recognition with embedded attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-End Text Recognition with Hybrid HMM Maxout Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PhotoOCR: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Character recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISAPP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Residual enhanced visual vectors for on-device image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Takacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
		<editor>Asilomar</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Whole is greater than sum of parts: Recognizing scene text words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="398" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6082</idno>
		<title level="m">Multi-digit number recognition from street view imagery using deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Supervised mid-level features for word image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
		<title level="m">ICDAR 2013 robust reading competition. In ICDAR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ICDAR 2005 text locating competition results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="80" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
		<title level="m">ICDAR 2003 robust reading competitions. In ICDAR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="770" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scene text localization and recognition with oriented stroke detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-lexicon attribute-consistent text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="752" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast keypoint recognition in ten lines of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozuysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with compressed fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Label embedding for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Rodriguez-Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ICDAR 2011 robust reading competition challenge 2: Reading text in scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1491" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3304" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Context-sensitive coding, associative memory, and serial order in (speech) behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wayne A Wickelgran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4042" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
