<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Motion Patterns in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cordelia Schmid</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><forename type="middle">Alahari</forename><surname>Inria</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cordelia Schmid</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Motion Patterns in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of determining whether an object is in motion, irrespective of camera motion, is far from being solved. We address this challenging task by learning motion patterns in videos. The core of our approach is a fully convolutional network, which is learned entirely from synthetic video sequences, and their ground-truth optical flow and motion segmentation. This encoder-decoder style architecture first learns a coarse representation of the optical flow field features, and then refines it iteratively to produce motion labels at the original high-resolution. We further improve this labeling with an objectness map and a conditional random field, to account for errors in optical flow, and also to focus on moving "things" rather than "stuff". The output label of each pixel denotes whether it has undergone independent motion, i.e., irrespective of camera motion. We demonstrate the benefits of this learning framework on the moving object segmentation task, where the goal is to segment all objects in motion. Our approach outperforms the top method on the recently released DAVIS benchmark dataset, comprising real-world sequences, by 5.6%. We also evaluate on the Berkeley motion segmentation database, achieving state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of analyzing motion patterns has a long history in computer vision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. This includes methods for motion estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref>, scene <ref type="bibr" target="#b36">[37]</ref> and optical <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref> flow computation, video segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41]</ref>; all of which aim to estimate or capitalize on motion cues in scenes. Despite this progress, the fundamental problem of identifying if an object is indeed moving, irrespective of camera motion, remains challenging. In this paper, we make significant advances to address this challenge, with a novel CNN-based framework to automatically learn motion patterns in videos, and use it to segment moving objects; see sample results in <ref type="figure">Figure 1</ref>.</p><p>To illustrate the task, consider <ref type="figure">Figure 2</ref>, a sequence from the FlyingThings3D dataset <ref type="bibr" target="#b22">[23]</ref>. It depicts a scene generated synthetically, involving a moving camera (can be easily observed by comparing the top left corners of the images (a) * Thoth team, Inria, Laboratoire Jean Kuntzmann, Grenoble, France. <ref type="figure">Figure 1</ref>. Results on the DAVIS dataset. Left: Optical flow field input to our MP-Net, computed with <ref type="bibr" target="#b5">[6]</ref>. Right: Our segmentation result overlaid on the video frame. Note that our approach accurately segments moving objects, and learns to distinguish between object and camera motions (seen in the flow fields). and (b)), with objects in motion, e.g., the three large objects in the centre of the frame (which are easier to spot in the ground-truth segmentation (d)). The goal of our work is to study such motion patterns in video sequences (using optical flow field (c)), and to learn to distinguish real motion of objects from camera motion. In other words, we target the moving object segmentation in (d).</p><p>The core of our approach is a trainable model, motion pattern network (MP-Net), for separating independent object and camera motion, which takes optical flow as input and outputs a per-pixel score for moving objects. Inspired by fully convolutional networks (FCNs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>, we propose a related encoder-decoder style architecture to accomplish this two-label classification task. The network is trained from scratch with synthetic data <ref type="bibr" target="#b22">[23]</ref>. Pixel-level ground-truth labels for training are generated automatically (see <ref type="figure">Figure 2</ref>(d)), and denote whether each pixel has moved in the scene. The input to the network is flow fields, such as the one shown in <ref type="figure">Figure 2</ref>(c). More details of the network, and how it is trained are provided in Section 3. With this training, our model learns to distinguish motion patterns of objects and background. We then refine these labels with objectness cues <ref type="bibr" target="#b28">[29]</ref> and a conditional random field (CRF) model <ref type="bibr" target="#b18">[19]</ref> (see §4), to demonstrate the efficacy of the entire framework on the moving object segmentation task (see §6). These refinement steps are im-(a) (b) (c) (d) <ref type="figure">Figure 2</ref>. (a,b) Two example frames from a sequence in the FlyingThings3D dataset <ref type="bibr" target="#b22">[23]</ref>. The camera is in motion in this scene, along with four independently moving objects. (c) Ground-truth optical flow of (a), which illustrates motion of both foreground objects and background with respect to the next frame (b). (d) Ground-truth segmentation of moving objects in this scene.</p><p>portant to account for errors in flow fields, and also to target moving objects, instead of stuff such as moving water. We evaluate on the densely annotated video segmentation (DAVIS) <ref type="bibr" target="#b27">[28]</ref> and the Freiburg/Berkeley motion segmentation datasets (BMS-26, FBMS) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>, all comprising real-data sequences. We obtain state-of-the-art results on these challenging datasets. In particular, we outperform previous video-level methods by over 5.6%, on the intersection over union score, on DAVIS, despite operating only on the frame level. We have made the source code and the trained models available online. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is related to the following tasks dealing with motion cues: motion and scene flow estimation, and video object segmentation. We will review the most relevant work on these topics, in addition to a review of related CNN architectures in the remainder of this section.</p><p>Motion estimation. Early attempts for estimating motion have focused on geometry-based approaches, such as <ref type="bibr" target="#b34">[35]</ref>, where the potential set of motions is identified with RANSAC. Recent approaches have relied on other cues to estimate moving object regions. For example, Papzouglou and Ferrari <ref type="bibr" target="#b26">[27]</ref> first extract motion boundaries by measuring changes in optical flow field, and use it to estimate moving regions. They also refine this initial estimate iteratively with appearance features. This approach produces interesting results, but is limited by its heuristic initialization. We show that incorporating our learning-based motion estimation into it improves the results significantly (see <ref type="table" target="#tab_3">Table 4</ref>).</p><p>Narayana et al. <ref type="bibr" target="#b23">[24]</ref> use optical flow orientations in a probabilistic model to assign pixels with labels that are consistent with their respective real-world motion. This approach assumes pure translational camera motion, and is prone to errors when the object and camera motions are consistent with each other. Bideau et al. <ref type="bibr" target="#b2">[3]</ref> presented an alternative to this, where initial estimates of foreground and background motion models are updated over time, with optical flow orientations of the new frames. This initialization is also heuristic, and lacks a robust learning framework. While we also set out with the goal of finding objects in motion, our solution to this problem is a novel 1 http://thoth.inrialpes.fr/research/mpnet learning-based method. Scene flow, i.e., 3D motion field in a scene <ref type="bibr" target="#b36">[37]</ref>, is another form of motion estimation, but is computed with additional information, such as disparity values computed from stereo images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40]</ref>, or estimated 3D scene models <ref type="bibr" target="#b37">[38]</ref>. None of these methods follows a CNN-based learning approach, in contrast to our MP-Net.</p><p>Video object segmentation. The task of segmenting objects in video is to associate pixels belonging to a class spatio-temporally; in other words, extract segments that respect object boundaries, as well as associate object pixels temporally whenever they appear in the video. This can be accomplished by propagating manual segment labels in one or more frames to the rest of the video sequence <ref type="bibr" target="#b1">[2]</ref>. This class of methods is not applicable to our scenario, where no manual segmentation is available.</p><p>Our approach to solve the segmentation problem does not require any manually-marked regions. Several methods in this paradigm generate an over-segmentation of videos <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>. While this can be a useful intermediate step for some recognition tasks in video, it has no notion of objects. Indeed, most of the extracted segments in this case do not directly correspond to objects, making it non-trivial to obtain video object segmentation from this intermediate result. An alternative to this is motion segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref>, which produces more coherent regions with point trajectories. They, however, assume homogeneity of motion over the entire object, which is not valid for nonrigid objects.</p><p>Another class of segmentation methods cast the problem as a foreground-background classification task <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. Some of these first estimate a region <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref> or regions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43]</ref>, which potentially correspond(s) to the foreground object, and then learn foreground/background appearance models. The learned models are then integrated with other cues, e.g., saliency maps <ref type="bibr" target="#b38">[39]</ref>, pairwise constraints <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43]</ref>, object shape estimates <ref type="bibr" target="#b19">[20]</ref>, to compute the final object segmentation. Alternatives to this framework have used: (i) long-range interactions between distinct parts of the video to overcome noisy initializations in low-quality videos <ref type="bibr" target="#b8">[9]</ref>, and (ii) occluder/occluded relations to obtain a layered segmentation <ref type="bibr" target="#b33">[34]</ref>. Our proposed method outperforms all the top ones from this class of segmentation approaches (see §6). Related CNN architectures. Our CNN model predicts labels for every pixel, similar to CNNs for other tasks, such as semantic segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>, optical flow <ref type="bibr" target="#b7">[8]</ref> and disparity/depth <ref type="bibr" target="#b22">[23]</ref> estimation. We adopt an encoder-decoder style network, inspired by the success of similar architectures in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>. They first learn a coarse representation with receptive fields of gradually increasing sizes, and then iteratively refine it with upconvolutional layers, i.e., by upsampling the feature maps and performing convolutions, to obtain an output at the original high-resolution. In contrast to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>, which predict labels in each of the upconvolutional layers, we concatenate features computed at different resolutions to form a strong representation, and estimate the labels in the last layer. Our architecture also has fewer channels in the layers in the encoding part, compared to <ref type="bibr" target="#b30">[31]</ref>, to accommodate larger training-set batches, and thus decrease the training time. More details of our architecture are presented in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Motion Patterns</head><p>Our MP-Net takes the optical flow field corresponding to two consecutive frames of a video sequence as input, and produces per-pixel motion labels. In other words, we treat each video as a sequence of frame pairs, and compute the labels independently for each pair. As shown in <ref type="figure">Figure</ref> 3, the network comprises several "encoding" (convolutional and max-pooling) and "decoding" (upsampling and convolutional) layers. The motion labels are produced by the last layer of the network, which are then rescaled to the original image resolution (see §3.1). We train the network entirely on synthetic data-a scenario where ground-truth motion labels can be acquired easily (see §3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network architecture</head><p>Our encoder-decoder style network is motivated by the goal of segmenting diverse motion patterns in flow fields, which requires a large receptive field as well as an output at the original image resolution. A large receptive field is critical to incorporate context into the model. For example, when the spatial region of support (for performing convolution) provided by a small receptive field falls entirely within an object with non-zero flow values, it is impossible to determine whether it is due to object or camera motion. On the other hand, a larger receptive field will include regions corresponding to the object as well as background, providing sufficient context to determine what is moving in the scene. The second requirement of output generated at the original image resolution is to capture fine details of objects, e.g., when only a part of the object is moving. Our network satisfies these two requirements with: (i) the encoder part learning features with receptive fields of increasing sizes, and (ii) the decoder part upsampling the intermediate layer outputs to finally predict labels at the full resolution. <ref type="figure" target="#fig_0">Figure 3</ref> illustrates our network architecture. Optical flow field input is processed by the encoding part of the network (denoted by (a) in the figure) to generate a coarse representation that is a 32 × 32 downsampled version of the input. Each 3D block here represents a feature map produced by a set of layers. In the encoding part, each feature map is a result of applying convolutions, followed by a ReLU non-linearity layer, and then a 2 × 2 max-pooling layer. The coarse representation learned by the final set of operations in this part, i.e., the 32 × 32 downsampled version, is gradually upsampled by the decoder part ((b) in the figure). In each decoder step, we first upsample the output of the previous step by 2 × 2, and concatenate it with the corresponding intermediate encoded representation, before max-pooling (illustrated with black arrows pointing down in the figure). This upscaled feature map is then processed with two convolutional layers, followed by non-linearities, to produce input for the next (higher-resolution) decoding step. The final decoder step produces a motion label map at half the original resolution. We perform a bilinear interpolation on this result to estimate labels at the original resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training with synthetic data</head><p>We need a large number of fully-labelled examples to train a convolutional network such as the one we propose. In our case, this data corresponds to videos of several types of objects, captured under different conditions (e.g., moving or still camera), with their respective moving object annota- tions. No large dataset of real-world scenes satisfying these requirements is currently available, predominantly due to the cost of generating ground-truth annotations and flow for every frame. We adopt the popular approach of using synthetic datasets, followed in other work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>. Specifically, we use the FlyingThings3D dataset <ref type="bibr" target="#b22">[23]</ref> containing 2250 video sequences of several objects in motion, with ground-truth optical flow. We augment this dataset with ground-truth moving object labels, which are accurately estimated using the disparity values and camera parameters available in the dataset, as outlined in Section 5. See We train the network with mini-batch SGD under several settings. The one trained with ground-truth optical flow as input shows the best performance. This is analyzed in detail in Section 6.2. Note that, while we use ground-truth flow for training and evaluating the network on synthetic datasets, all our results on real-world test data use only the estimated optical flow. After convergence of the training procedure, we obtain a learned model for motion patterns.</p><formula xml:id="formula_0">(a) (b) (c)</formula><p>Our approach capitalizes on the recent success of CNNs for pixel-level labelling tasks, such as semantic image segmentation, which learn feature representations at multiple scales in the RGB space. The key to their top performance is the ability to capture local patterns in images. Various types of object and camera motions also produce consistent local patterns in the flow field, which our model is able to learn to recognize. This gives us a clear advantage over other pixellevel motion estimation techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref> that can not detect local patterns. Motion boundary based heuristics used in <ref type="bibr" target="#b26">[27]</ref> can be seen as one particular type of pattern, representing independent object motion. Our model is able to learn many such patterns, which greatly improves the quality and robustness of motion estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Detecting Motion Patterns</head><p>We apply our trained model on synthetic (FlyingTh-ings3D) as well as real-world (DAVIS, BMS-26, FBMS) test data. <ref type="figure" target="#fig_1">Figure 4</ref> shows sample predictions of our model on the FlyingThings3D test set with ground-truth optical flow as input. Examples in the first two rows show that our model accurately identifies fine details in objects: thin structures even when they move subtlely, such as the neck of the guitar in the top-right corner in the first row (see the subtle motion in the optical flow field (b)), fine structures like leaves in the vase, and the guitar's headstock in the second row. Furthermore, our method successfully handles objects exhibiting highly varying motions in the second example. The third row shows a limiting case, where the receptive field of our network falls entirely within the interior of a large object, as the moving object dominates. Traditional approaches, such as RANSAC, do not work in this case either.</p><p>In order to detect motion patterns in real-world videos, we first compute optical flow with popular methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>. With this flow as input to the network, we estimate a motion label map, as shown in the examples in  <ref type="figure">figure)</ref>. We address these challenges by: (i) incorporating object proposals <ref type="bibr" target="#b28">[29]</ref> into our framework, and (ii) refining the result with a fully-connected conditional random field (CRF) <ref type="bibr" target="#b18">[19]</ref>. The following two sections present these in detail, and their influence is analyzed in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Segmenting real-world videos</head><p>As mentioned in the example above ( <ref type="figure" target="#fig_4">Figure 5</ref>, top row), real-world videos may contain stuff (water in this case) undergoing independent motion. While it is interesting to study this motion, and indeed, our model estimates it (see network prediction (c) in the first row), it is not annotated in any of the standard datasets for moving object segmentation. In order to perform a fair evaluation on standard benchmarks, we introduce the notion of objects, with an objectness score, computed from object proposals, to eliminate "moving stuff." We combine this score with our network output to obtain an updated prediction.</p><p>We first generate object proposals in each frame with a state-of-the-art method <ref type="bibr" target="#b28">[29]</ref>. We then use a pixel-level voting scheme to compute an objectness score. The score at a pixel i is the number of proposals that include it. This score is normalized by the total number of proposals to obtain o i , the objectness score at pixel i in the 0 − 1 range. In essence, we aggregate several proposals, which are likely to represent objects of interest, to obtain an objectness map, as shown in the examples in <ref type="figure" target="#fig_4">Figure 5(d)</ref>. We then combine this with the motion prediction of our MP-Net at pixel i, m i ∈ [0, 1], to obtain an updated prediction p i as: p i = min(m i * (k + o i ), 1), where k ∈ [0, 1] is a parameter controlling the influence of objectness. It is set to 0.5 to ensure that a high-confidence network prediction m i gets suppressed only when there are no objects proposals supporting it. In the example with the kiteboarder (top row in <ref type="figure" target="#fig_4">Figure 5</ref>), the objectness map (d) has no object proposals on water, the "moving stuff," and eliminates it, to obtain segmentation (e) that corresponds to the moving object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Refining the segmentation</head><p>As shown on the synthetic test sequences, <ref type="figure" target="#fig_1">Figure 4</ref>, our model produces accurate object boundaries in several cases. This is in part due to precise optical flow input; recall that we use ground-truth flow for synthetic data. Naturally, computed flow is less accurate than this, and can often fail to provide precise object boundaries (see <ref type="figure" target="#fig_4">Figure 5</ref>(b)), especially in low-texture regions. Such errors inevitably result in imprecise motion segments. To address this, we follow the common practice of refining segmentation results with a CRF <ref type="bibr" target="#b6">[7]</ref>. We use a fully-connected CRF <ref type="bibr" target="#b18">[19]</ref>, with our predictions updated with objectness scores as the unary terms, and standard colour-based pairwise terms. The refinement is shown qualitatively in <ref type="figure" target="#fig_4">Figure 5</ref>(f), which improves over the initial segmentation in (e), e.g., contours of the person pushing the pram in the middle row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Datasets</head><p>FlyingThings3D (FT3D). We train our network with the synthetic FlyingThings3D dataset <ref type="bibr" target="#b22">[23]</ref>. It contains videos of various objects flying along randomized trajectories, in randomly constructed scenes. The video sequences are generated with complex camera motion, which is also randomized. FT3D comprises 2700 videos, each containing 10 stereo frames. The dataset is split into training and test sets, with 2250 and 450 videos respectively. Ground-truth optical flow, disparity, intrinsic and extrinsic camera parame-ters, and object instance segmentation masks are provided for all the videos. No annotation is directly available to distinguish moving objects from stationary ones, which is required to train our network. We extract this from the data provided as follows. With the given camera parameters and the stereo image pair, we first compute the 3D coordinates of all the pixels in a video frame t. Using ground-truth flow between frames t and t + 1 to find a pair of corresponding pixels, we retrieve their respective 3D scene points. Now, if the pixel has not undergone any independent motion between these two frames, the scene coordinates will be identical (up to small rounding errors). We have made these labels publicly available on our project website. Performance on the test set is measured as the standard intersection over union score between the predicted segmentation and the ground-truth masks. DAVIS. We use the densely annotated video segmentation dataset <ref type="bibr" target="#b27">[28]</ref> exclusively for evaluating our approach. DAVIS is a very recent dataset containing 50 full HD videos, featuring diverse types of object and camera motion. It includes challenging examples with occlusion, motion blur and appearance changes. Accurate pixel-level annotations are provided for the moving object in all the video frames. Note that only a single object is annotated in each video, even if there are multiple moving objects in the scene. We evaluate our method on DAVIS with the three measures used in <ref type="bibr" target="#b27">[28]</ref>, namely intersection over union for region similarity, F-measure for contour accuracy, and temporal stability for measuring the smoothness of segmentation over time. We follow the protocol in <ref type="bibr" target="#b27">[28]</ref> and use images downsampled by a factor of two. Other datasets.</p><p>We also evaluate on sequences from Berkeley (BMS-26) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36]</ref> and Freiburg-Berkeley (FBMS) <ref type="bibr" target="#b25">[26]</ref> motion segmentation datasets. The BMS-26 dataset consists of 26 videos with ground-truth object segmentations for a selection of frames. Observing that annotations in some of these videos do not correspond to objects with independent motion, ten of them were excluded in <ref type="bibr" target="#b2">[3]</ref>. In order to compare with <ref type="bibr" target="#b2">[3]</ref>, we follow their experimental protocol, and evaluate on the same subset of BMS-26. FBMS is an extension of BMS- <ref type="bibr" target="#b25">26</ref> total, and a train and test split of 29 and 30 respectively. We use the test set in this paper. Performance on these two datasets is evaluated with F-measure, as done in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation details</head><p>Training. We use mini-batch SGD with a batch size of 13 images-the maximum possible due to GPU memory constraints. The network is trained from scratch with learning rate set to 0.003, momentum to 0.9, and weight decay to 0.005. Training is done for 27 epochs, and the learning rate and weight decay are decreased by a factor of 0.1 after every 9 epochs. We downsample the original frames of the FT3D training set by a factor 2, and perform data augmentation by random cropping and mirroring. Batch normalization <ref type="bibr" target="#b15">[16]</ref> is applied to all the convolutional layers of the network. Other details. We perform zero-mean normalization of the flow field vectors, similar to <ref type="bibr" target="#b31">[32]</ref>. When using flow angle and magnitude together (which we refer to as flow angle field), we scale the magnitude component, to bring the two channels to the same range. We use 100 proposals in each frame to compute the objectness score (see §4.1). Also, for a fair comparison to other methods on DAVIS, we do not learn the parameters of the fully-connected CRF on this dataset, and instead set them to values used for a related pixel-level segmentation task <ref type="bibr" target="#b6">[7]</ref>. Our model is implemented in the Torch framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Influence of input modalities</head><p>We first analyze the influence of different input modalities on training our network. Specifically, we use RGB data (single frame and image pair), optical flow field (ground truth and estimated one), directly as flow vectors, i.e., flow in x and y axes, or as angle field (flow vector angle concatenated with flow magnitude), and a combination of RGB data and flow. These results are presented on the FT3D test set and also on DAVIS, to study how well the observations on synthetic videos transfer to the real-world ones, in <ref type="table" target="#tab_0">Table 1</ref>. For computational reasons we train and test with different modalities on a smaller version of our MP-Net, with one decoder unit instead of four. Then we pick the best modality to train and test the full, deeper version of the network. From <ref type="table" target="#tab_0">Table 1</ref>, the performance on DAVIS is lower than on FT3D. This is expected as there is domain change from synthetic to real data, and that we use ground truth optical flow as input for FT3D test data, but estimated flow <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref> for DAVIS. As a baseline, we train on single RGB frames ('RGB single frame' in the table). Clearly, no motion patterns can be learned in this case, but the network performs reasonably on FT3D test (68.1), as it learns to correlate object appearance with its motion. This intuition is confirmed by the fact that 'RGB single frame' fails on DAVIS (12.7), where the appearance of objects and background is significantly different from FT3D. MP-Net trained on 'RGB pair', i.e., RGB data of two consecutive frames concatenated, performs slightly better on both FT3D (69.1) and DAVIS (16.6), suggesting that it captures some motion-like information, but continues to rely on appearance, as it does not transfer well to DAVIS.</p><p>Training on ground-truth flow vectors corresponding to the image pair ('GT flow') improves the performance on FT3D by 5.4% and on DAVIS significantly (27.7%). This shows that MP-Net learned on flow from synthetic examples can be transferred to real-world videos. We then experiment with flow angle as part of the input. As discussed in <ref type="bibr" target="#b23">[24]</ref>, flow orientations are independent of depth from the camera, unlike flow vectors, when the camera is undergoing only translational motion. Using the ground truth flow angle field (concatenation of flow angles and magnitudes) as input ('GT angle field'), we note a slight decrease in IoU score on FT3D (1.4%), where strong camera rotations are abundant, but in real examples, such motion is usually mild. Hence, 'GT angle field' improves IoU on DAVIS by 2.3%. We use angle field representation in all further experiments.</p><p>Using a concatenated flow and RGB representation ('RGB + GT angle field') performs better on FT3D (by 1.7%), but is poorer by 7% on DAVIS, re-confirming our observation that appearance features are not consistent between the two datasets. Finally, training on computed flow <ref type="bibr" target="#b5">[6]</ref> ('LDOF angle field') leads to significant drop on both the datasets: 9.9% on FT3D (with GT flow for testing) and 8.5% on DAVIS, showing the importance of highquality training data for learning accurate models. The full version of our MP-Net, with 4 decoder units, improves the Measure NLC <ref type="bibr" target="#b8">[9]</ref> CVOS <ref type="bibr" target="#b33">[34]</ref> TRC <ref type="bibr" target="#b9">[10]</ref> MSG <ref type="bibr" target="#b4">[5]</ref> KEY <ref type="bibr" target="#b19">[20]</ref> SAL <ref type="bibr" target="#b38">[39]</ref> FST <ref type="bibr" target="#b26">[27]</ref>   <ref type="table">Table 3</ref>. Comparison to state-of-the-art methods on DAVIS with intersection over union (J ), F-measure (F), and temporal stability (T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>Optical flow <ref type="bibr" target="#b5">[6]</ref> FST <ref type="bibr" target="#b26">[27]</ref> NLC <ref type="bibr" target="#b8">[9]</ref> Ours <ref type="figure">Figure 6</ref>. Qualitative comparison with top-performing methods on DAVIS. Left to right: ground truth, optical flow <ref type="bibr" target="#b5">[6]</ref>, results of FST <ref type="bibr" target="#b26">[27]</ref>, NLC <ref type="bibr" target="#b8">[9]</ref>, and our approach. The last row shows a failure case of our approach, i.e., a part of the motorbike is missing.</p><p>IoU by 12.8% on FT3D and 5.8% on DAVIS over its shallower one-unit equivalent.</p><p>Notice that the performance of our full model on FT3D is excellent, with the remaining errors mostly due to inherently ambiguous cases like objects moving close to the camera (see third row in <ref type="figure" target="#fig_1">Figure 4</ref>), or very strong object/camera motion. On DAVIS the results are considerably lower despite less challenging motion. To investigate the extent to which this is due to errors in flow estimation, we compute LDOF <ref type="bibr" target="#b5">[6]</ref> flow on the FT3D test set and evaluate our full model trained on ground-truth flow. We observe a significant drop in performance by 27.2% (from 85.9% to 58.7%). This confirms the impact of optical flow quality and suggests that improvements in flow estimation can increase the performance of our method on real-world videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Evaluation on real videos</head><p>We show the performance of our MP-Net on DAVIS in <ref type="table">Table 2</ref>, along with a study on the influence of additional cues and the flow used. First, we evaluate the importance of the estimated flow quality by comparing EpicFlow <ref type="bibr" target="#b29">[30]</ref>, a recent method, and LDOF <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref>, a more classical method. Using EpicFlow, which leverages motion contours, produces more accurate object boundaries, and improves over MP-Net using LDOF by 4.5%. Incorporating objectness cues with our network ('MP-Net + Objectness' in the table), as described in Section 4.1 improves the segmentation results over 'MP-Net' by 10.9% and 7.6% with LDOF and EpicFlow respectively. Refining these segmentation results with a fully-connected CRF ('MP-Net + Objectness + CRF'), as in Section 4.2, further improves the IoU by 6.4% and 3.5% with LDOF and EpicFlow respectively. This refinement has a significant impact when using LDOF flow, as it improves segmentation around object boundaries, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. On the other hand, EpicFlow already incorporates motion boundaries, and a CRF refinement on top of the results with this flow has a less-pronounced improvement. The overall method 'MP-Net + Objectness + CRF' performs better with LDOF (69.7) than EpicFlow (68.0). Although EpicFlow has better precision than LDOF around object boundaries, it tends to make larger errors in other regions, which cannot be corrected with CRF refinement. We thus use LDOF in the following experiments. <ref type="table">Table 3</ref> shows comparison with unsupervised state-ofthe-art methods on DAVIS. In addition to comparing with the methods reported in <ref type="bibr" target="#b27">[28]</ref>, we evaluate PCM <ref type="bibr" target="#b2">[3]</ref>, the top-performer on BMS-26, with source code provided by the authors. Note that methods which use supervision on DAVIS test sequences (e.g., annotation in the first frame) do perform better, but are not directly comparable to our method. Our frame-level approach 'MP-Net + Objectness + CRF' (Ours) outperforms all the methods significantly, notably by 5.6% on IoU (mean-J ) and 7% on F-measure (mean-F) over the best one in the evaluation <ref type="bibr" target="#b27">[28]</ref>, i.e., Measure CUT <ref type="bibr" target="#b16">[17]</ref> FST <ref type="bibr" target="#b26">[27]</ref>  Ground truth Optical flow <ref type="bibr" target="#b5">[6]</ref> FST <ref type="bibr" target="#b26">[27]</ref> PCM <ref type="bibr" target="#b2">[3]</ref> MP+Obj + FST <ref type="bibr" target="#b26">[27]</ref>  NLC <ref type="bibr" target="#b8">[9]</ref>. Note that the two top methods, NLC and FST <ref type="bibr" target="#b26">[27]</ref> perform a video-level inference by propagating motion labels through the video, unlike our approach using only a pair of video frames at a time. Our network shows the top performance, by a significant margin, with respect to mean and recall on the IoU and F-measure scores. All the methods perform similarly on the decay scores, which quantifies the performance loss/gain over time. As MP-Net uses limited temporal information (two-frame optical flow) and does not perform inference at the video level, it is not the best one on the temporal stability measure. This limitation can be addressed with a post-processing step, such as using a temporal CRF. <ref type="figure">Figure 6</ref> compares our approach qualitatively to the two top-performing methods, FST <ref type="bibr" target="#b26">[27]</ref> and NLC <ref type="bibr" target="#b8">[9]</ref>, on DAVIS. In the first row, FST localizes the moving boat, but its segmentation leaks into the background region around the boat, due to errors in motion tracking. NLC latches onto moving water, whereas our MP-Net segments the boat accurately. In the second row, our segmentation result is more precise and complete than both FST and NLC. The last row shows a failure case, where a part of the motorbike is missing due to highly imprecise flow estimation. <ref type="table" target="#tab_3">Table 4</ref> shows quantitative comparison on the subset of BMS-26 used in <ref type="bibr" target="#b2">[3]</ref>. We observed that objects are annotated in some of the sequences when they do not undergo independent motion. Thus, the results of our MP-Net with objectness ('MP+Obj' in the table) are not directly comparable to the other methods which use propagation between frames, though they are still on par with many of the previous methods. To account for this mismatch with MP-Net, which only segments moving objects, we incorporate our frame-level motion estimation results into a state-ofthe-art video segmentation method <ref type="bibr" target="#b26">[27]</ref>. This is achieved by replacing the location unary scores in <ref type="bibr" target="#b26">[27]</ref> with our motion prediction scores integrated with objectness. The results ('MP+Obj + FST <ref type="bibr" target="#b26">[27]</ref>' in the table) are significantly better than most previous methods, and on par with PCM <ref type="bibr" target="#b2">[3]</ref>. In particular, using our motion prediction in <ref type="bibr" target="#b26">[27]</ref> improves the result by 14%. We also evaluated this combination ('MP+Obj + FST') on the FBMS test set, where it achieves 77.5% in F-measure, and is better than state-ofthe-art methods: FST <ref type="bibr" target="#b26">[27]</ref> (69.2), CVOS <ref type="bibr" target="#b33">[34]</ref> (74.9), and CUT <ref type="bibr" target="#b16">[17]</ref> (76.8). <ref type="figure" target="#fig_5">Figure 7</ref> compares our results on BMS-26 with the topmethod on this dataset, PCM <ref type="bibr" target="#b2">[3]</ref>, and the baseline videolevel approach FST <ref type="bibr" target="#b26">[27]</ref>. In the first row, FST segments only one of the two moving cars in the foreground, due to very slow motion of the second car. Introducing our motion prediction into FST segments both these cars. This result is comparable to PCM. None of the methods segments the third car in the background however. In the second row, PCM fails to segment the woman, and FST segments only the jacket, but including our motion estimate into FST significantly improves the result. Tracking errors inherent in FST result in the segmentation leaking into the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Comparison to the state of the art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper introduces a novel approach for learning motion patterns in videos. Its strength is demonstrated for the task of moving object segmentation, where our method outperforms many complex approaches, that rely on engineered features. Future work includes: (i) development of end-to-end trainable models for video semantic segmentation, (ii) use of a memory module for video object segmentation, (iii) using additional information, e.g., subset of frames annotated by users, to handle ambiguous cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Our motion pattern network: MP-Net. The blue arrows in the encoder part (a) denote convolutional layers, together with ReLU and max-pooling layers. The red arrows in the decoder part (b) are convolutional layers with ReLU, 'up' denotes 2 × 2 upsampling of the output of the previous unit. The unit shown in green represents bilinear interpolation of the output of the last decoder unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Each row shows: (a) example frame from a sequence in FlyingThings3D, (b) ground-truth optical flow of (a), which illustrates motion of both foreground objects and background, with respect to the next frame, and (c) our estimate of moving objects in this scene with ground-truth optical flow as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2(d) for an illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 5(c). Although the prediction of our frame-pair feedforward model is accurate in several regions in the frame ((c) in the figure), we are faced with two challenges, which were not observed in the synthetic training set. The first one is motion of stuff [1] in a scene, e.g., patterns on the water due to the kiteboarder's motion (first row in the figure), which is irrelevant for moving object segmentation. The second one is significant errors in optical flow, e.g., in front of the pram ((b) in the bottom row in the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Sample results on the DAVIS dataset showing all the components of our approach. Each row shows: (a) video frame, (b) optical flow estimated with LDOF [6], (c) output of our MP-Net with LDOF flow as input, (d) objectness map computed with proposals [29], (e) initial moving object segmentation result, (f) segmentation refined with CRF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparison on two sample sequences from BMS-26. Left to right: ground truth, optical flow<ref type="bibr" target="#b5">[6]</ref>, results of FST<ref type="bibr" target="#b26">[27]</ref>, PCM<ref type="bibr" target="#b2">[3]</ref>, and our MP-Net + Objectness + FST ('MP+Obj + FST<ref type="bibr" target="#b26">[27]</ref>').</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>, with 59 sequences in # dec. Trained on FT3D with ... FT3D DAVIS Comparing the influence of different input modalities on the FlyingThings3D (FT3D) test set and DAVIS. Performance is shown as mean intersection over union scores. # dec. refers to the number of decoder units in our MP-Net. Ground-truth flow is used for evaluation on FT3D and LDOF flow for DAVIS.</figDesc><table><row><cell></cell><cell>RGB single frame</cell><cell>68.1</cell><cell>12.7</cell></row><row><cell></cell><cell>RGB pair</cell><cell>69.1</cell><cell>16.6</cell></row><row><cell>1</cell><cell>GT flow GT angle field</cell><cell>74.5 73.1</cell><cell>44.3 46.6</cell></row><row><cell></cell><cell>RGB + GT angle field</cell><cell>74.8</cell><cell>39.6</cell></row><row><cell></cell><cell>LDOF angle field</cell><cell>63.2</cell><cell>38.1</cell></row><row><cell>4</cell><cell>GT angle field</cell><cell>85.9</cell><cell>52.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>TRC<ref type="bibr" target="#b9">[10]</ref> MTM<ref type="bibr" target="#b41">[42]</ref> CMS<ref type="bibr" target="#b23">[24]</ref> PCM<ref type="bibr" target="#b2">[3]</ref> MP+Obj MP+Obj + FST<ref type="bibr" target="#b26">[27]</ref> Comparison to state-of-the-art methods on the subset of BMS-26 used in<ref type="bibr" target="#b2">[3]</ref> with F-measure. 'MP+Obj' is MP-Net with objectness.</figDesc><table><row><cell>F</cell><cell>73.0</cell><cell>64.1</cell><cell>72.8</cell><cell>66.0</cell><cell>62.5</cell><cell>78.2</cell><cell>71.8</cell><cell>78.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported in part by the ERC advanced grant ALLEGRO, the MSR-Inria joint project, a Google research award and a Facebook gift. We gratefully acknowledge the support of NVIDIA with the donation of GPUs used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On seeing stuff: The perception of materials by humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Label propagation in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learned-Miller. It&apos;s moving! A probabilistic model for causal motion segmentation in moving camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bideau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video object segmentation by tracking regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large displacement optical flow: descriptor matching in variational motion estimation. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A variational method for scene flow estimation from stereo sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Devernay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classifier based graph construction for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Track to the future: Spatio-temporal video segmentation with long-range motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learned-Miller. Coherent motion segmentation in moving camera videos using optical flow orientations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Higher order motion models and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dense point trajectories by gpu-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geometric motion segmentation and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. Royal Society of London A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A benchmark for the comparison of 3-D motion segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<title level="m">Three-dimensional scene flow. PAMI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3D scene flow estimation with a piecewise rigid scene model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stereoscopic scene flow computation for 3D motion understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vaudrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">LIBSVX: A supervoxel library and benchmark for early video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A multitransformational model for background subtraction with moving cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zamalieva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
