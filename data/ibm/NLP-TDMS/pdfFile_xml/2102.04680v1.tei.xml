<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TräumerAI: Dreaming Music with StyleGAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasaem</forename><surname>Jeong</surname></persName>
							<email>dasaem.jeong@sktbrain.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungheon</forename><surname>Doh</surname></persName>
							<email>seungheondoh@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taegyun</forename><surname>Kwon</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">T-Brain X SK Telecom Seoul</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Culture Technology KAIST Dajeon</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TräumerAI: Dreaming Music with StyleGAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although music is usually regarded as an audio domain, there are many commonly used visual representations for music, such as music notation, spectrogram, and piano roll. Because the music visualization can provide additional information via visual, there have been various music visualization schemes with different purposes, such as to visualize the emotion of music by selecting photos <ref type="bibr" target="#b0">[1]</ref>, to implement an active listening interface by visualizing structure <ref type="bibr" target="#b1">[2]</ref> or progress of music <ref type="bibr" target="#b2">[3]</ref>, or to create media art performance <ref type="bibr" target="#b3">[4]</ref>.</p><p>After the recent advances of generative models, there have been several works exploring the cross between music and visual domain using deep neural networks. An audio-reactive StyleGAN <ref type="bibr" target="#b4">[5]</ref> was introduced, which navigate the latent space of StyleGAN with controlled speed based on audio features such as digital filtering outputs and Nsynth <ref type="bibr" target="#b5">[6]</ref>. The limitation is that the starting images are manually selected for each generation, rather than automatically generated, and only the movement between images are controlled by the acoustic features. Also, selected audio features are focused on the loudness and timbral aspects of audio, the video does not match with high-level semantic features of music such as genre and mood.</p><p>Another recent work proposed a crossing between music and visual style based on artistic periods, such as mapping Debussy's music to French Impressionists' style <ref type="bibr" target="#b6">[7]</ref>. The mapping based on the era provides objective shared labels and helps to avoid arbitrariness in pairing music and art. However, as the authors themselves pointed out, the era label is not sufficient enough to bridging between music and paintings. Also, the model generates a visual style rather than an image, so it demands an additional reference image to be style transferred.</p><p>Our goal is to generate a visually appealing video that responds to music with a neural network so that each frame of the video reflects the musical characteristics of the corresponding audio clip. To achieve the goal, we propose a neural music visualizer directly mapping deep music embeddings to style embeddings of StyleGAN, named TräumerAI 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Audio Embedding and Image Generator</head><p>We utilized a music auto-tagging model as a fixed music encoder, which is a short-chunk CNN with residual connection based on <ref type="bibr" target="#b7">[8]</ref>, and trained the model with MagnaTagATune dataset <ref type="bibr" target="#b8">[9]</ref> and its top 50 tags. We used the output of the last CNN layer as an embedding of the audio.</p><p>For image generation, we used StyleGAN <ref type="bibr" target="#b9">[10]</ref> due to its capacity of generating high-resolution images of quality. Our system was made from a public PyTorch implementation 2 of StyleGAN2 <ref type="bibr" target="#b10">[11]</ref> and a pre-trained model that was trained with WikiArt Dataset 3 . <ref type="figure">Figure 1</ref>: Structure of the proposed TräumerAI system. Images on the right are generated output of our system from Queen's "Bohemian Rhapsody"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Manual labeling between Music and Image</head><p>Rather than establishing an objective metric between musical and visual semantics, we manually labeled the pairs in a subjective manner. One of the authors listened to 100 music clips of 10 seconds long and selected an image that suits the music among the 200 StyleGAN-generated examples. If the annotator could not find an appropriate image, he generated another 200 images in random or based on a single image from the previous step. The data covers various genres including classical, jazz, pop, ballad, R&amp;B, new age, K-pop, J-pop, rock, electronic, hip hop, and trot.</p><p>During the process, the annotator considered his emotional impressions such as arousal and valence, genre and era, and timbral characteristic such as instrumentation. We intentionally avoided mapping vocal sounds to a portrait unless the vocal is strongly dominant or any other instruments does not appear, because portraits have relatively low variance on visual style or perceptive emotion.</p><p>Based on the collected data, we trained a simple transfer function that converts an audio embedding z mu to a style embedding w st . This transfer function is similar to the one used for zero-shot learning between words and images in <ref type="bibr" target="#b11">[12]</ref>, with mean and deviation µ st , σ st of w st from random sampled z of StyleGAN.</p><p>L(w st , z mu ) = |w st − (2σ st tanh(W z mu + b) + µ st )| (1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Video Generation</head><p>The system takes audio input and extracts a sequence of audio embedding. The sequence is converted to a sequence of styles, which is then generated as a sequence of images by StyleGAN. During the experiment, linear interpolation between coarsely sampled sequence( 3 sec) results in abrupt acceleration. Therefore, we sampled the 30 audio embeddings per second so that each frame of video is generated from the corresponding audio embedding. Since our mapping conserves the temporal progress of embeddings, the progress of the video followed structural change of music. Also, smoothing with an averaging window is applied to the style sequence to prevent the generated images from changing too rapidly. The window size differs by style hierarchy, so that coarse, middle, and fine styles are smoothed with a window of 3 sec, 2 sec, and 0.3 sec, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion</head><p>The generated video on Queen's "Bohemian Rhapsody", which can be segmented into six different sub-genres, shows that the mapping between audio and video makes a certain level of intra-segment similarity and inter-segment dissimilarity as presented in <ref type="figure">Figure 1</ref>.</p><p>Although exploring objective mapping between different domains is interesting, subjective mapping can still be a reasonable solution due to the subjective nature of art. Therefore, making a user interface for efficient data labeling between music and painting can be valuable for implementing a personal version of the neural music visualizer. Designing an easily navigable system for the StyleGAN latent space, and applying an active learning method that helps annotators efficiently cover various music or painting styles will significantly reduce the time for the labeling process, which are remained for the future work along with a quantitative evaluation.</p><p>Since the mapping between music and image is done in subjective pairs, the generated results are heavily biased by the annotator's preference on music. Therefore, the system can generate bizarre or grotesque images from the music, which may deliver a biased impression on the music to viewers, thus may distort the original artist's intention or creativity.  Example of Generated Images <ref type="figure" target="#fig_1">Figure 3</ref> shows examples of generated images. Again, we encourage the readers to watch videos on https://jdasam.github.io/traeumerAI_demo/, since how the image is changed throughout the music is the main part of our contribution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example of Audio and Style Embedding Trajectory</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>List of 100 music and corresponding images that were paired during the annotation. Some of the Korean names and titles are translated by the author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Snapshots from example videos. Images in the same row are generated from different segments of the same music</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>demonstrates how audio embeddings and its corresponding mapped style embeddings changes as music progresses, represented in 2D PCA. The presented result shows that our audio encoder extracts different audio embedding for each sub-genre of the music, and also that the mapped style vectors are conserving the tendency in certain level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>2D PCA of audio embeddings and mapped style embeddings from Queen's "Bohemian Rhapsody". Each point represents 3.7 seconds of audio clip, which is sampled for every one third second</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is available on https://github.com/jdasam/traeumerAI. We encourage the readers to watch our demo videos on https://jdasam.github.io/traeumerAI_demo/ 2 https://github.com/rosinality/stylegan2-pytorch 3 https://github.com/pbaylies/stylegan2 4th Workshop on Machine Learning for Creativity and Design at NeurIPS 2020, Vancouver, Canada. arXiv:2102.04680v1 [cs.SD] 9 Feb 2021</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Example of the Labeling</head><p>The selected music and corresponding images during the annotation process are presented in <ref type="figure">Figure 2</ref>. The images were selected from random generation of StyleGAN without truncation to attempt to fully exploit its diversity in expression. Some images like No. 28 and No. 67 are extremely distant from other images in terms of style latent vector, as we did not take into account how extreme the image is in the style latent space. The annotation process took about 10 hours, and we did not modified any annotation after or during the process. To compensate these outliers, we used L1 loss and tanh non-linearity. To be clear, the 100 labeled music clips do not include any music of the artists who are selected for video demonstration.</p><p>The annotation process became time consuming when to cover extremely different genre like Korean trot. If an annotator limits the genre of music and style of image in certain level annotation process can become much faster. Therefore, We expect that other users can make their own version of audio-visual mapping by their preference in shorter time.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emotion-based music visualization using photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Jeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Multimedia Modeling</title>
		<meeting>of International Conference on Multimedia Modeling</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="358" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active music listening interfaces based on signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>of IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1441</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visualizing music in its entirety using acoustic features: Music flowgram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd International Conference on Technologies for Music Notation and Representation (TENOR)</title>
		<meeting>of the 2nd International Conference on Technologies for Music Notation and Representation (TENOR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time music visualization using responsive imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boulanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Torres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 8th International Conference on Virtual Reality</title>
		<meeting>of 8th International Conference on Virtual Reality</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="26" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stylizing audio reactive visuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2019 Workshop: Machine Learning for Creativity and Design</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural audio synthesis of musical notes with wavenet autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>of the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1068" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Crossing you in style: Cross-modal style transfer from music to visual arts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><forename type="middle">P</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08083</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluation of CNN-based automatic music tagging models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Won</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferraro</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanov</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 17th Sound and Music Computing</title>
		<meeting>of 17th Sound and Music Computing</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluation of algorithms using games: The case of music tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Downie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="387" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
