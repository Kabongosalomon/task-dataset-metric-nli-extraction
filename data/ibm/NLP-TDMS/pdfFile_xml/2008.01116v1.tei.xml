<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sub-Pixel Back-Projection Network For Lightweight Single Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supratik</forename><surname>Banerjee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Rawky Tech LLP</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagri</forename><surname>Ozcinar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Rana</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Smolic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Manzke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sub-Pixel Back-Projection Network For Lightweight Single Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>super-resolution</term>
					<term>convolutional neural network</term>
					<term>sub-pixel convolution</term>
					<term>iterative back-projection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural network (CNN)-based methods have achieved great success for single-image superresolution (SISR). However, most models attempt to improve reconstruction accuracy while increasing the requirement of number of model parameters. To tackle this problem, in this paper, we study reducing the number of parameters and computational cost of CNN-based SISR methods while maintaining the accuracy of super-resolution reconstruction performance. To this end, we introduce a novel network architecture for SISR, which strikes a good trade-off between reconstruction quality and low computational complexity. Specifically, we propose an iterative back-projection architecture using sub-pixel convolution instead of deconvolution layers. We evaluate the performance of computational and reconstruction accuracy for our proposed model with extensive quantitative and qualitative evaluations. Experimental results reveal that our proposed method uses fewer parameters and reduces the computational cost while maintaining reconstruction accuracy against state-of-the-art SISR methods over well-known four SR benchmark datasets. 1 Code is available at https://github.com/supratikbanerjee/ SubPixel-BackProjection_SuperResolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single image super-resolution (SISR) is the process of recovering the high-resolution (HR) image from a given low-resolution (LR) image <ref type="bibr" target="#b0">[1]</ref>. With the success in signal processing and machine learning, many learningbased SISR methods have been proposed in the literature, demonstrating promising results. Nowadays, these methods can be used in different applications <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> such as medical imaging, surveillance, face recognition, and virtual reality <ref type="bibr" target="#b3">[4]</ref>.</p><p>Given the advances in SISR, it remains a challenge to deploy the most existing SISR models in real-time applications, demanding compact deep neural network architectures. In particular, some emerging applications require faster SISR methods to boost the imaging performance. For example, modern graphic cards can raise a game's frame rates using SISR algorithm <ref type="bibr" target="#b4">[5]</ref>. In fact, most of the recent SISR algorithms are based upon very deep neural networks, requiring high number of parameters and computational cost for graphically-intensive workloads <ref type="bibr" target="#b5">[6]</ref>.</p><p>In this paper, we propose a new convolutional neural networks (CNNs)-based SISR method with an objective of factoring minimal reduction in perceptual quality while maintaining computational complexity. We use the previously developed SISR method in <ref type="bibr" target="#b6">[7]</ref>, and reduce its network parameters by simplifying the backprojection network architecture. For this, we replace the densely connected up-and down-projection units which comprise of several deconvolution and convolution layers by our proposed sub-pixel back-projection  (SPBP) block. Experimental results validate the effectiveness of our proposed method in reconstructing accurate SR images. The proposed model requires a small number of parameters and low computational cost against several state-of-the-art SISR methods over four well-known SR test datasets. In addition, we demonstrate two smaller variations of our network, SPBP-S (small) and SPBP-M (medium), which use even fewer parameters and has significantly lower computational cost. The rest of the paper is organized as follows: Section 2 discusses the related CNN-based SISR works. Section 3 explains our proposed SISR model. Experimental results are presented in Section 4. Finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Inspired by the performance improvements obtained by CNNs on computer vision tasks such as image-to-image translation <ref type="bibr" target="#b7">[8]</ref>, image captioning <ref type="bibr" target="#b8">[9]</ref>, Dong et al. proposed an SRCNN method <ref type="bibr" target="#b9">[10]</ref>. This work proposed a three-layer network to learn the mapping between the desired HR image and its bicubic up-sampled LR image. Motivated by SRCNN, many CNN-based research works have been shown to use deeper networks to increase representation power further. For instance, Kim et al. <ref type="bibr" target="#b10">[11]</ref> presented a very deep SR (VDSR) architecture to significantly improve the SR image reconstruction accuracy with the use of a 20 layer VGG network <ref type="bibr" target="#b11">[12]</ref> along with global residual learning. Recently, Haris et al. <ref type="bibr" target="#b6">[7]</ref> proposed a deep back-projection network (DBPN), which was based on the idea of iterative up-and down-sampling. However, their proposed network uses large filter sizes which increases the number of parameters, leading to higher computational complexity. Ahn et al. <ref type="bibr" target="#b12">[13]</ref> designed a cascading mechanism on residual networks, which effectively boost the performance with multi-level representation and multiple short-cut connections for learning residuals in LR feature space. Li et al. <ref type="bibr" target="#b13">[14]</ref> proposed (SRFBN) to improve reconstruction performance while having low parameters to reduce chances of over-fitting using a feedback mechanism, but it increases the computational cost of the network.</p><p>Computational efficiency of the neural networks designed for SISR is important. Dong et al. <ref type="bibr" target="#b14">[15]</ref>, for instance, designed an efficient network structure for fast SISR, called fast SR CNN (FSRCNN). With a similar aim, Shi et al. <ref type="bibr" target="#b15">[16]</ref> proposed an efficient sub-pixel CNN (ESPCN). In their work, pixel shuffle network was used to upscale the image at the final step of the SR process. Even though their network demonstrates realtime performance, it lacks high reconstruction quality due to its architectural simplicity. Recently, a few SR networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> have been proposed to have low parameters and low computational complexity, while maintaining state-of-the-art reconstruction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, our proposed network architecture consists of three main blocks, namely, i) feature extraction (FE), ii) non-linear mapping (NLM), and iii) reconstruction. At the first block, we extract shallow features from the LR image. The second block extracts deeper features using an iterative back-projection technique. The third block up-samples and refines the final SR image. In the following, we present details of each block where convolutions are denoted as C onv(k, n) with k being the filter size and n being the number of filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature extraction</head><p>The FE block consists of two convolution layers with PReLU as activation layers, similar to the architectures proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>. The FE block is defined as:</p><formula xml:id="formula_0">F 0 i n = C F E 0 (I LR ), and F 1 i n = C F E 1 (F 0 i n ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">C F E 0 = C onv(3, 4 f ) with C F E 1 = C onv(1, f ),</formula><p>and f is the base number of filters. The low-level representation, F 0 i n , is obtained from the LR image, I LR , and the refined feature F 1 i n is obtained by F 0 i n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Non-linear mapping</head><p>Next, we present details about our proposed NLM block, called SPBP. Here, we reduce the computational cost for SISR, building upon and simplifying the back-projection block developed in <ref type="bibr" target="#b6">[7]</ref>. This method, called DBPN, proposes the use of densely connected up and down projection units. These units make use of multiple convolution and deconvolution (Dconv) layers to back-project the feature maps, which makes the network computationally expensive. To reduce the model complexity of DBPN, we propose to replace these up-and down-projection units and their error feedback mechanism with up-and down-sampling layers as SPC and convolution layers. Our inspiration for this new approach of using SPC over Dconv is based on the work of Shi et al. <ref type="bibr" target="#b15">[16]</ref>, where it is described that the SPC layer is log 2 r 2 times faster than Dconv layer in the forward pass. Since SPC operates in LR space on a feature map of size n, W s , H s and Dconv layer operates in HR space on a feature map of size n s 2 ,W, H , where W and H are the dimensions of the input. We can represent the information contained in its feature maps as:</p><formula xml:id="formula_2">SPC = LR n × W s × H s and Dconv = H R n s 2 × W × H .</formula><p>The complexity of the layers with a filter size of k × k and scaling factor s will then be:</p><formula xml:id="formula_3">SPC = O n × n × k × k × W s × H s (2) Dconv = O n s 2 × n s 2 × sk × sk × W × H<label>(3)</label></formula><p>Thus, the number of parameters are:</p><formula xml:id="formula_4">SPC = LR (n × n × k × k) (4) Dconv = H R n s 2 × n s 2 × sk × sk<label>(5)</label></formula><p>For the same information retention and computational complexity, as shown in Eqs. <ref type="formula">(4)</ref> and <ref type="bibr" target="#b4">(5)</ref>. SPC contains larger number of parameters compared to Dconv, and therefore, upholds a higher representation power without adding computational complexity. For this reason, we propose to use SPC in order to reduces network parameters by simplifying the back-projection network architecture. This approach provides higher representation power and achieves an efficient feature mapping. <ref type="figure" target="#fig_1">Figure 2</ref> shows the design of SPBP, which comprises of an exterior and interior unit. The exterior unit is defined as:</p><formula xml:id="formula_5">H 0 = P S(C N LM 0,0 (F 1 i n ) ↑ s , and L 0 = C N LM 0,1 (H 0 ) ↓ s ,<label>(6)</label></formula><p>where ↑ s , ↓ s represent up-sample and down-sample operations respectively with a scale factor s. Also, C N LM</p><formula xml:id="formula_6">0,0 represents C onv(3, f s 2 ),</formula><p>where P S is the pixel-shuffle layer, which defines SPC. The SPBP block takes F 1 i n , which is the first LR feature map in this block as input and produces an HR feature map, H 0 . This is backprojected to a LR feature map L 0 using C N LM 0,1 , which represents C onv <ref type="bibr">(3, f )</ref>. This is a single group of the proposed SPBP block.</p><p>The use of DenseNet <ref type="bibr" target="#b18">[19]</ref> has demonstrated the alleviation of vanishing gradient problem. Also, the use of dense skip connections help to generate powerful high-level representations and encourages feature reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F out F in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv 1x1</head><p>Conv <ref type="formula">(</ref>  Inspired by this, we introduce the use of dense connections in SPBP block, as similar to <ref type="bibr" target="#b6">[7]</ref>, which forms the interior unit of the block. Thus the interior unit of G groups is formulated as:</p><formula xml:id="formula_7">H g = P S(C N LM g ,0 F 1 i n , L 0 , . . . , L g −1 ) ↑ s ,<label>(7)</label></formula><formula xml:id="formula_8">L g = C N LM g ,1 H 0 , H 1 , . . . , H g ↓ s ,<label>(8)</label></formula><formula xml:id="formula_9">F out = C out ([L 1 , L 2 , . . . , L G ]) .<label>(9)</label></formula><p>where [F 1 i n , L 0 , . . . , L g −1 ] refers to the concatenation of F 1 i n , LR feature maps 0, ..., g − 1 and H g and is the HR feature map produced by the up-projection layer in the g t h group. Similarly, [H 0 , H 1 , . . . , H g ] refers to the concatenation of HR feature maps 0, ..., g and L g is the LR feature map produced by the down-projection layer in the g t h group. C out is a compression unit representing C onv(1, f ) to generate the output F out by fusing LR features from the previous levels 1, ...,G of the SPBP block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reconstruction</head><p>This block uses a SPC layer which up-scales the LR feature map obtained from the SPBP block. This is followed a convolution layer which refines the up-sampled feature map. The reconstruction layer is defined as: </p><formula xml:id="formula_10">I SR = I Res 1 + f U P (I LR ),<label>(10)</label></formula><p>where I Res 0 is the residual upscale of P S(C R 0 (F out )) with input F out . I Res 1 is the refined residual HR feature map derived from C R 1 (.), which is a C onv <ref type="bibr">(3, f out</ref> ) where, f out = 3 is the output feature map "RGB". Inspired by <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref> the super-resolved image is constructed by adding the refined HR feature map with f U P (.), which is bicubic up-sample of the LR image. Since the LR image contains abundant low-frequency information <ref type="bibr" target="#b20">[21]</ref>, this allows the network to bypass the LR information and focus only on the residual component from the HR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section, we first describe our training details, and then we evaluate our proposed SISR method with state-of-the-art SISR methods using quantitative and qualitative experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Details</head><p>All experimentation was carried out on ×2 scaling factor between LR and HR. The LR images were obtained by down-sampling HR images from the training set of DIV2K <ref type="bibr" target="#b21">[22]</ref> dataset with bicubic interpolation. For training, the LR image-crop size was set as 48 × 48 with 40 random crops per image. The mini-batch size was set to 40 for all network configurations. Each proposed model was trained using the ADAM optimizer with L1 loss for 1000 epochs, with β 1 = 0.9, β 2 = 0.999. The learning rate was initialized as 10 −4 and decayed by a factor of 2 in every 200 epochs. Image augmentation was was used for training by randomly flipping horizontally or vertically and rotating the training images like <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref>. Three different settings for the proposed SISR model, SPBP-S (small), SPBP-M (medium) and SPBP-L (large) use ( f = 16,G = 1), ( f = 16,G = 10), and ( f = 32,G = 10) configurations, respectively. The proposed models have been implemented using the PyTorch library <ref type="bibr" target="#b23">[24]</ref>. The training was performed using NVIDIA Titan-Xp GPU with 12 GB memory on Intel core i7-7700 machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>To validate our proposed SPBP method, we performed a thorough experimental analysis using nine CNNbased state-of-the-art SISR algorithms: SRCNN <ref type="bibr" target="#b9">[10]</ref>, FSRCNN <ref type="bibr" target="#b14">[15]</ref>, ESPCN <ref type="bibr" target="#b15">[16]</ref>, VDSR <ref type="bibr" target="#b10">[11]</ref>, DBPN-SS <ref type="bibr" target="#b6">[7]</ref>, CARN <ref type="bibr" target="#b12">[13]</ref>, IDN <ref type="bibr" target="#b16">[17]</ref>, SRFBNs <ref type="bibr" target="#b13">[14]</ref>, FLSR <ref type="bibr" target="#b17">[18]</ref>. As our focus is to develop a lightweight network for SISR, for simplicity, we do not show results for the published networks which are known to have a more complex model than CARN <ref type="bibr" target="#b12">[13]</ref>. Each model was tested with four datasets, namely, Set5 <ref type="bibr" target="#b24">[25]</ref>, Set14 <ref type="bibr" target="#b25">[26]</ref>, BSDS100 <ref type="bibr" target="#b26">[27]</ref>, and Urban100 <ref type="bibr" target="#b27">[28]</ref>.</p><p>In the following, we compare the performance between our proposed methods (SPBP-S, SPBP-M, SPBP-L, SPBP-L+), and state-of-the-art SISR methods using quantitative and qualitative analysis. Similar to other SISR methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>, we applied the self-ensemble strategy during testing on SPBP-L to further improve the reconstruction performance, we denote this method as SPBP-L+. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Quantitative</head><p>We measured the performance of each method for its reconstructed accuracy of the SR image using PSNR and SSIM. Here, similar to previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>, we cropped 2 pixels near image boundary and estimated quality scores using only the luminance channel (Y) of images. Also, we measure the computational complexity in terms of the number of operations with Mult-Adds, which is the number of composite multiply-accumulate operations. <ref type="table" target="#tab_2">Table 1</ref> compares the performance of the proposed SPBP-S, SPBP-M, and SPBP-L models with state-of-the-art methods in terms of # of parameters, computational complexity, and objective quality metrics. We also examined the computational complexity of our model in comparison to other state-of-the-art methods concerning PSNR over the datasets. <ref type="figure">Fig. 3</ref> shows trade-off between reconstruction accuracy (in terms of PSNR) versus number of operations and parameters over three datasets: (a)-Set5, BSDS100, and Urban100. In the experiment, the calculations were performed for HR image of size 720p (1280 × 720). Looking at the results, we see that our proposed models (SPBP-S, SPBP-M, SPBP-L, and SPBP-L+) outperform state-of-the-art methods in terms of PSNR for comparable parameter size and has a much lower computational cost.</p><p>Overall, our SPBP-L+ model, which has nearly 629K parameters, shows the best reconstruction accuracy performance in most of the benchmark datasets in terms of objective quality scores. Further, we observe that SPBP-M which has only 159K parameters performs very close in most of the benchmark datasets to FLSR, SRFBN, IDN and CARN, all of which have about double or more parameters. Comparing models with less than 100K parameters, we can clearly see SPBP-S outperforms all existing models <ref type="figure">(SRCNN, FSRCNN, ESPCN)</ref>. These results prove that our developed models handle the image feature better than the other state-of-the-art methods with fewer parameters and lower computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Qualitative</head><p>To provide qualitative visual comparison between methods, <ref type="figure">Fig. 4</ref> shows some examples of reconstructed images from the Urban100 dataset. We see that the proposed model can construct HR images with higher quality, compared to most of the state-of-the-art methods. Also, we observe that the proposed models have visually similar or better results compared to other state-of-the-art networks, such as CARN, IDN, VDSR, but with lower parameters and computational expense. Especially, the proposed SPBP models construct high frequency patterns with subjectively closer to the original HR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel sub-pixel convolution-based dense iterative back-projection network architecture for single-image super-resolution tasks. We showed the reconstruction accuracy and computational efficiency of employing our proposed models (SPBP-S, SPBP-M, SPBP-L) in terms of model parameters, quantitative quality measures (in terms of PSNR, SSIM), and qualitative evaluations. We also compared our proposed model with nine state-of-the-art SISR methods over well-known SR datasets and demonstrated that our proposed approach provides lower computational complexity while maintaining high reconstruction performance. This can be very well observed with SPBP-S which stands out to be the best performing network under 100K parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Proposed network architecture for SISR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sub-Pixel Back-Projection Block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>out ) ↑ s , and I Res 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Trade-off between reconstruction accuracy versus number of operations and parameters on three datasets. The xaxis and the y-axis denote the Multi-Adds and PSNR [dB], and the size of the circle represents the number of parameters. The Mult-Adds is computed for HR image of size 720p. Qualitative comparison of our SPBP models with other works on "img_074" and "img_059" example images from the Urban100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative Results on four datasets. The highest reconstruction accuracy is indicated in red and second highest reconstruction accuracy in blue. [×2 upscaling]</figDesc><table><row><cell>Datasets</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under the Grant Number 15/RP/2776.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Super-resolution image reconstruction: a technical overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine (SPM)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="36" />
			<date type="published" when="2003-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Single-image super-resolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep learning for image super-resolution: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno>arXiv cs.CV 1902.06068</idno>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-resolution of omnidirectional images using adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ozcinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Nvidia dlss: Control and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Edelsten</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/en-us/geforce/news/dlss-control-and-beyond/" />
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for single image super-resolution: A brief review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep tone mapping operator for high dynamic range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valenzise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aesthetic image captioning from weakly-labelled photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koustav</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2016-02" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast, accurate, and lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhyuk</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungkon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Ah</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="256" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszãąr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast and accurate single image super-resolution via information distillation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiumei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and lightweight image super-resolution based on dense residuals two-channel network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="2826" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Single image super-resolution using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korobchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Foco</surname></persName>
		</author>
		<ptr target="https://gwmt.nvidia.com/super-res/about" />
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="294" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop Autodiff Submission in Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<editor>Jean-Daniel Boissonnat, Patrick Chenin, Albert Cohen, Christian Gout, Tom Lyche, Marie-Laurence Mazure, and Larry Schumaker</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
