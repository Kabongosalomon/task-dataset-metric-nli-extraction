<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Antipodal Robotic Grasping using Generative Residual Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sulabh</forename><surname>Kumra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirin</forename><surname>Joshi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferat</forename><surname>Sahin</surname></persName>
						</author>
						<title level="a" type="main">Antipodal Robotic Grasping using Generative Residual Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a modular robotic system to tackle the problem of generating and performing antipodal robotic grasps for unknown objects from the nchannel image of the scene. We propose a novel Generative Residual Convolutional Neural Network (GR-ConvNet) model that can generate robust antipodal grasps from n-channel input at real-time speeds (∼20ms). We evaluate the proposed model architecture on standard datasets and a diverse set of household objects. We achieved state-of-the-art accuracy of 97.7% and 94.6% on Cornell and Jacquard grasping datasets, respectively. We also demonstrate a grasp success rate of 95.4% and 93% on household and adversarial objects, respectively, using a 7 DoF robotic arm.</p><p>Authors are with Multi-Agent Bio-Robotics Laboratory (MABL), Rochester Institute of Technology, Rochester, NY, USA. Sulabh Kumra is also with OSARO Inc.,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Robotic manipulators are constantly compared to humans due to the inherent characteristics of humans to instinctively grasp an unknown object rapidly and with ease based on their own experiences. As increasing research is being done to make the robots more intelligent, there exists a demand for a generalized technique to infer fast and robust grasps for any kind of object that the robot encounters. The major challenge is being able to precisely transfer the knowledge that the robot learns to novel real-world objects.</p><p>We present a modular robot agnostic approach to tackle this problem of grasping unknown objects. We propose a Generative Residual Convolutional Neural Network (GR-ConvNet) that generates antipodal grasps for every pixel in an n-channel input image. We use the term generative to distinguish our method from other techniques that output a grasp probability or classify grasp candidates in order to predict the best grasp.</p><p>Unlike the previous work done in robotic grasping <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, where the required grasp is predicted as a grasp rectangle calculated by choosing the best grasp from multiple grasp probabilities, our network generates three images from which we can infer grasp rectangles for multiple objects. Additionally, it is possible to infer multiple grasp rectangles for multiple objects from the output of GR-ConvNet in oneshot thereby decreasing the overall computational time. <ref type="figure">Fig.1</ref> shows an overview of the proposed system architecture. It consists of two main modules: the inference module and the control module. The inference module acquires RGB and aligned depth images of the scene from the RGB-D camera. The images are pre-processed to match the input format of the GR-ConvNet. The network generates quality, <ref type="figure">Fig. 1</ref>: Proposed system overview. Inference module predict suitable grasp poses for the objects in the camera's field of view. Control module uses these grasp poses to plan and execute robot trajectories to perform antipodal grasps. Video: https://youtu.be/cwlEhdoxY4U angle, and width images, which are then used to infer antipodal grasp poses. The control module consists of a task controller that prepares and executes a plan to perform a pick and place task using the grasp pose generated by the inference module. It communicates the required actions to the robot through a ROS interface using a trajectory planner and controller.</p><p>The main contributions of this paper can be summarized as follows:</p><p>• We present a modular robotic system that predicts, plans, and performs antipodal grasps for the objects in the scene. We open-sourced the implementation of the proposed inference 1 and control 2 modules. • We propose a novel generative residual convolutional neural network architecture that predicts suitable antipodal grasp configurations for objects in the camera's field of view. • We evaluate our model on publicly available grasping datasets and achieved state-of-the-art accuracy of 97.7% and 94.6% on Cornell and Jacquard grasping datasets, respectively. • We demonstrate that the proposed model can be de-ployed on a robotic arm to perform antipodal grasps at real-time speeds with a success rate of 95.4% and 93% on household and adversarial objects, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Robotic Grasping: There has been extensive on-going research in the field of robotics, especially robotic grasping. Although the problem seems to just be able to find a suitable grasp for an object, the actual task involves multifaceted elements such as-the object to be grasped, the shape of the object, physical properties of the object and the gripper with which it needs to be grasped among others. Early research in this field involved hand-engineering the features <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, which can be a tedious and time-consuming task but can be helpful for learning to grasp objects with multiple fingers such as <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>Initially for obtaining a stable grasp, the mechanics and contact kinematics of the end effector in contact with the object were studied and the grasp analysis was performed as seen from the survey by <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Prior work <ref type="bibr" target="#b10">[11]</ref> in robotic grasping for novel objects involved using supervised learning which was trained on synthetic data but it was limited to environments such as office, kitchen, and dishwasher. Satish et al. <ref type="bibr" target="#b11">[12]</ref> introduced a Fully Convolutional Grasp Quality Convolutional Neural Network (FC-GQ-CNN) which predicted a robust grasp quality by using a data collection policy and synthetic training environment. This method enabled an increase in the number of grasps considered to 5000 times in 0.625s. However, the current research relies more on using the RGB-D data to predict grasp poses. These approaches depend wholly on deep learning techniques.</p><p>Deep learning for grasping: Deep learning has been a hot topic of research since the advent of ImageNet success and the use of GPU's and other fast computational techniques. Also, the availability of affordable RGB-D sensors enabled the use of deep learning techniques to learn the features of objects directly from image data. Recent experimentations using deep neural networks <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> have demonstrated that they can be used to efficiently compute stable grasps. Pinto et al. <ref type="bibr" target="#b2">[3]</ref> used an architecture similar to AlexNet which shows that by increasing the size of the data, their CNN was able to generalize better to new data. Varley et al. <ref type="bibr" target="#b14">[15]</ref> propose an interesting approach to grasp planning through shape completion where a 3D CNN was used to train the network on the 3D prototype of objects on their own dataset captured from various viewpoints. Guo et al. <ref type="bibr" target="#b15">[16]</ref> used tactile data along with visual data to train a hybrid deep architecture. Mahler et al. <ref type="bibr" target="#b16">[17]</ref> proposed a Grasp Quality Convolutional Neural Network (GQ-CNN) that predicts grasps from synthetic point cloud data trained on Dex-Net 2.0 grasp planner dataset. Levine et al. <ref type="bibr" target="#b17">[18]</ref> discuss the use of monocular images for hand to eye coordination for robotic grasping using a deep learning framework. They use a CNN for grasp success prediction and further use continuous servoing to continuously servo the manipulator to correct mistakes. Antanas et al. <ref type="bibr" target="#b18">[19]</ref> discuss an interesting approach known as a probabilistic logic framework that is said to improve the grasping capability of a robot with the help of semantic object parts. This framework combines high-level reasoning with low-level grasping. The high-level reasoning comprises object affordances, its categories, and task-based information while low-level reasoning uses visual shape features. This has been observed to work well in kitchen-related scenarios.</p><p>Grasping using Uni-modal data : Johns et al. <ref type="bibr" target="#b23">[24]</ref> used a simulated depth image to predict a grasp outcome for every grasp pose predicted and select the best grasp by smoothing the predicted pose using a grasp uncertainty function. A generative approach to grasping is discussed by Morrison et al. <ref type="bibr" target="#b19">[20]</ref>. The Generative grasp CNN architecture generates grasp poses using a depth image and the network computes grasp on a pixel-wise basis. <ref type="bibr" target="#b19">[20]</ref> suggests that it reduces existing shortcomings of discrete sampling and computational complexity. Another recent approach that merely relies on depth data as the sole input to the deep CNN is as seen in <ref type="bibr" target="#b12">[13]</ref>.</p><p>Grasping using multi-modal data: There are different ways of handling objects multi-modalities. Many have used separate features to learn the modalities which can be computationally exhaustive. Wang et al. <ref type="bibr" target="#b25">[25]</ref> proposed methods that consider multi-modal information as the same. Jiang et al. <ref type="bibr" target="#b26">[26]</ref> used RGB-D images to infer grasps based on a two-step learning process. The first step was used to narrow down the search space and the second step was used to compute the optimal grasp rectangle from the top grasps obtained using the first method. Lenz et al. <ref type="bibr" target="#b0">[1]</ref> used a similar twostep approach but with a deep learning architecture which however could not work well on all types of objects and often predicted a grasp location that was not the best grasp for that particular object such as in <ref type="bibr" target="#b26">[26]</ref> the algorithm predicted the grasp for a shoe was from its laces which in practice failed when the robot tried to grasp using the shoelaces while in <ref type="bibr" target="#b0">[1]</ref> the algorithm sometimes could not predict grasps which are more practical using just the local information as well as due to the RGB-D sensor used. Yan et al. <ref type="bibr" target="#b27">[27]</ref> used point cloud prediction network to generate a grasp by first preprocessing the data by obtaining the color, depth, and masked image and then obtaining a 3D point cloud of the object to be fed into a critic network to predict a grasp. Chu et al. <ref type="bibr" target="#b20">[21]</ref> propose a novel architecture that can predict multiple grasps for multiple objects simultaneously rather than for a single object. For this, they used a multi-object dataset of their own. The model was also tested on Cornell Grasp Dataset. A robotic grasping method that consists of a ConvNet for object recognition and a grasping method for manipulating the objects is discussed by Ogas et al. <ref type="bibr" target="#b28">[28]</ref>. The grasping method assumes an industry assembly line where the object parameters are assumed to be known in advance. Kumra et al. <ref type="bibr" target="#b3">[4]</ref> proposed a Deep CNN architecture that uses residual layers for predicting robust grasps. The paper demonstrates that a deeper network along with residual layers learns better features and performs faster. Asif et al. <ref type="bibr" target="#b29">[29]</ref> introduced a consolidated framework known as EnsembleNet in which the grasp generation network generates four grasp representations and EnsembleNet synthesizes these generated grasps to produce grasp scores from which the grasp with the highest score gets selected.</p><p>Our work is based on similar concepts and is designed to advance the research done in this area. <ref type="table" target="#tab_0">Table I</ref> provides a comparison of our work to recent related work in robotic grasping for unknown objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION</head><p>In this work, we define the problem of robotic grasping as predicting antipodal grasps for unknown objects from an n-channel image of the scene and executing it on a robot.</p><p>Instead of the 5 dimensional grasp representation used in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, we use an improved version of grasp representation similar to the one proposed by Morrison et al. in <ref type="bibr" target="#b19">[20]</ref>. We denote the grasp pose in robot frame as:</p><formula xml:id="formula_0">G r = (P, Θ r ,W r , Q)<label>(1)</label></formula><p>where, P = (x, y, z) is tool tip's center position, Θ r is tools rotation around the z-axis, W r is the required width for the tool, and Q is the grasp quality score. We detect a grasp from an n-channel image I = R n×h×w with height h and width w, which can be defined as:</p><formula xml:id="formula_1">G i = (x, y, Θ i ,W i , Q)<label>(2)</label></formula><p>where (x, y) corresponds to the center of grasp in image coordinates, Θ i is the rotation in camera's frame of reference, W i is the required width in image coordinates, and Q is the same scalar as in equation <ref type="formula" target="#formula_0">(1)</ref>. The grasp quality score Q is the quality of the grasp at every point in the image and is indicated as a score value between 0 and 1 where a value that is in proximity to 1 indicates a greater chance of grasp success. Θ i indicates the antipodal measurement of the amount of angular rotation required at each point to grasp the object of interest and is represented as a value in the range [ −π 2 , π 2 ]. W i is the required width which is represented as a measure of uniform depth and indicated as a value in the range of [0,W max ] pixels. W max is the maximum width of the antipodal gripper.</p><p>To execute a grasp obtained in the image space on a robot, we can apply the following transformations to convert the image coordinates to robot's frame of reference.</p><formula xml:id="formula_2">G r = T rc (T ci (G i ))<label>(3)</label></formula><p>where, T ci is a transformation that converts image space into camera's 3D space using the intrinsic parameters of the camera, and T rc converts camera space into the robot space using the camera pose calibration value. This notation can be scaled for multiple grasps in an image. The collective group of all the grasps can be denoted as:</p><formula xml:id="formula_3">G = (Θ Θ Θ, W, Q) ∈ R 3×h×w (4)</formula><p>where Θ Θ Θ, W, and Q represents three images in the form of grasp angle, grasp width and grasp quality score respectively calculated at every pixel of an image using equation <ref type="formula" target="#formula_1">(2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPROACH</head><p>We propose a dual-module system to predict, plan and perform antipodal grasps for the objects in the scene. The overview of the proposed system is shown in <ref type="figure">fig.1</ref>. The inference module is used to predict suitable grasp poses for the objects in the camera's field of view. The control module uses these grasp poses to plan and execute robot trajectories to perform antipodal grasps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inference module</head><p>The inference module consists of three parts. First, the input data is pre-processed where it is cropped, resized, and normalized. If the input has a depth image, it is inpainted to obtain a depth representation <ref type="bibr" target="#b30">[30]</ref>. The 224 × 224 n-channel processed input image is fed into the GR-ConvNet. It uses n-channel input that is not limited to a particular type of input modality such as a depth-only or RGB-only image as our input image. Thus, making it generalized for any kind of input modality. The second generates three images as grasp angle, grasp width, and grasp quality score as the output using the features extracted from the pre-processed image using GR-ConvNet. The third infers grasp poses from the three output images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Control module</head><p>The control module mainly incorporates a task controller that performs tasks such as pick-and-place and calibration. The controller requests a grasp pose from the inference module which returns the grasp pose with the highest quality score. The grasp pose is then converted from camera coordinates into robot coordinates using the transform calculated from hand-eye calibration <ref type="bibr" target="#b31">[31]</ref>. Further, the grasp pose in robot frame is used to plan a trajectory to perform the pick and place action using inverse kinematics through a ROS interface. The robot then executes the planned trajectory. Due to our modular approach and ROS integration, this system can be adapted for any robotic arm. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the proposed GR-ConvNet model, which is a generative architecture that takes in an n-channel input image and generates pixel-wise grasps in the form of three images. The n-channel image is passed through three convolutional layers, followed by five residual layers and convolution transpose layers to generate four images. These output images consist of grasp quality score, required angle in the form of cos 2Θ, and sin 2Θ as well as the required width of the end effector. Since the antipodal grasp is uniform around ± π 2 , we extract the angle in the form of two elements cos 2Θ and sin 2Θ that output distinct values that are combined to form the required angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model architecture</head><p>The convolutional layers extract the features from the input image. The output of the convolutional layer is then fed into 5 residual layers. As we know, accuracy increases with increasing the number of layers. However, it is not true when you exceed a certain number of layers, which results in the problem of vanishing gradients and dimensionality error, thereby causing saturation and degradation in the accuracy. Thus, using residual layers enables us to better learn the identity functions by using skip connections. After passing the image through these convolutional and residual layers, the size of the image is reduced to 56 × 56, which can be difficult to interpret. Therefore, to make it easier to interpret and retain spatial features of the image after convolution operation, we up-sample the image by using a convolution transpose operation. Thus, we obtain the same size of the image at the output as the size of the input.</p><p>Our network has a total of 1,900,900 parameters which indicate that our network is comparatively shorter as opposed to other networks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b29">[29]</ref>. Thereby making it computationally less expensive and faster in contrast to other architectures using similar grasp prediction techniques that contain millions of parameters and complex architectures. The lightweight nature of the model makes it suitable for closed-loop control at a rate of up to 50 Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training methodology</head><p>For a dataset having objects D = {D 1 . . . D n }, input scene images I = I 1 . . . I n and successful grasps in image frame G i = g 1 1 . . . g 1 m 1 . . . g 2 1 . . . g n m n , we can train our model endto-end to learn the mapping function γ(I, D) = G i by minimizing the negative log-likelihood of G i conditioned on the input image scene I, which is given by:</p><formula xml:id="formula_4">− 1 n n ∑ i=1 1 m i m i ∑ j=1 logγ(g j i |I i )<label>(5)</label></formula><p>The models were trained using the Adam optimizer <ref type="bibr" target="#b32">[32]</ref> and standard backpropagation and mini-batch SGD technique <ref type="bibr" target="#b33">[33]</ref>. The learning rate was set as 10 −3 and a mini-batch size of 8 was used. We trained the model using three random seeds, and report the average of the three seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss function</head><p>We analyzed the performance of various loss functions for our network and after running a few trials found that in order to handle exploding gradients, the smooth L1 loss also known as Huber loss works best. We define our loss as :</p><formula xml:id="formula_5">L (G i , G i ) = 1 n k ∑ z k<label>(6)</label></formula><p>where z i is given by:</p><formula xml:id="formula_6">z k =    0.5(G i k − G i k ) 2 , i f G i k − G i k &lt; 1 G i k − G i k − 0.5 otherwise<label>(7)</label></formula><p>G i is the grasp generated by the network and G i is the ground truth grasp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION A. Datasets</head><p>There are a limited number of publicly available antipodal grasping datasets. <ref type="table" target="#tab_0">Table II</ref> shows a summary of the publicly available antipodal grasping datasets. We used two of these datasets for training and evaluating our model. The first one is the Cornell grasp dataset <ref type="bibr" target="#b26">[26]</ref>, which is the most common grasping dataset used to benchmark results, and the second one is a more recent Jacquard grasping dataset <ref type="bibr" target="#b34">[34]</ref>, which is more than 50 times bigger the Cornell grasp dataset. The extended version of Cornell Grasp Dataset comprises of 1035 RGB-D images with a resolution of 640 × 480 pixels of 240 different real objects with 5110 positive and 2909 negative grasps. The annotated ground truth consists of several grasp rectangles representing grasping possibilities per object. However, it is a small dataset for training our GR-ConvNet model, therefore we create an augmented dataset using random crops, zooms, and rotations which effectively has 51k grasp examples. Only positively labeled grasps from the dataset were considered during training.</p><p>The Jacquard Grasping Dataset is built on a subset of ShapeNet which is a large CAD models dataset. It consists of 54k RGB-D images and annotations of successful grasping positions based on grasp attempts performed in a simulated environment. In total, it has 1.1M grasp examples. As this dataset was large enough to train our model, no augmentation was performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Grasp Detection Metric</head><p>For a fair comparison of our results, we use the rectangle metric <ref type="bibr" target="#b26">[26]</ref> proposed by Jiang et al. to report the performance of our system. According to the proposed rectangle metric, a grasp is considered to be valid when it satisfies the following two conditions:</p><p>• The intersection over union (IoU) score between the ground truth grasp rectangle and the predicted grasp rectangle is more than 25%. • The offset between the grasp orientation of the predicted grasp rectangle and the ground truth rectangle is less than 30 • . This metric requires a grasp rectangle representation, but our model predicts image-based grasp representation G i using equation 2. Therefore, in order to convert from imagebased grasp representation to rectangle representation, the value corresponding to each pixel in the output image is mapped to its equivalent rectangle representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>In our experiments, we evaluate our approach on: (i) two standard datasets, (ii) household objects, (iii) adversarial objects and (iv) objects in clutter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Setup</head><p>To get the scene image for the real-world experiments, we used the Intel RealSense Depth Camera D435 that uses stereo vision to calculate depth. It consists of a pair of RGB sensors, depth sensors, and an infrared projector. The experiments were conducted on the 7-DoF Baxter Robot by Rethink Robotics. A two-fingered parallel gripper was used for grasping the test objects. The camera was mounted behind the robot arm looking over the shoulder.</p><p>The execution times for our proposed GR-ConvNet are measured on a system running Ubuntu 16.04 with an Intel Core i7-7800X CPU clocked at 3.50 GHz and an NVIDIA GeForce GTX 1080 Ti graphics card with CUDA 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Household test objects</head><p>A total of 35 household objects were chosen for testing the performance of our system. Each object was tested individually for 10 different positions and orientations which resulted in 350 grasp attempts. The objects were chosen such that each object represented different shape, size, and geometry; and had minimum or no resemblance with each other. We created a mix of deformable, difficult to grasp, reflective, and small objects that need high precision. <ref type="figure" target="#fig_1">Fig. 3a</ref> shows the set of objects that were used for the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Adversarial test objects</head><p>Another set consisting of 10 adversarial objects with complex geometry was used to evaluate the accuracy of our proposed system. These 3D printed objects have abstract geometry with indefinite surfaces and edges that are hard to perceive and grasp. Each of these objects was tested in isolation for 10 different orientations and positions and made up of a total of 100 grasp attempts. <ref type="figure" target="#fig_1">Fig. 3b</ref> shows the adversarial objects used during the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Objects in clutter</head><p>Industrial applications such as warehouses require objects to be picked in isolation as well as from a clutter. Therefore, to perform our experiments on cluttered objects we carried out 10 runs with 60 unseen objects. A set of distinct objects for each run was selected from the previously unseen novel objects to create a cluttered scene. An example of this is shown in <ref type="figure" target="#fig_3">fig. 5</ref>. Each run is terminated when there are no objects in the camera's field of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RESULTS</head><p>In this section, we discuss the results of our experiments. We evaluate GR-ConvNet on both the Cornell and the Jacquard dataset to examine the outcomes for each of the datasets based on factors such as the size of the dataset, type of training data and demonstrate our model's capacity to generalize to any kind of object. Further, we show that our model is able to not only generate a single grasp for isolated objects but also multiple grasps for multiple objects in clutter. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the qualitative results obtained on previously unseen objects. The figure consists of output in image representation G i in the form of grasp quality score Q, the required angle for grasping Θ i , and the required gripper width W i . It also includes the output in the form of a rectangle grasp representation projected on the RGB image.</p><p>Further, we demonstrate the viability of our method in comparison to other methods by gauging the performance  of our network on different types of objects. Additionally, we evaluate the performance of our network on different input modalities. The modalities that the model was tested on included uni-modal input such as depth only and RGB only input images; and multi-modal input such as RGB-D images. <ref type="table" target="#tab_0">Table III</ref> show that our network performed better on multi-modal data as compared to uni-modal data since multiple input modalities enabled better learning of the input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cornell Dataset</head><p>We follow a cross-validation setup as in previous works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b15">[16]</ref>, using image-wise (IW) and objectwise (OW) data splits. <ref type="table" target="#tab_0">Table III</ref> shows the performance of our system for multiple modalities in comparison to other techniques used for grasp prediction. We obtained state-ofthe-art accuracy of 97.7% on Image-wise split and 96.6% on Object-wise split using RGB-D data, outperforming all competitive methods as seen in table III. The results obtained on the previously unseen objects in the dataset depict that our network can predict robust grasps for different types of objects in the validation set. The data augmentation performed on the Cornell grasp dataset improved the overall performance of the network. Further, the recorded prediction speed of 20ms per image suggests that GR-ConvNet is suitable for real-time closed-loop applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Jacquard Dataset</head><p>For the Jacquard dataset, we trained our network on 90% of the dataset images and validated on 10% of the remaining dataset. As the Jacquard dataset is much larger than the Cornell dataset, no data augmentation was required.</p><p>We performed experiments on the Jacquard dataset using multiple modalities and obtained state-of-the-art results with an accuracy of 94.6% using RGB-D data as the input. <ref type="table" target="#tab_0">Table  IV</ref> shows that our network not only gives the best results on the Cornell grasp dataset but also outperforms other methods on the Jacquard dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Grasping novel objects</head><p>Along with the state-of-the-art results on two standard datasets, we also demonstrate that our system equally outperforms in robotic grasping experiments for novel real-world objects. We used 35 household and 10 adversarial objects to evaluate the performance of our system in the physical world using the Baxter robotic arm. Each of the objects was tested for 10 different positions and orientations. The robot performed 334 successful grasps of the total 350 grasp attempts on household objects resulting in an accuracy of 95.4% and 93 successful grasps out of 100 grasp attempts on adversarial objects giving an accuracy of 93%. <ref type="table" target="#tab_4">Table V</ref> shows our results in comparison to other deep learning based approaches in robotic grasping.</p><p>The results obtained in table V and <ref type="figure" target="#fig_2">fig. 4</ref> indicates that GR-ConvNet is able to generalize well to new objects that it has never seen before. The model was able to generate grasps for all the objects except for a transparent bottle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Objects in clutter</head><p>Along with predicting optimum grasps for novel real objects, our robust model is able to predict multiple antipodal grasps for multiple objects in clutter. Each run was performed with as well as without object replacement, and we achieved a grasp success of 93.5% by averaging grasp success for every successful grasp attempt in each run. Despite the model being trained only on isolated objects, it was able to efficiently predict grasps for manifold objects. Moreover, <ref type="figure" target="#fig_2">fig. 4(d)</ref> shows grasps predicted for multiple objects and <ref type="figure" target="#fig_3">fig. 5</ref> illustrates robot grasping household and adversarial objects in cluttered environments. This demonstrates that GR-ConvNet generalizes to all types of objects and can predict robust grasps for multiple objects in clutter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Failure case analysis</head><p>In our experimental results, there are only a few cases that can be accounted for as failures. Of them, the objects that had extremely low grasp scores and those that slipped from the gripper in spite of the gripper being closed were the most common ones. This could be attributed to the inaccurate depth information coming from the camera and the gripper misalignment due to collision between the gripper and nearby objects.</p><p>Another case where the model was unable to produce a good grasp was for the transparent bottle as seen in <ref type="figure" target="#fig_2">fig. 4(e)</ref>. This could be due to inaccurate depth data captured by the  camera because of possible object reflections. However, by combining depth data along with RGB data, the model was still able to generate a fairly good grasp for the transparent objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>We presented a modular solution for grasping novel objects using our Generative Residual Convolutional Neural Network that uses n-channel input data to generate images that can be used to infer grasp rectangles for each pixel in an image. We evaluated the GR-ConvNet on two standard datasets, the Cornell grasp dataset and the Jacquard dataset and obtained state-of-the-art results on both the datasets. We also validated the proposed system on novel real objects in clutter using a robotic arm. The results demonstrate that our system can predict and perform accurate grasps for previously unseen objects. Moreover, the low inference time of our model makes the system suitable for closed-loop robotic grasping.</p><p>In future work, we would like to extend our solution for different types of grippers used such as single and multiple suction cups and multi-fingered grippers. We would also like to use depth prediction techniques to accurately predict depth for reflective objects, which can aid in improving the grasp prediction accuracy for reflective objects like the bottle.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Proposed Generative Residual Convolutional Neural Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Objects used for robotic grasping experiments. (a) Household test objects. (b) Adversarial test objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative results. Quality, angle and width are the output of GR-ConNet which are used to infer grasp rectangle. (a) Unseen objects from Cornell dataset (b) Unseen objects from Jacquard dataset (c) Single household object (d) Multiple grasps for multiple objects (e) Poor grasp for transparent object</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Example of robot grasping adversarial (top) and household (bottom) objects. See attached video for complete run. (a) Grasp pose generated by inference module. (b) Robot approaching the grasp pose. (c) Robot grasping the object. (d) Robot retracting after successful grasp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell>: A comparison of related work</cell><cell></cell></row><row><cell>[1] [2] [4] [20] [21] [22]</cell><cell>[23] Ours</cell></row><row><cell>Real Robot Experiments</cell><cell></cell></row><row><cell>Adversarial Objects</cell><cell></cell></row><row><cell>Clutter</cell><cell></cell></row><row><cell>Results on Cornell</cell><cell></cell></row><row><cell>Results on Jacquard</cell><cell></cell></row><row><cell>Code Available</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Summary of Antipodal Grasping Datasets</figDesc><table><row><cell>Dataset</cell><cell cols="3">Modality Objects Images</cell><cell>Grasps</cell></row><row><cell>Cornell</cell><cell>RGB-D</cell><cell>240</cell><cell>1035</cell><cell>8019</cell></row><row><cell>Dexnet</cell><cell>Depth</cell><cell>1500</cell><cell>6.7M</cell><cell>6.7M</cell></row><row><cell>Jacquard</cell><cell>RGB-D</cell><cell>11k</cell><cell>54k</cell><cell>1.1M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Results on the Cornell Dataset</figDesc><table><row><cell>Authors</cell><cell>Algorithm</cell><cell cols="2">Accuracy (%)</cell><cell>Speed</cell></row><row><cell></cell><cell></cell><cell>IW</cell><cell>OW</cell><cell>(ms)</cell></row><row><cell>Jiang [26]</cell><cell>Fast Search</cell><cell>60.5</cell><cell>58.3</cell><cell>5000</cell></row><row><cell>Lenz [1]</cell><cell>SAE, struct. reg.</cell><cell>73.9</cell><cell>75.6</cell><cell>1350</cell></row><row><cell>Redmon [2]</cell><cell>AlexNet, MultiGrasp</cell><cell>88.0</cell><cell>87.1</cell><cell>76</cell></row><row><cell>Wang [25]</cell><cell cols="2">Two-stage closed-loop 85.3</cell><cell>-</cell><cell>140</cell></row><row><cell>Asif [35]</cell><cell>STEM-CaRFs</cell><cell>88.2</cell><cell>87.5</cell><cell>-</cell></row><row><cell>Kumra [4]</cell><cell>ResNet-50x2</cell><cell>89.2</cell><cell>88.9</cell><cell>103</cell></row><row><cell>Morrison [20]</cell><cell>GG-CNN</cell><cell>73.0</cell><cell>69.0</cell><cell>19</cell></row><row><cell>Guo [16]</cell><cell>ZF-net</cell><cell>93.2</cell><cell>89.1</cell><cell>-</cell></row><row><cell>Zhou [22]</cell><cell>FCGN, ResNet-101</cell><cell>97.7</cell><cell>96.6</cell><cell>117</cell></row><row><cell>Karaoguz [36]</cell><cell>GRPN</cell><cell>88.7</cell><cell>-</cell><cell>200</cell></row><row><cell>Asif [23]</cell><cell>GraspNet</cell><cell>90.2</cell><cell>90.6</cell><cell>24</cell></row><row><cell></cell><cell>GR-ConvNet-D</cell><cell>93.2</cell><cell>94.3</cell><cell>19</cell></row><row><cell>Our</cell><cell>GR-ConvNet-RGB</cell><cell>96.6</cell><cell>95.5</cell><cell>19</cell></row><row><cell></cell><cell>GR-ConvNet-RGB-D</cell><cell>97.7</cell><cell>96.6</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Results on the Jacquard Dataset</figDesc><table><row><cell>Authors</cell><cell>Algorithm</cell><cell>Accuracy (%)</cell></row><row><cell>Depierre [34]</cell><cell>Jacquard</cell><cell>74.2</cell></row><row><cell cols="2">Morrison [20] GG-CNN2</cell><cell>84</cell></row><row><cell>Zhou [22]</cell><cell>FCGN, ResNet-101</cell><cell>91.8</cell></row><row><cell></cell><cell>GR-ConvNet -D</cell><cell>93.7</cell></row><row><cell>Our</cell><cell>GR-ConvNet -RGB</cell><cell>91.8</cell></row><row><cell></cell><cell>GR-ConvNet -RGB-D</cell><cell>94.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Results from robotic grasping experiments</figDesc><table><row><cell cols="3">Approach Household Objects Adversarial Objects</cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell>Accuracy (%)</cell></row><row><cell>[1]</cell><cell>89 (89/100)</cell><cell>-</cell></row><row><cell>[3]</cell><cell>73 (109/150)</cell><cell>-</cell></row><row><cell>[20]</cell><cell>92 (110/120)</cell><cell>84 (67/80)</cell></row><row><cell>[21]</cell><cell>89 (89/100)</cell><cell>-</cell></row><row><cell>Ours</cell><cell>95.4 (334/350)</cell><cell>93 (93/100)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors acknowledge Research Computing <ref type="bibr" target="#b37">[37]</ref> at the Rochester Institute of Technology for providing computational resources and support that have contributed to the research results reported in this publication.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning for detecting robotic grasps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="705" to="724" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time grasp detection using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1316" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3406" to="3413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robotic grasp detection using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cloth grasp point detection based on multiple-view geometric cues with application to robotic towel folding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maitin-Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cusumano-Towner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2308" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust visual servoing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The international journal of robotics research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="923" to="939" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One-shot learning and generation of dexterous grasps for novel objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kopicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Detry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adjigble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Wyatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="959" to="976" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data-driven grasp synthesisa survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="309" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robotic grasping and contact: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bicchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065)</title>
		<meeting>2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia (Cat. No. 00CH37065)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robot grasp synthesis algorithms: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Shimoga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="266" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robotic grasping of novel objects using vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Driemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On-policy dataset synthesis for learning robot grasping policies using fully convolutional deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1357" to="1364" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Grasping of unknown objects using deep convolutional neural networks based on depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vahrenkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wächter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6831" to="6838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and crossdomain image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Donlon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shape completion enabled robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dechant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2442" to="2447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A hybrid deep architecture for robotic grasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1609" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ojea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09312</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="421" to="436" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic and geometric reasoning for robotic grasping: a probabilistic logic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antanas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>De Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1393" to="1418" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning robust, real-time, reactive robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leitner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="page">0278364919859066</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-world multiobject, multigrasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Vela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3355" to="3362" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional grasp detection network with oriented anchor box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7223" to="7230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graspnet: An efficient convolutional neural network for real-time grasp detection for low-powered devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="4875" to="4882" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep learning a grasp function for grasping under gripper pose uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4461" to="4468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robot grasp detection using multimodal deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Mechanical Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1687814016668077</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient grasping from rgbd images: Learning using a new rectangle representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moseson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Data-efficient learning for sim-to-real robotic grasping using deep point cloud prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08989</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A robotic grasping method using convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ogas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larregay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Argentine Conference on Electronics (CAE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ensemblenet: Improving grasp detection using an ensemble of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Depth image inpainting: Improving low rank matrix completion with low gradient regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4311" to="4320" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimal hand-eye calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hirzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE/RSJ international conference on intelligent robots and systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4647" to="4653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic proximal gradient descent with acceleration techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nitanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1574" to="1582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Jacquard: A large scale dataset for robotic grasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Depierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dellandréa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition and grasp detection using hierarchical cascaded forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object detection approach for robot grasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karaoguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4953" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Research computing services</title>
		<ptr target="https://www.rit.edu/researchcomputing/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Rochester Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
