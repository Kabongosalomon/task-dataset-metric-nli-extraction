<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Construct Dynamic Graphs for Hand Gesture Recognition via Spatial-Temporal Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University New Jersey</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University New Jersey</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
							<email>xipeng@udel.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware Delaware</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Yuan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester New York</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University New Jersey</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Construct Dynamic Graphs for Hand Gesture Recognition via Spatial-Temporal Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>YUXIAO CHEN: DYNAMIC GRAPHS FOR HAND GESTURE RECOGNITION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a Dynamic Graph-Based Spatial-Temporal Attention (DG-STA) method for hand gesture recognition. The key idea is to first construct a fully-connected graph from a hand skeleton, where the node features and edges are then automatically learned via a self-attention mechanism that performs in both spatial and temporal domains. We further propose to leverage the spatial-temporal cues of joint positions to guarantee robust recognition in challenging conditions. In addition, a novel spatial-temporal mask is applied to significantly cut down the computational cost by 99%. We carry out extensive experiments on benchmarks  and prove the superior performance of our method compared with the state-of-the-art methods. The source code can be found at https://github.com/yuxiaochen1103/DG-STA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hand gesture recognition has been an active research area due to its wide range of applications such as human computer interaction, gaming and nonverbal communication analysis including sign language recognition <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b37">37]</ref>. Previous work can be classified into two categories based on the modality of their inputs: image-based <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b38">38]</ref> and skeletonbased <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b21">21]</ref> methods. Image-based methods take RGB or RGB-D images as inputs and rely on image-level features for recognition. On the other hand, skeleton-based methods make predictions by a sequence of hand joints with 2D or 3D coordinates. They are more robust to varying lighting conditions and occlusions given the accurate joint coordinates. Thanks to the low-cost depth cameras (e.g., Microsoft Kinect or Intel RealSense) and great progress made on hand pose estimation <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24]</ref>, accurate coordinates of hand joints are easy to be obtained. Therefore, we follow the skeleton-based method in this work. c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.  Conventional methods <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b25">25]</ref> of skeleton-based hand gesture recognition aim to design powerful feature descriptors to model the action of hands. However, these hand-crafted features have limited generalization capability. Recent studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b21">21]</ref> have achieved significant improvement using deep learning. They usually concatenate the joint coordinates into a tensor which is fed into a neural network, and then hand features are directly learned by the network during training. Nevertheless, the spatial structures and temporal dynamics of hand skeletons are not explicitly exploited in these deep learning based approaches.</p><p>More recent studies <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42]</ref> attempt to incorporate structures and dynamics of skeletons based on skeleton graphs. Specifically, given a sequence of skeletons, they define a spatial-temporal graph where structures and dynamics of skeletons are embedded. The feature representation of the graph is then extracted for action recognition. However, a predefined graph with fixed structure lacks the flexibility to capture the variance and dynamics across different actions, yielding sub-optimal performance in practice.</p><p>To this end, we propose a Dynamic Graph-Based Spatial Temporal Attention (DG-STA) model for hand gesture recognition. The key idea is to perform a self-attention mechanism in both spatial and temporal domains to modify a unified graph dynamically in order to model different actions. <ref type="figure" target="#fig_1">Figure 1</ref> gives an overview of our approach. There are three crucial designs that distinguish our approach from previous methods. First, instead of a pre-defined graph with fixed structure, we propose to construct a unified graph where the edges and nodes are dynamically optimized according to different actions. This makes it is possible to achieve action-specific graphs with improved expressive power. Second, we propose spatial-temporal position embedding which improves the temporal position embedding <ref type="bibr" target="#b34">[34]</ref>. It encodes the identity and temporal order information of each node in the graph. Combining node features with their position embeddings can further improve the performance of our approach. Third, to implement our DG-STA more efficiently, we present a novel spatial-temporal mask operation which is directly applied to the matrix of scaled dot-products among all nodes. It significantly improves the computational efficiency of our model and allows easier input data arrangement.</p><p>To evaluate the effectiveness of our approach, we conduct comprehensive experiments on two standard benchmarks: DHG-14/28 Dataset <ref type="bibr" target="#b8">[8]</ref> and SHREC'17 Track Dataset <ref type="bibr" target="#b9">[9]</ref>. The results demonstrate that our method outperforms the state-of-the-art methods. In summary, our main contributions are summarised as follows:</p><p>• We propose Dynamic Graph-Based Spatial-Temporal Attention (DG-STA) for skeletonbased hand gesture recognition. The structures and dynamics of hand skeletons can be learned automatically and more efficiently by our approach.</p><p>• We propose spatial-temporal position embedding which encodes the identity and temporal order information of nodes to boost the performance of our model, and a spatialtemporal mask operation for efficient implementation of DG-STA.</p><p>• We conduct comprehensive experiments to validate our approach on two standard benchmarks. The proposed DG-STA achieves the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review recent work on self-attention and recent developments in skeletonbased action recognition, which motivated our approach. Self-Attention. The self-attention mechanism is widely used in computer vision and natural language processing tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40]</ref>. Vaswani et al. <ref type="bibr" target="#b34">[34]</ref> proposed to apply the self-attention module to model temporal and semantic relationships among words within a sentence for machine translation. Instead, we study applying the self-attention mechanism to learn spatial-temporal information contained in hand skeletons represented by graphs, which are largely different from sequences. Graph Attention Networks (GATs) <ref type="bibr" target="#b35">[35]</ref> employed self-attention to learn node embeddings of graphs. By contrast, our approach is able to capture additional temporal information as well as node identities.</p><p>Skeleton-Based Hand Gesture Recognition. Skeleton-based hand gesture is a wellstudied but still challenging task. Traditional methods <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b25">25]</ref> mainly focus on designing powerful hand feature descriptors. Smedt et al. <ref type="bibr" target="#b8">[8]</ref> proposed the Shape of Connected Joints descriptor to represent the hand shape of hand skeleton. Recent studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b21">21]</ref> apply deep neural networks for this task and achieve significant performance improvement. Convolutional Neural Networks and Long Short-Term Memory are leveraged to learn the spatial and temporal features from the sequence of hand joints for hand gesture classification in <ref type="bibr" target="#b21">[21]</ref>. One limitation of these learning-based methods is that they do not explicitly explore the structures and dynamics of human hands.</p><p>Skeleton-Based Human Action Recognition. Recent studies in skeleton-base human action recognition <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b41">41]</ref> started to incorporate structures and dynamics of human bodies by building skeleton graphs <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42]</ref>. This idea is first introduced by <ref type="bibr" target="#b11">[11]</ref> which employs Graph CNNs. Recently, Yan et al. <ref type="bibr" target="#b39">[39]</ref> built a skeleton graph based on the natural structure of human body, and extract its representation by Graph Convolution Networks <ref type="bibr" target="#b15">[15]</ref> for action recognition. Nevertheless, it is difficult to define an optimal skeleton graph which represents all action-specific structures and dynamics information. Instead, our method can automatically learn multiple action-specific graphs with the multi-head attention mechanism <ref type="bibr" target="#b34">[34]</ref>, which efficiently encode structures and dynamics of hand gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The overview of our approach is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. First, a fully-connected skeleton graph is constructed from the input sequence of hand skeletons as described in Section 3.1. In Section 3.2, we devise DG-STA to learn the edge weights and node embeddings within the graph. The learned node features by DG-STA are then average-pooled into a vector which captures the structures and dynamics of the input skeleton graph. We use it for hand gesture classification. Section 3.3 presents the spatial-temporal position embedding which is combined with node features to incorporate node identity and temporal order information contained in the hand skeletons. Moreover, a spatial-temporal mask operation is introduced in Section 3.4 which implements our proposed DG-STA more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Skeleton Graph Initialization</head><p>Given a video of T frames, N hand joints are extracted from each frame to represent the hand skeleton. Then a fully-connected skeleton graph G = (V, E) is constructed from this sequence of hand skeletons. Let V = {v (t,i) |t = 1, . . . , T, i = 1, . . . , N} denote the node set where v (t,i) represents the i-th hand joint at the time step t. The node features are represented by F = {f (t,i) |t = 1, . . . , T, i = 1, . . . , N}, where f (t,i) indicates the feature vector of the node v (t,i) . They are extracted from the 3D coordinates of nodes. Note that each node is connected with all other nodes including itself. For clarity, we define three types of edges on the edge set E as follows.</p><formula xml:id="formula_0">• A spatial edge v (t,i) → v (t, j) (i = j) connects two different nodes at the same time step. • A temporal edge v (t,i) → v (k, j) (t = k) connects two nodes at different time steps. • A self-connected edge v (t,i) → v (t,i) connects the node with itself.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Graph Construction via Spatial-Temporal Attention</head><p>The proposed DG-STA consists of the spatial attention model A S and temporal attention model A T which are employed to extract spatial and temporal information from the hand skeleton graph respectively. Both A S and A T are based on multi-head attention <ref type="bibr" target="#b34">[34]</ref>. A S first takes the initial node features F as the input and updates them to encode spatial information. The updated node features are then fed to A T to further learn temporal information. Finally, the results are average-pooled to a vector which is used as the feature representation of the skeleton graph for classification.</p><p>Specifically, given the input feature f (t,i) of the node v (t,i) in the skeleton graph, the h-th spatial attention head first applies three fully-connected layers to map f (t,i) into the key, query and value vectors respectively, which are formulated as: The spatial attention head computes the weights of the spatial and self-connected edges in two steps. First, it calculates the "scaled dot-product" <ref type="bibr" target="#b34">[34]</ref> between the query vectors and key vectors of the nodes within the same time step. Then it normalizes the results by a Softmax function. These two steps are formulated as:</p><formula xml:id="formula_1">K h (t,i) = W h K f (t,i) , Q h (t,i) = W h Q f (t,i) , V h (t,i) = W h V f (t,i) ,<label>(1)</label></formula><formula xml:id="formula_2">where K h (t,i) , Q h (t,i) and V h (t,</formula><formula xml:id="formula_3">u h (t,i)→(t, j) = Q h (t,i) , K h (t, j) √ d , α h (t,i)→(t, j) = exp u h (t,i)→(t, j) ∑ N n=1 exp u h (t,i)→(t,n) ,<label>(2)</label></formula><p>where d is the dimension of the key, query and value vectors; u h (t,i)→(t, j) is the scaled dotproduct of the node v (t,i) and v (t, j) ; ·, · represents the inner product operation; α h (t,i)→(t, j)</p><p>is the attention weight between the node v (t,i) and v (t, j) , which measures the importance of information from node v (t, j) to node v (t,i) . Meanwhile, the weights for all temporal edges are set to 0 in order to block the information passing in the temporal domain. As a result, each spatial attention head produces a weighted skeleton graph which represents a specific type of spatial structure of the hand.</p><p>The attention head calculates the spatial attention feature of the node v (t,i) as the weighted sum of the value vectors within the same time step, which is defined as:</p><formula xml:id="formula_4">f h (t,i) = N ∑ j=1 α h (t,i)→(t, j) · V h (t, j) ,<label>(3)</label></formula><p>wheref h (t,i) denotes the spatial attention feature of the node v (t,i) . Intuitively, the computation mechanism of the spatial attention features is essentially the process that each node in the graph sends some information to the others within the same time step and then aggregates the received information based on the learned edge weights.</p><p>The spatial attention model A S finally concatenates the spatial attention features learned by all spatial attention heads intof (t,i) which is employed as the spatial feature of the node</p><formula xml:id="formula_5">v (t,i) :f (t,i) = Concate f 1 (t,i) ,f 2 (t,i) , ...,f H (t,i) ,<label>(4)</label></formula><p>where H is the number of spatial attention heads. The obtained node features encode multiple types of structural information represented by the weighted skeleton graphs which are learned by different spatial attention heads. The temporal attention model A T takes the output node features from the spatial attention model as the input, and then applies the above multi-head attention mechanism in the temporal domain. The node feature which is the output from the temporal attention model encodes both spatial and temporal information carried by the input sequence of the hand skeletons. We average-pool these node features to a vector as the feature representation of the input sequence for hand gesture recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spatial-Temporal Position Embedding</head><p>The original node features F extracted from the coordinates of the input hand skeletons do not contain spatial identity information describing which hand joint a node corresponds to, and temporal information indicating which time step a node is at. To incorporate these messages, we propose the spatial-temporal position embedding.</p><p>Specifically, our spatial-temporal position embedding is made up of the spatial position embedding and the temporal position embedding. The spatial one consists of N vectors and each represents a hand joint. Meanwhile, the temporal one is composed of N × T distinct vectors and each of them corresponds to a node in the hand skeleton graph. The feature vector of a specific node is added with the corresponding spatial and temporal position embedding vectors before fed into the DG-STA. Therefore, we have:</p><formula xml:id="formula_6">f (t,i) = A T p T (t,i) + A S f (t,i) + p S (i) ,<label>(5)</label></formula><p>wheref (t,i) denotes the final output feature of node v (t,i) , p S (i) is the spatial position embedding of the i-th hand joint, and p T (t,i) denotes the temporal position embedding of the i-th hand joint at t-th time step. These embeddings are with the same dimension as f (t,i) , and their values are set using the sine and cosine functions of different frequencies following <ref type="bibr" target="#b34">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Efficient Implementation</head><p>It is not straightforward to implement the proposed DG-STA because the input data have to be arranged in a complex format. However, we find that the computation of attention weights and features without domain constraints is straightforward, which can be implemented efficiently using matrix multiplication operations. Therefore, we propose a novel scheme to facilitate the implementation of the DG-STA. The main idea is to first compute the matrix of the scaled dot-products among all nodes and then apply the proposed spatial-temporal mask operation to the matrix in order to let the model focus on the spatial or temporal domain. An illustration of our mask operation is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. For a specific attention head, we compute a query matrix Q where each row represents the query vector of each node, and a key matrix K where each row corresponds to the key vector of each node. The matrix of the scaled dot-products W (i.e., the edge weights before normalization) can be obtained by:</p><formula xml:id="formula_7">W = Q ⊗ K ,<label>(6)</label></formula><p>where ⊗ is the matrix multiplication, and denotes the matrix transpose operation. Then the proposed spatial mask operation sets the value of each element in W which represents the temporal edge to η (i.e., a number close to negative infinity) and keeps the values of other elements unchanged. Therefore, the resulting matrix after the spatial mask operationW S is calculated:W</p><formula xml:id="formula_8">S = φ (W M S + (1 − M S ) × η) ,<label>(7)</label></formula><p>where denotes the element-wise dot operation, M S is the spatial mask where the elements are 1 if they represent the spatial or self-connect edges and 0 otherwise. We set η to −9 × 10 15 in our implementation. The Softmax activation φ essentially normalizes weights across spatial edges, because the exponential value of the η is close to 0. As a result, all weights of the temporal edges inW S are set to 0. Equations (6) and <ref type="bibr" target="#b6">(7)</ref> efficiently implement the calculation of edge weights in the spatial domain formulated in Equation <ref type="bibr" target="#b1">(2)</ref>. Moreover, the matrixW S can be directly employed to implement the computation of node features represented by Equation <ref type="formula" target="#formula_4">(3)</ref> by performing the matrix multiplication operation with the matrix of value vectors.</p><p>We define the temporal mask operation following the same way as Equation <ref type="formula" target="#formula_8">(7)</ref>. The difference is that we use the temporal mask M T instead of M S to compute the matrix after the temporal mask operationW T . The elements of M T are 1 if they represent the temporal or self-connect edges and 0 otherwise. With the help of the proposed spatial-temporal mask operation, we experimentally find that the computation time is reduced by 99%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first describe our network structure in Section 4.1. In Section 4.2, we introduce the datasets and settings employed in the experiments. Then we conduct ablation studies in Section 4.3 to evaluate the effectiveness of each component proposed in our method. Finally, we report our results and comparisons with the state of the art in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Our network structure is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We set the head number of the spatial and temporal attention models to 8. The dimension d of the query, key and value vector is set to 32. Layer Normalization <ref type="bibr" target="#b16">[16]</ref> is utilized to normalize the intermediate outputs of our network. The input 3D coordinate of a hand joint is projected into an initial node feature of 128 dimension. It is then added with the corresponding spatial position embedding and fed into the spatial attention model, which produces a node feature of 256 dimension. This node feature is projected into a vector of 128 dimension which is added with the corresponding temporal position embedding. The temporal attention model takes it as the input and generates the final node feature. Finally, we average-pool the features of all nodes into a vector and feed it into a fully-connected layer for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dropout, 0.2 FC Layer, 128</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReLU</head><p>LayerNorm 1D <ref type="figure" target="#fig_3">(N, T, 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets and Settings</head><p>We evaluate our method on the DHG-14/28 Dataset <ref type="bibr" target="#b8">[8]</ref> and the SHREC'17 Track Dataset <ref type="bibr" target="#b9">[9]</ref>. Both datasets contain 2800 video sequences of 14 hand gestures which are performed in two configurations: using one single finger or the whole hand. The videos of the two datasets are captured by the Intel Realsense camera. The 3D coordinates of 22 hand joints in real-world space are provided per frame for network training and evaluation. Network Training. The proposed DG-STA is implemented based on the PyTorch platform. The Adam <ref type="bibr" target="#b14">[14]</ref> optimizer with a learning rate of 0.001 is employed to train our model. The batch size is set to 32 and the dropout rate <ref type="bibr" target="#b30">[30]</ref> is set to 0.2. We uniformly sample 8 frames from each video as the input. For fair comparison, we perform data augmentation by applying the same operations as proposed in <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b21">21]</ref> including scaling, shifting, time interpolation and adding noise. We also subtract every skeleton sequence by the palm position of the first frame following Smedt et al. <ref type="bibr" target="#b9">[9]</ref> for alignment.</p><p>Evaluation Protocols. On the DHG-14/28 Dataset, models are evaluated by using the leave-one-subject-out cross-validation strategy <ref type="bibr" target="#b8">[8]</ref>. Specifically, we perform one experiment for each subject in this dataset. In each experiment, one subject is selected for testing and the remaining 19 subjects are used for training. The average accuracy of 14 gestures (without the single-finger configuration) or 28 gestures (with the single-finger configuration) over the 20 cross-validation folds are reported. For the SHREC'17 Track Dataset, we use the same data split as provided by <ref type="bibr" target="#b9">[9]</ref> and report the accuracy of both 14 and 28 gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Our proposed approach consists of three major components, including the fully-connected skeleton graph structure (FSG), the spatial-temporal attention model (STA) and the spatialtemporal position embedding (STE). We validate the effectiveness of these components in this section. The results are shown in <ref type="table">Table 1</ref>. Evaluation of Fully-Connected Graph Structure. We compare the proposed FSG with the sparse skeleton graph structure (SSG) introduced by Yan et al. <ref type="bibr" target="#b39">[39]</ref>, where spatial edges are defined based on the natural connections of the hand joints and temporal edges connect the same joints between consecutive frames. We can see that our model significantly outperforms the one trained on SSG. This is because SSG may be sub-optimal for some hand gestures, while FSG has little constrains on the model so that it is able to learn action-specific graph structures.</p><p>Evaluation of Spatial-Temporal Attention. The proposed STA downgrades to Graph Attention (GAT) <ref type="bibr" target="#b35">[35]</ref> if only one attention model is applied to the whole graph without distinguishing the spatial and temporal domains. We implement GAT by replacing the spatial and temporal attention models in our network with one attention model, and train it under the same setting of our model. We can observe that the STA-based model achieves better performance than the GAT-based model, which demonstrates the effectiveness of STA.</p><p>Evaluation of Spatial-Temporal Position Embedding. We validate the effectiveness of the proposed STE by training a variant of our method where STE is removed. We can see that our model outperforms the model without STE, which demonstrates the importance of the identity and temporal order information encoded by STE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Previous Methods</head><p>We compare our method with the state-of-the-art methods on the DHG-14/28 Dataset <ref type="bibr" target="#b8">[8]</ref> and the SHREC'17 Track Dataset <ref type="bibr" target="#b9">[9]</ref>, respectively. The compared state-of-the-art methods include traditional hand-crafted feature approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>, deep learning based approaches <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b21">21]</ref> and a graph-based method <ref type="bibr" target="#b39">[39]</ref>. The results are shown in Tables 2 and 3. Note that for ST-GCN <ref type="bibr" target="#b39">[39]</ref>, we implement it following the distance partitioning setting and use a three-layer ST-GCN with 128 channels for fair comparison. We collect the results of other baseline methods from <ref type="bibr" target="#b13">[13]</ref>.  Results on DHG-14/28 Dataset. From <ref type="table" target="#tab_4">Table 2</ref>, we can see that our method achieves the state-of-the-arts performance under both 14-gesture and 28-gesture setting. Moreover, both our method and ST-GCN <ref type="bibr" target="#b39">[39]</ref> outperform other methods which do not explicitly exploit structures and dynamics of hands, which demonstrates that these messages are important for skeleton-based hand gesture recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>14 Gestures 28 Gestures Oreifej et al. <ref type="bibr" target="#b26">[26]</ref> 78.5 74.0 Devanne et al. <ref type="bibr" target="#b10">[10]</ref> 79.4 62.0 Classify Sequence by Key Frames <ref type="bibr" target="#b9">[9]</ref> 82.9 71.9 Ohn-Bar et al. <ref type="bibr" target="#b25">[25]</ref> 83.9 76.5 SoCJ+Direction+Rotation <ref type="bibr" target="#b6">[7]</ref> 86.9 84.2 SoCJ+HoHD+HoWR <ref type="bibr" target="#b8">[8]</ref> 88.2 81.9 Caputo et al. <ref type="bibr" target="#b1">[2]</ref> 89.5 -Boulahia et al. <ref type="bibr" target="#b0">[1]</ref> 90.5 80.5 Res-TCN <ref type="bibr" target="#b13">[13]</ref> 91.1 87.3 STA-Res-TCN <ref type="bibr" target="#b13">[13]</ref> 93.6 90.7 ST-GCN <ref type="bibr" target="#b39">[39]</ref> 92.7 87.7 DG-STA (Ours) 94.4 90.7 Track Dataset provides raw captured video sequences with noisy frames, and hence is more challenging. We can see that our method achieves the state-of-the-arts performance under the 14-gesture setting, and obtains comparable performance with STA-Res-TCN <ref type="bibr" target="#b13">[13]</ref> under the 28-gesture setting. In addition, we can observe that our method and ST-GCN <ref type="bibr" target="#b39">[39]</ref> outperform all other methods which do not explicitly exploit structures and dynamics of hands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed a graph-based spatial-temporal attention method for skeletonbased hand-gesture recognition. It utilizes two attention models in the spatial and temporal domains of the fully-connected hand skeleton graph to learn edge weights and extract spatial and temporal information for hand gesture recognition. Extensive experiments demonstrate the effectiveness of our framework. Our proposed method provides a general framework that can be further used for other tasks aiming to learn spatial and temporal information from graph-based data, e.g., skeleton-based human action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of our method. The nodes in the graph correspond to hand joints and the dashed lines represent disconnected edges. The proposed DG-STA calculates edge weights and learn node features in both spatial and temporal domains of the hand skeleton graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed spatial and temporal mask operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The network architecture of the proposed DG-STA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>i) represent the key, query and value vectors of the node; W h K , W h Q and W h V are the corresponding weight matrices of the three fully-connected layers of the h-th spatial attention head.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of accuracy (%) on DHG-14/28 Dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of accuracy (%) on SHREC'17 Track Dataset. Results on SHREC'17 Track Dataset. Different from the DHG-14/28 Dataset where videos are cropped by human-labeled beginnings and ends of the gestures [8], the SHREC'17</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>This work was funded partly by ARO-MURI-68985NSMUR and NSF 1763523, 1747778, 1733843, 1703883 to Dimitris N. Metaxas.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic hand gesture recognition based on 3D pattern assembled trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Said Yacine Boulahia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Anquetil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Multon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulpa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing Theory, Tools and Applications (IPTA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing 3D trajectories for simple mid-air gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Prebianca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><forename type="middle">D</forename><surname>Carcangiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Spano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giachetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="17" to="25" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">When e-commerce meets social media: Identifying business on wechat moment using bilateral-attention lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Wide Web Conference (WWW)</title>
		<meeting>the World Wide Web Conference (WWW)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="343" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">factual&quot;or&quot;emotional&quot;: Stylized image captioning with adaptive learning and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="519" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Motion feature augmented recurrent neural network for skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Twitter sentiment analysis via bi-sense emoji embedding and attention-based lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference on Multimedia Conference (MM)</title>
		<meeting>the ACM Multimedia Conference on Multimedia Conference (MM)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="117" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic hand gesture recognition-From traditional handcrafted to recent deep learning approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sciences et</title>
		<imprint/>
		<respStmt>
			<orgName>Université de Lille 1</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cristal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Vandeborre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SHREC&apos;17 Track: 3D hand gesture recognition using a depth and skeletal dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Vandeborre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mohamed Daoudi, and Alberto Del Bimbo. 3-D human action recognition by shape analysis of motion trajectories on riemannian manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Devanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Pala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1340" to="1352" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph-based CNN for human action recognition from 3D pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference Workshop: Deep Learning on Irregular Domains</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Orientation histograms for hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial-temporal attention Res-TCN for skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxuan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="273" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using multiple cues for hand tracking and model refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Oliensis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hand gesture recognition with 3D convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sign language recognition using image based hand gesture recognition techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ashish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nikam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ambekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Green Engineering and Technologies (IC-GET)</title>
		<meeting>the International Conference on Green Engineering and Technologies (IC-GET)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks and long short-term memory for skeleton-based human activity and hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Juan C Núñez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cabido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">S</forename><surname>Pantrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José F</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vélez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="80" to="94" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepprior++: Improving fast and accurate 3D hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hands deep in deep learning for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06807</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training a feedback loop for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3316" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint angles similarities and HOG2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshed</forename><surname>Ohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">HON4D: Histogram of oriented 4D normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vision based hand gesture recognition for human computer interaction: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Rautaray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quantized densely connected U-Nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="339" to="354" />
		</imprint>
	</monogr>
	<note>Shaoting Zhang, and Dimitris Metaxas</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning complete representations for multi-view generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cr-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="942" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10569</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vision-based handgesture applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Wachs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Kölsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helman</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Edan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="71" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Superpixel-based hand gesture recognition with kinect depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shing-Chow</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selfattention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to forecast and refine residual motion for image-to-video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="387" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3D human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
