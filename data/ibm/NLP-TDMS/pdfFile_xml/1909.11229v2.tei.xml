<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pretraining boosts out-of-domain robustness for pose estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mathis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EPFL</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Biasi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EPFL</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Yüksekgönül</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Rogers</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Performance Genetics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mackenzie</forename><forename type="middle">W</forename><surname>Mathis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EPFL</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pretraining boosts out-of-domain robustness for pose estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks are highly effective tools for pose estimation. However, as in other computer vision tasks, robustness to out-of-domain data remains a challenge, especially for small training sets that are common for real-world applications. Here, we probe the generalization ability with three architecture classes (MobileNetV2s, ResNets, and Ef-ficientNets) for pose estimation. We developed a dataset of 30 horses that allowed for both "within-domain" and "outof-domain" (unseen horse) benchmarking-this is a crucial test for robustness that current human pose estimation benchmarks do not directly address. We show that better ImageNet-performing architectures perform better on both within-and out-of-domain data if they are first pretrained on ImageNet. We additionally show that better ImageNet models generalize better across animal species. Furthermore, we introduce Horse-C, a new benchmark for common corruptions for pose estimation, and confirm that pretraining increases performance in this domain shift context as well. Overall, our results demonstrate that transfer learning is beneficial for out-of-domain robustness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pose estimation is an important tool for measuring behavior, and thus widely used in technology, medicine and biology <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">40,</ref><ref type="bibr">30,</ref><ref type="bibr">35]</ref>. Due to innovations in both deep learning algorithms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">24,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b7">8]</ref> and large-scale datasets <ref type="bibr">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> pose estimation on humans has become very powerful. However, typical human pose estimation benchmarks, such as MPII pose and COCO <ref type="bibr">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>, contain many different individuals (&gt;10k) in different contexts, but only very few example postures per individual. In real world applications of pose estimation, users often want to create customized networks that estimate the location of user-defined bodyparts by only labeling a few hundred frames on a small subset of individuals, yet want this to gen- <ref type="bibr">*</ref>   eralize to new individuals <ref type="bibr">[40,</ref><ref type="bibr">30,</ref><ref type="bibr">35,</ref><ref type="bibr">43]</ref>. Thus, one naturally asks the following question: Assume you have trained an algorithm that performs with high accuracy on a given (individual) animal for the whole repertoire of movementhow well will it generalize to different individuals that have slightly or dramatically different appearances? Unlike in common human pose estimation benchmarks, here the setting is that datasets have many (annotated) poses per individual (&gt;200) but only a few individuals (≈10).</p><p>To allow the field to tackle this challenge, we developed a novel benchmark comprising 30 diverse Thoroughbred horses, for which 22 body parts were labeled by an expert in 8114 frames (Dataset available at http://horse10. deeplabcut.org). Horses have various coat colors and the "in-the-wild" aspect of the collected data at various Thoroughbred farms added additional complexity. With this dataset we could directly test the effect of pretraining on out-of-domain data. Here we report two key insights: <ref type="bibr">Figure 2</ref>. Horse Dataset: Example frames for each Thoroughbred horse in the dataset. The videos vary in horse color, background, lighting conditions, and relative horse size. The sunlight variation between each video added to the complexity of the learning challenge, as well as the handlers often wearing horse-leg-colored clothing. Some horses were in direct sunlight while others had the light behind them, and others were walking into and out of shadows, which was particularly problematic with a dataset dominated by dark colored coats. To illustrate the Horse-10 task we arranged the horses according to one split: the ten leftmost horses were used for train/test within-domain, and the rest are the out-of-domain held out horses.</p><p>(1) ImageNet performance predicts generalization for both within domain and on out-of-domain data for pose estimation; (2) While we confirm that task-training can catch up with fine-tuning pretrained models given sufficiently large training sets <ref type="bibr" target="#b9">[10]</ref>, we show this is not the case for out-ofdomain data <ref type="figure" target="#fig_0">(Figure 1</ref>). Thus, transfer learning improves robustness and generalization. Furthermore, we contrast the domain shift inherent in this dataset with domain shift induced by common image corruptions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">36]</ref>, and we find pretraining on ImageNet also improves robustness against those corruptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pose and keypoint estimation</head><p>Typical human pose estimation benchmarks, such as MPII pose and COCO <ref type="bibr">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> contain many different individuals (&gt; 10k) in different contexts, but only very few example postures per individual. Along similar lines, but for animals, Cao et al. created a dataset comprising a few thousand images for five domestic animals with one pose per individual <ref type="bibr" target="#b5">[6]</ref>. There are also papers for facial keypoints in horses <ref type="bibr">[42]</ref> and sheep <ref type="bibr">[48,</ref><ref type="bibr" target="#b15">16]</ref> and recently a large scale dataset featuring 21.9k faces from 334 diverse species was introduced <ref type="bibr" target="#b19">[20]</ref>. Our work adds a dataset comprising multiple different postures per individual (&gt;200) and comprising 30 diverse race horses, for which 22 body parts were labeled by an expert in 8114 frames. This pose estimation dataset allowed us to address within and out of domain generalization. Our dataset could be important for further testing and developing recent work for domain adapation in animal pose estimation on a real-world dataset [28, 43, 37].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transfer learning</head><p>Transfer learning has become accepted wisdom: finetuning pretrained weights of large scale models yields best results <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">49,</ref><ref type="bibr">25,</ref><ref type="bibr">33,</ref><ref type="bibr">27,</ref><ref type="bibr">50]</ref>. He et al. nudged the field to rethink this accepted wisdom by demonstrating that for various tasks, directly training on the task-data can match performance <ref type="bibr" target="#b9">[10]</ref>. We confirm this result, but show that on held-out individuals ("out-of-domain") this is not the case. Raghu et al. showed that for target medical tasks (with little similarity to ImageNet) transfer learning offers little benefit over lightweight architectures <ref type="bibr">[41]</ref>. Kornblith et al. showed for many object recognition tasks, that better Im-ageNet performance leads to better performance on these other benchmarks <ref type="bibr" target="#b22">[23]</ref>. We show that this is also true for pose-estimation both for within-domain and out-of-domain data (on different horses, and for different species) as well as for corruption resilience.</p><p>What is the limit of transfer learning? Would ever larger . This highlights the importance of the task, rather than the sheer size as a crucial factor. Further corroborating this insight, Li et al. showed that pretraining on large-scale object detection task can improve performance for tasks that require fine, spatial information like segmentation <ref type="bibr">[27]</ref>. Thus, one interesting future direction to boost robustness could be to utilize networks pretrained on Open-Images, which contains bounding boxes for 15 million instances and close to 2 million images [26].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Robustness</head><p>Studying robustness to common image corruptions based on benchmarks such as ImageNet-C <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">36,</ref><ref type="bibr">45</ref>] is a fruitful avenue for making deep learning more robust. Apart from evaluating our pose estimation algorithms on novel horses (domain-shift), we also investigate the robustness with respect to image corruptions. Hendrycks et al. study robustness to out-of distribution data on CIFAR 10, CIFAR 100 and TinyImageNet (but not pose estimation). The authors report that pretraining is important for adversarial robustness <ref type="bibr" target="#b14">[15]</ref>. Shah et al. found that pose estimation algorithms are highly robust against adversarial attacks [46], but neither directly test out-of-domain robustness on different individuals, nor robustness to common image corruptions as we do in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and evaluation metrics</head><p>We developed a novel horse data set comprising 8114 frames across 30 different horses captured for 4-10 seconds with a GoPro camera (Resolution: 1920 × 1080, Frame Rate: 60 FPS), which we call Horse-30 ( <ref type="figure">Figure 2</ref>). We downsampled the frames by a factor of 15% to speed-up the benchmarking process (288 × 162 pixels; one video was downsampled to 30%). We annotated 22 previously established anatomical landmarks for equines <ref type="bibr">[31,</ref><ref type="bibr" target="#b1">2]</ref>. The following 22 body parts were labeled in 8114 frames: Nose, Eye, Nearknee, Nearfrontfetlock, Nearfrontfoot, Offknee, Offfrontfetlock, Offfrontfoot, Shoulder, Midshoulder, Elbow, Girth, Wither, Nearhindhock, Nearhindfetlock, Nearhindfoot, Hip, Stifle, Offhindhock, Offhindfetlock, Offhindfoot, Ischium. We used the DeepLabCut 2.0 toolbox [38] for labeling. We created 3 splits that contain 10 randomly selected training horses each (referred to as Horse-10). For each training set we took a subset of 5 % (≈160 frames), and 50 % (≈ 1470 frames) of the frames for training, and then evaluated the performance on the training, test, and unseen (defined as "out-of-domain") horses (i.e. the other horses that were not in the given split of Horse-10). As the horses could vary dramatically in size across frames, due to the "in-the-wild" variation in distance from the camera, we normalized the raw pixel errors by the eye-to-nose distance and report the fraction of this distance (normalized error) as well as percent correct keypoint metric <ref type="bibr" target="#b3">[4]</ref>; we used a matching threshold of 30 % of the head segment length (nose to eye per horse; see <ref type="figure">Figure 3A</ref>).</p><p>For the generalization experiments, we also tested on the Animal Pose dataset <ref type="bibr" target="#b5">[6]</ref> to test the generality of our find-ings ( <ref type="figure" target="#fig_1">Figure 4</ref>). We extracted all single animal images from this dataset, giving us 1091 cat, 1176 dog, 486 horse, 237 cow, and 214 sheep images. To note, we corrected errors in ground truth labels for the dog's (in about 10 % of frames). Because nearly all images in this dataset are twice the size of the Horse-10 data, we also downsampled the images by a factor of 2 before training and testing. Given the lack of a consistent eye-to-nose distance across the dataset due to the varying orientations, we normalized as follows: the raw pixel errors were normalized by the square root of the bounding box area for each individual. For training the various architectures, the best schedules from cross validation on Horse-10 were used (see Section 3.2).</p><p>We also applied common image corruptions <ref type="bibr" target="#b13">[14]</ref> to the Horse-10 dataset, yielding a variant of the benchmark which we refer to as Horse-C. Horse-C images are corrupted with 15 forms of digital transforms, blurring filters, point-wise noise or simulated weather conditions. All conditions are applied following the evaluation protocol and implementation by Michaelis et al. <ref type="bibr">[36]</ref>. In total, we arrived at 75 variants of the dataset (15 different corruptions at 5 different severities), yielding over 600k images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architectures and Training Parameters</head><p>We utilized the pose estimation toolbox called DeepLab-Cut <ref type="bibr">[33,</ref><ref type="bibr">38,</ref><ref type="bibr" target="#b17">18]</ref>, and added MobileNetV2 [44] and Ef-ficientNet backbones [47] to the ResNets <ref type="bibr" target="#b12">[13]</ref> that were present, as well as adding imgaug for data augmentation <ref type="bibr" target="#b18">[19]</ref>. The TensorFlow [1]-based network architectures could be easily exchanged while keeping data loading, training, and evaluation consistent. The feature detectors in DeepLabCut consist of a backbone followed by deconvolutional layers to predict pose scoremaps and location refinement maps (offsets), which can then be used for predicting the pose while also providing a confidence score. As previously, for the ResNet backbones we utilize an output stride of 16 and then upsample the filter banks with deconvolutions by a factor of two to predict the heatmaps and locationrefinement at 1/8th of the original image size scale <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">33]</ref>. For MobileNetV2 [44], we configured the output-stride as 16 (by changing the last stride 2 convolution to stride 1).</p><p>We utilized four variants of MobileNetV2 with different expansion ratios (0.35, 0.5, 0.75 and 1) as this ratio modulates the ImageNet accuracy from 60.3 % to 71.8 %, and pretrained models on ImageNet from TensorFlow <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">44]</ref>. The baseline EfficientNet model was designed by Tan et al.</p><p>[47] through a neural architecture search to optimize for accuracy and inverse FLOPS. From B0 to B6, compound scaling is used to increase the width, depth, and resolution of the network, which directly corresponds to an increase in ImageNet performance <ref type="bibr">[47]</ref>. We used the AutoAugment pretrained checkpoints from TensorFlow as well as adapted the EfficientNet's output-stride to 16 (by changing the (oth-erwise) last stride 2 convolution to stride 1).</p><p>The training loss is defined as the cross entropy loss for the scoremaps and the location refinement error via a Huber loss with weight 0.05 <ref type="bibr">[33]</ref>. The loss is minimized via ADAM with batch size 8 <ref type="bibr" target="#b20">[21]</ref>. For training, a cosine learning rate schedule, as in <ref type="bibr" target="#b22">[23]</ref> with ADAM optimizer and batchsize 8 was used; we also performed augmentation, using imgaug <ref type="bibr" target="#b18">[19]</ref>, with random cropping and rotations. Initial learning rates and decay target points were cross-validated for MobileNetV2 0.35 and 1.0, ResNet-50, EfficientNet B0, B3, and B5 for the pretrained and from scratch models (see <ref type="figure">Supplementary Material</ref>). For each model that was not cross validated (MobileNetV2 0.5 and 0.75, ResNet-101, EfficientNet B1, B2, B4, B6), the best performing training parameters from the most similar cross validated model was used (i.e. the cross validated EfficientNet-B0 schedule was used for EfficientNet-B1; see Supplementary Material). For MobileNetV2s, we trained the batch normalization layers too (this had little effect on task performance for MobileNetV2-0.35). Pretrained models were trained for 30k iterations (as they converged), while models from scratch were trained for 180k iterations. From scratch variants of the architectures used He-initialization <ref type="bibr" target="#b11">[12]</ref>, while all pretrained networks were initialized from their ImageNet trained weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross Validation of Learning Schedules</head><p>To fairly compare the pose estimation networks with different backbones, we cross-validated the learning schedules. For models with pretraining and from scratch, we cross validated the cosine learning rate schedules by performing a grid search of potential initial learning rates and decay targets to optimize their performance on out of domain data. Given that our main result is that while tasktraining can catch up with fine-tuning pretrained models given sufficiently large training sets on within-domain-data (consistent with <ref type="bibr" target="#b9">[10]</ref>), we will show that this is not the case for out-of-domain data. Thus, in order to give models trained from scratch the best shot, we optimized the performance on out of domain data. Tables in the Supplementary Material describe the various initial learning rates explored during cross validation as well as the best learning schedules for each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Similarity Analysis</head><p>To elucidate the differences between pretrained models and models trained from scratch, we analyze the representational similarity between the variants. We use linear centered kernel alignment (CKA) <ref type="bibr" target="#b21">[22]</ref> to compare the image representations at various depths in the backbone networks. The results were aggregated with respect to the similarity of representations of within domain images versus out of domain images, and averaged over the three shuffles. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>To test within and out-of-domain performance we created a new dataset of 30 different Thoroughbreds that are led by different humans, resulting in a dataset of 8114 images with 22 labeled body parts each. These videos differ strongly in horse appearance, context, and background ( <ref type="figure">Figure 2)</ref>. Thus, this dataset is ideal for testing robustness and out-of-sample generalization. We created 3 splits containing 10 random horses each, and then varied the amount of training data from these 10 horses (referred to as Horse-10, see Methods). As the horses could vary dramatically in size across frames, due to the "in-the-wild" variation in distance from the camera, we used a normalized pixel error; i.e. we normalized the raw pixel errors by the eye-to-nose distance and report the fraction within this distance (see Methods).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet performance vs task performance</head><p>To probe the impact of ImageNet performance on pose estimation robustness, we selected modern convolutional architectures as backbones with a wide range of ImageNet performance (see Methods; 13 models spanning from 60% to 84% ImageNet performance). To fairly compare the Mo-bileNetV2, ResNet and EfficientNet backbones, we cross validated the learning schedules for each model (see Methods). In total, we found that all ImageNet-pretrained architectures exhibited strong performance on Horse-10 within domain, i.e. low average errors, and high average percent correct key points (aPCK; <ref type="figure">Figure 3</ref>, <ref type="table" target="#tab_2">Table 1</ref>). Performance on Horse-10 within domain also closely matched performance on Horse-30 (see Supplementary Material). Next, we directly compared the ImageNet performance to their respective performance on this pose estimation task. We found Top-1% ImageNet accuracy correlates with pose estimation test error (linear fit for test: slope −0.33%, R 2 = 0.93, p = 1.4 × 10 −7 ; <ref type="figure">Figure 3</ref>). Results for different bodyparts are displayed in <ref type="table" target="#tab_3">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generalization to novel horses</head><p>Next, we evaluated the performance of the networks on different horses in different contexts, i.e. out-of-domain horses (Figures 3A-C). Most strikingly, on out-of-domain horses, the relationship between ImageNet performance and performance on Horse-10 was even stronger. This can be quantified by comparing the linear regression slope for outof-domain test data: −0.93% pose-estimation improvement per percentage point of ImageNet performance, R 2 = 0.93, p = 9 × 10 −8 vs. within-domain test data −0.33%, R 2 = 0.93, p = 1.4 × 10 −7 (for 50% training data). Results for several different bodyparts of the full 22 are displayed in Table 3, highlighting that better models also generalized better in a bodypart specific way. In other words, less powerful models overfit more on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generalization across species</head><p>Does the improved generalization to novel individuals also hold for a more difficult out-of-domain generalization, namely, across species? Thus, we turned to a poseestimation dataset comprising multiple species. We evaluated the performance of the various architectures on the  Animal Pose dataset from Cao et. al <ref type="bibr" target="#b5">[6]</ref>. Here, images and poses of horses, dogs, sheep, cats, and cows allow us to test performance across animal classes. Using ImageNet pretraining and the cross validated schedules from our Horse-10 experiments, we trained on individual animal classes or multiple animal classes (holding out sheep/cows) and examined how the architectures generalized to sheep/cows, respectively ( <ref type="figure" target="#fig_1">Figure 4</ref>). For both cows and sheep, better ImageNet architectures, trained on the pose data of other animal classes performed better, in most cases. We mused that this improved generalization could be a consequence of the ImageNet pretraining or the architectures themselves. Therefore, we turned to Horse-10 and trained the different architectures directly on horse pose estimation from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Task-based training from scratch</head><p>To assess the impact of ImageNet pretraining we also trained several architectures from scratch. Thereby we could directly test if the increased slope for out-of-domain performance across networks was merely a result of more powerful network architectures. He et al. demonstrated that training Mask R-CNN with ResNet backbones directly on the COCO object detection, instance segmentation and key point detection tasks, catches-up with the performance of ImageNet-pretrained variants if training for substantially more iterations than typical training schedules <ref type="bibr" target="#b9">[10]</ref>. However, due to the nature of these tasks, they could not test this relationship on out-of-domain data. For fine-tuning from ImageNet pretrained models, we trained for 30k iterations (as the loss had flattened; see <ref type="figure" target="#fig_2">Figure 5</ref>). First, we searched for best performing schedules for training from scratch while substantially increasing the training time (6X longer). We found that cosine decay with restart was the best for outof-domain performance (see Methods; <ref type="figure" target="#fig_2">Figure 5</ref>).</p><p>Using this schedule, and consistent with He et al. <ref type="bibr" target="#b9">[10]</ref>, we found that randomly initialized networks could closely match the performance of pretrained networks, given enough data and time ( <ref type="figure" target="#fig_2">Figure 5</ref>). As expected, for smaller training sets (5 % training data; 160 images), this was not the case ( <ref type="figure" target="#fig_2">Figure 5</ref>). While task-training could therefore match the performance of pretrained networks given enough training data, this was not the case for novel horses (out-of-domain data). The trained from-scratch networks never caught up and indeed plateaued early ( <ref type="figure" target="#fig_0">Figure 5; Figure 1)</ref>. Quantitatively, we also found that for stronger networks (ResNets and EfficientNets) generalization was worse if trained from scratch <ref type="figure" target="#fig_0">(Figure 1</ref>). Interestingly that was not the case for the lightweight models, i.e. Mo-bileNetV2s (cf. [41]). and EfficientNet-B0 using 50% of the training data. Test errors when training from scratch (solid lines) closely match the transfer learning (dashed lines) performance after many iterations. Crucially, out-of-domain testing does not approach performance for pretrained network (stars). Bottom Row: Same as Middle but using 5% of the training data; note, however, for just 5% training data, the test errors do not approach the test error of pretrained models for larger models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Network similarity analysis</head><p>We hypothesized that the differences in generalization are due to more invariant representations in networks with higher ImageNet-performance using Centered Kernel Alignment (CKA) <ref type="bibr" target="#b21">[22]</ref>. We first verified that the representations change with task training (Supplementary Material <ref type="figure" target="#fig_0">Figure 1</ref>). We compared the representations of withindomain and out-of-domain images across networks trained from ImageNet vs. from scratch. We found that early blocks are similar for from scratch vs transfer learning for both sets of horses. In later layers, the representations diverge, but comparisons between within-domain and out of domain trends were inconclusive as to why e.g., EfficientNets generalize better (Supplementary Material <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Horse-C: Robustness to image corruptions</head><p>To elucidate the difficulty of the Horse-10 benchmark, we more broadly evaluate pose estimation performance under different forms of domain shift ( <ref type="figure" target="#fig_3">Figure 6</ref>). Recently, Schneider, Rusak et al. demonstrated that simple unsupervised domain adaptation methods can greatly enhance the performance on corruption robustness benchmarks <ref type="bibr">[45]</ref>. We therefore settled on a full adaptation evaluation protocol: We re-trained MobileNetV2 0.35 and 1.0, ResNet50, as well as EfficientNet B0 and B3 with batch normalization enabled. During evaluation we then re-computed separate batch norm statistics for each horse and corruption type.</p><p>We use batch norm adaptation [45] during our evaluation on Horse-C. On clean out-of-domain data, we see improvements for MobileNetV2s and ResNets when using pre-trained networks, and for all models when training models from scratch <ref type="figure" target="#fig_4">(Figure 7)</ref>. On common corruptions, utilizing adaptation is crucial to final performance (see full results in Supplementary Material). In the batch norm adapted models, we compared four test conditions comprised of within-domain and out-of domain for both original (clean) and corrupted images ( <ref type="figure" target="#fig_3">Figure 6</ref>). First, we find that even with batch norm adapted models, Horse-C is as hard as Horse-10; namely performance is significantly affected on corrupted data <ref type="figure" target="#fig_5">(Figure 8</ref>). Secondly, we find the corruption plus "out-of-domain" identity, is even harder-the performance degradation induced by different horse identities is on the same order of magnitude as the mean error induced on the corrupted dataset. Finally, and consistent with our other results, we found a performance gain by using pretrained networks <ref type="figure" target="#fig_5">(Figure 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and conclusions</head><p>We developed a novel pose estimation benchmark for out-of-domain robustness (Horse-10), and for testing com-   . Impact of distribution shift introduced by horse identities and common corruptions. We tested within identity (i.e., equivalent to within-domain in Horse-10 (left), or out-of-domain identity (right). Lines with alpha transparency represent corrupted images, whereas solid is the original (clean) image. We show mean ± SEM across 3 splits for clean images, and across 3 splits, 15 corruptions and 5 severities for corrupted images. mon image corruptions on pose estimation (Horse-C). The data and benchmarks are available at http://horse10. deeplabcut.org. Furthermore, we report two key findings: (1) pretrained-ImageNet networks offer known advantages: shorter training times, and less data requirements, as well as a novel advantage: robustness on out-of-domain data, and (2) pretained networks that have higher ImageNet performance lead to better generalization. Collectively, this sheds a new light on the inductive biases of "better Ima-geNet architectures" for visual tasks to be particularly beneficial for robustness.</p><p>We introduced novel DeepLabCut model variants, part of https://github.com/DeepLabCut/ DeepLabCut, that can achieve high accuracy, but with higher inference speed (up to double) than the original ResNet backbone (see Supplementary Material for an inference speed benchmark).</p><p>In summary, transfer learning offers multiple advantages. Not only does pretraining networks on ImageNet allow for using smaller datasets and shorter training time ( <ref type="figure" target="#fig_2">Figure 5</ref>), it also strongly improves robustness and generalization, especially for more powerful, over-parameterized models. In fact, we found a strong correlation between generalization and ImageNet accuracy <ref type="figure">(Figure 3</ref>). While we found a significant advantage (&gt;2X boost) of using pretrained networks vs. from scratch for out-of-domain robustness, there is still a 3-fold difference in performance between within domain and out of domain ( <ref type="figure" target="#fig_0">Figure 1</ref>). We believe that this work demonstrates that transfer learning approaches are powerful to build robust architectures, and are particularly important for further developing performance improvements on real-world datasets, such as Horse-10 and derived benchmarks such as Horse-C. Furthermore, by sharing our animal pose robustness benchmark dataset, we also believe that the community can collectively work towards closing the gap. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional information on the Horse-10 dataset</head><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Learning schedule cross validation</head><p>Because of the extensive resources required to cross validate all models, we only underwent the search on MobileNetV2s 0.35 and 1.0, ResNet 50, and EfficientNets B0, B3, and B5 for the pretraining and from scratch variants. For all other models, the parameters from the most similar networks were used for training (i.e. EfficientNet-B1 used the parameters for EfficientNet-B0). The grid search started with the highest possible initial learning rate that was numerically stable for each model; lower initial learning rates were then tested to fine tune the schedule. Zero and nonzero decay target levels were tested for each initial learning rate. In addition to the initial learning rates and decay targets, we experimented with shortening the cosine decay and incorporating restarts. All cross validation experiments were performed on the three splits with 50% of the data for training.</p><p>For training, a cosine learning rate schedule, as in <ref type="bibr" target="#b22">[23]</ref> with ADAM optimizer <ref type="bibr" target="#b20">[21]</ref> and batchsize 8 was used. For the learning schedules we use the following abbreviations: Initial Learning Rates (ILR) and decay target (DT).</p><p>The tables below list the various initial learning rates explored during cross validation for each model with pretraining. For the ImageNet pretrained case, the learning rate schedule without restarts was optimal on out of domain data, and the resulting optimal parameters are as follows:</p><p>The initial learning rates explored for the from scratch models during cross validation are as follows:</p><formula xml:id="formula_0">MODEL ILR MOBILENETV2-0.35 1E-2 5E-3 1E-3 5E-4 MOBILENETV2-1.0 1E-2 5E-3 1E-3 5E-4 RESNET-50 1E-3 5E-4 1E-4 5E-5 EFFICIENTNET-B0 2.5E-3 1E-3 7.5E-4 5E-4 EFFICIENTNET-B3 1E-3 5E-4 1E-4 5E-5 EFFICIENTNET-B5 5E-4 1E-4 MODELS ILR &amp; DT MOBILENETV2S 0.35, 0.5 1E-2 0 MOBILENETV2S 0.75, 1.0 1E-2 1E-4 RESNETS 50, 101 1E-4 1E-5 EFFICIENTNETS B0, B1 5E-4 1E-5 EFFICIENTNETS B2,B3,B4 5E-4 0 EFFICIENTNETS B5,B6 5E-4 1E-5 MODEL ILR MOBILENETV2 0.35 1E-2 5E-3 1E-3 5E-4 MOBILENETV2 1.0 1E-1 1E-2 1E-3 1E-4 RESNET 50 1E-3 5E-4 1E-4 5E-5 EFFICIENTNET-B0 1E-3 5E-4 1E-4 5E-5 EFFICIENTNET-B3 1E-3 5E-4 1E-4 5E-5</formula><p>For models trained from scratch, we found that using restarts lead to the best performance on out of domain data. The optimal learning rates found during the search are as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Performance on Horse-30</head><p>For comparison to Horse-10, we provide the train and test normalized errors for models trained on Horse-30. Here, Horse-30 was split into 3 shuffles each containing a train/test split of 50% of the horse images. Compared to Horse-10, we train these models for twice as long (60,000 iterations) but with the same cross-validated cosine schedules from Horse-10. Errors below are averaged over the three shuffles. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance (PCK per bodypart) for all networks on Horse-10</head><p>The tables below show the PCK for several bodyparts for all backbones that we considered. They complete the abridged tables in the main text ( <ref type="table" target="#tab_3">Table 2</ref> and 3) Thereby the bodyparts are abbreviated as follows: (FF=front foot; HF = Hind foot; HH = Hind Hock). D. CKA analysis of training &amp; trained vs. from scratch networks <ref type="figure" target="#fig_7">Figure 9</ref> shows a linear centered kernel alignment (CKA) <ref type="bibr" target="#b21">[22]</ref> comparison of representations for task-training vs. Ima-geNet trained (no task training) for ResNet-50  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results of within domain performance on Animal Pose</head><p>In Main <ref type="figure" target="#fig_1">Figure 4</ref> we show the performance when we train on only 1 species and testing on another (or all without cow/sheep vs. sheep.cow). Here, as a baseline we report the performance within domain, i.e. for each out-of-domain test species (cow and sheep) we trained on 90% of the cow data and tested on 10% cow data (see tables 6 and 7). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Full results on Horse-C</head><p>We show the full set of results for the Horse-C benchmark. We compute the corruptions proposed by Hendrycks et al. <ref type="bibr" target="#b13">[14]</ref> using the image corruptions library proposed by Michaelis et al. <ref type="bibr">[36]</ref>.</p><p>The original Horse-30 dataset is processed once for each of the corruptions and severities. In total, Horse-C is comprised of 75 evaluation settings with 8,114 images each, yielding a total of 608,550 images. For a visual impression of the impact of different corruptions and severities, see  For evaluation, we consider MobileNetV2-0.35, MobileNetV2-1.0, ResNet-50 and the B0 and B3 variants of EfficientNet. All models are either trained on Horse-10 from scratch or pre-trained on ImageNet and fine-tuned to Horse-10, using the three validation splits used throughout the paper. In contrast to our other experiments, we now fine-tune the BatchNorm layers for these models. For both the w.d. and o.o.d. settings, this yields comparable performance, but enables us to use the batch adaptation technique proposed by Schneider, Rusak et al. <ref type="bibr">[45]</ref> during evaluation on Horse-C, allowing a better estimate of model robustness. On the clean data, using batch norm adaptation yields slightly improved performance for MobileNetV2s on clean withindomain data and deteriorates performance for EfficientNet models. Performance on clean ood. data is improved of all model variants when training from scratch, and improved for MobileNets and ResNets when using pre-trained weights.</p><p>We evaluate the normalized errors for the non-adapted model (Base) and after estimating corrected batch normalization statistics (Adapt). The corrected statistics are estimated for each horse identity and corruption as proposed in <ref type="bibr">[45]</ref>. We average the normalized metrics across shuffles (and horses as usual). We present the full results for a pre-trained ResNet50 model for all four corruption classes in <ref type="table" target="#tab_2">Tables 10 and 11</ref> and contrast this to the within-domain/out-of-domain evaluation setting in <ref type="table" target="#tab_2">Table 12</ref>.</p><p>For the ResNet50 model considered in detail, we find that batch normalization helps most for noise and weather corruptions, where we typically found improvements of 60 − 90% and of 30 − 70%, respectively. In contrast, blur corruptions and digital corruptions (apart from contrast, defocus blur) saw more modest improvements. It is notable that some of the corruptions-such as elastic transform or pixelation-likely also impact the ground truth posture.</p><p>Batch norm adaptation slightly improves the prediction performance when evaluating on different horse identities, but fails to close the gap between the w.d. and ood. setting. In contrast, batch adaptation considerably improves prediction performance on all considered common corruptions.</p><p>In summary, we provide an extensive suite of benchmarks for pose estimation and our experiments suggest that domain shift induced by different individuals is difficult in nature (as it is difficult to fix). This further highlights the importance of benchmarks such as Horse-10. Full results for other model variants are depicted in <ref type="table" target="#tab_10">Table 8</ref> and <ref type="table" target="#tab_11">Table 9</ref>. We report average scores on Horse-C all models in the main text.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Inference Speed Benchmarking</head><p>We introduced new DeepLabCut variants that can achieve high accuracy but with higher speed than the original ResNet backbone <ref type="bibr">[33]</ref>. Here we provide a simple benchmark to document how fast the EfficientNet and MobileNetv2 backbones are ( <ref type="figure" target="#fig_0">Figure 15</ref>). We evaluated the inference speed for one video with 11, 178 frames at resolutions 512 × 512, 256 × 256 and 128 × 128. We used batch sizes: <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">32,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref>, and ran all models for all 3 (training set shuffles) trained with 50% of the data in a pseudo random order on a NVIDIA Titan RTX. We also updated the inference code from its numpy implementation [34] to TensorFlow, which brings a 2 − 10% gain in speed. <ref type="figure" target="#fig_0">Figure 15</ref>. Speed Benchmarking for MobileNetV2s, ResNets and EfficientNets: Inference speed for videos of different dimensions for all the architectures. A-C: FPS vs. batchsize, with video frame sizes as stated in the title. Three splits are shown for each network. MobileNetV2 gives a more than 2X speed improvement (over ResNet-50) for offline processing and about 40% for batchsize=1 on a Titan RTX GPU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Transfer Learning boosts performance, especially on out-of-domain data. Normalized pose estimation error vs. Ima-geNet Top 1% accuracy with different backbones. While training from scratch reaches the same task performance as fine-tuning, the networks remain less robust as demonstrated by poor accuracy on out-of-domain horses. Mean ± SEM, 3 shuffles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Generalization Across Species. Normalized pose estimation error vs. ImageNet Top 1% accuracy with different backbones (as inFigure 1), but for 10 additional out-of-domain tests. Training on either a single species or four species while holding one species (either cow or sheep) out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Training randomly initialized networks longer cannot rescue out-of-domain performance. Top Row: Best performing (cross-validated) learning schedules used for training. Middle: Normalized error vs. training iterations for MobileNetV2-0.35, ResNet-50</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Measuring the impact of common image corruptions on pose estimation (Horse-C): We adapt the image corruptions considered by Hendrycks et al. and contrast the impact of common image corruptions with that of out of domain evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Impact of test time normalization. Models trained with adaptive BN layers slightly outperform our baseline models for the MobileNetV2 and ResNet architecture out-of-domain evaluation. Lines with alpha transparency represent fixed models (vs. adapted). We show mean ± SEM computed across 3 data splits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8</head><label>8</label><figDesc>Figure 8. Impact of distribution shift introduced by horse identities and common corruptions. We tested within identity (i.e., equivalent to within-domain in Horse-10 (left), or out-of-domain identity (right). Lines with alpha transparency represent corrupted images, whereas solid is the original (clean) image. We show mean ± SEM across 3 splits for clean images, and across 3 splits, 15 corruptions and 5 severities for corrupted images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>CKA comparison of representations for task-training vs. ImageNet trained (no task training) for ResNet-50. Left: Linear CKA on within domain horses (used for training) when trained from scratch vs. plain ImageNet trained (no horse pose estimation task training). Right: Same, but for Transfer Learning vs. from ImageNet. Matrices are the averages over the three splits. In short, task training changes representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>CKA comparison of representations when trained from scratch vs. from ImageNet initialization. A: Top: Linear CKA between layers of individual networks of different depths on within domain horses (used for training the models). Bottom: Same, but for out-of-domain horses (not used in training). Matrices are the averages over the three splits. B: Quantification of similarity ratio plotted against depth of the networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figures 11-14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Noise corruptions for all five different severities (1 to 5, left to right). Top to bottom: Gaussian Noise, Shot Noise, Impulse Noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .Figure 13 .Figure 14 .</head><label>121314</label><figDesc>Blur corruptions for all five different severities (1 to 5, left to right). Top to bottom: Defocus Blur, Motion Blur, Zoom Blur Weather corruptions for all five different severities (1 to 5, left to right). Top to bottom: Snow, Frost, Fog, Brightness Digital corruptions for all five different severities (1 to 5, left to right). Top to bottom: Contrast, Elastic Transform, Pixelate, Jpeg Compression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Transfer learning (using pretrained ImageNet models), gives a 2X boost on out-of-domain data vs. from scratch training</figDesc><table><row><cell></cell><cell></cell><cell>Test (out of domain),</cell></row><row><cell></cell><cell></cell><cell>trained from scratch</cell></row><row><cell></cell><cell></cell><cell>Test (out of domain),</cell></row><row><cell></cell><cell></cell><cell>w/ transfer learning</cell></row><row><cell>MobileNetV2-0.35</cell><cell>EfficientNet-B0</cell><cell></cell></row><row><cell>MobileNetV2-1</cell><cell>ResNet-50</cell><cell>EfficientNet-B3</cell></row><row><cell></cell><cell></cell><cell>Train</cell></row><row><cell></cell><cell></cell><cell>Test (within domain)</cell></row><row><cell></cell><cell></cell><cell>w/transfer learning trained from scratch</cell></row></table><note>Equal contribution.† Correspondence: alexander.mathis@epfl.ch‡ Correspondence: mackenzie.mathis@epfl.ch</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>average PCK@0.3 (%)</figDesc><table><row><cell>MODELS</cell><cell>WITHIN DOMAIN</cell><cell>OUT-OF-D.</cell></row><row><cell>MOBILENETV2-0.35</cell><cell>95.2</cell><cell>63.5</cell></row><row><cell>MOBILENETV2-0.5</cell><cell>97.1</cell><cell>70.4</cell></row><row><cell>MOBILENETV2-0.75</cell><cell>97.8</cell><cell>73.0</cell></row><row><cell>MOBILENETV2-1</cell><cell>98.8</cell><cell>77.6</cell></row><row><cell>RESNET-50</cell><cell>99.8</cell><cell>81.3</cell></row><row><cell>RESNET-101</cell><cell>99.9</cell><cell>84.3</cell></row><row><cell>EFFICIENTNET-B0</cell><cell>99.9</cell><cell>81.6</cell></row><row><cell>EFFICIENTNET-B1</cell><cell>99.9</cell><cell>84.5</cell></row><row><cell>EFFICIENTNET-B2</cell><cell>99.9</cell><cell>84.3</cell></row><row><cell>EFFICIENTNET-B3</cell><cell>99.9</cell><cell>86.6</cell></row><row><cell>EFFICIENTNET-B4</cell><cell>99.9</cell><cell>86.9</cell></row><row><cell>EFFICIENTNET-B5</cell><cell>99.9</cell><cell>87.7</cell></row><row><cell>EFFICIENTNET-B6</cell><cell>99.9</cell><cell>88.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>PCK@0.3 (%) for several bodyparts and architectures on within domain horses (FF=front foot; HF = Hind foot; HH = Hind Hock).</figDesc><table><row><cell></cell><cell>Nose</cell><cell>Eye</cell><cell cols="5">Shoulder Wither Elbow NearFF OffFF</cell><cell>Hip</cell><cell cols="3">NearHH NearHF OffHF</cell></row><row><cell>MobileNetV2 0.35</cell><cell>90.7</cell><cell>94.1</cell><cell>97.6</cell><cell>96.9</cell><cell>96.7</cell><cell>92.3</cell><cell>93.7</cell><cell>96.4</cell><cell>94.1</cell><cell>94.2</cell><cell>92.5</cell></row><row><cell>MobileNetV2 0.5</cell><cell>94.1</cell><cell>96.1</cell><cell>99.2</cell><cell>98.3</cell><cell>98.0</cell><cell>93.8</cell><cell>95.4</cell><cell>96.7</cell><cell>97.2</cell><cell>97.2</cell><cell>97.0</cell></row><row><cell>MobileNetV2 0.75</cell><cell>96.0</cell><cell>97.5</cell><cell>99.2</cell><cell>98.0</cell><cell>99.0</cell><cell>96.6</cell><cell>96.8</cell><cell>98.8</cell><cell>97.6</cell><cell>98.0</cell><cell>97.4</cell></row><row><cell>MobileNetV2 1.0</cell><cell>97.7</cell><cell>98.8</cell><cell>99.7</cell><cell>99.1</cell><cell>99.0</cell><cell>97.6</cell><cell>97.3</cell><cell>99.4</cell><cell>98.4</cell><cell>98.5</cell><cell>98.9</cell></row><row><cell>ResNet 50</cell><cell>99.9</cell><cell>100.0</cell><cell>99.8</cell><cell>99.9</cell><cell>99.8</cell><cell>99.8</cell><cell>99.6</cell><cell>99.9</cell><cell>99.9</cell><cell>99.6</cell><cell>99.8</cell></row><row><cell>ResNet 101</cell><cell>99.9</cell><cell>100.0</cell><cell>99.9</cell><cell>99.8</cell><cell>99.9</cell><cell>99.8</cell><cell>99.7</cell><cell>99.8</cell><cell>99.9</cell><cell>99.7</cell><cell>99.9</cell></row><row><cell>EfficientNet-B0</cell><cell>99.7</cell><cell>99.9</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0</cell><cell>99.6</cell><cell>99.5</cell><cell>100.0</cell><cell>99.9</cell><cell>99.7</cell><cell>99.7</cell></row><row><cell>EfficientNet-B1</cell><cell>99.8</cell><cell>99.9</cell><cell>100.0</cell><cell>99.8</cell><cell>99.9</cell><cell>99.5</cell><cell>99.8</cell><cell>100.0</cell><cell>99.8</cell><cell>99.8</cell><cell>99.8</cell></row><row><cell>EfficientNet-B2</cell><cell>99.9</cell><cell>99.9</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0</cell><cell>99.8</cell><cell>99.7</cell><cell>99.9</cell><cell>99.8</cell><cell>99.7</cell><cell>99.7</cell></row><row><cell>EfficientNet-B3</cell><cell>99.9</cell><cell>99.9</cell><cell>99.9</cell><cell>99.9</cell><cell>99.9</cell><cell>99.7</cell><cell>99.6</cell><cell>99.7</cell><cell>99.8</cell><cell>99.6</cell><cell>99.9</cell></row><row><cell>EfficientNet-B4</cell><cell cols="2">100.0 100.0</cell><cell>99.9</cell><cell>99.8</cell><cell>99.9</cell><cell>99.6</cell><cell>99.7</cell><cell>99.9</cell><cell>99.7</cell><cell>99.8</cell><cell>99.8</cell></row><row><cell>EfficientNet-B5</cell><cell>99.9</cell><cell>99.9</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0</cell><cell>99.7</cell><cell>99.8</cell><cell>99.6</cell><cell>99.8</cell><cell>99.8</cell><cell>99.9</cell></row><row><cell>EfficientNet-B6</cell><cell>99.9</cell><cell>99.9</cell><cell>99.9</cell><cell>99.8</cell><cell>100.0</cell><cell>99.8</cell><cell>99.9</cell><cell>99.8</cell><cell>99.8</cell><cell>99.7</cell><cell>99.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>PCK@0.3 (%) for several bodyparts and architectures on out-of-domain horses (FF=front foot; HF = Hind foot; HH = Hind Hock).</figDesc><table><row><cell></cell><cell>Nose</cell><cell>Eye</cell><cell cols="5">Shoulder Wither Elbow NearFF OffFF</cell><cell>Hip</cell><cell cols="3">NearHH NearHF OffHF</cell></row><row><cell>MobileNetV2 0.35</cell><cell>45.6</cell><cell>53.1</cell><cell>65.5</cell><cell>68.0</cell><cell>69.1</cell><cell>56.4</cell><cell>57.6</cell><cell>65.9</cell><cell>65.9</cell><cell>60.5</cell><cell>62.5</cell></row><row><cell>MobileNetV2 0.5</cell><cell>52.7</cell><cell>61.0</cell><cell>76.7</cell><cell>69.7</cell><cell>78.3</cell><cell>62.9</cell><cell>65.4</cell><cell>73.6</cell><cell>70.8</cell><cell>68.1</cell><cell>69.7</cell></row><row><cell>MobileNetV2 0.75</cell><cell>54.2</cell><cell>65.6</cell><cell>78.3</cell><cell>73.2</cell><cell>80.5</cell><cell>67.3</cell><cell>68.9</cell><cell>80.0</cell><cell>74.1</cell><cell>70.5</cell><cell>70.2</cell></row><row><cell>MobileNetV2 1.0</cell><cell>59.0</cell><cell>67.2</cell><cell>83.8</cell><cell>79.7</cell><cell>84.0</cell><cell>70.1</cell><cell>72.1</cell><cell>82.0</cell><cell>79.9</cell><cell>76.0</cell><cell>76.7</cell></row><row><cell>ResNet 50</cell><cell>68.2</cell><cell>73.6</cell><cell>85.4</cell><cell>85.8</cell><cell>88.1</cell><cell>72.6</cell><cell>70.2</cell><cell>89.2</cell><cell>85.7</cell><cell>77.0</cell><cell>74.1</cell></row><row><cell>ResNet 101</cell><cell>67.7</cell><cell>72.4</cell><cell>87.6</cell><cell>86.0</cell><cell>89.0</cell><cell>79.9</cell><cell>78.0</cell><cell>92.6</cell><cell>87.2</cell><cell>83.4</cell><cell>80.0</cell></row><row><cell>EfficientNet-B0</cell><cell>60.3</cell><cell>62.5</cell><cell>84.9</cell><cell>84.6</cell><cell>87.2</cell><cell>77.0</cell><cell>75.4</cell><cell>86.7</cell><cell>86.7</cell><cell>79.6</cell><cell>79.4</cell></row><row><cell>EfficientNet-B1</cell><cell>67.4</cell><cell>71.5</cell><cell>85.9</cell><cell>85.7</cell><cell>89.6</cell><cell>80.0</cell><cell>81.1</cell><cell>86.7</cell><cell>88.4</cell><cell>81.8</cell><cell>81.6</cell></row><row><cell>EfficientNet-B2</cell><cell>68.7</cell><cell>74.8</cell><cell>84.5</cell><cell>85.2</cell><cell>89.2</cell><cell>79.7</cell><cell>80.9</cell><cell>88.1</cell><cell>88.0</cell><cell>82.3</cell><cell>81.7</cell></row><row><cell>EfficientNet-B3</cell><cell>71.7</cell><cell>76.6</cell><cell>88.6</cell><cell>88.7</cell><cell>92.0</cell><cell>80.4</cell><cell>81.8</cell><cell>90.6</cell><cell>90.8</cell><cell>85.0</cell><cell>83.6</cell></row><row><cell>EfficientNet-B4</cell><cell>71.1</cell><cell>75.8</cell><cell>88.1</cell><cell>87.4</cell><cell>91.8</cell><cell>83.3</cell><cell>82.9</cell><cell>90.8</cell><cell>90.3</cell><cell>86.7</cell><cell>85.5</cell></row><row><cell>EfficientNet-B5</cell><cell>74.8</cell><cell>79.5</cell><cell>89.6</cell><cell>89.5</cell><cell>93.5</cell><cell>82.2</cell><cell>84.1</cell><cell>91.8</cell><cell>90.9</cell><cell>86.6</cell><cell>85.2</cell></row><row><cell>EfficientNet-B6</cell><cell>74.7</cell><cell>79.7</cell><cell>90.3</cell><cell>89.8</cell><cell>92.8</cell><cell>83.6</cell><cell>84.4</cell><cell>92.1</cell><cell>92.1</cell><cell>87.8</cell><cell>85.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>[ 24 ]</head><label>24</label><figDesc>Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi. Pifpaf: Composite fields for human pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11977-11986, 2019. [25] Matthias Kümmerer, Thomas SA Wallis, and Matthias Bethge. 37] Jiteng Mu, Weichao Qiu, Gregory D Hager, and Alan L Yuille. Learning from synthetic animals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12386-12395, 2020. [38] Tanmay Nath, Alexander Mathis, An Chi Chen, Amir Patel, Matthias Bethge, and Mackenzie Weygandt Mathis. Using deeplabcut for 3d markerless pose estimation across species and behaviors.</figDesc><table><row><cell>Deepgaze ii: Reading fixations from deep</cell><cell></cell><cell></cell></row><row><cell>features trained on object recognition. arXiv preprint</cell><cell></cell><cell></cell></row><row><cell>arXiv:1610.01563, 2016.</cell><cell cols="2">Nature Protocols, 14:2152-2176, 2019.</cell></row><row><cell>[26] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-</cell><cell cols="2">[39] Guanghan Ning, Jian Pei, and Heng Huang. Lighttrack: A</cell></row><row><cell>jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan</cell><cell cols="2">generic framework for online top-down human pose track-</cell></row><row><cell>Popov, Matteo Malloci, Tom Duerig, et al. The open im-</cell><cell cols="2">ing. In Proceedings of the IEEE/CVF Conference on Com-</cell></row><row><cell>ages dataset v4: Unified image classification, object detec-</cell><cell cols="2">puter Vision and Pattern Recognition Workshops, pages</cell></row><row><cell>tion, and visual relationship detection at scale. arXiv preprint</cell><cell>1034-1035, 2020.</cell><cell></cell></row><row><cell>arXiv:1811.00982, 2018.</cell><cell cols="2">[40] Mirela Ostrek, Helge Rhodin, Pascal Fua, Erich Müller, and</cell></row><row><cell>[27] Hengduo Li, Bharat Singh, Mahyar Najibi, Zuxuan Wu, and</cell><cell cols="2">Jörg Spörri. Are existing monocular computer vision-based</cell></row><row><cell>Larry S Davis. An analysis of pre-training on object detec-</cell><cell cols="2">3d motion capture approaches ready for deployment? a</cell></row><row><cell>tion. arXiv preprint arXiv:1904.05871, 2019.</cell><cell cols="2">methodological study on the example of alpine skiing. Sen-</cell></row><row><cell>[28] Siyuan Li, Semih Gunel, Mirela Ostrek, Pavan Ramdya, Pas-</cell><cell>sors, 19(19):4323, 2019.</cell><cell></cell></row><row><cell></cell><cell cols="2">[41] Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy</cell></row><row><cell></cell><cell cols="2">Bengio. Transfusion: Understanding transfer learning for</cell></row><row><cell></cell><cell cols="2">medical imaging. In Advances in Neural Information Pro-</cell></row><row><cell></cell><cell>cessing Systems, pages 3342-3352, 2019.</cell><cell></cell></row><row><cell></cell><cell cols="2">[42] Maheen Rashid, Xiuye Gu, and Yong Jae Lee. Interspecies</cell></row><row><cell></cell><cell cols="2">knowledge transfer for facial keypoint detection. In Proceed-</cell></row><row><cell></cell><cell cols="2">ings of the IEEE Conference on Computer Vision and Pattern</cell></row><row><cell>-755.</cell><cell>Recognition, pages 6894-6903, 2017.</cell><cell></cell></row><row><cell>Springer, 2014.</cell><cell cols="2">[43] Artsiom Sanakoyeu, Vasil Khalidov, Maureen S McCarthy,</cell></row><row><cell>[30] Pablo Maceira-Elvira, Traian Popa, Anne-Christine Schmid,</cell><cell>Andrea Vedaldi, and Natalia Neverova.</cell><cell>Transferring</cell></row><row><cell>and Friedhelm C Hummel. Wearable technology in stroke</cell><cell cols="2">dense pose to proximal animal classes. arXiv preprint</cell></row><row><cell>rehabilitation: towards improved diagnosis and treatment of</cell><cell>arXiv:2003.00080, 2020.</cell><cell></cell></row><row><cell>upper-limb motor impairment. Journal of neuroengineering</cell><cell cols="2">[44] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-</cell></row><row><cell>and rehabilitation, 16(1):142, 2019.</cell><cell cols="2">moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted</cell></row><row><cell>[31] Lars-Erik Magnusson and B Thafvellin. Studies on the con-</cell><cell cols="2">residuals and linear bottlenecks. In Proceedings of the IEEE</cell></row><row><cell>formation and related traits of standardbred trotters in swe-</cell><cell cols="2">Conference on Computer Vision and Pattern Recognition,</cell></row><row><cell>den. Journal of Animal Physiology and Animal Nutrition</cell><cell>pages 4510-4520, 2018.</cell><cell></cell></row><row><cell>(Germany, FR), 1990.</cell><cell cols="2">[45] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring-</cell></row><row><cell>[32] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,</cell><cell cols="2">mann, Wieland Brendel, and Matthias Bethge. Removing</cell></row><row><cell>Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,</cell><cell cols="2">covariate shift improves robustness against common corrup-</cell></row><row><cell>and Laurens van der Maaten. Exploring the limits of weakly</cell><cell cols="2">tions. Thirty-fourth Conference on Neural Information Pro-</cell></row><row><cell>supervised pretraining. In Proceedings of the European Con-</cell><cell>cessing Systems (NeurIPS), 2020.</cell><cell></cell></row><row><cell>ference on Computer Vision (ECCV), pages 181-196, 2018.</cell><cell cols="2">[46] Sahil Shah, Abhishek Sharma, Arjun Jain, et al. On</cell></row><row><cell>[33] Alexander Mathis, Pranav Mamidanna, Kevin M Cury, Taiga</cell><cell cols="2">the robustness of human pose estimation. arXiv preprint</cell></row><row><cell>Abe, Venkatesh N Murthy, Mackenzie Weygandt Mathis,</cell><cell>arXiv:1908.06401, 2019.</cell><cell></cell></row><row><cell>and Matthias Bethge. Deeplabcut: markerless pose estima-</cell><cell cols="2">[47] Mingxing Tan and Quoc V Le. Efficientnet: Rethinking</cell></row><row><cell>tion of user-defined body parts with deep learning. Nature</cell><cell cols="2">model scaling for convolutional neural networks. arXiv</cell></row><row><cell>Neuroscience, 21(9):1281-1289, 2018.</cell><cell>preprint arXiv:1905.11946, 2019.</cell><cell></cell></row><row><cell>[34] Alexander Mathis and Richard A. Warren. On the infer-</cell><cell cols="2">[48] Heng Yang, Renqiao Zhang, and Peter Robinson. Human</cell></row><row><cell>ence speed and video-compression robustness of deeplabcut.</cell><cell cols="2">and sheep facial landmarks localisation by triplet interpo-</cell></row><row><cell>BioRxiv, 2018.</cell><cell cols="2">lated features. In 2016 IEEE Winter Conference on Applica-</cell></row><row><cell>[35] Mackenzie Weygandt Mathis and Alexander Mathis. Deep</cell><cell cols="2">tions of Computer Vision (WACV), pages 1-8. IEEE, 2016.</cell></row><row><cell>learning tools for the measurement of animal behavior in</cell><cell cols="2">[49] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.</cell></row><row><cell>neuroscience. Current Opinion in Neurobiology, 60:1-11,</cell><cell cols="2">How transferable are features in deep neural networks? In</cell></row><row><cell>2020.</cell><cell cols="2">Advances in neural information processing systems, pages</cell></row><row><cell></cell><cell>3320-3328, 2014.</cell><cell></cell></row></table><note>cal Fua, and Helge Rhodin. Deformation-aware unpaired im- age translation for pose estimation on laboratory animals. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 13158-13168, 2020. [29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740[36] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S. Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking ro- bustness in object detection: Autonomous driving when win- ter is coming. arXiv preprint arXiv:1907.07484, 2019.[[50] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. arXiv preprint arXiv:1911.02685, 2019.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>table lists the following statistics: labeled frames, scale (nose-to-eye distance in pixels), and whether a horse was within domain (w.d) or out-of-domain (o.o.d.) for each shuffle.</figDesc><table><row><cell>Horse Identifier</cell><cell cols="5">samples nose-eye dist shuffle 1 shuffle 2 shuffle 3</cell></row><row><cell>BrownHorseinShadow</cell><cell>308</cell><cell>22.3</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>w.d.</cell></row><row><cell>BrownHorseintoshadow</cell><cell>289</cell><cell>17.4</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Brownhorselight</cell><cell>306</cell><cell>15.57</cell><cell>o.o.d</cell><cell>w.d.</cell><cell>o.o.d</cell></row><row><cell>Brownhorseoutofshadow</cell><cell>341</cell><cell>16.22</cell><cell>o.o.d</cell><cell>w.d.</cell><cell>w.d.</cell></row><row><cell>ChestnutHorseLight</cell><cell>318</cell><cell>35.55</cell><cell>w.d.</cell><cell>w.d.</cell><cell>o.o.d</cell></row><row><cell>Chestnuthorseongrass</cell><cell>376</cell><cell>12.9</cell><cell>o.o.d</cell><cell>w.d.</cell><cell>w.d.</cell></row><row><cell>GreyHorseLightandShadow</cell><cell>356</cell><cell>14.41</cell><cell>w.d.</cell><cell>w.d.</cell><cell>o.o.d</cell></row><row><cell>GreyHorseNoShadowBadLight</cell><cell>286</cell><cell>16.46</cell><cell>w.d.</cell><cell>o.o.d</cell><cell>w.d.</cell></row><row><cell>TwoHorsesinvideobothmoving</cell><cell>181</cell><cell>13.84</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>w.d.</cell></row><row><cell>Twohorsesinvideoonemoving</cell><cell>252</cell><cell>16.51</cell><cell>w.d.</cell><cell>w.d.</cell><cell>w.d.</cell></row><row><cell>Sample1</cell><cell>174</cell><cell>24.78</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample2</cell><cell>330</cell><cell>16.5</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample3</cell><cell>342</cell><cell>16.08</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample4</cell><cell>305</cell><cell>18.51</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>w.d.</cell></row><row><cell>Sample5</cell><cell>295</cell><cell>16.89</cell><cell>w.d.</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample6</cell><cell>376</cell><cell>12.3</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample7</cell><cell>262</cell><cell>18.52</cell><cell>w.d.</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample8</cell><cell>388</cell><cell>12.5</cell><cell>w.d.</cell><cell>w.d.</cell><cell>o.o.d</cell></row><row><cell>Sample9</cell><cell>359</cell><cell>12.43</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample10</cell><cell>235</cell><cell>25.18</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample11</cell><cell>256</cell><cell>19.16</cell><cell>o.o.d</cell><cell>w.d.</cell><cell>o.o.d</cell></row><row><cell>Sample12</cell><cell>288</cell><cell>17.86</cell><cell>w.d.</cell><cell>o.o.d</cell><cell>w.d.</cell></row><row><cell>Sample13</cell><cell>244</cell><cell>25.78</cell><cell>w.d.</cell><cell>w.d.</cell><cell>w.d.</cell></row><row><cell>Sample14</cell><cell>168</cell><cell>25.55</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample15</cell><cell>154</cell><cell>26.53</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample16</cell><cell>212</cell><cell>15.43</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample17</cell><cell>240</cell><cell>10.04</cell><cell>w.d.</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>Sample18</cell><cell>159</cell><cell>29.55</cell><cell>o.o.d</cell><cell>w.d.</cell><cell>o.o.d</cell></row><row><cell>Sample19</cell><cell>134</cell><cell>13.44</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>w.d.</cell></row><row><cell>Sample20</cell><cell>180</cell><cell>28.57</cell><cell>o.o.d</cell><cell>o.o.d</cell><cell>o.o.d</cell></row><row><cell>mean</cell><cell>270.47</cell><cell>18.89</cell><cell></cell><cell></cell><cell></cell></row><row><cell>STD</cell><cell>73.04</cell><cell>6.05</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>PCK@0.3 (%) for several bodyparts and all evaluated architectures on within domain horses.</figDesc><table><row><cell></cell><cell>Nose</cell><cell>Eye</cell><cell cols="5">Shoulder Wither Elbow NearFF OffFF</cell><cell>Hip</cell><cell cols="3">NearHH NearHF OffHF</cell></row><row><cell>MobileNetV2 0.35</cell><cell>90.7</cell><cell>94.1</cell><cell>97.6</cell><cell>96.9</cell><cell>96.7</cell><cell>92.3</cell><cell>93.7</cell><cell>96.4</cell><cell>94.1</cell><cell>94.2</cell><cell>92.5</cell></row><row><cell>MobileNetV2 0.5</cell><cell>94.1</cell><cell>96.1</cell><cell>99.2</cell><cell>98.3</cell><cell>98.0</cell><cell>93.8</cell><cell>95.4</cell><cell>96.7</cell><cell>97.2</cell><cell>97.2</cell><cell>97.0</cell></row><row><cell>MobileNetV2 0.75</cell><cell>96.0</cell><cell>97.5</cell><cell>99.2</cell><cell>98.0</cell><cell>99.0</cell><cell>96.6</cell><cell>96.8</cell><cell>98.8</cell><cell>97.6</cell><cell>98.0</cell><cell>97.4</cell></row><row><cell>MobileNetV2 1.0</cell><cell>97.7</cell><cell>98.8</cell><cell>99.7</cell><cell>99.1</cell><cell>99.0</cell><cell>97.6</cell><cell>97.3</cell><cell>99.4</cell><cell>98.4</cell><cell>98.5</cell><cell>98.9</cell></row><row><cell>ResNet 50</cell><cell>99.9</cell><cell>100.0</cell><cell>99.8</cell><cell>99.9</cell><cell>99.8</cell><cell>99.8</cell><cell>99.6</cell><cell>99.9</cell><cell>99.9</cell><cell>99.6</cell><cell>99.8</cell></row><row><cell>ResNet 101</cell><cell>99.9</cell><cell>100.0</cell><cell>99.9</cell><cell>99.8</cell><cell>99.9</cell><cell>99.8</cell><cell>99.7</cell><cell>99.8</cell><cell>99.9</cell><cell>99.7</cell><cell>99.9</cell></row><row><cell>EfficientNet-B0</cell><cell>99.7</cell><cell>99.9</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0</cell><cell>99.6</cell><cell>99.5</cell><cell>100.0</cell><cell>99.9</cell><cell>99.7</cell><cell>99.7</cell></row><row><cell>EfficientNet-B1</cell><cell>99.8</cell><cell>99.9</cell><cell>100.0</cell><cell>99.8</cell><cell>99.9</cell><cell>99.5</cell><cell>99.8</cell><cell>100.0</cell><cell>99.8</cell><cell>99.8</cell><cell>99.8</cell></row><row><cell>EfficientNet-B2</cell><cell>99.9</cell><cell>99.9</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0</cell><cell>99.8</cell><cell>99.7</cell><cell>99.9</cell><cell>99.8</cell><cell>99.7</cell><cell>99.7</cell></row><row><cell>EfficientNet-B3</cell><cell>99.9</cell><cell>99.9</cell><cell>99.9</cell><cell>99.9</cell><cell>99.9</cell><cell>99.7</cell><cell>99.6</cell><cell>99.7</cell><cell>99.8</cell><cell>99.6</cell><cell>99.9</cell></row><row><cell>EfficientNet-B4</cell><cell cols="2">100.0 100.0</cell><cell>99.9</cell><cell>99.8</cell><cell>99.9</cell><cell>99.6</cell><cell>99.7</cell><cell>99.9</cell><cell>99.7</cell><cell>99.8</cell><cell>99.8</cell></row><row><cell>EfficientNet-B5</cell><cell>99.9</cell><cell>99.9</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0</cell><cell>99.7</cell><cell>99.8</cell><cell>99.6</cell><cell>99.8</cell><cell>99.8</cell><cell>99.9</cell></row><row><cell>EfficientNet-B6</cell><cell>99.9</cell><cell>99.9</cell><cell>99.9</cell><cell>99.8</cell><cell>100.0</cell><cell>99.8</cell><cell>99.9</cell><cell>99.8</cell><cell>99.8</cell><cell>99.7</cell><cell>99.8</cell></row><row><cell></cell><cell cols="9">Table 5. PCK@0.3 (%) for several bodyparts and all architectures on out-of-domain horses.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="11">Nose Eye Shoulder Wither Elbow NearFF OffFF Hip NearHH NearHF OffHF</cell></row><row><cell cols="3">MobileNetV2 0.35 45.6 53.1</cell><cell>65.5</cell><cell>68.0</cell><cell>69.1</cell><cell>56.4</cell><cell>57.6</cell><cell>65.9</cell><cell>65.9</cell><cell>60.5</cell><cell>62.5</cell></row><row><cell>MobileNetV2 0.5</cell><cell cols="2">52.7 61.0</cell><cell>76.7</cell><cell>69.7</cell><cell>78.3</cell><cell>62.9</cell><cell>65.4</cell><cell>73.6</cell><cell>70.8</cell><cell>68.1</cell><cell>69.7</cell></row><row><cell cols="3">MobileNetV2 0.75 54.2 65.6</cell><cell>78.3</cell><cell>73.2</cell><cell>80.5</cell><cell>67.3</cell><cell>68.9</cell><cell>80.0</cell><cell>74.1</cell><cell>70.5</cell><cell>70.2</cell></row><row><cell>MobileNetV2 1.0</cell><cell cols="2">59.0 67.2</cell><cell>83.8</cell><cell>79.7</cell><cell>84.0</cell><cell>70.1</cell><cell>72.1</cell><cell>82.0</cell><cell>79.9</cell><cell>76.0</cell><cell>76.7</cell></row><row><cell>ResNet 50</cell><cell cols="2">68.2 73.6</cell><cell>85.4</cell><cell>85.8</cell><cell>88.1</cell><cell>72.6</cell><cell>70.2</cell><cell>89.2</cell><cell>85.7</cell><cell>77.0</cell><cell>74.1</cell></row><row><cell>ResNet 101</cell><cell cols="2">67.7 72.4</cell><cell>87.6</cell><cell>86.0</cell><cell>89.0</cell><cell>79.9</cell><cell>78.0</cell><cell>92.6</cell><cell>87.2</cell><cell>83.4</cell><cell>80.0</cell></row><row><cell>EfficientNet-B0</cell><cell cols="2">60.3 62.5</cell><cell>84.9</cell><cell>84.6</cell><cell>87.2</cell><cell>77.0</cell><cell>75.4</cell><cell>86.7</cell><cell>86.7</cell><cell>79.6</cell><cell>79.4</cell></row><row><cell>EfficientNet-B1</cell><cell cols="2">67.4 71.5</cell><cell>85.9</cell><cell>85.7</cell><cell>89.6</cell><cell>80.0</cell><cell>81.1</cell><cell>86.7</cell><cell>88.4</cell><cell>81.8</cell><cell>81.6</cell></row><row><cell>EfficientNet-B2</cell><cell cols="2">68.7 74.8</cell><cell>84.5</cell><cell>85.2</cell><cell>89.2</cell><cell>79.7</cell><cell>80.9</cell><cell>88.1</cell><cell>88.0</cell><cell>82.3</cell><cell>81.7</cell></row><row><cell>EfficientNet-B3</cell><cell cols="2">71.7 76.6</cell><cell>88.6</cell><cell>88.7</cell><cell>92.0</cell><cell>80.4</cell><cell>81.8</cell><cell>90.6</cell><cell>90.8</cell><cell>85.0</cell><cell>83.6</cell></row><row><cell>EfficientNet-B4</cell><cell cols="2">71.1 75.8</cell><cell>88.1</cell><cell>87.4</cell><cell>91.8</cell><cell>83.3</cell><cell>82.9</cell><cell>90.8</cell><cell>90.3</cell><cell>86.7</cell><cell>85.5</cell></row><row><cell>EfficientNet-B5</cell><cell cols="2">74.8 79.5</cell><cell>89.6</cell><cell>89.5</cell><cell>93.5</cell><cell>82.2</cell><cell>84.1</cell><cell>91.8</cell><cell>90.9</cell><cell>86.6</cell><cell>85.2</cell></row><row><cell>EfficientNet-B6</cell><cell cols="2">74.7 79.7</cell><cell>90.3</cell><cell>89.8</cell><cell>92.8</cell><cell>83.6</cell><cell>84.4</cell><cell>92.1</cell><cell>92.1</cell><cell>87.8</cell><cell>85.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Test performance on cow when trained on 90% of cow data</figDesc><table><row><cell></cell><cell>Normalized Error</cell></row><row><cell>MobileNetV2 0.35</cell><cell>0.136</cell></row><row><cell>MobileNetV2 1.0</cell><cell>0.093</cell></row><row><cell>ResNet 50</cell><cell>0.062</cell></row><row><cell>EfficientNet-B0</cell><cell>0.060</cell></row><row><cell>EfficientNet-B3</cell><cell>0.054</cell></row><row><cell cols="2">Table 7. Test performance on sheep when trained on 90% of sheep data</cell></row><row><cell></cell><cell>Normalized Error</cell></row><row><cell>MobileNetV2 0.35</cell><cell>0.385</cell></row><row><cell>MobileNetV2 1.0</cell><cell>0.248</cell></row><row><cell>ResNet 50</cell><cell>0.186</cell></row><row><cell>EfficientNet-B0</cell><cell>0.124</cell></row><row><cell>EfficientNet-B3</cell><cell>0.159</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Summary results for evaluation of all models on the Horse-C dataset. Results are averaged across all five severities and three validation splits of the data. Adaptive batch normalization (adapt) is crucial for attaining good performance compared to fixing the statistics during evaluation (base). Best viewed in the digital version. base adapt base adapt base adapt base adapt base adapt base adapt base adapt base adapt base</figDesc><table><row><cell>Net Type</cell><cell cols="2">mobilenet_v2_0.35</cell><cell cols="2">mobilenet_v2_1.0</cell><cell>resnet_50</cell><cell></cell><cell cols="2">efficientnet-b0</cell><cell cols="2">efficientnet-b3</cell></row><row><cell>Pretrained</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>True</cell></row><row><cell cols="3">Condition adapt base adapt Corruption</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>brightness</cell><cell cols="10">0.34 1.76 0.29 0.87 0.27 1.67 0.21 0.94 0.33 1.29 0.17 0.24 0.40 1.40 0.19 0.72 0.33 1.38 0.19 0.70</cell></row><row><cell>contrast</cell><cell cols="10">0.47 8.02 0.30 4.78 0.36 8.63 0.22 4.93 0.38 8.57 0.18 2.41 0.41 6.95 0.20 4.01 0.37 7.94 0.20 3.43</cell></row><row><cell>defocus_blur</cell><cell cols="10">0.81 3.22 0.69 2.20 0.61 3.51 0.66 3.35 0.83 3.44 0.60 1.54 0.67 2.05 0.51 2.54 0.71 2.64 0.54 2.01</cell></row><row><cell cols="11">elastic_transform 0.38 0.96 0.36 0.83 0.32 0.96 0.32 0.92 0.35 0.50 0.26 0.29 0.39 0.91 0.29 0.76 0.38 0.90 0.29 0.77</cell></row><row><cell>fog</cell><cell cols="10">1.57 6.62 0.41 1.55 1.17 7.20 0.30 2.23 1.09 7.51 0.26 0.56 1.63 5.31 0.27 1.15 1.28 6.80 0.25 1.11</cell></row><row><cell>frost</cell><cell cols="10">2.27 6.74 1.10 3.78 1.97 6.81 1.00 3.84 1.68 6.44 0.60 1.80 1.39 6.44 0.71 2.91 1.43 7.04 0.67 2.43</cell></row><row><cell>gaussian_noise</cell><cell cols="10">2.65 5.90 1.68 6.98 2.11 6.19 1.91 7.51 0.97 3.53 0.82 5.25 1.65 5.13 1.22 5.77 1.71 5.82 1.25 5.89</cell></row><row><cell>glass_blur</cell><cell cols="10">0.60 2.03 0.63 1.57 0.50 2.01 0.67 2.34 0.54 1.69 0.50 0.95 0.53 1.35 0.53 1.56 0.56 1.45 0.59 1.39</cell></row><row><cell>impulse_noise</cell><cell cols="10">2.36 5.75 1.73 6.88 1.86 6.07 1.91 7.46 0.83 3.47 0.81 5.56 1.45 4.83 0.89 5.46 1.46 5.80 0.86 5.71</cell></row><row><cell cols="11">jpeg_compression 0.64 1.32 0.52 1.12 0.50 1.49 0.48 1.30 0.39 0.62 0.34 0.39 0.51 1.10 0.43 1.06 0.47 1.13 0.45 1.00</cell></row><row><cell>motion_blur</cell><cell cols="10">0.83 2.81 0.68 1.84 0.73 2.99 0.68 2.69 0.80 2.29 0.56 1.08 0.68 1.88 0.56 1.72 0.66 2.07 0.56 1.66</cell></row><row><cell>none</cell><cell cols="10">0.30 0.88 0.26 0.72 0.25 0.87 0.20 0.73 0.27 0.40 0.17 0.19 0.33 0.84 0.18 0.66 0.30 0.83 0.18 0.66</cell></row><row><cell>pixelate</cell><cell cols="10">0.34 0.99 0.33 0.84 0.28 0.96 0.28 0.99 0.31 0.47 0.23 0.28 0.35 0.89 0.27 0.78 0.33 0.86 0.27 0.76</cell></row><row><cell>shot_noise</cell><cell cols="10">2.27 5.31 1.29 6.52 1.65 5.57 1.29 6.95 0.72 2.83 0.63 4.40 1.28 4.55 0.82 4.94 1.32 5.63 0.80 4.90</cell></row><row><cell>snow</cell><cell cols="10">0.89 4.14 0.82 2.55 0.75 4.38 0.76 3.55 0.71 4.89 0.46 1.69 0.70 3.51 0.53 1.75 0.63 4.32 0.51 1.79</cell></row><row><cell>zoom_blur</cell><cell cols="10">0.98 2.34 0.82 1.74 0.88 2.58 0.89 2.39 0.93 2.16 0.69 1.11 0.93 1.75 0.70 1.60 1.02 1.95 0.71 1.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>Full result table on Horse-C. All results are averaged across the three validation splits. "none" denotes the uncorrupted Horse-10 dataset. Best viewed in the digital version. adapt base adapt base adapt base adapt base adapt base adapt base adapt base adapt base adapt base Corruption Severity</figDesc><table><row><cell cols="2">Net Type mobilenet_v2_0.35</cell><cell cols="2">mobilenet_v2_1.0</cell><cell>resnet_50</cell><cell></cell><cell cols="2">efficientnet-b0</cell><cell cols="2">efficientnet-b3</cell></row><row><cell>Pretrained False</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>True</cell></row><row><cell>Condition adapt base</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .Table 11 .Table 12 .</head><label>101112</label><figDesc>Improvements using batch adaptation on the Horse-C Noise and Blur corruption subsets for a pre-trained ResNet50 model. Improvements using batch adaptation on the Horse-C Weather and Digital corruptions subsets for a pre-trained ResNet50 model. Small improvements by using batch adaptation on the identity shift task for a pre-trained ResNet50 model. Note that the o.o.d. performance is still substantially worse (higher normalized error) than the within-domain performance.</figDesc><table><row><cell>Noise</cell><cell></cell><cell></cell><cell cols="4">Base Adapt</cell><cell cols="2">∆abs ∆rel</cell><cell>Blur</cell><cell>Base Adapt</cell><cell>∆abs ∆rel</cell></row><row><cell>Corruption</cell><cell></cell><cell>Severity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Corruption</cell><cell>Severity</cell></row><row><cell cols="3">Gaussian Noise 1</cell><cell cols="2">0.427</cell><cell cols="4">0.138 0.289 67.7%</cell><cell>Defocus Blur 1</cell><cell>0.137</cell><cell>0.100 0.037 27.0%</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="2">2.187</cell><cell cols="4">0.201 1.986 90.8%</cell><cell>2</cell><cell>0.169</cell><cell>0.127 0.042 24.9%</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell cols="2">5.556</cell><cell cols="4">0.314 5.242 94.3%</cell><cell>3</cell><cell>0.369</cell><cell>0.233 0.136 36.9%</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell cols="2">7.843</cell><cell cols="4">0.649 7.194 91.7%</cell><cell>4</cell><cell>1.569</cell><cell>0.446 1.123 71.6%</cell></row><row><cell></cell><cell></cell><cell>5</cell><cell cols="2">8.894</cell><cell cols="4">1.410 7.484 84.1%</cell><cell>5</cell><cell>3.480</cell><cell>0.763 2.717 78.1%</cell></row><row><cell cols="2">Impulse Noise</cell><cell>1</cell><cell cols="2">1.079</cell><cell cols="4">0.201 0.878 81.4%</cell><cell>Motion Blur</cell><cell>1</cell><cell>0.213</cell><cell>0.160 0.053 24.9%</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="2">3.432</cell><cell cols="4">0.276 3.156 92.0%</cell><cell>2</cell><cell>0.290</cell><cell>0.224 0.066 22.8%</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell cols="2">5.393</cell><cell cols="4">0.360 5.033 93.3%</cell><cell>3</cell><cell>0.340</cell><cell>0.335 0.005 1.5%</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell cols="2">7.839</cell><cell cols="4">0.663 7.176 91.5%</cell><cell>4</cell><cell>0.864</cell><cell>0.501 0.363 42.0%</cell></row><row><cell></cell><cell></cell><cell>5</cell><cell cols="2">8.923</cell><cell cols="4">1.339 7.584 85.0%</cell><cell>5</cell><cell>1.596</cell><cell>0.645 0.951 59.6%</cell></row><row><cell>Shot Noise</cell><cell></cell><cell>1</cell><cell cols="2">0.191</cell><cell cols="4">0.114 0.077 40.3%</cell><cell>Zoom Blur</cell><cell>1</cell><cell>0.331</cell><cell>0.288 0.043 13.0%</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="2">0.986</cell><cell cols="4">0.152 0.834 84.6%</cell><cell>2</cell><cell>0.536</cell><cell>0.436 0.100 18.7%</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell cols="2">3.618</cell><cell cols="4">0.244 3.374 93.3%</cell><cell>3</cell><cell>0.654</cell><cell>0.467 0.187 28.6%</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell cols="2">7.225</cell><cell cols="4">0.516 6.709 92.9%</cell><cell>4</cell><cell>0.974</cell><cell>0.620 0.354 36.3%</cell></row><row><cell></cell><cell></cell><cell>5</cell><cell cols="2">8.365</cell><cell cols="4">0.894 7.471 89.3%</cell><cell>5</cell><cell>1.217</cell><cell>0.640 0.577 47.4%</cell></row><row><cell>Weather</cell><cell></cell><cell cols="4">Base Adapt</cell><cell cols="2">∆abs ∆rel</cell><cell>Digital</cell><cell>Base Adapt</cell><cell>∆abs ∆rel</cell></row><row><cell cols="3">Corruption Severity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Corruption</cell><cell>Severity</cell></row><row><cell cols="2">Brightness 1</cell><cell cols="2">0.120</cell><cell cols="5">0.084 0.036 30.0%</cell><cell>Contrast</cell><cell>1</cell><cell>0.151</cell><cell>0.085 0.066 43.7%</cell></row><row><cell></cell><cell>2</cell><cell cols="2">0.127</cell><cell cols="5">0.083 0.044 34.6%</cell><cell>2</cell><cell>0.211</cell><cell>0.084 0.127 60.2%</cell></row><row><cell></cell><cell>3</cell><cell cols="2">0.141</cell><cell cols="5">0.084 0.057 40.4%</cell><cell>3</cell><cell>0.840</cell><cell>0.083 0.757 90.1%</cell></row><row><cell></cell><cell>4</cell><cell cols="2">0.165</cell><cell cols="5">0.089 0.076 46.1%</cell><cell>4</cell><cell>3.700</cell><cell>0.085 3.615 97.7%</cell></row><row><cell></cell><cell>5</cell><cell cols="2">0.205</cell><cell cols="5">0.097 0.108 52.7%</cell><cell>5</cell><cell>6.406</cell><cell>0.103 6.303 98.4%</cell></row><row><cell>Fog</cell><cell>1</cell><cell cols="2">0.156</cell><cell cols="5">0.097 0.059 37.8%</cell><cell>Elastic Transform</cell><cell>1</cell><cell>0.121</cell><cell>0.092 0.029 24.0%</cell></row><row><cell></cell><cell>2</cell><cell cols="2">0.191</cell><cell cols="5">0.107 0.084 44.0%</cell><cell>2</cell><cell>0.127</cell><cell>0.101 0.026 20.5%</cell></row><row><cell></cell><cell>3</cell><cell cols="2">0.289</cell><cell cols="5">0.126 0.163 56.4%</cell><cell>3</cell><cell>0.139</cell><cell>0.116 0.023 16.5%</cell></row><row><cell></cell><cell>4</cell><cell cols="2">0.330</cell><cell cols="5">0.137 0.193 58.5%</cell><cell>4</cell><cell>0.154</cell><cell>0.133 0.021 13.6%</cell></row><row><cell></cell><cell>5</cell><cell cols="2">0.764</cell><cell cols="5">0.176 0.588 77.0%</cell><cell>5</cell><cell>0.175</cell><cell>0.157 0.018 10.3%</cell></row><row><cell>Frost</cell><cell>1</cell><cell cols="2">0.193</cell><cell cols="5">0.125 0.068 35.2%</cell><cell>Jpeg Compression 1</cell><cell>0.136</cell><cell>0.108 0.028 20.6%</cell></row><row><cell></cell><cell>2</cell><cell cols="2">0.672</cell><cell cols="5">0.249 0.423 62.9%</cell><cell>2</cell><cell>0.152</cell><cell>0.128 0.024 15.8%</cell></row><row><cell></cell><cell>3</cell><cell cols="2">1.447</cell><cell cols="5">0.393 1.054 72.8%</cell><cell>3</cell><cell>0.170</cell><cell>0.138 0.032 18.8%</cell></row><row><cell></cell><cell>4</cell><cell cols="2">1.680</cell><cell cols="5">0.449 1.231 73.3%</cell><cell>4</cell><cell>0.216</cell><cell>0.189 0.027 12.5%</cell></row><row><cell></cell><cell>5</cell><cell cols="2">2.375</cell><cell cols="5">0.573 1.802 75.9%</cell><cell>5</cell><cell>0.305</cell><cell>0.276 0.029 9.5%</cell></row><row><cell>Snow</cell><cell>1</cell><cell cols="2">0.229</cell><cell cols="5">0.155 0.074 32.3%</cell><cell>Pixelate</cell><cell>1</cell><cell>0.117</cell><cell>0.087 0.030 25.6%</cell></row><row><cell></cell><cell>2</cell><cell cols="2">0.737</cell><cell cols="5">0.252 0.485 65.8%</cell><cell>2</cell><cell>0.117</cell><cell>0.089 0.028 23.9%</cell></row><row><cell></cell><cell>3</cell><cell cols="2">0.720</cell><cell cols="5">0.270 0.450 62.5%</cell><cell>3</cell><cell>0.125</cell><cell>0.100 0.025 20.0%</cell></row><row><cell></cell><cell>4</cell><cell cols="2">1.873</cell><cell cols="5">0.386 1.487 79.4%</cell><cell>4</cell><cell>0.142</cell><cell>0.112 0.030 21.1%</cell></row><row><cell></cell><cell>5</cell><cell cols="2">2.146</cell><cell cols="5">0.348 1.798 83.8%</cell><cell>5</cell><cell>0.156</cell><cell>0.132 0.024 15.4%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Base Adapt</cell><cell>∆abs ∆rel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Identity (wd)</cell><cell>0.115</cell><cell>0.086 0.029 25.2%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Identity (ood) 0.271</cell><cell>0.247 0.024 8.9%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Longitudinal development of equine conformation from weanling to age 3 years in the thoroughbred</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcilwraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Equine veterinary journal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="563" to="570" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5167" to="5176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluation of the leap motion controller as a new contactfree pointing device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rinkenauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="214" to="233" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-domain adaptation for animal pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9498" to="9507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DeCaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08883</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Rethinking imagenet pre-training. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pose-informed face alignment for extreme head pose variations in animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwa</forename><surname>Mahmoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeeperCut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">B</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Crall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Graving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Reinders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Vecsei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirka</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Vallentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semen</forename><surname>Zhydenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Pfeiffer</surname></persName>
		</author>
		<editor>Weng, Abner Ayala-Acevedo, Raphael Meudec, Matias Laporte</editor>
		<imprint>
			<date type="published" when="2020-02" />
			<pubPlace>Ben Cook, Ismael Fernández, François-Michel De Rainville</pubPlace>
		</imprint>
	</monogr>
	<note>et al. imgaug. https:// github.com/aleju/imgaug, 2020. Online; accessed 01</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Animalweb: A large-scale hierarchical dataset of annotated animal faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Muhammad Haris Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Shahabuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6939" to="6948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00414</idno>
		<title level="m">Similarity of neural network representations revisited</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2661" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
