<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Monocular 3D Vehicle Detection and Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hou-Ning</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sinovation Ventures AI Institute</orgName>
								<address>
									<addrLine>3 UC Berkeley 4 MIT 5</addrLine>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
						</author>
						<title level="a" type="main">Joint Monocular 3D Vehicle Detection and Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vehicle 3D extents and trajectories are critical cues for predicting the future location of vehicles and planning future agent ego-motion based on those predictions. In this paper, we propose a novel online framework for 3D vehicle detection and tracking from monocular videos. The framework can not only associate detections of vehicles in motion over time, but also estimate their complete 3D bounding box information from a sequence of 2D images captured on a moving platform. Our method leverages 3D box depth-ordering matching for robust instance association and utilizes 3D trajectory prediction for re-identification of occluded vehicles. We also design a motion learning module based on an LSTM for more accurate long-term motion extrapolation. Our experiments on simulation, KITTI, and Argoverse datasets show that our 3D tracking pipeline offers robust data association and tracking. On Argoverse, our imagebased method is significantly better for tracking 3D vehicles within 30 meters than the LiDAR-centric baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Autonomous driving motivates much of contemporary visual deep learning research. However, many commercially successful approaches to autonomous driving control rely on a wide array of views and sensors, reconstructing 3D point clouds of the surroundings before inferring object trajectories in 3D. In contrast, human observers have no difficulty in perceiving the 3D world in both space and time from simple sequences of 2D images rather than 3D point clouds, even though human stereo vision only reaches several meters. Recent progress in monocular object detection and scene segmentation offers the promise to make low-cost mobility widely available. In this paper, we explore architectures and datasets for developing similar capabilities using deep neural networks. * Work was done while Hou-Ning Hu, Qi-Zhi Cai and Ji Lin were at the Berkeley DeepDrive Center  Monocular 3D detection and tracking are inherently illposed. In the absence of depth measurements or strong priors, a single view does not provide enough information to estimate 3D layout of a scene accurately. Without a good layout estimate, tracking becomes increasingly difficult, especially in the presence of large ego-motion (e.g., a turning car). The two problems are inherently intertwined. Robust tracking helps 3D detection, as information along consecutive frames is integrated. Accurate 3D detection helps to track, as ego-motion can be factored out.</p><p>In this paper, we propose an online network architecture to jointly track and detect vehicles in 3D from a series of monocular images. <ref type="figure" target="#fig_1">Figure 1</ref> provides an overview of our 3D tracking and detection task. After detecting 2D bounding boxes of objects, we utilize both world coordinates and reprojected camera coordinates to associate instances across frames. Notably, we leverage novel occlusion-aware associ-ation and depth-ordering matching algorithms to overcome the occlusion and reappearance problems in tracking. Finally, we capture the movement of instances in a world coordinate system and update their 3D poses using LSTM motion estimation along a trajectory, integrating single-frame observations associated with the instance over time.</p><p>Like any deep network, our model is data hungry. The more data we feed it, the better it performs. However, existing datasets are either limited to static scenes <ref type="bibr" target="#b51">[51]</ref>, lack the required ground truth trajectories <ref type="bibr" target="#b34">[35]</ref>, or are too small to train contemporary deep models <ref type="bibr" target="#b15">[16]</ref>. To bridge this gap, we resort to realistic video games. We use a new pipeline to collect large-scale 3D trajectories, from a realistic synthetic driving environment, augmented with dynamic meta-data associated with each observed scene and object.</p><p>To the best of our knowledge, we are the first to tackle the estimation of complete 3D vehicle bounding box tracking information from a monocular camera. We jointly track the vehicles across frames based on deep features and estimate the full 3D information of the tracks including position, orientation, dimensions, and projected 3D box centers of each object. The depth ordering of the tracked vehicles constructs an important perceptual cue to reduce the mismatch rate. Our occlusion-aware data association provides a strong prior for occluded objects to alleviate the identity switch problem. Our experiments show that 3D information improves predicted association in new frames compared to traditional 2D tracking, and that estimating 3D positions with a sequence of frames is more accurate than single-frame estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Object tracking has been explored extensively in the last decade <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b49">49]</ref>. Early methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref> track objects based on correlation filters. Recent ConvNet-based methods typically build on pre-trained object recognition networks. Some generic object trackers are trained entirely online, starting from the first frame of a given video <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25]</ref>. A typical tracker will sample patches near the target object which are considered as foreground and some farther patches as background. These patches are then used to train a foreground-background classifier. However, these online training methods cannot fully utilize a large amount of video data. Held et al. <ref type="bibr" target="#b21">[22]</ref> proposed a regression-based method for offline training of neural networks, tracking novel objects at test-time at 100 fps. Siamese networks also found in use, including tracking by object verification <ref type="bibr" target="#b50">[50]</ref>, tracking by correlation <ref type="bibr" target="#b2">[3]</ref>, tracking by detection <ref type="bibr" target="#b12">[13]</ref>. Yu et al. <ref type="bibr" target="#b53">[53]</ref> enhance tracking by modeling a track-let into different states and explicitly learns an Markov Decision Process (MDP) for state transition. Due to the absence of 3D information, it just uses 2D location to decide whether a track-let is occluded.</p><p>All those methods only take 2D visual features into consideration, where the search space is restricted near the orig-inal position of the object. This works well for a static observer, but fails in a dynamic 3D environment. Here, we further leverage 3D information to narrow down the search space, and stabilize the trajectory of target objects.</p><p>Sharma et al. <ref type="bibr" target="#b48">[48]</ref> uses 3D cues for 2D vehicle tracking. Scheidegger et al. <ref type="bibr" target="#b47">[47]</ref> also adds 3D kalman filter on the 3D positions to get more consistent 3D localization results. Because the goals are for 2D tracking, 3D box dimensions and orientation are not considered. Osep et al. <ref type="bibr" target="#b36">[37]</ref> and Li et al. <ref type="bibr" target="#b30">[31]</ref> studies 3D bounding box tracking with stereo cameras. Because the 3D depth can be perceived directly, the task is much easier, but in many cases such as ADAS, large-baseline stereo vision is not possible. Object detection reaped many of the benefits from the success of convolutional representation. There are two mainstream deep detection frameworks: 1) two-step detectors: R-CNN <ref type="bibr" target="#b17">[18]</ref>, Fast R-CNN <ref type="bibr" target="#b16">[17]</ref>, and Faster R-CNN <ref type="bibr" target="#b40">[41]</ref>. 2) onestep detectors: YOLO <ref type="bibr" target="#b37">[38]</ref>, SSD <ref type="bibr" target="#b32">[33]</ref>, and YOLO9000 <ref type="bibr" target="#b38">[39]</ref>.</p><p>We apply Faster R-CNN, one of the most popular object detectors, as our object detection input. The above algorithms all rely on scores of labeled images to train on. In 3D tracking, this is no different. The more training data we have, the better our 3D tracker performs. Unfortunately, getting a large amount of 3D tracking supervision is hard. Driving datasets have attracted a lot of attention in recent years. KITTI <ref type="bibr" target="#b15">[16]</ref>, Cityscapes <ref type="bibr" target="#b9">[10]</ref>, Oxford RobotCar <ref type="bibr" target="#b33">[34]</ref>, BDD100K <ref type="bibr" target="#b57">[57]</ref>, NuScenes <ref type="bibr" target="#b5">[6]</ref>, and Argoverse <ref type="bibr" target="#b6">[7]</ref> provide well annotated ground truth for visual odometry, stereo reconstruction, optical flow, scene flow, object detection and tracking. However, their provided 3D annotation is very limited compared to virtual datasets. Accurate 3D annotations are challenging to obtain from humans and expensive to measure with 3D sensors like LiDAR. Therefore these real-world datasets are typically small in scale or poorly annotated.</p><p>To overcome this difficulty, there has been significant work on virtual driving datasets: virtual KITTI <ref type="bibr" target="#b14">[15]</ref>, SYN-THIA <ref type="bibr" target="#b44">[44]</ref>, GTA5 <ref type="bibr" target="#b42">[43]</ref>, VIPER <ref type="bibr" target="#b41">[42]</ref>, CARLA <ref type="bibr" target="#b10">[11]</ref>, and Free Supervision from Video Games (FSV) <ref type="bibr" target="#b25">[26]</ref>. The closest dataset to ours is VIPER <ref type="bibr" target="#b41">[42]</ref>, which provides a suite of videos and annotations for various computer vision problems while we focus on object tracking. We extend FSV <ref type="bibr" target="#b25">[26]</ref> to include object tracking in both 2D and 3D, as well as finegrained object attributes, control signals from driver actions.</p><p>In the next section, we describe how to generate 3D object trajectories from 2D dash-cam videos. Considering the practical requirement of autonomous driving, we primarily focus on online tracking systems, where only the past and current frames are accessible to a tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Joint 3D Detection and Tracking</head><p>Our goal is to track objects and infer their precise 3D location, orientation, and dimension from a single monocular video stream and a GPS sensor. <ref type="figure">Figure 2</ref> shows an overview  <ref type="figure">Figure 2</ref>: Overview of our monocular 3D tracking framework. Our online approach processes monocular frames to estimate and track region of interests (RoIs) in 3D (a). For each ROI, we learn 3D layout (i.e., depth, orientation, dimension, a projection of 3D center) estimation (b). With 3D layout, our LSTM tracker produces robust linking across frames leveraging occlusion-aware association and depth-ordering matching (c). With the help of 3D tracking, the model further refines the ability of 3D estimation by fusing object motion features of the previous frames (d).</p><p>of our system. Images are first passed through a detector network trained to generate object proposals and centers. These proposals are then fed into a layer-aggregating network which infers 3D information. Using 3D re-projection to generate similarity metric between all trajectories and detected proposals, we leverage estimated 3D information of current trajectories to track them through time. Our method also solves the occlusion problem in tracking with the help of occlusion-aware data association and depth-ordering matching. Finally, we re-estimate the 3D location of objects using the LSTM through the newly matched trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>We phrase the 3D tracking problem as a supervised learning problem. We aim to find N trajectories {τ 1 , . . . , τ N }, one for each object in a video. Each trajectory τ i links a sequence of detected object states {s</p><formula xml:id="formula_0">(i) a , s (i) a+1 , . . . , s (i) b }</formula><p>starting at the first visible frame a and ending at the last visible frame b. The state of an object at frame a is given by s a = (P, O, D, F, ∆P ), where P defines the 3D world location (x, y, z) of the object, and ∆P stands for its velocity (ẋ,ẏ,ż). O, D, F denotes for object orientation θ, dimension (l, w, h) and appearance feature f app , respectively. In addition, we reconstruct a 3D bounding box X for each object, with estimated P, O, D and the projection c = (x c , y c ) of 3D box's center in the image. The bounding boxes enable the use of our depth-ordering matching and occlusion-aware association. Each bounding box X also forms a projected 2D box M (X) = {x min , y min , x max , y max } projected onto a 2D image plane using camera parameters M = K[R|t].</p><p>The intrinsic parameter K can be obtained from camera calibration. The extrinsic parameter [R|t] can be calculated from the commonly equipped GPS or IMU sensor. The whole system is powered by a convolutional network pipeline trained on a considerable amount of ground truth supervision. Next, we discuss each component in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Candidate Box Detection</head><p>In the paper, we employ Faster R-CNN <ref type="bibr" target="#b40">[41]</ref> trained on our dataset to provide object proposals in the form of bounding boxes. Each object proposal <ref type="figure">(Figure 2</ref>(a)) corresponds to a 2D bounding box d = {x min , y min , x max , y max } as well as an estimated projection of the 3D box's center c. The detection results are used to locate the candidate vehicles and extract their appearance features. However, the centers of objects' 3D bounding boxes usually do not project directly to the center of their 2D bounding boxes. As a result, we have to provide an estimation of the 3D box center for better accuracy. More details about the estimation of the 3D center can be found in the supplementary material 1 . Projection of 3D box center. To estimate the 3D layout from single images more accurately, we extend the regression process to predict a projected 2D point of the 3D bounding box's center from an ROIpooled feature F using L1 loss. Estimating a projection of 3D center is crucial since a small gap in the image coordinate will cause a gigantic shift in 3D. It is worth noting that our pipeline can be used with any off-the-shelf detector and our 3D box estimation module is extendable to estimate projected 2D points even if the detector is replaced. With the extended ROI head, the model regresses both a bounding box d and the projection of 3D box's center c from an anchor point. ROIalign <ref type="bibr" target="#b20">[21]</ref> is used instead of ROIpool to get the regional representation given the detected regions of interest (ROIs). This reduces the misalignment of two-step quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Box Estimation</head><p>We estimate complete 3D box information <ref type="figure">(Figure 2</ref>(b)) from an ROI in the image via a feature representation of the pixels in the 2D bounding box. The ROI feature vector F is extracted from a 34-layer DLA-up <ref type="bibr" target="#b56">[56]</ref> using ROIalign. Each of the 3D information is estimated by passing the ROI features through a 3-layer 3x3 convolution sub-network, which extends the stacked Linear layers design of Mousavian et al. <ref type="bibr" target="#b35">[36]</ref>. We focus on 3D location estimation consisting of object center, orientation, dimension and depth, whereas <ref type="bibr" target="#b35">[36]</ref> focus on object orientation and dimension from 2D boxes. Besides, our approach integrates with 2D detection and has the potential to jointly training, while <ref type="bibr" target="#b35">[36]</ref> crops the input image with pre-computed boxes. This network is trained using ground truth depth, 3D bounding box center projection, dimension, and orientation values. A convolutional network is used to preserve spatial information. In the case that the detector is replaced with another architecture, the center c can be obtained from this sub-network. More details of c can be found at Appendix A. 3D World Location. Contrasting with previous approaches, we also infer 3D location P from monocular images. The network regresses an inverse depth value 1/d, but is trained to minimize the L1 loss of the depth value d and the projected 3D location P . A projected 3D location P is calculated using an estimated 2D projection of the 3D object center c as well as the depth d and camera transformation M . Vehicle Orientation. Given the coordinate distancex = x c − w 2 to the horizontal center of an image and the focal length f , we can restore the global rotation θ in the camera coordinate from θ l with simple geometry, θ = (θ l + arctanx f ) mod 2π. Following <ref type="bibr" target="#b35">[36]</ref> for θ l estimation, we first classify the angle into two bins and then regress the residual relative to the bin center using Smooth L1 loss. Vehicle Dimension. In driving scenarios, the high variance of the distribution of the dimensions of different categories of vehicles (e.g., car, bus) results in difficulty classifying various vehicles using unimodal object proposals. Therefore, we regress a dimension D to the ground truth dimension over the object feature representation using L1 loss.</p><p>The estimation of an object's 3D properties provides us with an observation for its location P with orientation θ, dimension D and 2D projection of its 3D center c. For any new tracklet, the network is trained to predict monocular object state s of the object by leveraging ROI features. For any previously tracked object, the following association network is able to learn a mixture of a multi-view monocular 3D estimates by merging the object state from last visible frames and the current frame. First, we need to generate such a 3D trajectory for each tracked object in world coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Data Association and Tracking</head><p>Given a set of tracks {τ J , . . . , τ K } at frame a where 1 ≤ J ≤ K ≤ M from M trajectories, our goal is to associate each track with a candidate detection, spawn new tracks, or end a track <ref type="figure">(Figure 2</ref>(c)) in an online fashion.</p><p>We solve the data association problem by using a weighted bipartite matching algorithm. Affinities between tracks and new detections are calculated from two criteria: overlap between projections of current trajectories forward in time and bounding boxes candidates; and the similarity of the deep representation of the appearances of new and existing object detections. Each trajectory is projected forward in time using the estimated velocity of an object and camera ego-motion. Here, we assume that ego-motion is given by a sensor, like GPS, an accelerometer, gyro and/or IMU.</p><p>We define an affinity matrix A(τ a , s a ) between the information of an existing track τ a and a new candidate s a as a joint probability of appearance and location correlation.</p><formula xml:id="formula_1">A deep (τ a , s a ) = exp(−||F τa , F sa || 1 ) (1) A 2D (τ a , s a ) = d τa ∩ d sa d τa ∪ d a (2) A 3D (τ a , s a ) = M(X τa ) ∩ M(X sa ) M(X τa ) ∪ M(X sa ) ,<label>(3)</label></formula><p>where F τa , F sa are the concatenation of appearance feature f app , dimension D, center c, orientation θ and depth d. X τa and X sa are the tracked and predicted 3D bounding boxes, M is the projection matrix casting the bounding box to image coordinates, A 2D and A 3D is the Intersection of Union (IoU).</p><formula xml:id="formula_2">A(τ a , s a ) = w deep A app (τ a , s a ) + w 2D A 2D (τ a , s a ) + w 3D A 3D (τ a , s a )<label>(4)</label></formula><p>w deep , w 2D , w 3D are the weights of appearance, 2D overlap, and 3D overlap. We utilize a mixture of those factors as the affinity across frames, similar to the design of POI <ref type="bibr" target="#b55">[55]</ref>. Comparing to 2D tracking, 3D-oriented tracking is more robust to ego-motion, visual occlusion, overlapping, and re-appearances. When a target is temporally occluded, the corresponding 3D motion estimator can roll-out for a period of time and relocate 2D location at each new point in time via the camera coordinate transformation. Depth-Ordering Matching. We introduce instance depth ordering for assigning a detection to neighbor tracklets, which models the strong prior of relative depth ordering found in human perception. For each detection of interest (DOI), we consider potential associated tracklets in order of their depths. From the view of each DOI, we obtain the IOU  <ref type="figure">Figure 3</ref>: Illustration of depth-ordering matching. Given the tracklets and detections, we sort them into a list by depth order. For each detection of interest (DOI), we calculate the IOU between DOI and non-occluded regions of each tracklet. The depth order naturally provides higher probabilities to tracklets near the DOI.</p><p>of two non-occluded overlapping map from both ascending and descending ordering. To cancel out the ordering ambiguity of a distant tracklet, we filter out those tracklets with a larger distance to a DOI than a possible matching length. So Equation 3 becomes</p><formula xml:id="formula_3">A 3D (τ a , s a ) = 1 × φ(M(X τa )) ∩ M(X sa ) φ(M(X τa )) ∪ M(X sa ) ,<label>(5)</label></formula><p>where 1 denotes if the tracklets is kept after depth filtering, and the overlapping function</p><formula xml:id="formula_4">φ(·) = arg min x {x|ord(x) &lt; ord(x 0 )∀x 0 ∈ M(X τa ))}</formula><p>captures pixels of non-occluded tracklets region with the nearest depth order. It naturally provides higher probabilities of linking neighbor tracklets than those layers away. In this way, we obtain the data association problem of moving objects with the help of 3D trajectories in world coordinates. <ref type="figure">Figure 3</ref> depicts the pipeline of depth ordering. Finally, we solve data association using the Kuhn-Munkres algorithm.</p><p>Occlusion-aware Data Association. Similar to previous state-of-the-art methods <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b45">45]</ref>, we model the lifespan of a tracker into four major subspaces in MDP state space: {birth, tracked, lost, death}. For each new set of detections, the tracker is updated using pairs with the highest affinities score (Equation <ref type="formula" target="#formula_2">4</ref>). Each unmatched detection spawns a new tracklet; however, an unmatched tracklet is not immediately terminated, as tracklets can naturally disappear in occluded region and reappear later. We address the dynamic object inter-occlusion problem by separating a new state called "occluded" from a lost state. An object is considered occluded when covered by another object in the front with over 70% overlap. An occluded tracklet will not update its lifespan or its feature representation until it is clear from occlusion, but we still predict its 3D location using the estimated motion. <ref type="figure">Figure 4</ref> illustrates how the occlusion-aware association works. More details of data association can be found at Appendix B.</p><p>In the next subsection, we show how to estimate that distance leveraging the associated tracklet and bounding box using a deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Motion Model</head><p>Deep Motion Estimation and Update. To exploit the temporal consistency of certain vehicles, we associate the information across frames by using two LSTMs. We embed a 3D location P to a 64-dim location feature and use 128-dim hidden state LSTMs to keep track of a 3D location from the 64-dim output feature.</p><p>Prediction LSTM (P-LSTM) models dynamic object location in 3D coordinates by predicting object velocity from previously updated velocitiesṖ T −n:T −1 and the previous location P T − 1. We use previous n = 5 frames of vehicle velocity to model object motion and acceleration from the trajectory. Given the current expected location of the object from 3D estimation module, the Updating LSTM (U-LSTM) considers both currentP T and previously predicted locatioñ P T −1 to update the location and velocity <ref type="figure">(Figure 2(c)</ref>).</p><p>Modeling motion in 3D world coordinates naturally cancels out adverse effects of ego-motion, allowing our model to handle missed and occluded objects. The LSTMs continue to update the object state</p><formula xml:id="formula_5">s (i) a = s (i) a−1 + α(s * a − s (i) a−1 )</formula><p>using the observation of matched detection state s * a with an updating ratio α = 1 − A deep (τ i a , s * a ), while assuming a linear velocity model if there is no matched bounding box. Therefore, we model 3D motion <ref type="figure">(Figure 2(d)</ref>) in world coordinates allowing occluded tracklet to move along motion plausible paths while managing the birth and death of moving objects. More details can be found at Appendix C.</p><p>Concretely, our pipeline consists of a single-frame monocular 3D object detection model for object-level pose inference and recurrent neural networks for inter-frame object association and matching. We extend the region processing to include 3D estimation by employing multi-head modules for each object instance. We introduced occlusion-aware association to solve inter-object occlusion problem. For tracklet matching, depth ordering lowers mismatch rate by filtering out distant candidates from a target. The LSTM motion estimator updates the velocity and states of each object independent of camera movement or interactions with other objects. The final pipeline produces accurate and dense object trajectories in 3D world coordinate system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">3D Vehicle Tracking Simulation Dataset</head><p>It is laborious and expensive to annotate a large-scale 3D bounding box image dataset even in the presence of Li-DAR data, although it is much easier to label 2D bounding boxes on tens of thousands of videos <ref type="bibr" target="#b57">[57]</ref>. Therefore, no such dataset collected from real sensors is available to the research community. To resolve the data problem, we turn to driving simulation to obtain accurate 3D bounding box annotations at no cost of human efforts. Our data collection and annotation pipeline extend the previous works like VIPER <ref type="bibr" target="#b41">[42]</ref> and FSV <ref type="bibr" target="#b25">[26]</ref>, especially in terms of linking identities across frames. Details on the thorough comparison to prior data collection efforts, and dataset statistics can be found in the Appendix D.</p><p>Our simulation is based on Grand Theft Auto V, a modern game that simulates a functioning city and its surroundings in a photo-realistic three-dimensional world. To associate object instances across frames, we utilize in-game API to capture global instance id and corresponding 3D annotations directly. In contrast, VIPER leverages a weighted matching algorithm based on a heuristic distance function, which can lead to inconsistencies. It should be noted that our pipeline is real-time, providing the potential of large-scale data collection, while VIPER requires expensive off-line processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our 3D detection and tracking pipeline on Argoverse Tracking benchmark <ref type="bibr" target="#b6">[7]</ref>, KITTI MOT benchmark <ref type="bibr" target="#b15">[16]</ref> and our large-scale dataset, featuring real-world driving scenes and a wide variety of road conditions in a diverse virtual environment, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training and Evaluation</head><p>Dataset. Our GTA raw data is recorded at 12 FPS, which is helpful for temporal aggregation. With the goal of autonomous driving in mind, we focus on vehicles closer than 150m, and also filtered out the bounding boxes whose areas are smaller than 256 pixels. The dataset is then split into train, validation and test set with ratio 10 : 1 : 4. The KITTI Tracking benchmark provides real-world driving scenario. Our 3D tracking pipeline train on the whole training set and evaluate the performance on the public testing benchmark. The Argoverse Tracking benchmark offers novel 360 degree driving dataset. We train on the training set and evaluate the performance on the validation benchmark since the evaluation server is not available upon the time of submission. Training Procedure. We train our 3D estimation network (Section 3.3) on each training set, separately. 3D estimation network produces feature maps as the input of ROIalign <ref type="bibr" target="#b20">[21]</ref>. The LSTM motion module (Section 3.5) is trained on the same set with a sequence of 10 images per batch. For GTA, all the parameters are searched using validation set with detection bounding boxes from Faster R-CNN. The training is conducted for 100 epochs using Adam optimizer with an initial learning rate 10 −3 , momentum 0.9, and weight decay 10 −4 . Each GPU has 5 images and each image with no more than 300 candidate objects before NMS. More training details can be found in Appendix E. 3D Estimation. We adapt depth evaluation metrics <ref type="bibr" target="#b11">[12]</ref> from image-level to object-level, leveraging both error and accuracy metrics. Error metrics include absolute relative difference (Abs Rel), squared relative difference (Sq Rel), root mean square error (RMSE) and RMSE log. Accuracy metrics are percents of y i that max( yi</p><formula xml:id="formula_6">y * i , y * i yi ) &lt; δ where δ = 1.25, 1.25 2 , 1.25 3 .</formula><p>Following the setting of KITTI <ref type="bibr" target="#b15">[16]</ref>, we use orientation score (OS) for orientation evaluation.</p><p>We propose two metrics for evaluating estimated object dimension and 3D projected center position. A Dimension Score (DS) measures how close an object volume estimation to a ground truth. DS is defined as DS = min( Vpred Vgt , Vgt Vpred ) with an upper bound 1, where V is the volume of a 3D box by multiplying its dimension l * w * h. A Center Score (CS) measures distance of a projected 3D center and a ground truth. CS is calculated by CS = (1 + cos(a gt − a pd ))/2, with a upper bound 1, where a depicts an angular distance ((x gt −x pd )/w pd , (y gt −y pd )/h pd ), weighted by corresponding box width and height in the image coordinates. Object Tracking. We follow the metrics of CLEAR <ref type="bibr" target="#b1">[2]</ref>, including multiple object tracking accuracy (MOTA), multiple object tracking precision (MOTP), miss-match (MM), false positive (FP), and false negative (FN), etc. Overall Evaluation. We evaluated the 3D IoU mAP of 3D layout estimation with refined depth estimation of different tracking methods. The metric reflects the conjunction of all 3D components, dimension, rotation, and depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>3D for tracking. The ablation study of tracking performance could be found in <ref type="table">Table 1</ref>. Adding deep feature distinguishes two near-overlapping objects, our false negative (FN) rate drops with an observable margin. With depth-order   <ref type="table" target="#tab_4">Table 2</ref>.</p><p>matching and occlusion-aware association, our model filters out 6 − 8% possible mismatching trajectories. For a full ablation study, please refer to the Appendix F Motion Modeling. We propose to use LSTM to model the vehicle motion. To analyze its effectiveness, we compare our LSTM model with traditional 3D Kalman filter (KF3D) and single frame 3D estimation using 3D IoU mAP. <ref type="table" target="#tab_4">Table 2</ref> shows that KF3D provides a small improvement via trajectory smoothing within prediction and observation. On the other hand, our LSTM module gives a learned estimation based on past n velocity predictions and current frame observation, which may compensate for the observation error. Our LSTM module achieves the highest accuracy among the   <ref type="table" target="#tab_5">Table 3</ref>: Importance of using projection of 3D bounding box center estimation on KITTI training set. We evaluate our proposed model using different center inputs c to reveal the importance of estimating projection of a 3D center. The reduction of ID Switch (IDS), track fragmentation (FRAG), and the increase of MOTA suggest that the projection of a 3D center benefits our tracking pipeline over the 2D center.</p><p>other methods with all the IoU thresholds. 3D Center Projection Estimation. We estimate the 3D location of a bounding box through predicting the projection of its center and depth, while Mousavian et al. <ref type="bibr" target="#b35">[36]</ref> uses the center of detected 2D boxes directly.   <ref type="table">Table 5</ref>: Tracking performance on the validation set of Argoverse tracking benchmark <ref type="bibr" target="#b6">[7]</ref>. Note that the LiDAR-centric baseline <ref type="bibr" target="#b6">[7]</ref> uses LiDAR sweeps, HD maps for evaluation.  comparison of these two methods on KITTI dataset. The result indicates the correct 3D projections provides higher tracking capacity for motion module to associate candidates and reduces the ID switch (IDS) significantly. Amount of Data Matters. We train the depth estimation module with 1%, 10%, 30% and 100% training data. The results show how we can benefit from more data in <ref type="table" target="#tab_6">Table 4</ref>, where there is a consistent trend of performance improvement as the amount of data increases. The trend of our results with a different amount of training data indicates that large-scale 3D annotation is helpful, especially with accurate ground truth of far and small objects.</p><formula xml:id="formula_7">Methods MOTA ↑ MOTP ↑ MT ↑ ML ↓ FP ↓ FN ↓<label>Ours</label></formula><p>Real-world Evaluation. Besides evaluating on synthetic data, we resort to Argoverse <ref type="bibr" target="#b6">[7]</ref> and KITTI <ref type="bibr" target="#b15">[16]</ref> tracking benchmarks to compare our model abilities. For Argoverse, we use Faster R-CNN detection results of mmdetection <ref type="bibr" target="#b7">[8]</ref> implementation pre-trained on COCO <ref type="bibr" target="#b31">[32]</ref> dataset. Major results are listed in <ref type="table" target="#tab_8">Table 5 and Table 6</ref>. For a full evaluation explanation, please refer to the Appendix F. Our monocular 3D tracking method outperforms all the published methods on KITTI and beats LiDAR-centric baseline methods on 50m and 30m ranges of the Argoverse tracking validation set upon the time of submission. It is worth noting that the baseline methods on Argoverse tracking benchmark leveraging HD maps, locations, and Li-DAR points for 3D detection, in addition to images. Our monocular 3D tracking approach can reach competitive results with image stream only. It is interesting to see that the 3D tracking accuracy based on images drops much faster with increasing range threshold than LiDAR-based method. This is probably due to different error characteristics of the two measurement types. The farther objects occupy smaller number of pixels, leading to bigger measurement variance on the images. However, the distance measurement errors of LiDAR hardly change for faraway objects. At the same time, the image-based method can estimate the 3D positions and associations of nearby vehicles accurately. The comparison reveals the potential research directions to combine imaging and LiDAR signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we learn 3D vehicle dynamics from monocular videos. We propose a novel framework, combining spatial visual feature learning and global 3D state estimation, to track moving vehicles in a 3D world. Our pipeline consists of a single-frame monocular 3D object inference model and motion LSTM for inter-frame object association and updating. In data association, we introduced occlusion-aware association to solve inter-object occlusion problem. In tracklet matching, depth ordering filters out distant candidates from a target. The LSTM motion estimator updates the velocity of each object independent of camera movement. Both qualitative and quantitative results indicate that our model takes advantage of 3D estimation leveraging our collection of dynamic 3D trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Foreword</head><p>This appendix provides technical details about our monocular 3D detection and tracking network and our virtual GTA dataset, and more qualitative and quantitative results. Section A shows the importance of learning a 3D center projection. Section B provides more details about occlusion aware association. Section C will talk about details of two LSTMs predicting and updating in 3D coordinates. Section D offers frame-and object-based statistical summaries. Section E describes our training procedure and network setting of each dataset. Section F illustrates various comparisons, including ablation, inference time and network settings, of our method on GTA, Argoverse and KITTI data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Projection of 3D Center</head><p>We estimate the vehicle 3D location by first estimating the depth of the detected 2D bounding boxes. Then the 3D position is located based on the observer's pose and camera calibration. We find that accurately estimating the vehicle 3D center and its projection is critical for accurate 3D box localization. However, the 2D bounding box center can be far away from the projection of 3D center for several reasons. First, there is always a shift from the 2D box center if the 3D bounding box is not axis aligned in the observer's local coordinate system. Second, the 2D bounding box is only detecting the visible area of the objects after occlusion and truncation, while the 3D bounding box is defined on the object's full physical dimensions. The projected 3D center can be even out of the detected 2D boxes. For instance, the 3D bounding box of a truncated object can be out of the camera view. These situations are illustrated in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visible Invisible</head><p>2D Center 3D re-projection 2D projection 3D Center <ref type="figure">Figure 6</ref>: The illustration of the difference in 2D bounding box and 3D projected centers. A visible 2D center projection point may wrongly locate above or under the ground plane in the 3D coordinates. Other states, occluded or truncated, would inevitably suffer from the center shift problem. The center shift causes misalignment for GT and predicted tracks and harms 3D IOU AP performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Association Details</head><p>Due to the length limitation in the main paper, we supplement more details of data association here. Occlusion-Aware Association. We continue to predict the 3D location of unmatched tracklets until they disappear from our tracking range (e.g. 0.15m to 100m) or die out after 20 time-steps. The sweeping scheme benefits tracker from consuming a huge amount of memory to store all the occluded tracks.</p><p>Depth-Ordering Matching. The equation mentioned in the main paper</p><formula xml:id="formula_8">A 3D (τ a , s a ) = 1 × φ(M(X τa )) ∩ M(X sa ) φ(M(X τa )) ∪ M(X sa ) ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">1 = (d τa − d sa ) &lt; l τa + w τa + l sa + w sa</formula><p>denotes if the tracklets is kept after depth filtering, which can be interpreted as a loose bound</p><formula xml:id="formula_10">l 2 τa + w 2 τa + l 2 sa + w 2 sa &lt; l τa + w τa + l sa + w sa</formula><p>of two car within a reachable distance range, i.e. the diagonal length of two car. The overlapping function</p><formula xml:id="formula_11">φ(·) = arg min x {x|ord(x) &lt; ord(x 0 )∀x 0 ∈ M(X τa ))}</formula><p>captures pixels of non-occluded tracklets region with the nearest depth order ord(·). Note that the ord(·) returns the same order if the distance is within 1 meter. Tracklets closer to the DOI occlude the overlapped area of farther ones. So each DOI matches to the tracklet with highest IOU while implicitly modeling a strong prior of depth sensation called "Occultation".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Motion Model</head><p>Kalman Filter. In our experiments, we compare a variant of approaches in estimating locations. 2D Kalman filter (KF2D) models {x, y, s, a, ∆x, ∆y, ∆a} in 2D coordinates, where s stands for the width/high ratio and a for the area of the object. KF2D often misses tracking due to substantial dynamic scene changes from sudden egomotion. We also propose a 3D Kalman filter (KF3D) which models {x, y, z, ∆x, ∆y, ∆z} in 3D coordinates. However, Kalman filter tries to find a position in between observation and prediction location that minimize a mean of square error, while LSTM does not be bounded in such a restriction.</p><p>Deep Motion. The proposed final motion model consists of two LSTMs, one for predicting and the other for updating. The latter is in charge of refining 3D locations. Predicting LSTM (P-LSTM) learns a velocity in 3D {∆x, ∆y, ∆z} using hidden state based on the previously n updated velocitiesṖ T −n:T −1 and location P T −1 and predicts a smoothed step toward an estimated locationP T − 1. We embed velocity for the past n = 5 consecutive frames with an initial value of all 0 to model object motion and acceleration from the trajectory. As the next time step comes, the velocity array updates a new velocity into the array, and discards the oldest one.</p><p>For Updating LSTM (U-LSTM) module, we encode both predicted locationP T −1 and current single-frame observa-tionP T into two location features. By concatenating them together, we obtained a 128-dim feature as the input of U-LSTM. The LSTM learns a smooth estimation of location difference from the observationP T to the predicted loca-tionP T −1 and updates the 3D location P T and velocitẏ P T −n+1:T from hidden state leveraging previous observations.</p><p>Both LSTM modules are trained with ground truth location projected using the depth value, a projection of 3D center, camera intrinsic and extrinsic using 2 losses. The L1 loss L 1 = |Ṗ T −Ṗ T −1 | reduces the distance of estimated and ground truth location. The linear motion loss L linear = (1 − cos(Ṗ T ,Ṗ T −1 )) + L1(Ṗ T ,Ṗ T −1 ) focuses on the smooth transition of location estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dataset Statistics</head><p>To help understand our dataset and its difference, we show more statistics. Dataset Comparison. <ref type="table" target="#tab_10">Table 7</ref> demonstrates a detailed comparison with related datasets, including detection, tracking, and driving benchmarks. KITTI-D <ref type="bibr" target="#b15">[16]</ref> and KAIST <ref type="bibr" target="#b23">[24]</ref> are mainly detection datasets, while KITTI-T <ref type="bibr" target="#b15">[16]</ref>, MOT15 <ref type="bibr" target="#b27">[28]</ref>, MOT16 <ref type="bibr" target="#b34">[35]</ref>, and UA-DETRAC <ref type="bibr" target="#b51">[51]</ref> are primarily 2D tracking benchmarks. The common drawback could be the limited scale, which cannot meet the growing demand for training data. Compared to related synthetic dataset, Virtual KITTI <ref type="bibr" target="#b14">[15]</ref> and VIPER <ref type="bibr" target="#b41">[42]</ref>, we additionally provide finegrained attributes of object instances, such as color, model, maker attributes of vehicle, motion and control signals, which leaves the space for imitation learning system. Weather and Time of Day. <ref type="figure">Figure 7</ref> shows the distribution of weather, hours of our dataset. It features a full weather cycle and time of a day in a diverse virtual environment. By collecting various weather cycles <ref type="figure">(Figure 7a</ref>), our model learns to track with a higher understanding of environments. With different times of a day <ref type="figure">(Figure 7b</ref>), the network handles changeable perceptual variation. Number of Instances in Each Category. The vehicle diversity is also very large in the GTA world, featuring 15 fine-grained subcategories. We analyzed the distribution of the 15 subcategories in <ref type="figure">Figure 8a</ref>. Besides instance categories, we also show the distribution of occluded ( <ref type="figure">Figure 8c</ref>) and truncated <ref type="figure">(Figure 8b)</ref> instances to support the problem of partially visible in the 3D coordinates. Dataset Statistics. Compared to the others, our dataset has more diversity regarding instance scales <ref type="figure">(Figure 9a</ref>) and closer instance distribution to real scenes <ref type="figure">(Figure 9b</ref>). Examples of Our Dataset. <ref type="figure" target="#fig_1">Figure 10</ref> shows some visual examples in different time, weather and location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training Details</head><p>We apply different training strategies to our model to optimize the performance on different datasets. Training Procedure. We train on 4 GPUs (effective minibatch size is 20   <ref type="figure">Figure 9</ref>: Statistical summary of our dataset in comparison of KITTI <ref type="bibr" target="#b15">[16]</ref>, VKITTI <ref type="bibr" target="#b14">[15]</ref>, VIPER <ref type="bibr" target="#b41">[42]</ref>, and Cityscapes <ref type="bibr" target="#b9">[10]</ref> we estimate both 2D detection and 3D center on the Faster RCNN pr-trained on Pascal VOC dataset. For KITTI, we use RRC <ref type="bibr" target="#b39">[40]</ref> detection results following a state-of-the-art method, BeyondPixels <ref type="bibr" target="#b48">[48]</ref>. For Argoverse, we use Faster RCNN detection results of mmdetection <ref type="bibr" target="#b7">[8]</ref> implementation pre-trained on COCO <ref type="bibr" target="#b31">[32]</ref> dataset. Since both the results do not come with projected 3D centers, we estimate the center c from the network in Section 3.3 of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experiments</head><p>Visual Odometry vs Ground Truth. To analyze the impact of ego-motion on tracking, we compare tracking results based on poses from GT camera and Mono-VO approach in  <ref type="table" target="#tab_11">Table 8</ref>: Ablation study of tracking performance with estimated (VO) or ground truth (GT) camera pose in KITTI training set. We find that VO can lead to similar results as GT.</p><p>Inference Time. The total inference time is 203 ms on a single P100 GPU (see <ref type="table">Table 9</ref> for details). Note that the inference time on KITTI benchmark focus only on the nondetection part (92 ms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection 3D Estimation Tracking</head><p>Run Time 0.111 0.062 0.030 <ref type="table">Table 9</ref>: Inference Time (second) of our proposed framework on KITTI tracking benchmark. Note that elapsed time for object detection is not included in the specified runtime of KITTI benchmark.</p><p>Tracking policy. In the association, we keep all tracklets until they disappear from our tracking range (e.g. 10m to 100m) or die out after 20 time-steps. <ref type="table" target="#tab_13">Table 10</ref> shows the ablation comparison of using location and max-age. Since the max-age affects the length of each tracklet, we prefer to keep the tracklet a longer period (i.e. &gt; 15) to re-identify possible detections using 3D location information. Data Association Weights. During our experiment, we use different weights of appearance, 2D overlap, and 3D overlap for corresponding methods. We select weight ratios based on the results of our validation set. For None and 2D, we use w app = 0.0,w 2D = 1.0, w 3D = Tracking Performance on KITTI dataset. As mentioned in the main paper, we resort to Argoverse <ref type="bibr" target="#b6">[7]</ref> and KITTI <ref type="bibr" target="#b15">[16]</ref>    Ablation Study. The ablation study of tracking performance can be found in <ref type="table" target="#tab_4">Table 12</ref>. From the ablation study, we observe the huge performance improvement, especially in track recall (TR) for a 8% and Accuracy (MOTA) for a 18% relative improvement, from 2D to 3D, which indicates 3D information helps to recover unmatched pairs of objects. We implement our 2D baselines deriving from Sort <ref type="bibr" target="#b3">[4]</ref>, Deep Sort <ref type="bibr" target="#b52">[52]</ref> tracker for inter-frame ROIs matching in 2D. Modeling motion in 3D coordinate is more robust in linking candidates than 2D motion. Moreover, adding deep feature distinguishes two near-overlapping objects, our false negative (FN) rate drops with an observable margin. With depth-order matching and occlusion-aware association, our model filters out 6 − 8% possible mismatching trajectories. Qualitative results. We show our evaluation results in <ref type="figure" target="#fig_1">Figure 11</ref> on our GTA test set and in <ref type="figure" target="#fig_1">Figure 12</ref> on Argoverse validation set. The method comparison on GTA shows that our proposed 3D Tracking outperforms the strong baseline which using 3D Kalman Filter.  <ref type="table" target="#tab_4">Table 12</ref>: Ablation study of tracking performance with different methods and features in GTA dataset. The upper half of the table shows evaluation of targets within 100 meters while the lower within 150 meters. Modeling motion in 3D coordinate with the help of our depth estimation is more robust in linking candidate tracklets than 2D motion. Using deep feature in correlation can reduce the false negative (FN) rate. Using depth-order matching and occlusion-aware association filter out relatively 6 − 8% mismatching trajectories. Replacing 3D kalman filter with LSTM module to re-estimate the location using temporal information.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Joint online detection and tracking in 3D. Our dynamic 3D tracking pipeline predicts 3D bounding box association of observed vehicles in image sequences captured by a monocular camera with an ego-motion sensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Frame</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of 3D Estimation on KITTI testing set. We show predicted 3D layout colored with tracking IDs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>The statistics of scene in our dataset. The statistics of object in our dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Examples of our dataset. 0.0. For 3D related methods, we give w 3D = 1.0 and w 2D = 0.0. For Deep related methods, we times w 2D or w 3D with 0.7 and 0.3 for w app .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Qualitative comparison results of 3D Kalman and our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Qualitative results of our method on Argoverse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Predict Update Associate Trackers Proposals Predict Update Associate Trackers Proposals Predict Update Associate Trackers Proposals</head><label></label><figDesc></figDesc><table><row><cell>Input</cell><cell>Region</cell><cell>Monocular</cell><cell>Deep</cell><cell>Multi-frame</cell></row><row><cell>Image</cell><cell>Proposals</cell><cell>3D Estimation</cell><cell>Association</cell><cell>Refinement</cell></row><row><cell></cell><cell>Angle</cell><cell></cell><cell></cell><cell></cell></row><row><cell>T-2</cell><cell></cell><cell>Depth</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Angle</cell><cell></cell><cell></cell><cell></cell></row><row><cell>T-1</cell><cell></cell><cell>Depth</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Angle</cell><cell></cell><cell></cell><cell></cell></row><row><cell>T</cell><cell></cell><cell>Depth</cell><cell></cell><cell></cell></row><row><cell>Frame</cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>2D Center</cell><cell>315</cell><cell>497</cell><cell>401</cell><cell>1435</cell><cell>91.06</cell><cell>90.36</cell></row><row><cell>3D Center</cell><cell>190</cell><cell>374</cell><cell>401</cell><cell>1435</cell><cell>91.58</cell><cell>90.36</cell></row></table><note>Comparison of tracking performance on 3D IoU AP 25, 50, 70 in GTA dataset. Noted that KF3D slightly improves the AP performance compare to single-frame es- timation (None), while LSTM grants a clear margin. The difference comes from how we model object motion in the 3D coordinate. Instead of using Kalman filter smoothing be- tween prediction and observation, we directly model vehicle movement using LSTMs.IDS ↓ FRAG ↓ FP ↓ FN ↓ MOTA ↑ MOTP ↑</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Sq Rel ↓ RMSE ↓ RMSElog ↓ δ &lt; 1.25 ↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑</figDesc><table><row><cell>shows the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance of 3D box estimation. The evaluation demonstrates the effectiveness of our model from each separate metrics. The models are trained and tested on the same type of dataset, either GTA or KITTI. With different amounts of training data in our GTA dataset, the results suggest that large data capacity benefits the performance of a data-hungry network.</figDesc><table><row><cell cols="2">Range Method</cell><cell cols="2">MOTA ↑ MM ↓</cell><cell>#FP ↓</cell><cell>#FN ↓</cell></row><row><cell>30m</cell><cell>LiDAR [7] Ours</cell><cell>73.02 77.93</cell><cell cols="2">19.75 5.29 104.29 92.80</cell><cell>350.50 395.33</cell></row><row><cell>50m</cell><cell>LiDAR [7] Ours</cell><cell>52.74 53.48</cell><cell cols="3">31.60 12.25 194.67 99.70 1308.25 857.08</cell></row><row><cell>100m</cell><cell>LiDAR [7] Ours</cell><cell>37.98 15.59</cell><cell cols="3">32.55 105.40 2455.30 19.83 338.54 1572.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Tracking performance on the testing set of KITTI tracking benchmark. Only published methods are reported.</figDesc><table /><note>† sign means 3D information is used.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>). To keep the original image resolution can be divided by 32, we use 1920 × 1080 resolution for GTA, 1920 × 1216 for Argoverse, and 1248 × 384 for KITTI. The anchors of RPN in Faster R-CNN span 4 scales and 3 ratios, in order to detect the small objects at a distance. For GTA,</figDesc><table><row><cell>Dataset</cell><cell cols="8">Task Object Frame Track Boxes 3D Weather Occlusion Ego-Motion</cell></row><row><cell>KITTI-D [16]</cell><cell>D</cell><cell>C,P</cell><cell>7k</cell><cell>-</cell><cell>41k</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>KAIST [24]</cell><cell>D</cell><cell>P</cell><cell>50k</cell><cell>-</cell><cell>42k</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>KITTI-T [16]</cell><cell>T</cell><cell>C</cell><cell>8k</cell><cell>938</cell><cell>65k</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>MOT15-3D [28]</cell><cell>T</cell><cell>P</cell><cell>974</cell><cell>29</cell><cell>5632</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>MOT15-2D [28]</cell><cell>T</cell><cell>P</cell><cell>6k</cell><cell>500</cell><cell>40k</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MOT16 [35]</cell><cell>T</cell><cell>C,P</cell><cell>5k</cell><cell>467</cell><cell>110k</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="2">UA-DETRAC [51] D,T</cell><cell>C</cell><cell>84k</cell><cell>6k</cell><cell>578k</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="2">Virtual KITTI [15] D,T</cell><cell>C</cell><cell>21k</cell><cell>2610</cell><cell>181k</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VIPER [42]</cell><cell>D,T</cell><cell>C,P</cell><cell>184k</cell><cell>8272</cell><cell>5203k</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>D,T</cell><cell>C,P</cell><cell>688k</cell><cell cols="2">325k 10068k</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison to related dataset for detection and tracking (Upper half: real-world, Lower half: synthetic). We only count the size and types of annotations for training and validation (D=detection, T=tracking, C=car, P=pedestrian). To our knowledge, our dataset is the largest 3D tracking benchmark for dynamic scene understanding, with control signals of driving, sub-categories of object.</figDesc><table><row><cell>(a) Instance scale</cell><cell>(b) Instances per image</cell></row><row><cell>(c) Frames of tracking</cell><cell>(d) Vehicle distance</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>The proposed method maintains similar MOTA using estimated camera pose. Note that 3D tracking is compensated with GT ego-motion in the text.</figDesc><table><row><cell cols="4">DATA POSE MOTA↑ MOTP↑ MM↓</cell></row><row><cell>KITTI</cell><cell>VO GT</cell><cell>71.079 71.215</cell><cell>88.233 0.784 88.210 0.642</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Comparison of performance using different location and maxage in our tracking pipeline. The hyphenrepresents not using the information in tracking.real-world scenario. Our monocular 3D tracking method outperforms all the published methods upon the time of submission. Results are listed inTable 11.Benchmark MOTA ↑ MOTP ↑ MT ↑ ML ↓ FP ↓ FN ↓</figDesc><table><row><cell>Ours</cell><cell>84.52</cell><cell>85.64 73.38</cell><cell>2.77</cell><cell>705 4242</cell></row><row><cell>BeyondPixels  † [48]</cell><cell>84.24</cell><cell>85.73 73.23</cell><cell>2.77</cell><cell>705 4247</cell></row><row><cell>PMBM  † [47]</cell><cell>80.39</cell><cell>81.26 62.77</cell><cell cols="2">6.15 1007 5616</cell></row><row><cell>extraCK [19]</cell><cell>79.99</cell><cell>82.46 62.15</cell><cell>5.54</cell><cell>642 5896</cell></row><row><cell>MCMOT-CPD [29]</cell><cell>78.90</cell><cell cols="2">82.13 52.31 11.69</cell><cell>316 6713</cell></row><row><cell>NOMT* [9]</cell><cell>78.15</cell><cell cols="3">79.46 57.23 13.23 1061 6421</cell></row><row><cell>MDP [53]</cell><cell>76.59</cell><cell cols="2">82.10 52.15 13.38</cell><cell>606 7315</cell></row><row><cell>DSM [14]</cell><cell>76.15</cell><cell>83.42 60.00</cell><cell>8.31</cell><cell>578 7328</cell></row><row><cell>SCEA [23]</cell><cell>75.58</cell><cell cols="3">79.39 53.08 11.54 1306 6989</cell></row><row><cell>CIWT [37]</cell><cell>75.39</cell><cell cols="2">79.25 49.85 10.31</cell><cell>954 7345</cell></row><row><cell>NOMT-HM* [9]</cell><cell>75.20</cell><cell cols="3">80.02 50.00 13.54 1143 7280</cell></row><row><cell>mbodSSP [30]</cell><cell>72.69</cell><cell>78.75 48.77</cell><cell cols="2">8.77 1918 7360</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Tracking performance on the testing set of KITTI tracking benchmark. Only published methods are reported. † sign means 3D information is used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>The light dashed rectangular represents the ground truth vehicle while the solid rectangular stands for the predicted vehicle in bird's eye view. The qualitative results on Argoverse demonstrate our monocular 3D tracking ability under day and night road conditions without the help of LiDAR or HD maps. The figures are best viewed in color. Evaluation Video. We have uploaded a demo video which summaries our main concepts and demonstrates video inputs with estimated bounding boxes and tracked trajectories in bird's eye view. We show the comparison of our method with baselines in both our GTA dataset and real-world videos.</figDesc><table><row><cell>Motion</cell><cell cols="4">Deep Order MOTA ↑ MOTP ↑</cell><cell>TP ↑</cell><cell>TR ↑</cell><cell cols="3">MM ↓ NM ↓ RM ↓</cell><cell>FP ↓</cell><cell>FN ↓</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.056</cell><cell>67.726</cell><cell cols="2">79.445 71.102</cell><cell>6.31</cell><cell>1.636</cell><cell>5.839 25.997 62.637</cell></row><row><cell>2D [4]</cell><cell>-</cell><cell>-</cell><cell>57.042</cell><cell>82.794</cell><cell cols="3">86.905 77.058 3.739</cell><cell>1.064</cell><cell>3.418</cell><cell>6.085 33.134</cell></row><row><cell>2D  *  [52]</cell><cell></cell><cell>-</cell><cell>65.892</cell><cell>81.86</cell><cell cols="3">90.607 87.186 4.796</cell><cell>4.096</cell><cell>3.898 10.099 19.213</cell></row><row><cell>3D</cell><cell>-</cell><cell>-</cell><cell>69.616</cell><cell>84.776</cell><cell cols="3">85.947 84.202 1.435</cell><cell>0.511</cell><cell>1.283</cell><cell>7.651 21.298</cell></row><row><cell>3D</cell><cell></cell><cell>-</cell><cell>69.843</cell><cell>84.523</cell><cell cols="3">90.242 87.113 2.269</cell><cell>1.798</cell><cell>1.672</cell><cell>9.341 18.547</cell></row><row><cell>3D</cell><cell>-</cell><cell></cell><cell>70.061</cell><cell>84.845</cell><cell cols="3">85.952 84.377 1.317</cell><cell>0.403</cell><cell>1.184</cell><cell>7.303 21.319</cell></row><row><cell>3D</cell><cell></cell><cell></cell><cell>70.126</cell><cell>84.494</cell><cell cols="3">90.155 87.432 2.123</cell><cell>1.717</cell><cell>1.512</cell><cell>9.574 18.177</cell></row><row><cell>LSTM</cell><cell></cell><cell></cell><cell>70.439</cell><cell>84.488</cell><cell cols="3">90.651 87.374 1.959</cell><cell>1.547</cell><cell>1.37</cell><cell>9.508 18.094</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.609</cell><cell>71.064</cell><cell cols="5">74.941 83.594 14.535 3.735 13.834 31.326 49.531</cell></row><row><cell>2D [4]</cell><cell>-</cell><cell>-</cell><cell>42.748</cell><cell>81.878</cell><cell cols="3">70.003 84.292 9.172</cell><cell>2.077</cell><cell>8.755 16.731 31.350</cell></row><row><cell>2D  *  [52]</cell><cell></cell><cell>-</cell><cell>48.518</cell><cell>81.479</cell><cell cols="3">66.381 88.222 7.270</cell><cell>2.683</cell><cell>6.738 21.223 22.989</cell></row><row><cell>3D</cell><cell>-</cell><cell>-</cell><cell>54.324</cell><cell>83.914</cell><cell cols="3">64.986 90.869 3.032</cell><cell>0.799</cell><cell>2.820 23.574 19.070</cell></row><row><cell>3D</cell><cell></cell><cell>-</cell><cell>54.855</cell><cell>83.812</cell><cell cols="3">68.235 91.687 2.087</cell><cell>0.776</cell><cell>1.864 25.223 17.835</cell></row><row><cell>3D</cell><cell>-</cell><cell></cell><cell>54.738</cell><cell>84.045</cell><cell cols="3">64.001 90.623 3.242</cell><cell>0.533</cell><cell>3.102 22.120 19.899</cell></row><row><cell>3D</cell><cell></cell><cell></cell><cell>55.218</cell><cell>83.751</cell><cell cols="3">68.628 92.132 1.902</cell><cell>0.723</cell><cell>1.688 25.578 17.302</cell></row><row><cell>LSTM</cell><cell></cell><cell></cell><cell>55.150</cell><cell>83.780</cell><cell cols="3">69.860 92.040 2.150</cell><cell>0.800</cell><cell>1.920 25.220 17.470</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Supplementary material of Joint Monocular 3D Vehicle Detection and Tracking can be found at https://eborboihuc.github.io/Mono-3DT/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>The authors gratefully acknowledge the support of Berkeley AI Research, Berkeley DeepDrive and MOST-107 2634-F-007-007, MOST Joint Research Center for AI Technology and All Vista Healthcare.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>JIVP)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>David S Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yui Man</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Fang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<idno>ArXiv:1906.07155</idno>
		<editor>Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Carla: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end learning of multi-sensor 3d tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davi</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A lightweight online multiple object vehicle tracking method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gültekin</forename><surname>Gündüz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tankut</forename><surname>Acarman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<title level="m">Struck: Structured output tracking with kernels. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online multi-object tracking via structural constraint event aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Hong Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ryeol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multispectral pedestrian detection: Benchmark dataset and baseline. Integrated Computer-Aided Engineering (ICAE)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soonmin</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukyung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tracking-learning-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zdenek</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Free supervision from video games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luka</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno>ArXiv:1504.01942</idno>
		<title level="m">Motchallenge 2015: Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-class multi-object tracking using changing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enkhbayar</forename><surname>Erdenee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songguo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><forename type="middle">Young</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Giu</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phill-Kyu</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops (ECCV Workshops)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Followme: Efficient online min-cost flow tracking with bounded memory and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stereo visionbased semantic 3d object and ego-motion tracking for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">1 year, 1000 km: The oxford robotcar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Mot16: A benchmark for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno>ArXiv:1603.00831</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combined image-and world-space tracking in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Playing for benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="300" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptive appearance modeling for video tracking: Survey and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mono-camera 3d multi-object tracking using deep learning detections and pmbm filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Benjaminsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrit</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Granstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Beyond pixels: Leveraging geometry and shape cues for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaid</forename><surname>Ahmed Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Krishna</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Madhava Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dung</forename><forename type="middle">M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Ua-detrac: A new benchmark and protocol for multi-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<idno>ArXiv:1511.04136</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alper</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Poi: multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vashisht</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>ArXiv:1805.04687</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
