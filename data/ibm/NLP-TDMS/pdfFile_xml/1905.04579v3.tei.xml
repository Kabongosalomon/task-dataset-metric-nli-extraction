<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
							<email>tingchen@cs.ucla.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bian</surname></persName>
							<email>songbian@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
							<email>yzsun@cs.ucla.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are. In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a set of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks. 2 Recent years have seen increasing attention to Graph Neural Nets (GNNs) [1-4], which have achieved superior performance in many graph tasks, such as node classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and graph classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Different from traditional neural networks that are defined on regular structures such as sequences or images, graphs provide a more general abstraction for structured data, which subsume regular structures as special cases. The power of GNNs is that they can directly define learnable compositional function on (arbitrary) graphs, thus extending classic networks (e.g. CNNs, RNNs) to more irregular and general domains.</p><p>Despite their success, it is unclear what GNNs have learned, and how sophisticated the learned graph functions are. It is shown in [8] that traditional CNNs used in image recognition have learned complex hierarchical and compositional features, and that deep non-linear computation can be beneficial <ref type="bibr" target="#b8">[9]</ref>. Is this also the case when applying GNNs to common graph problems? Recently, <ref type="bibr" target="#b4">[5]</ref> showed that, for common node classification benchmarks, non-linearity can be removed in GNNs without suffering * Now at Google. 2 Code at: https://github.com/chentingpc/gfn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction much loss of performance. The resulting linear GNNs collapse into a logistic regression on graph propagated features. This raises doubts on the necessity of complex GNNs, which require much more expensive computation, for node classification benchmarks. Here we take a step further dissecting GNNs, and examine the necessity of complex GNN parts on more challenging graph classification benchmarks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>To better understand GNNs on graph classification, we dissect it into two parts/stages: 1) the graph filtering part, where graph-based neighbor aggregations are performed, and 2) the set function part, where a set of hidden node features are composed for prediction. We aim to test the importance of both parts separately, and seek answers to the following questions. Do we need a sophisticated graph filtering function for a particular task or dataset? And if we have a powerful set function, is it enough to use a simple graph filtering function?</p><p>To answer these questions, we propose to linearize both parts separately. We first linearize graph filtering part, resulting Graph Feature Network (GFN): a simple lightweight neural net defined on a set of graph augmented features. Unlike GNNs, which learn a multi-step neighbor aggregation function on graphs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, the GFN only utilizes graphs in constructing its input features. It first augments nodes with graph structural and propagated features, and then learns a neural net directly on the set of nodes (i.e. a bag of graph pre-processed feature vectors), which make it more efficient. We then further linearize set function in GFN, and arrive at Graph Linear Network (GLN), which is a linear function of augmented graph features.</p><p>Empirically, we perform evaluations on common graph classification benchmarks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>, and find that GFN can match or exceed the best accuracies produced by recently proposed GNNs, at a fraction of the computation cost. GLN performs much poorly than both GFN and recent GNNs. This result casts doubts on the necessity of non-linear graph filtering, and suggests that the existing GNNs may not have learned more sophisticated graph functions than linear neighbor aggregation on these benchmarks. Furthermore, we find non-linear set function plays an important role, as its linearization can hurt performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Graph classification problem. We use G = (V, E) ∈ G to denote a graph, where V is a set of vertices/nodes, and E is a set of edges. We further denote an attributed graph as G X = (G, X) ∈ G X , where X ∈ R n×d are node attributes with n = |V |. It is assumed that each attributed graph is associated with some label y ∈ Y, where Y is a set of pre-defined categories. The goal in graph classification problem is to learn a mapping function f : G X → Y, such that we can predict the target class for unseen graphs accurately. Many real world problems can be formulated as graph classification problems, such as social and biological graph classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Graph neural networks. Graph Neural Networks (GNNs) define functions on the space of attributed graph G X . Typically, the graph function, GNN(G, X), learns a multiple-step transformation of the original attributes/signals for final node level or graph level prediction. In each of the step t, a new node presentation, h (t) v is learned. Initially, h <ref type="bibr" target="#b0">(1)</ref> v is initialized with the node attribute vector, and during each subsequent step, a neighbor aggregation function is applied to generate the new node representation. More specifically, common neighbor aggregation functions for the v-th node take the following form:</p><formula xml:id="formula_0">h (t) v = f h (t−1) v , h (t−1) u |u ∈ N (v) ,<label>(1)</label></formula><p>where N (v) is a set of neighboring nodes of node v. To instantiate this neighbor aggregation function, <ref type="bibr" target="#b3">[4]</ref> proposes the Graph Convolutional Network (GCN) aggregation scheme as follows.</p><formula xml:id="formula_1">h (t+1) v = σ u∈N (v)Ã uv (W (t) ) T h (t) u ,<label>(2)</label></formula><p>where W (t) ∈ R d×d is the learnable transformation weight,Ã =D −1/2 (A + I)D −1/2 is the normalized adjacency matrix with as a constant ( = 1 in <ref type="bibr" target="#b3">[4]</ref>) andD ii = j A ij + . σ(·) is a non-linear activation function, such as ReLU. This transformation can also be written as H (t+1) = σ(ÃH (t) W (t) ), where H (t) ∈ R n×d are the hidden states of all nodes at t-th step.</p><p>More sophisticated neighbor aggregation schemes are also proposed, such as GraphSAGE <ref type="bibr" target="#b13">[14]</ref> which allows pooling and recurrent aggregation over neighboring nodes. Most recently, in Graph Isomorphism Network (GIN) <ref type="bibr" target="#b14">[15]</ref>, a more powerful aggregation function is proposed as follows.</p><formula xml:id="formula_2">h (t) v = MLP (t) 1 + (t) h (t−1) v + u∈N (v) h (t−1) u ,<label>(3)</label></formula><p>where MLP abbreviates for multi-layer perceptrons and (t) can either be zero or a learnable parameter.</p><p>Finally, in order to generate graph level representation h G , a readout function is used, which generally takes the following form:</p><formula xml:id="formula_3">h G = g h (T ) v |v ∈ G .<label>(4)</label></formula><p>This can be instantiated by a global sum pooling, i.e. h G = n v=1 h (T ) v followed by fully connected layers to generate the categorical or numerical output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph feature network</head><p>Motivated by the question that, with a powerful graph readout function, whether we can simplify the sophisticated multi-step neighbor aggregation functions (such as Eq. 2 and 3). Therefore we propose Graph Feature Network (GFN): a neural set function defined on a set of graph augmented features.</p><p>Graph augmented features. In GFN, we replace the sophisticated neighbor aggregation functions (such as Eq. 2 and 3) with graph augmented features based on G X . Here we consider two categories as follows: 1) graph structural/topological features, which are related to the intrinsic graph structure, such as node degrees, or node centrality scores 3 , but do not rely on node attributes; 2) graph propagated features, which leverage the graph as a medium to propagate node attributes. The graph augmented features X G can be seen as the output of a feature extraction function defined on the attributed graph, i.e. X G = γ(G, X), and Eq. 5 below gives a specific form, which combine node degree features and multi-scale graph propagated features as follows:</p><formula xml:id="formula_4">X G = γ(G, X) = d, X,Ã 1 X,Ã 2 X, · · · ,Ã K X ,<label>(5)</label></formula><p>where d ∈ R n×1 is the degree vector for all nodes, andÃ is again the normalized adjacency matrix (Ã =D −1/2 (A + I)D −1/2 ), but other designs of propagation operator are possible <ref type="bibr" target="#b15">[16]</ref>. Features separated by comma are concatenated to form X G .</p><p>Neural set function. To build a powerful graph readout function based on graph augmented features X G , we use a neural set function. The neural set function discards the graph structures and learns purely based on the set of augmented node features. Motivated by the general form of a permutationinvariant set function shown in <ref type="bibr" target="#b16">[17]</ref>, we define our neural set function for GFN as follows:</p><formula xml:id="formula_5">GFN(G, X) = ρ v∈V φ X G v .<label>(6)</label></formula><p>Both φ(·) and ρ(·) are parameterized by neural networks. Concretely, we parameterize the function φ(·) as a multi-layer perceptron (MLP), i.e. φ(x) = σ(σ(· · · σ(x T W (1) ) · · · )W (T ) ). Note that a single layer of φ(·) resembles a graph convolution layer H (t+1) = σ(ÃH (t) W (t) ) with the normalized adjacency matrixÃ replaced by identity matrix I (a.k.a. 1 × 1 convolution). As for the function ρ(·), we parameterize it with another MLP (i.e. fully connected layers in this case).</p><p>Computation efficiency. GFN provides a way to approximate GNN with less computation overheads, especially during the training process. Since the graph augmented features can be pre-computed before training starts, the graph structures are not involved in the iterative training process. This brings the following advantages. First, since there is no neighbor aggregation step in GFN, it reduces computational complexity. To see this, one can compare a single layer feature transformation function in GFN, i.e. σ(HW ), against the neighbor aggregation function in GCN, i.e. σ(ÃHW ). Secondly, since graph augmented features of different scales are readily available from the input layer, GFN can leverage them much earlier, thus may require fewer transformation layers. Lastly, it also eases the implementation related overhead, since the neighbor aggregation operation in graphs are typically implemented by sparse matrix operations.</p><p>Graph Linear Network. When we use a linear set function instead of the generic one used in Eq. 6, we arrive at graph linear network, which can be expressed as follows.</p><formula xml:id="formula_6">GLN(G, X) = σ W v∈V X G v .<label>(7)</label></formula><p>Where W is a weight matrix, and σ(·) is softmax function produce class probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">From GNN to GFN and GLN: a dissection of GNNs</head><p>To better understand GNNs on graph classification, we propose a formal dissection/decomposition of GNNs into two parts/stages: the graph filtering part and the set function part. As we shall see shortly, the simplification of the graph filtering part allows us to derive GFN from GNN, and also be able to assess the importance of the two GNN parts separately.</p><p>To make concepts more clear, we first give formal definitions of the two GNN parts in the dissection. Definition 1. (Graph filtering) A graph filtering function, Y = F G (X), performs a transformation of input signals based on the graph G, which takes a set of signals X ∈ R n×d and outputs another set of filtered signals Y ∈ R m×d .</p><p>Graph filtering in most existing GNNs consists of multi-step neighbor aggregation operations, i.e. multiple steps of Eq. 1. For example, in GCN <ref type="bibr" target="#b3">[4]</ref>, the multi-step neighbor aggregation can be expressed as</p><formula xml:id="formula_7">H (T ) = σ(Aσ(...σ(AXW (1) )...)W (T ) ). Definition 2. (Set function) A set function, y = T (Y )</formula><p>, takes a set of vectors Y ∈ R m×d where their order does not matter, and outputs a task specific prediction y ∈ Y.</p><p>The graph readout function in Eq. 4 is a set function, which enables the graph level prediction that is permutation invariant w.r.t. nodes in the graph. Although a typical readout function is simply a global pooling <ref type="bibr" target="#b14">[15]</ref>, the set function can be as complicated as Eq. 6. Claim 1. A GNN that is a mapping of G X → Y can be decomposed into a graph filtering function followed by a set function, i.e. GNN(G,</p><formula xml:id="formula_8">X) = T • F G (X).</formula><p>This claim is obvious for the neighbor aggregation framework defined by Eq. 1 and 4, where most existing GNN variants such as GCN, GraphSAGE and GIN follow. This claim is also general, even for unforeseen GNN variants that do not explicitly follow this framework <ref type="bibr" target="#b3">4</ref> .</p><p>We aim to assess the importance of two GNN parts separately. However, it is worth pointing out that the above decomposition is not unique in general, and the functionality of the two parts can overlap: if the graph filtering part has fully transformed graph features, then a simple set function may be used for prediction. This makes it challenging to answer the question: do we need a sophisticated graph filtering part for a particular task or dataset, especially when a powerful set function is used? To better disentangle these two parts and study their importance more independently, similar to <ref type="bibr" target="#b4">[5]</ref>, we propose to simplify the graph filtering part by linearizing it. Definition 3. (Linear graph filtering) We say a graph filtering function F G (X) is linear w.r.t. X iff it can be expressed as F G (X) = Γ(G, X)θ, where Γ(G, X) is a linear map of X, and θ is the only learnable parameter.</p><p>Intuitively, one can construct a linear graph filtering by removing the non-linear operations from graph filtering part in existing GNNs, such as non-linear activation function σ(·) in Eq. 2 or 3. By doing so, the graph filtering becomes linear w.r.t. X, thus multi-layer weights collapse into a single linear transformation, described by θ. More concretely, let us consider a linearized GCN <ref type="bibr" target="#b3">[4]</ref>, its K-th layer can be written as H (K) =Ã K X(Π K k=1 W (k) ), and we can rewrite the weights with θ = Π K k=1 W (k) . The linearization of graph filtering part enables us to disentangle graph filtering and the set function more thoroughly: the graph filtering part mainly constructs graph augmented features (by setting γ(G, X) = Γ(G, X)), and the set function learns to compose them for the graph-level prediction. This leads to the proposed GFN. In other words, GNNs with a linear graph filtering part can be expressed as GFN with appropriate graph augmented features. This is shown more formally in the following proposition 1.</p><formula xml:id="formula_9">Proposition 1. Let GNN lin (G, X) be a mapping of G X → Y that has a linear graph filtering part, i.e. F G (X) = Γ(G, X)θ, then we have GNN lin (G, X) = GFN(G, X), where γ(G, X) = Γ(G, X).</formula><p>The proof can be found in the appendix. Noted that a GNN with a linear graph filtering can be seen as a GFN, but the reverse may not be true. General GFN can also have non-linear graph filtering, e.g. when the feature extraction function γ(G, X) is not a linear map of X (Eq. 5 is a linear map of X).</p><p>Why GFN? GFN can help us understand the functions that GNNs learned on current benchmarks. First, by comparing GNN with linear graph filtering (i.e. GFN) against standard GNN with nonlinear graph filtering, we can assess the importance of non-linear graph filtering part. Secondly, by comparing GFN with linear set function (i.e. GLN) against GFN with non-linear set function, we can assess the importance of non-linear set function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and settings</head><p>Datasets. The main datasets we consider are commonly used graph classification benchmarks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15]</ref>. The graphs in the collection can be categorized into two categories biological graphs and social graphs. It is worth noting that the social graphs have no node attributes, while the biological graphs come with categorical node attributes. The detailed statistics for biological and social graph datasets can be found in Appendix.</p><p>Baselines. We compare with two families of baselines. The first family of baselines are kernelbased, namely the Weisfeiler-Lehman subtree kernel (WL) <ref type="bibr" target="#b17">[18]</ref>, Deep Graph Kernel (DGK) <ref type="bibr" target="#b9">[10]</ref> and AWE <ref type="bibr" target="#b18">[19]</ref> that incorporate kernel-based methods with learning-based approach to learn embeddings. Two recent work alone this line are RetGK <ref type="bibr" target="#b19">[20]</ref> and GNTK <ref type="bibr" target="#b20">[21]</ref>.</p><p>The second family of baselines are GNN-based models, which include recently proposed PATCHY-SAN (PSCN) <ref type="bibr" target="#b21">[22]</ref>, Deep Graph CNN (DGCNN) <ref type="bibr" target="#b10">[11]</ref>, CapsGNN <ref type="bibr" target="#b6">[7]</ref> and GIN <ref type="bibr" target="#b14">[15]</ref>. GNN based methods are usually more scalable compared to graph kernel based ones, as the complexity is typically quadratic in the number of graphs and nodes for kernels while linear for GNNs.</p><p>For the above baselines, we use their accuracies reported in the original papers, following the same evaluation setting as in <ref type="bibr" target="#b14">[15]</ref>. Architecture and hyper-parameters can make a difference, so to enable a better controlled comparison between GFN and GNN, we also implement Graph Convolutional Networks (GCN) from <ref type="bibr" target="#b3">[4]</ref>. More specifically, our GCN model contains a dense feature transformation layer, i.e. H (2) = σ(XW (1) ), followed by three GCN layers, i.e. H (t+1) = σ(ÃH (t) W (t) ). We also vary the number of GCN layers in our ablation study. To enable graph level prediction, we add a global sum pooling, followed by two fully-connected layers that produce categorical probability over pre-defined categories.</p><p>Model configurations. For the proposed GFN, we mirror our GCN model configuration to allow direct comparison. Therefore, we use the same architecture, parameterization and training setup, but replace the GCN layer with feature transformation layers (totaling four such layers). Converting GCN layer to feature transformation layer is equivalent to setting A = I in in GCN layers. We also construct a faster GFN, namely "GFN-light", that contains only a single feature transformation layer, which can further reduce the training time while maintaining similar performance.</p><p>For both our GCN and GFN, we utilize ReLU activation and batch normalization <ref type="bibr" target="#b22">[23]</ref>, and fix the hidden dimensionality to 128. No regularization is applied. Furthermore we use batch size of 128, a fixed learning rate of 0.001, and the Adam optimizer <ref type="bibr" target="#b23">[24]</ref>. GLN follows the same setting as GFN, but contains no feature transform layer. It only has the global sum pooling of graph features followed by a single fully connected layer. To compare with existing work, we follow <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref> and perform 10-fold  cross validation. We run the model for 100 epochs, and select the epoch in the same way as <ref type="bibr" target="#b14">[15]</ref>, i.e., a single epoch with the best cross-validation accuracy averaged over the 10 folds is selected. We report the average and standard deviation of test accuracies at the selected epoch over 10 folds.</p><p>In terms of input node features for GFN and GLN, by default, we use both degree and multi-scale propagated features (up to K = 3), that is [d, X,Ã 1 X,Ã 2 X,Ã 3 X]. We turn discrete features into one-hot vectors, and also discretize degree features into one-hot vectors, as suggested in <ref type="bibr" target="#b24">[25]</ref>. We set X = 1 for the social graphs we consider as there are no node attributes. By default, we also augment node features in our GCN with an extra node degree feature (to counter that the normalized adjacency matrix may lose the degree information). Other graph augmented features are also studied for GCN (which has minor effects). All experiments are run on Nvidia GTX 1080 Ti GPU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GFN performs similarly or better compared to its GNN variants across 10 datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">While GCN trains better, GFN generalizes better</head><p>To better understand the training dynamic, we plot the training/test curves for both GCN and GFN in <ref type="figure">Figure 1</ref>. We observe that GCN usually performs better than GFN during the training, but GFN can generalize similarly or better than GCN in the test set. This shows that GFN works well not because it is easier to optimize, but its generalization capability. This observation also indicates that linear graph filtering, as used in GFN, may be a good inductive bias for the tested graph classification datasets.</p><p>Since GCN overfits, one may wonder if increasing data size could make a difference. To test the impact of dataset size, we take the largest graph dataset available in the benchmark, RE-M12K, which has 11929 graphs. And we then construct nine new datasets by randomly sampling the original dataset with different ratios, ranging from 10% to 100% of all graphs. We compute both training and test accuracies over 10 fold cross-validation for both our GFN and GCN. For each dataset size (10 fold cross validation), we consider two ways to extract performance: <ref type="bibr" target="#b0">(1)</ref>    <ref type="figure" target="#fig_2">Figure 2</ref> shows the results. We can see that as data size increases, 1) it is harder for both models to overfit (training accuracy decreases), but it seems GCN still overfits more if trained longer (to the last epoch); 2) at the best epoch, both models performance almost identical, no sign of differences as training data size increases.</p><p>Through these experiments, we conclude that GFN generalizes similarly or better than GCN, and this may be related to the good inductive bias of linear graph filtering, or the inadequacy of the benchmark datasets in testing powerful GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">GFNs are more efficient than GCNs</head><p>Since GFN's performance is on par with GCN's, we further compare the training time of our GCN and the proposed GFNs. <ref type="figure" target="#fig_0">Figure 3</ref> shows that a significant speedup (from 1.4× to 6.7× as fast) by utilizing GFN compared to GCN, especially for datasets with denser edges such as the COLLAB dataset. Also since our GFN can work with fewer transformation layers, GFN-light can achieve better speedup by reducing the number of transformation layers. Note that our GCN is already very efficient as it is built on a highly optimized framework <ref type="bibr" target="#b24">[25]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablations</head><p>Node features. To better understand the impact of features, we test both models with different input node features. <ref type="table" target="#tab_3">Table 3</ref> shows that 1) graph features are very important for both GFN and GCN, 2) the node degree feature is surprisingly important, and multi-scale features can further improve on that, and 3) even with multi-scale features, GCN still performs similarly to GFN, which further suggests that linear graph filtering is enough. More detailed results (per dataset) can be found in the appendix. Architecture depth. We vary the number of convolutional layers (with two FC-layers after sum pooling kept the same). <ref type="table" target="#tab_4">Table 4</ref> shows that 1) GCN benefits from multiple grpah convolutional layers with a significant diminishing return, 2) GFN with single feature transformation layer works pretty well already, likely due to the availability of multi-scale input node features, which otherwise require multiple GCN layers to obtain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work studies the effectiveness of GNNs via linearization, and propose a simple, effective and efficient architecture. Two family of work are most relevant to ours.</p><p>Many new graph neural networks architectures (for graph classification) have been proposed recently, such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. Many of these networks aims to improve the expressiveness or trainability of GNNs. We do not aim to propose a more powerful architecture, our GFN aims for simplicity and efficiency, at the same time can obtain high performance on graph classification benchmark that is on-par with state-of-the-art.</p><p>There have been relatively fewer work on understanding graph neural network. Existing work mainly focus on studying the expressiveness of GNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, in theoretical limit. We take a more practical route by examining the effects of linearization on real graph classification problems, leading to a scalable approach (i.e. GFN) that works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this work, we conduct a dissection of GNNs on common graph classification benchmarks. We first decompose GNNs into two parts, and linearize the graph filtering part resulting GFN. We then further linearize the set function of GFN resulting GLN. In our extensive experiments, we find GFN can match or exceed the best results by recently proposed GNNs, with a fraction of computation cost. The linearization of graph filtering (i.e. GFN) has little impact on performance, while linearization of both graph filtering and set function (i.e. GLN) leads to worse performance.</p><p>Since GCN usually achieve better training accuracies while not better test accuracies, we conjecture that the linear graph filtering may be a good inductive bias for tested datasets, though this is speculative and requires more future investigations. It also casts doubts on the adequacy of existing graph classification benchmarks. It is possible that the complexity of current graph classification benchmarks is limited, so that linear graph filtering is enough, and moving to datasets or problems with higher structural complexity could require sophisticated non-linear graph filtering. <ref type="table" target="#tab_5">Table 5</ref> summarizes the comparisons between GCN and its linearized variants. The efficiency and performance are concluded from our experiments on graph classification benchmarks. Noted that a GNN with a linear graph filtering can be seen as a GFN, but the reverse may not be true. General GFN can have non-linear graph filtering, e.g. when the feature extraction function γ(G, X) is not a linear map of X. Thus we use GFN lin in <ref type="table" target="#tab_5">Table 5</ref> to denote such subtle difference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Comparisons of different linearizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B GFN can be a generic framework for functions over graphs</head><p>Beyond as a tool to study GNN parts, GFN is also more efficient than GNN counterpart, which makes it a fast approximation. Furthermore, GFNs can be a very powerful framework without restriction on the feature extraction function γ(G, X) and the exact forms of the set function. The potential expressiveness of a GFN is demonstrated by the following proposition.</p><p>Proposition 2. For any GNN F defined in G X , there exists a graph to set mapping M : G → S where S is a set space, and a set function T that approximates F to arbitrary precision, i.e. ∀G ∈ G X , F (G) ≈ T (M(G)).</p><p>The proof is provided below. We want to provide an intuitive interpretation here. There exists some way(s) that we can encode any graph into a set, and learn a generic set function on it. As long as the set contains the graph information, a powerful set function can learn to integrate it in a flexible way. So a well constructed GFN can be as powerful as, if not more powerful than, the most powerful GNNs. This shows the potential of the GFN framework in modeling arbitrary graph data.</p><p>Here we provide the proof for Proposition 2.</p><p>Proof. We show the existence of the mapping T by constructing it as follows. First, we assign a unique ID to each of the node, then we add its ID and its neighbors' IDs in the end of node features. If there are edges with features, we also treat them as nodes and apply the same above procedure. This procedure results in a set of nodes with features that preserve the same original information (since we can reconstruct the original graph).</p><p>We now show the existence of a set function that can mimic any graph functions operated on G, again, by constructing a specific one. Since the set of nodes preserve the whole graph information, the set function can first reconstruct the graph by decoding the node's feature vectors. At every computation step, the set function find neighbors of each node in the set, and compute the aggregation function in exactly the same way as the graph function would do with the neighbors of a node. This procedure is repeated until the graph function produces its output.</p><p>Hence, the above constructed example proves the existence of M and a set function T such that ∀G ∈ G X , F(G) ≈ T (M(G)). We also note that the specially constructed examples above are feasible but likely not optimal. A better solution is to have a set function that learns to adaptively leverage the graph structure as well as node attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof for proposition 1</head><p>Here we provide the proof for Proposition 1.</p><p>Proof. According to claim 1 and definition 3, a GNN(G, X) with a linear graph filtering part, denoted by GNN lin (G, X), can be written as follows.</p><formula xml:id="formula_10">GNN lin (G, X) = T • F G (X) = T (Γ(G, X)θ) = T (Γ(G, X)),</formula><p>where θ is absorbed into the set function T (·). According to GFN's definition in Eq. 6 and general set function result from <ref type="bibr" target="#b16">[17]</ref>, we have GFN(G, X) = T (X G ) = T (γ(G, X)).</p><p>By setting γ(G, X) = Γ(G, X), we arrive at GNN lin (G, X) = GFN(G, X).    <ref type="figure" target="#fig_3">Figure 4</ref> shows more training/test curves for both GCN and GFN. The conclusion is consistent with main text that GFN works well not because it is easier to optimize. G Detailed performances with different architecture depths <ref type="table" target="#tab_11">Table 9</ref> shows performance per datasets under different number of layers.        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Dataset details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Detailed performances with different features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Graph visualizations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Training time comparisons. The annotation, e.g. 1.0×, denotes speedup compared to GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>selecting best epoch averaged over 10 fold cross validation, or (2) selecting the last epoch (i.e. the 100-th epoch).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Performances on subsets of RE-M12K with different number of training graphs. We see no benefits of increased dataset size for GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Training and test performance versus training epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 , 6 , 8 ,</head><label>568</label><figDesc>and 7 show the random and mis-classified samples for MUTAG, PROTEINS, IMDB-B, and IMDB-M, respectively. In general, it is difficult to find the patterns of each class by visually examining the graphs. And the mis-classified patterns are not visually distinguishable, except for IMDB-B/IMDB-M datasets where there are some graphs seem ambiguous. (a) Random samples. (b) Mis-classified samples by GFN. (c) Random samples. (d) Mis-classified samples by GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Random and mis-classified samples from MUTAG. Each row represents a (true) class. (a) Random samples (b) Mis-classified samples by GFN. (c) Random samples. (d) Mis-classified samples by GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Random and mis-classified samples from PROTEINS. Each row represents a (true) class. (a) Random samples. (b) Mis-classified samples by GFN. (c) Random samples. (d) Mis-classified samples by GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Random and mis-classified samples from IMDB-B. Each row represents a (true) class. (c) Random samples. (d) Mis-classified samples by GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Random and mis-classified samples from IMDB-M. Each row represents a (true) class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test accuracies (%) for biological graphs. The best results per dataset and in average are highlighted. -means the results are not available for a particular dataset. 19±0.18 74.68±0.49 79.78±0.36 52.22±1.26</figDesc><table><row><cell>Algorithm</cell><cell>MUTAG</cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>D&amp;D</cell><cell cols="2">ENZYMES Average</cell></row><row><cell>WL</cell><cell>82.05±0.36</cell><cell cols="5">82.74.18</cell></row><row><cell>AWE</cell><cell>87.87±9.76</cell><cell>-</cell><cell>-</cell><cell cols="2">71.51±4.02 35.77±5.93</cell><cell>-</cell></row><row><cell>DGK</cell><cell>87.44±2.72</cell><cell cols="4">80.31±0.46 75.68±0.54 73.50±1.01 53.43±0.91</cell><cell>74.07</cell></row><row><cell>RetGKI</cell><cell>90.30±1.10</cell><cell cols="4">84.50±0.20 75.80±0.60 81.60±0.30 60.40±0.80</cell><cell>78.52</cell></row><row><cell>RetGKII</cell><cell>90.10±1.00</cell><cell cols="4">83.50±0.20 75.20±0.30 81.00±0.50 59.10±1.10</cell><cell>77.78</cell></row><row><cell>GNTK</cell><cell>90.00±8.50</cell><cell cols="2">84.20±1.50 75.60±4.20</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PSCN</cell><cell>88.95±4.37</cell><cell cols="3">76.34±1.68 75.00±2.51 76.27±2.64</cell><cell>-</cell><cell>-</cell></row><row><cell>DGCNN</cell><cell>85.83±1.66</cell><cell cols="4">74.44±0.47 75.54±0.94 79.37±0.94 51.00±7.29</cell><cell>73.24</cell></row><row><cell>CapsGNN</cell><cell>86.67±6.88</cell><cell cols="4">78.35±1.55 76.28±3.63 75.38±4.17 54.67±5.67</cell><cell>74.27</cell></row><row><cell>GIN</cell><cell>89.40±5.60</cell><cell cols="2">82.70±1.70 76.20±2.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GCN</cell><cell>87.20±5.11</cell><cell cols="4">83.65±1.69 75.65±3.24 79.12±3.07 66.50±6.91</cell><cell>78.42</cell></row><row><cell>GLN</cell><cell cols="5">82.85±12.15 68.61±2.31 75.65±4.43 76.75±5.00 43.83±5.16</cell><cell>69.54</cell></row><row><cell>GFN</cell><cell>90.84±7.22</cell><cell cols="4">82.77±1.49 76.46±4.06 78.78±3.49 70.17±5.58</cell><cell>79.80</cell></row><row><cell>GFN-light</cell><cell>89.89±7.14</cell><cell cols="4">81.43±1.65 77.44±3.77 78.62±5.43 69.50±7.37</cell><cell>79.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>02±1.77 73.40±4.63 49.33±4.75 49.44±2.36 38.18±1.30 57.87 AWE 73.93±1.94 74.45±5.83 51.54±3.61 50.46±1.91 39.20±2.09 57.92 DGK 73.09±0.25 66.96±0.56 44.55±0.52 41.27±0.18 32.22±0.10 51.62 RetGKI 81.00±0.30 71.90±1.00 47.70±0.30 56.10±0.50 48.70±0.20 61.08 RetGKII 80.60±0.30 72.30±0.60 48.70±0.60 55.30±0.30 47.10±0.30 60.80 GNTK 83.60±1.00 76.90±3.60 52.80±4.60 ---PSCN 72.60±2.15 71.00±2.29 45.23±2.84 49.10±0.70 41.32±0.42 55.85 DGCNN 73.76±0.49 70.03±0.86 47.83±0.85 48.70±4.54 --CapsGNN 79.62±0.91 73.10±4.83 50.27±2.65 52.88±1.48 46.62±1.90 60.50 GIN 80.20±1.90 75.10±5.10 52.30±2.80 57.50±1.50 --GCN 81.72±1.64 73.30±5.29 51.20±5.13 56.81±2.37 49.31±1.44 62.47 GLN 75.72±2.51 73.10±3.18 50.40±5.61 52.97±2.58 39.84±0.95 58.41 GFN 81.50±2.42 73.00±4.35 51.80±5.16 57.59±2.40 49.43±1.36 62.66 GFN-light 81.34±1.73 73.00±4.29 51.20±5.71 57.11±1.46 49.75±1.19 62.48</figDesc><table><row><cell>Algorithm</cell><cell>COLLAB</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>RE-M5K</cell><cell>RE-M12K</cell><cell>Average</cell></row><row><cell>WL</cell><cell>79.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Test accuracies (%) for social graphs. The best results per dataset and in average are highlighted. -means the results are not available for a particular dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>and 2 show the results of different methods in both biological and social datasets. It is worth noting that in both datasets, GFN achieves similar performances with our GCN, and match or</figDesc><table><row><cell></cell><cell>$FFXUDF\</cell><cell></cell><cell>$FFXUDF\</cell><cell></cell><cell></cell><cell>$FFXUDF\</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>$FFXUDF\</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>*&amp;1WUDLQ</cell><cell cols="2">*&amp;1WUDLQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>*&amp;1WUDLQ</cell><cell></cell><cell>*&amp;1WUDLQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell>*&amp;1WHVW</cell><cell cols="2">*&amp;1WHVW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>*&amp;1WHVW</cell><cell></cell><cell>*&amp;1WHVW</cell></row><row><cell></cell><cell></cell><cell></cell><cell>*)1WUDLQ</cell><cell cols="2">*)1WUDLQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>*)1WUDLQ</cell><cell></cell><cell>*)1WUDLQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell>*)1WHVW</cell><cell cols="2">*)1WHVW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>*)1WHVW</cell><cell></cell><cell>*)1WHVW</cell></row><row><cell></cell><cell></cell><cell>(SRFK</cell><cell></cell><cell>(SRFK</cell><cell></cell><cell></cell><cell></cell><cell>(SRFK</cell><cell></cell><cell></cell><cell>(SRFK</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(a) NCI1</cell><cell cols="2">(b) IMDB-BINARY</cell><cell></cell><cell cols="5">(c) REDDIT-MULTI-12K</cell><cell cols="2">(d) PROTEINS</cell></row><row><cell></cell><cell></cell><cell cols="10">Figure 1: Training and test performance versus training epoch.</cell><cell></cell></row><row><cell>Time per fold (sec)</cell><cell>0 100 200 300 400</cell><cell>PROTEINS 2.3X 1.6X 1.0X Model GFN-light GFN GCN</cell><cell>IMDB-B 2.3X 1.7X 1.0X</cell><cell>IMDB-M 2.2X 1.7X 1.0X</cell><cell>2.7X</cell><cell>DD 1.4X</cell><cell>1.0X</cell><cell>1.9X</cell><cell>NCI1 1.5X</cell><cell>1.0X</cell><cell>COLLAB 6.7X 4.2X 1.0X</cell><cell>RE-M5K 1.0X 4.0X 1.4X</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>exceed existing state-of-the-art results on multiple datasets, while GLN performs worse in most of the datasets. This result suggests the importance of non-linear set function, while casting doubt on the necessity of non-linear graph filtering for these benchmarks. Not only that GFN matches GCN performance, but it also performs comparably, if not better, than existing approaches on most of the datsets tested.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracies (%) under various augmented features. Averaged results over multiple datasets are shown here. A 1,2,3 X is abbreviated for A 1 X, A 2 X, A 3 X, and default node feature X is always used (if available) but not displayed to reduce clutter. Best results per row/block are highlighted. X A 1,2 X A 1,2,3 X d, A 1 X d, A 1,2 X d, A 1,2,3 X</figDesc><table><row><cell cols="4">Graphs Model None GCN 78.52 A 1 Bio. d 78.51 78.23 GFN 76.27 77.84 78.78</cell><cell>78.24 79.09</cell><cell>78.68 79.17</cell><cell>79.10 78.71</cell><cell>79.26 79.21</cell><cell>79.69 79.13</cell></row><row><cell>Soical</cell><cell>GCN GFN</cell><cell>34.02 30.45</cell><cell>62.35 59.20 60.79 58.04</cell><cell>60.39 59.83</cell><cell>60.28 60.09</cell><cell>62.45 62.47</cell><cell>62.71 62.63</cell><cell>62.77 62.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracies (%) under different number of Conv. layers. GFN requires fewer layers to achieve similar results as GCN.</figDesc><table><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>Bio.</cell><cell>GCN GFN</cell><cell>77.17 79.59</cell><cell>79.38 79.77</cell><cell>78.86 79.78</cell><cell>78.75 78.99</cell><cell>78.21 78.14</cell></row><row><cell>Soical</cell><cell>GCN GFN</cell><cell>60.69 62.70</cell><cell>62.12 62.88</cell><cell>62.37 62.81</cell><cell>62.70 62.80</cell><cell>62.46 62.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of different linearizations for GNN/GCN. Here we denote the GFN as GFN lin since the GFN we study in this work has linear graph filtering, but general GFN can also have a non-linear graph filtering function.</figDesc><table><row><cell cols="3">Method Graph filtering Set function</cell><cell cols="2">Efficiency Performance</cell></row><row><cell>GLN</cell><cell>Linear</cell><cell>Linear</cell><cell>High</cell><cell>Low</cell></row><row><cell>GFN lin</cell><cell>Linear</cell><cell>Non-linear</cell><cell>High</cell><cell>High</cell></row><row><cell>GCN</cell><cell>Non-linear</cell><cell cols="2">Linear/Non-linear Low</cell><cell>High</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>data statistics for biological graph, including MUTAG, NCI1, PROTEINS, D&amp;D, ENZYMES.</cell></row><row><cell>Table 7 shows data statistics for social graphs, including COLLAB, IMDB-Binary (IMDB-B), IMDB-</cell></row><row><cell>Multi (IMDB-M), Reddit-Multi-5K (RE-M5K), Reddit-Multi-12K (RE-M12K). It is worth noting</cell></row><row><cell>that ENZYMES dataset we use has 3 node label + 18 continuous features, and PROTEINS dataset</cell></row><row><cell>we use has 3 node label plus 1 continuous feature. And we use all available features as input to our</cell></row><row><cell>GCN, GFN as well as GLN.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Data statistics of Biological dataset</figDesc><table><row><cell>Dataset</cell><cell cols="3">MUTAG NCI1 PROTEINS</cell><cell>D&amp;D</cell><cell>ENZYMES</cell></row><row><cell># graphs</cell><cell>188</cell><cell>4110</cell><cell>1113</cell><cell>1178</cell><cell>600</cell></row><row><cell># classes</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>6</cell></row><row><cell># features</cell><cell>7</cell><cell>37</cell><cell>4</cell><cell>82</cell><cell>21</cell></row><row><cell>Avg # nodes</cell><cell>17.93</cell><cell>29.87</cell><cell>39.06</cell><cell>284.32</cell><cell>32.63</cell></row><row><cell>Avg # edges</cell><cell>19.79</cell><cell>32.30</cell><cell>72.82</cell><cell>715.66</cell><cell>62.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Data statistics of Social dataset</figDesc><table><row><cell>Dataset</cell><cell cols="5">COLLAB IMDB-B IMDB-M RE-M5K RE-M12K</cell></row><row><cell># graphs</cell><cell>5000</cell><cell>1000</cell><cell>1500</cell><cell>4999</cell><cell>11929</cell></row><row><cell># classes</cell><cell>3</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>11</cell></row><row><cell># features</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Avg # nodes</cell><cell>74.49</cell><cell>19.77</cell><cell>13.00</cell><cell>508.52</cell><cell>391.41</cell></row><row><cell>Avg # edges</cell><cell>2457.78</cell><cell>96.53</cell><cell>65.94</cell><cell>594.87</cell><cell>456.89</cell></row><row><cell cols="5">E Extra curves on training / test performance vs epoch</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>show the performances under different graph features for GNNs and GFNs. It is evident that both model benefit significantly from graph features, especially GFNs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Accuracies (%) under various augmented features. A 1..3 X is abbreviated for A 1 X, A 2 X, A 3 X, and default node feature X is always used (if available) but not displayed to reduce clutter. X A 1,2 X A 1..3 X d, A 1 X d, A 1,2 X d, A 1..3 X</figDesc><table><row><cell cols="3">Dataset A 1 MUTAG Model None d GCN 83.48 87.09 83.35 GFN 82.21 89.31 87.59</cell><cell>83.43 87.17</cell><cell>85.56 86.62</cell><cell>87.18 89.42</cell><cell>87.62 89.28</cell><cell>88.73 88.26</cell></row><row><cell>NCI1</cell><cell>GCN GFN</cell><cell>80.15 83.24 82.62 70.83 75.50 80.95</cell><cell>83.11 82.80</cell><cell>82.60 83.50</cell><cell>83.38 81.92</cell><cell>83.63 82.41</cell><cell>83.50 82.84</cell></row><row><cell>PROTEINS</cell><cell>GCN GFN</cell><cell>74.49 76.28 74.48 74.93 76.63 76.01</cell><cell>75.47 75.74</cell><cell>76.54 76.64</cell><cell>77.09 76.37</cell><cell>76.91 76.46</cell><cell>77.45 77.09</cell></row><row><cell>DD</cell><cell>GCN GFN</cell><cell>79.29 78.78 78.70 78.70 77.77 77.85</cell><cell>77.67 77.43</cell><cell>78.18 78.28</cell><cell>78.35 77.34</cell><cell>78.79 76.92</cell><cell>79.12 78.11</cell></row><row><cell>ENZYMES</cell><cell>GCN GFN</cell><cell>75.17 67.17 72.00 74.67 70.00 71.50</cell><cell>71.50 72.33</cell><cell>70.50 70.83</cell><cell>69.50 68.50</cell><cell>69.33 71.00</cell><cell>69.67 69.33</cell></row><row><cell>COLLAB</cell><cell>GCN GFN</cell><cell>39.69 82.14 76.62 31.57 80.36 76.40</cell><cell>76.98 77.08</cell><cell>77.22 77.04</cell><cell>82.14 81.28</cell><cell>82.24 81.62</cell><cell>82.20 81.26</cell></row><row><cell>IMDB-B</cell><cell>GCN GFN</cell><cell>51.00 73.00 70.30 50.00 73.30 72.30</cell><cell>71.10 71.30</cell><cell>72.20 71.70</cell><cell>73.50 74.40</cell><cell>73.80 73.20</cell><cell>73.70 73.90</cell></row><row><cell>IMDB-M</cell><cell>GCN GFN</cell><cell>35.00 50.33 45.53 33.33 51.20 46.80</cell><cell>46.33 46.67</cell><cell>45.73 46.47</cell><cell>50.20 51.93</cell><cell>50.73 51.93</cell><cell>51.00 51.73</cell></row><row><cell>RE-M5K</cell><cell>GCN GFN</cell><cell>28.48 56.99 54.97 20.00 54.23 51.11</cell><cell>57.43 55.85</cell><cell>56.55 56.35</cell><cell>56.67 56.45</cell><cell>56.75 57.01</cell><cell>57.01 56.71</cell></row><row><cell>RE-M12K</cell><cell>GCN GFN</cell><cell>15.93 49.28 48.58 17.33 44.86 43.61</cell><cell>50.11 48.25</cell><cell>49.71 48.87</cell><cell>49.73 48.31</cell><cell>50.03 49.37</cell><cell>49.92 49.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Accuracies (%) under different number of Conv. layers. Flat denotes the collapsed GFN into a linear model (i.e. linearizing the set function).</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Flat</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>MUTAG</cell><cell>GCN GFN</cell><cell cols="6">-82.85 90.34 89.39 88.18 87.59 87.18 88.32 90.89 87.65 88.31 87.68</cell></row><row><cell>NCI1</cell><cell>GCN GFN</cell><cell cols="6">-68.61 81.77 83.09 82.85 82.80 83.09 75.62 81.41 83.04 82.94 83.31</cell></row><row><cell>PROTEINS</cell><cell>GCN GFN</cell><cell cols="6">-75.65 77.71 77.09 77.17 76.28 75.92 76.91 76.99 77.00 76.19 75.29</cell></row><row><cell>DD</cell><cell>GCN GFN</cell><cell cols="6">-76.75 78.44 78.78 79.04 78.45 76.32 77.34 77.93 78.95 79.46 78.77</cell></row><row><cell>ENZYMES</cell><cell>GCN GFN</cell><cell cols="6">-43.83 69.67 70.50 71.67 69.83 68.17 67.67 69.67 67.67 66.83 66.00</cell></row><row><cell>COLLAB</cell><cell>GCN GFN</cell><cell cols="6">-75.72 81.24 82.04 81.36 82.18 81.72 80.36 81.86 81.40 81.90 81.78</cell></row><row><cell>IMDB-B</cell><cell>GCN GFN</cell><cell cols="6">-73.10 73.50 73.30 74.00 73.90 73.60 72.60 72.30 73.30 73.80 73.40</cell></row><row><cell>IMDB-M</cell><cell>GCN GFN</cell><cell cols="6">-50.40 51.73 52.13 51.93 51.87 51.40 51.53 51.07 50.87 51.53 50.60</cell></row><row><cell>RE-M5K</cell><cell>GCN GFN</cell><cell cols="6">-52.97 57.45 57.13 57.21 56.61 57.03 54.05 56.49 56.83 56.73 56.89</cell></row><row><cell>RE-M12K</cell><cell>GCN GFN</cell><cell cols="6">-39.84 49.58 49.82 49.54 49.44 49.27 44.91 48.87 49.45 49.52 49.61</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We only use node degree in this work as it is very efficient to calculate during both training and inference.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For some unforeseen cases, we can absorb the set function T into FG. That is, let the output Y = FG(·) be final logits for pre-defined classes and set T (·) to softmax function with zero temperature, i.e. exp(x/τ )/Z with τ → 0.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Random samples.(b) Mis-classified samples by GFN.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Yunsheng Bai and Zifeng Kang for their help in a related project prior to this work. We also thank Jascha Sohl-dickstein, Jiaxuan You, Kevin Swersky, Yasaman Bahri, Yewen Wang, Ziniu Hu and Allan Zhou for helpful discussions and feedbacks. This work is partially supported by NSF III-1705169, NSF CAREER Award 1741634, and Amazon Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Capsule graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xinyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Byl8BnRcYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining neural networks with personalized pagerank for classification on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1gL-2A9Ym" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11921</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Anonymous walk embeddings. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Retgk: Graph kernels based on return probabilities of random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijian</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arye</forename><surname>Nehorai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3964" to="3974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangcheng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanczosnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01484</idno>
		<title level="m">Multi-scale deep graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05178</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Graph u-nets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding the representation power of graph neural networks in learning graph topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Dehmamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15387" to="15397" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
