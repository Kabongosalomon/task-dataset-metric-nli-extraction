<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Focusing and Diffusion: Bidirectional Attentive Graph Convolutional Networks for Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Gao</surname></persName>
							<email>jialingao@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
							<email>zhouxi@cloudwalk.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Ge</surname></persName>
							<email>geshiming@iie.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Focusing and Diffusion: Bidirectional Attentive Graph Convolutional Networks for Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A collection of approaches based on graph convolutional networks have proven success in skeleton-based action recognition by exploring neighborhood information and dense dependencies between intra-frame joints. However, these approaches usually ignore the spatial-temporal global context as well as the local relation between interframe and intra-frame. In this paper, we propose a focusing and diffusion mechanism to enhance graph convolutional networks by paying attention to the kinematic dependence of articulated human pose in a frame and their implicit dependencies over frames. In the focusing process, we introduce an attention module to learn a latent node over the intra-frame joints to convey spatial contextual information. In this way, the sparse connections between joints in a frame can be well captured, while the global context over the entire sequence is further captured by these hidden nodes with a bidirectional LSTM. In the diffusing process, the learned spatial-temporal contextual information is passed back to the spatial joints, leading to a bidirectional attentive graph convolutional network (BAGCN) that can facilitate skeleton-based action recognition. Extensive experiments on the challenging NTU RGB+D and Skeleton-Kinetics benchmarks demonstrate the efficacy of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition is a challenging vision problem that has been studied for years. It plays an essential role in various applications, such as video surveillance <ref type="bibr" target="#b23">[24]</ref>, patient monitoring <ref type="bibr" target="#b19">[20]</ref>, robotics <ref type="bibr" target="#b39">[40]</ref>, human-machine interaction <ref type="bibr" target="#b9">[10]</ref> and so on <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28]</ref>. Previous methods mainly rely on two different data modalities: RGB images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref> and skeleton points <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. Compared with 2D images, 3D (skeleton) points directly encode object shape and motion <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6]</ref> and thus are <ref type="bibr">Figure 1</ref>. Previous methods ignore the implicit inter-frame and intra-frame connections, which inspired us to learn a latent transformer node and then convey the global spatial-temporal context information back to spatial joints in each frame. more robust to nuisances like fast motions, camera viewpoint changes, etc.</p><p>Conventional attempts <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22]</ref> take skeletons as a sequence of vectors generated by encoding all joints in the same frame together. Some deep-learning-based methods, such as CNNs, employ convolution operation directly on skeletons, the irregular grid data. These approaches ignore the intrinsically kinematic dependencies and structural information of human pose, leading to unsatisfactory performance.</p><p>To exploit the structure of skeleton data, Yan et al. <ref type="bibr" target="#b38">[39]</ref> first introduces the spatial-temporal Graph Convolutional Network (GCN) based on coordinated joints and the natural topology of the human body. However, this predefined graph structure suffers from two defects, as shown in <ref type="figure">Fig.1: 1</ref>) the graph only considers physically structural information but ignores the semantic connections among intra-frame joints. For example, the implicit correlation between hands and head is crucial to recognize the action of "drinking." 2) the inter-frame edges only connect the same joints over consecutive frames, which lack modeling the connections between inter-frame joints. Another challenge is that a complete action lasts from several seconds to minutes. Thus, it is crucial to design a model that is capable of reducing noise from irrelevant joints and focusing on informative ones. Temporal context modeling is usually expected to be helpful for addressing this issue, inspired by which in this paper we intend to capture both the physical connections and semantic dependencies beyond the spatial and temporal neighborhood restrictions of GCN.</p><p>Several methods, like AS-GCN <ref type="bibr" target="#b17">[18]</ref> and 2s-AGCN <ref type="bibr" target="#b29">[30]</ref>, attempt to overcome the aforementioned first drawback but they introduce additional challenges to be solved. For obtaining kinematic dependency, 2s-AGCN introduces a module to learn the additional data-driven morphing graph over layers, and AS-GCN proposes an encoder-decoder structure to capture action-specific dependencies combined with physically structural links between joints. Nevertheless, it is inefficient to directly model the relationship between nonneighbor joints over a fully-connected graph. These potentially dense connections are also tricky to learn because of the noise from irrelevant ones, which leads to difficulty in optimization and unsatisfied performance. Besides, they also ignore the correlations between inter-frame joints so that they cannot take advantage of the temporal contextual information.</p><p>To overcome such limitations, we introduce a focusing and diffusion mechanism to enhance the graph convolutional networks to receive information beyond the spatial and temporal neighborhood restrictions. We first represent the human skeleton in each frame by constructing a focus graph and diffusion graph. Then we propose an efficient way to propagate information over these graphs though a learnable latent transformer node. This way captures a sparse but informative connections between joints in each frame, since not all the implicit correlations are activated in an action. It also agrees with the fact that informative connections may also change over the frames. During the training stage, the latent transformer node learns how to connect the action-specific related joints and passes message forth and back though the focusing and diffusion graphs.</p><p>However, there remains a problem of how to model the relation between intra-frame and inter-frame joints. To solve this, we take advantage of the latent transformer node again. We empirically show that the node learns the informative spatial context in each frame, generating a sequence of latent spatial nodes. To capture the temporal contextual information over frames, we introduce a context-aware module consisting of bidirectional LSTM cells to model temporal dynamics and dependencies based on the learned spatial latent nodes.</p><p>With the focusing and diffusion mechanism, a graph convolutional network can selectively focus on the informative joints in each frame, capture the global spatialtemporal contextual information and further convey it back to augment node embedding in each frame. We also develop a Bidirectional Attentive Graph Convolutional Network (BAGCN) as an exemplar and evaluate its effectiveness on two challenging public benchmarks: NTH-RGB+D and Skeleton-Kinetics. In summary, our main contributions are three-fold:</p><p>1) We propose a novel representation of the human skeleton data in a single frame by constructing two opposite-direction graphs and also introduce an efficient way of message passing in the graph.</p><p>2) We design a new network architecture, Bidirectional Attentive Graph Convolutional Network (BAGCN), that learns spatial-temporal context from human skeleton sequences leveraging a graph convolutional networks based focusing and diffusion mechanism.</p><p>3) Our BAGCN model is compared against other state-ofthe-arts methods on the challenging NTU-RGB+D and Skeleton-Kinetics benchmarks, and demonstrate superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Skeleton-based action recognition: The methods can be divided into handcraft feature based <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9]</ref> and deep learning based <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39]</ref>. The former encounters challenges in designing representative features, which results in limited recognition accuracy. But the latter is data-driven and improves the performance by a great margin. There are three types of deep models used widely: RNNs, CNNs and GCNs. RNN-based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> methods usually take the skeletons as a sequence of vectors and models their temporal dependencies over frames. CNN-based approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> employ convolution on skeleton data by regarding it as a pseudo-image.</p><p>Recently, graph-based methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b17">18]</ref> perform superior classification accuracies. ST-GCN <ref type="bibr" target="#b38">[39]</ref> is the first one to employ graph convolution. SR-TSL <ref type="bibr" target="#b30">[31]</ref> uses gated recurrent unit (GRU) to propagate messages on graphs and uses LSTM to learn the temporal features. AS-GCN <ref type="bibr" target="#b17">[18]</ref> learns actional links simultaneously with structural links in their graph model. 2s-AGCN <ref type="bibr" target="#b29">[30]</ref> introduces an adaptive module to construct a data-driven morphing graphs over layers. However, these two methods meet the same difficulties in optimization due to noise from irrelevant joints or bones. Further, DGNN <ref type="bibr" target="#b28">[29]</ref> constructs a directed graph and define the message passing rules for node and edge updating. However, they ignore the implicit correlations between intra-frame and inter-frame joints. Graph convolutional networks: Graph convolutional networks (GCNs), which generalizes convolution from image to graph, has been successfully applied in many applications. The way of constructing GCNs on graph can be split into 2 categories: the spectral <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15]</ref> and spatial <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref> perspective. The former utilizes graph Fourier transform performing convolution in frequency domain. Differently, the latter directly performs the convolution filters on the graph according to predefined rules and neighbors. We follow the second way to construct the CNN filters on the spatial domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>To obtain the physically and semantically connections sparsely, we introduce a focusing and diffusion mechanism to enhance the graph convolutional networks' ability in receiving information from other joints of intra-frame and inter-frame. First, we define focusing and diffusion graphs based on the skeletons. Then we formulate how to convey message over constructed graphs. Afterwards, we introduce an exemplar of our focusing and diffusion mechanism, developing a bidirectional attentive graph convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Definition</head><p>The raw skeleton data consists of a sequential articulated human pose coordinates. Previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref> construct an undirected spatial temporal graph the same as <ref type="bibr" target="#b38">[39]</ref>. Given a spatial-temporal graph G = {V, E}, where V = {v t i |t = 1, · · · , T, i = 1, · · · , V } is the set of V body joints and E = {E S , E T } is the set of intra-skeleton and inter-frame bones. E S contains the natural connections in articulated human pose and E T consists of the joints' trajectories between consecutive two frames. As shown in the left part of <ref type="figure" target="#fig_0">Fig.2</ref>, the vertexes denote body joints and the naturally connections between them represent the bone.</p><p>Different from previous works, we define an undirected spatial temporal graph as two directed graphs with opposite direction for each edges inspired by <ref type="bibr" target="#b13">[14]</ref>, where G = {{V, − → E }, {V, ← − E }}, as shown in the right part of <ref type="figure" target="#fig_0">Fig.2</ref>. The message passing between joints is bi-directional, which is expected to exploit the kinematics dependencies and help the center joint and peripheral joints receive information from each other. <ref type="bibr" target="#b38">[39]</ref> indicates that a graph with spatial configuration partitioning strategy exploits more location information. We follow this conclusion and split the neighbors into three subsets: 1) the root node itself; 2) the centripetal group; 3) the centrifugal group. That we have S = {root, closer, far}. It is according to the distance from the gravity center of the skeleton. Hence, we define the message passing in focus graphs from a joint v i close to the center of gravity to a joint v j far from the center of gravity and that direction is opposite in diffusion graphs. The bone information e ij is obtained by calculating the coordinates difference between these two points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ST-GCN</head><p>Let A ∈ {0, 1} n×n be the adjacency matrix of our graph G, where A = {A f , A d } represents the corresponding adjacency matrices to focus graph {V, − → E } and diffusion graph {V, ← − E } , respectively. For each sub-graph, we have</p><formula xml:id="formula_0">s∈S A f s = A f and s∈S A d s = A d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Building Block with Graph Convolution</head><p>We have defined the sequential skeletons as two directed spatial-temporal graphs. The problem becomes how to passing messages in these directed graphs. We adopt a similar implementation of spatial graph convolution as in <ref type="bibr" target="#b14">[15]</ref>, which employs a weighted average of neighboring features for updating intra-frame joints' embedding. According to the definition in G, each graph convolution layer is followed by a temporal convolution, so that we only need to implement graph convolution operation in a single frame case.</p><p>For message passing, we first convey information though focus graphs and then transform the updated features back via diffusion graphs. Let X in ∈ R V ×T ×C be the nodes' embedding of our focus graph from a certain intermediate layer. Before the focus stage, the node embedding is first <ref type="bibr">Figure 3</ref>. Illustration of our bidirectional attentive graph convolutional network for skeleton-based action recognition. Besides the stacked spatial-temporal graph convolution, we introduce a focusing and diffusion mechanism to learn a latent transformer node and bidirectionally convey the local context over frames. Once capturing the global spatial-temporal contextual information, spatial joints learn how much context should be absorbed for updating node embeddings in each frame. updated via a graph convolution</p><formula xml:id="formula_1">X mid = s∈S M s ⊗Â f s X in W s ,<label>(1)</label></formula><formula xml:id="formula_2">whereÂ f s = Λ 1 2 s A f s Λ 1 2 s ∈ R V ×V is the normalized adja- cent matrix for each group, M s ∈ R V ×V is a learnable edge importance weighting and ⊗ denotes element-wise produc- tion between two matrix. W s ∈ R C×C ×1×1 is the weight matrix, which is implemented via 1 × 1 convolution opera- tion.</formula><p>Then, these feature maps will be transformed to learn a latent node during the focus stage, which performs a transformer to pass message to arbitrary nodes beyond the temporal and spatial restrictions. This latent node also can be regarded as the spatial context over a single frame graph. In order to capture the temporal contextual information, we propose to learn the context though a context-aware module, denoted as CAM . Afterward, the learned spatial-temporal contextual information can be conveyed back to the spatial node in each frame. Every node learns how much context should be used for embedding during the diffusion stage. For notation, F f and F d represent the focusing and diffusion function, respectively. Hence, the latent node G S and temporal context G T during the focusing and diffusion can be formulated as:</p><formula xml:id="formula_3">           G S = F f (X mid ) G ST = CAM (G S ) X g = F d (G ST ) X mid = F ([Xmid, X g ]),<label>(2)</label></formula><p>where F indicates a simple fully-connected layer and [·, ·] denotes concatenating operation.</p><p>The latent node models global spatial-temporal contextual information which is selectively transferred to update the node features in a single frame graph. GivenX mid enhanced with our focusing and diffusion mechanism, another graph convolution operates on the diffusion graph A d so that the updated node embedding as:</p><formula xml:id="formula_4">X out = s∈S M s ⊗Â d sXmid W s ,<label>(3)</label></formula><formula xml:id="formula_5">whereÂ d s = Λ 1 2 s A d s Λ 1 2</formula><p>s ∈ R V ×V is the normalized adjacent matrix for each group in diffusion graphs, M s ∈ R V ×V is a learnable edge importance weighting and ⊗ denotes element-wise multiplication operator. W s ∈ R C ×C ×1×1 is the weight matrix. Similar to the conventional building block, we add a BN layer and a ReLU layer atfer each graph convolution. With this building blocks, we can construct our bidirectional attentive graph convolutional network as shown in <ref type="figure">Fig.3</ref>, the details of which implementation is shown in next session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Context-aware Graph Focusing and Diffusion</head><p>In this section, we illustrate an exemplar of our focusing and diffusion mechanism in details, which is shown in <ref type="figure" target="#fig_1">Fig.4</ref>. First, we introduce attentive focus module and then a temporal context-aware module. After learning the spatialtemporal contextual information with a latent node, we develop a dynamic diffusion to convey the global context back to spatial nodes in each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Dynamic Attentive Focusing</head><p>The purpose of the focusing stage is to learn a latent node as a transformer, which carries information between nodes in a single frame graph beyond spatial neighborhood restrictions. Besides, this latent transformer node endows a model with the ability of selectively focusing on the informative joints in each frame.</p><p>Specifically, given an input f in ∈ R V ×T ×C , our attentive focus learns a latent node G S ∈ R 1×T ×C , which sat-</p><formula xml:id="formula_6">isfies G t S = W t f f t in W 1<label>(4)</label></formula><p>where W 1 ∈ R C ×C ×1×1 is a 1 × 1 convolution operation for node embedding and W f ∈ R 1×T ×V is a learnable matrix to focus the information over a single frame graph. It is equivalent to the attention mechanism combining features from all nodes over a spatial graph. In our spatial-temporal graphs, it is also temporally dynamic and each spatial graph in a sequence has its own attention causing a series of global spatial contexts. A model can learn how to selectively transform features to the latent node by our attentive focus and receive their feedback in the diffusion stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Temporal Context-aware Module</head><p>We exploit the temporal dependencies between learned latent nodes in the graph sequence via Context-Aware Module (CAM) for transforming global spatial-temporal context. Once each spatial graph obtains its G S , the left problem is how to model the temporal contextual information based on these. Long-Short-term Memory (LSTM) <ref type="bibr" target="#b7">[8]</ref> has been validated its strengths in modeling the dependencies and dynamics in sequential data. Hence, we utilize it to build our context-aware module. A LSTM cell contains an input gate i t , a forget gate f t , a output gate o t and an input cell g t , which are computed by the following functions:</p><formula xml:id="formula_7">     i t f t o t g t      =      σ σ σ tanh      W Ĝ t S H t−1<label>(5)</label></formula><p>where H t−1 is the previous hidden state at time t, W is a transformation matrix with learnable parameters, σ and tanh are the activation functions.Ĝ t S is the projected latent node features, which isĜ S = G S W 2 with a weight matrix W 2 ∈ R C ×C ×1×1 . The memory cell c t and hidden state H t are updated by :</p><formula xml:id="formula_8">c t = f t ⊗ c t−1 + i t ⊗ g t H t = o t ⊗ tanh(c t )<label>(6)</label></formula><p>where ⊗ represents element-wise multiplication operator.</p><p>Considering the directed message passing, we stack two bidirectional LSTM cells as our temporal context-aware module. Thus, the learned spatial-temporal context can be formulated as</p><formula xml:id="formula_9">G ST = [ ← − H, − → H] ∈ R 1×T ×Ĉ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Dynamic Attentive Diffusion</head><p>For diffusion stage, we expect the neural network to learn how much spatial-temporal context should be passed back when updating node features. With this diffusion, one node in a single graph can receive the information from interframe and intra-frame nodes, which indicates the learned latent node can be regarded as a transformer. In details, the net firstly learns a weight matrix W d ∈ R V ×T ×1 for conveying global context to every spatial node in each frame. Then the node v i absorbs the diffused contextual information f vi g to concatenate with its node embedding f vi in . This combined node features are embedded via a 1 × 1 convolution operation with a learnable W 3 ∈ R (C +Ĉ)×C ×1×1 . Hence, the diffusion is formulated as:</p><formula xml:id="formula_10">f t g = W t d G t ST f out = [f in , f g ]W 3<label>(7)</label></formula><p>where [·, ·] denotes concatenating along channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the effectiveness of our approach, we conduct extensive experiments to compare with state-of-the-art methods on two challenging benchmarks: NTU-RGB+D <ref type="bibr" target="#b27">[28]</ref> and Skeleton-Kinetics <ref type="bibr" target="#b38">[39]</ref>. Considering the limited computing resources, we choose the relative smaller NTU-RGB+D dataset to analyze the effect of components in our Bidirectional Attentive Graph Convolutional Network(BAGCN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Setting</head><p>NTU-RGB+D: NTU-RGB+D <ref type="bibr" target="#b27">[28]</ref> is collected by Microsoft Kinect v2 with joints annotations. It consists of 56,880 video samples from 60 categories of human actions. 40 distinct subjects perform various actions, which are captured by three cameras simultaneously with different horizontal angles: -45 o , 0 o , 45 o . The human pose are articulated as 25 joints and their coordinates are labeled as annotations. The evaluation of this dataset is divided into two standard protocols: Cross-Subject (X-sub) and Cross-View (X-view). In the former configuration, half of the 40 subjects consists of the training set and the other for testing. For the latter, training and testing groups are split in terms of the camera views, where the training group has 37920 video samples from camera 2 and 3. Only top-1 accuracy is reported on both of the two protocols. Data preprocessing follows that in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Skeleton-Kinetics: DeepMind Kinetics <ref type="bibr" target="#b10">[11]</ref> human action video dataset contains 400 human action classes, with at least 400 video clips for each action taken from the Internet. Its activity categories contains various indoors and outdoors daily actions, like driving car and dying hair. The skeleton-kinetics data was obtained by estimating the location of 18 articulated joints on every frame with public available OpenPose <ref type="bibr" target="#b1">[2]</ref> toolbox. The annotation of these joints consists of their 2D coordinates and 1D confidence score. We follow the same data preparation strategy in ST-GCN <ref type="bibr" target="#b38">[39]</ref> for fair comparison and select two bodies from multi-person cases according to the highest average joints confidence. All the samples are resized to 300 frames by padding. Top-1 and Top-5 classification accuracy are chosen to serve as evaluation metric with 240,000 and 20,000 clips for training and testing, respectively.</p><p>Implementation details: Here, we introduce how to implement our Bidirectional Attentive Graph Convolutional Network (BAGCN) and the training details. We concatenate joints and bones along the channel dimension following <ref type="bibr" target="#b29">[30]</ref> and fed it into networks for almost the experiments.</p><p>Our BAGCN consists of 9 building blocks and the first layer have 64 channels for output. In the 4-th and 7-th blocks, we double the number of channels while downsample the temporal length by a factor 2, as the same in ST-GCN <ref type="bibr" target="#b38">[39]</ref>. A data BN layer performs the normalization at the beginning and a global average pooling layer follows the last block to generate a 256 dimension vector for each sequence, which is then classified by a softmax classifier. For the context-aware focusing and diffusion, C and C is the input and output channels of corresponding blocks, where C = 1 4 C andĈ = 128. Besides, W f and W d share the weight and learn from f in directly using a C × 1 × 1 × 1 convolution. Stochastic gradient descent with 0.9 momentum, weight decay with 0.0001 and Cross-entropy are applied for training. We initialize the base learning rate as 0.1 <ref type="table">Table 1</ref>. Top-1 accuracy of action recognition on NTU-RGB+D compared with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Subject Cross-View</head><p>Lie Group <ref type="bibr" target="#b34">[35]</ref> 50.1% 82.8% H-RNN <ref type="bibr" target="#b2">[3]</ref> 59.1% 64.0% Deep LSTM <ref type="bibr" target="#b27">[28]</ref> 60.7% 67.3% ST-LSTM <ref type="bibr" target="#b20">[21]</ref> 69.2% 77.7% STA-LSTM <ref type="bibr" target="#b31">[32]</ref> 73.4% 81.2% Temporal Conv <ref type="bibr" target="#b12">[13]</ref> 74.3% 84.1% Clip+CNN+MTLN <ref type="bibr" target="#b11">[12]</ref> 79.6% 84.8% Synthesized CNN <ref type="bibr" target="#b21">[22]</ref> 80.0% 87.2% ST-GCN <ref type="bibr" target="#b38">[39]</ref> 81.5% 88.3% Two-Stream CNN <ref type="bibr" target="#b15">[16]</ref> 83.2% 89.3% SR-TSL <ref type="bibr" target="#b30">[31]</ref> 84.8% 92.4% HCN <ref type="bibr" target="#b16">[17]</ref> 86.5% 91.1% AS-GCN <ref type="bibr" target="#b17">[18]</ref> 86.8% 94.2% 2s-AGCN <ref type="bibr" target="#b29">[30]</ref> 88.5% 95.1% DGNN <ref type="bibr" target="#b28">[29]</ref> 89.9% 96.1% BAGCN (Ours) 90.3% 96.3%</p><p>and decay it by 10 at 30 th and 40 th epoch of 50 epochs for NTU-RGB+D dataset while 20 th , 40 th 55 th of 65 epochs for Skeleton-Kinetics dataset. Besides, we also explore the spatial and motion information referred in DGNN <ref type="bibr" target="#b28">[29]</ref>. The motion stream takes the movements of joints and the deformations of bones as input, which is exploited only in the last part in ablation study. Otherwise, all the other experiments are based on the spatial stream for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Compared with State-of-the-art Methods</head><p>To validate the superior performance of the BAGCN, we compare it with state-of-the-arts on both NTU-RGB+D and Skeleton-Kinetics datasets.</p><p>For the former, we report the cross-subject and crossview protocols of all the compared methods in Tab.1. These approaches can be divided into handcraft-based <ref type="bibr" target="#b34">[35]</ref>, RNNbased <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref>, CNN-based <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22]</ref> and recent GCN-based <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29]</ref>. Our model outperforms previous methods. Compared with our baseline ST-GCN, the context-aware focusing and diffusion mechanism enhances the performance by a great margin, which increases from 81.5% to 90.3% and 88.3% to 96.3% in terms of cross-subject and cross-view, respectively. This performance gain verifies the superiority and effectiveness of BAGCN. The effect of components in BAGCN is explored in the later ablation study.</p><p>For Skeleton-Kinetics datasets, we compare with other methods in terms of top-1 and top-5 accuracies in Tab.2. We see that our BAGCN achieves the best performance, which is identical to the former. From these two table, it is found that graph-based methods exploit better kinematic  <ref type="bibr" target="#b17">[18]</ref> 34.8% 56.5% 2s-AGCN <ref type="bibr" target="#b29">[30]</ref> 36.1% 58.7% DGNN <ref type="bibr" target="#b28">[29]</ref> 36.9% 59.6% BAGCN(Ours) 37.3% 60.2% <ref type="table">Table 3</ref>. Comparison with the baseline ST-GCNs and focusing ways on NTU-RGB+D validated by Cross-Subjects protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Accuracy</head><p>ST-GCN * 81.5% ST-GCN <ref type="bibr" target="#b38">[39]</ref> 84.8%</p><p>A-link GCN + 83.2% S-link GCN + 84.2% AS-GCN <ref type="bibr" target="#b17">[18]</ref> 86.1% BAGCN (wo/F) 86.4% BAGCN (max) 87.4% BAGCN (avg) 87.8% BAGCN (att) 89.4% * Original results reported in <ref type="bibr" target="#b38">[39]</ref> with only joints information + Actional links (A-link) and Structural links (S-link) learned in <ref type="bibr" target="#b17">[18]</ref> dependency than RNN/CNN based ones and our focusing and diffusion mechanism can achieve the message passing between intra-frame joints and inter-frame joints for better recognition accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this session, we evaluate the effect of components in our bidirectional attentive graph convolutional network. In details, we analyze each component by testing recognition performance of BAGCN without that part on NTU-RGB+D. Besides, we also exploit the impact from different modalities. Compared with the X-view protocol, the X-subject requires a model the ability to recognize actions performed by unseen subjects, which is more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Attentive Focusing and Diffusion</head><p>We analyze the importance of our attentive focusing and diffusion by exploring the different ways of focusing. First, we delete it from our Bidirectional Attentive Graph Convolutional network (BAGCN) and then investigate three types for the focusing stage, results shown in Tab.3.</p><p>Without attentive focusing and diffusion. The origi-nal performance in ST-GCN <ref type="bibr" target="#b38">[39]</ref> is increased to 84.8% with the concatenated joints and bones information, serving as our baseline method. According to the graph construction, our BAGCN degrades to bidirectional ST-GCN without the focusing and diffusion, denoted as BAGCN (wo/F). The message first flows out from the center joint to peripheral joints and then flows back after nodes updating. This message passing way improves the performance from 84.8% to 86.4%, that indicates the necessity for a joint node receiving information from spatial non-neighbor joints. AS-GCN <ref type="bibr" target="#b17">[18]</ref> also captures intra-frame dependencies beyond physical connection to update node embeddings. It learns actional links (A-link) and structural links (S-link) simultaneously, which causes difficulty in optimizing. The limited performance of AS-GCN (86.1%) illustrates this problem and it demonstrates the effectiveness and efficiency of our proposed way of message passing in turn. On the other hand, it indicates that dense dependencies make it hard for the net to overcome noise from those irrelevant ones.</p><p>With other ways of focusing and diffusion. For the focus stage, a latent transformer node first passes the spatial contextual information in a frame and then captures temporal dynamics over frames for spatial-temporal contextual information. This global context will be conveyed back to each spatial skeleton graph in the diffusion stage. Two more types of focus way, average and max-pool, are investigated for proving the importance of attentive focus. Recognition performances are shown in the last three lines of Tab.3.</p><p>It is observed that attentive focusing and diffusion can significantly improve performance. For the max-pool configuration, our BAGCN takes embedding features of maximum response node to initialize the latent node in each frame and then captures temporal contextual information between those maximum response nodes. This captured spatial-temporal context is demonstrated useful in augmenting with the spatial node embedding for every frame, owing to the performance increased by 1.0% compared with BAGCN (wo/F). However, It is not reasonable to only focus on the maximum responsive node and ignore those physically associated or semantically associated joint nodes in an action. Hence, we can find that the other two configurations bring higher performance improvement. The average focus way, denoted as BAGCN (avg), emphasizes the mean statistics of the joint nodes' response in every graph convolutional layer and finally achieves 87.8%. Further, BAGCN (att) leans dynamic spatial attention over graph layers for each frame to selectively pass message forth and back between informative joints. It leads to around 6% performance gain compared with our baseline ST-GCN. This attentive focusing and diffusion learns a sequence of latent nodes to capture spatial and temporal contextual information from implicit connections and allows spatial nodes learn how much context should be used when updating node <ref type="figure">Figure 5</ref>. Illustration of the associated joints with our learned latent transformer node in three action samples. Actions from left to right are reading, hand waving and face wiping, respectively. The red points represent the informative joint nodes and activated joints associated with specific action. The larger circle denotes more informative and relative irrelevant joints are ignored in this <ref type="figure">figure.</ref> embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Latent Edge Visualization</head><p>Various actions my refer to activate different joints while ignore the irrelevant ones. <ref type="figure">Fig.5</ref> shows some action samples and their associated joints with our learned latent transformer node. We choose the last graph convolutional layer to display the implicitly sparse dependencies between joint nodes. The attention matrix W f ∈ R T ×V measures the informative degree of connections between the latent transformer node and joints nodes. In the attentive focusing, if the value of W vi f is larger than 0.8, the connection between latent and joint will be drawn. All the connected joints reflects the activated joint nodes related to a certain action.</p><p>It can be seen that reading action, left part in <ref type="figure">Fig.5</ref>, refers to hands and head joints, which agrees with the kinematic dependency. The middle part in <ref type="figure">Fig.5</ref> displays the informative joints in hand waving and the right part of <ref type="figure">Fig.5</ref> illustrates action-specific related body parts in face wiping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Temporal Context-aware or not</head><p>The latent node captures spatial contextual information in a frame and passes its message back after node updating. In this section, we evaluate the effect of temporal contextaware module, which is used to extract the global context in temporal dimension. First, we delete the context-aware and then employ two different types of LSTM for modeling temporal dynamics. The configuration of only learning a latent node with focusing and diffusion graphs obtains the 86.6% classification accuracy in Tab.4. Its performance is only 0.2% higher than BAGCN (wo/F). In contrast, results in the last two lines demonstrates the power of temporal context-aware module. BAGCN (1-Ca) represents one direction temporal context module achieving 87.3% and BAGCN (2-Ca) denotes bi-directional one can obtain 89.4% recognition accuracy, a higher improvement. These improvements validate the effectiveness and neces- sity of capturing spatial-temporal contextual information for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Two-stream or not</head><p>Inspired by previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29]</ref>, we test the supplementary effect of motion information in our bidirectional attentive graph convolutional network. We perform experiments to train the spatial and motion branch separately and then employ late fusion on NTU-RGB+D dataset in Cross-Subjects and Cross-View protocols, which are shown in Tab.5. From this table, it can be found that the spatial branch achieves higher classification performance than the motion one while fusing them together can still improve the accuracy from 89.4% to 90.3% in Cross-subject and 95.6% to 96.3% in cross-view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduce a focusing and diffusion mechanism to enhance the graph convolutional network for receiving the information beyond the spatial and temporal neighbors restrictions. Besides, we propose a context-aware module to capture the global spatial-temporal contextual information for modeling implicit correlations between interframe and intra-frame joints. We then develop a Bidirectional Attentive Graph Convolutional Network (BAGCN) as an exemplar. This graph-based model is enhanced with our context-aware focusing and diffusion mechanism. Its superior performance is demonstrated on two public challenging skeleton-based action recognition benchmarks: NTU-RGB+D and Skeleton-Kinetics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Undirected graphs (left) can be modeled by explicitly assigning two directed edges in opposite direction for each undirected edge, leading to two directed graphs(right). The black node denotes the center node and arrow indicates the passing direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Implementation details of the context-aware focusing and diffusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-art methods of on Skeleton-Kinetics in terms of Top-1 and Top-5 classification accuracies.</figDesc><table><row><cell>Methods</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>Feature Enc. [4]</cell><cell cols="2">14.9% 25.8%</cell></row><row><cell>Deep LSTM [28]</cell><cell cols="2">16.4% 35.3%</cell></row><row><cell cols="3">Temporal Conv[13] 20.3% 40.0%</cell></row><row><cell>ST-GCN [39]</cell><cell cols="2">30.7% 52.8&amp;</cell></row><row><cell>AS-GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Comparison of classification performance with or withoutContext-aware (Ca) on NTU-RGB+D validation dataset in Cross-Subjects protocol</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell>Accuracy</cell><cell></cell></row><row><cell></cell><cell>ST-GCN [39]</cell><cell></cell><cell>84.8%</cell><cell></cell></row><row><cell cols="3">BAGCN (wo/F)</cell><cell>86.4%</cell><cell></cell></row><row><cell cols="3">BAGCN (wo/Ca)</cell><cell>86.6%</cell><cell></cell></row><row><cell cols="3">BAGCN (1-Ca)</cell><cell>87.3%</cell><cell></cell></row><row><cell cols="3">BAGCN (2-Ca)</cell><cell>89.4%</cell><cell></cell></row><row><cell cols="5">Table 5. Comparison of classification performance with spatial in-</cell></row><row><cell cols="5">formation, motion information and their fusion on NTU-RGB+D</cell></row><row><cell cols="5">validation dataset in Cross-Subjects and Cross-views protocols</cell></row><row><cell cols="4">and Skeleton-Kinetics in top-1,5 accuracies</cell><cell></cell></row><row><cell>Methods</cell><cell>X-Sub</cell><cell>X-View</cell><cell>Top-1</cell><cell>Top5</cell></row><row><cell>Spatial</cell><cell>89.4%</cell><cell>95.6%</cell><cell>36.4%</cell><cell>58.9%</cell></row><row><cell>Motion</cell><cell>86.3%</cell><cell>94.1%</cell><cell>31.8%</cell><cell>54.9%</cell></row><row><cell>Fusion</cell><cell>90.3%</cell><cell>96.3%</cell><cell>37.3%</cell><cell>60.2%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geonet: Deep geodesic networks for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chihao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6888" to="6897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mono3d++: Monocular 3d vehicle detection with two-scale 3d hypotheses and task priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Mohamed E Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motaz</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Third International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human action recognition in unconstrained videos by explicit motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3781" to="3795" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ferdous Sohel, and Farid Boussaid. A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senjian</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE conference on computer vision and pattern recognition workshops (CVPRW)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04687</idno>
		<title level="m">Neural relational inference for interacting systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cooccurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06055</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple to complex transfer learning for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuoyang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunmei</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="949" to="960" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning scene flow in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Twostream adaptive graph convolutional networks for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtyfirst AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning non-rigid 3d shape from 2d motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1555" to="1562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-person interaction detection using body-pose features and multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiwon</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Honorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debaleena</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
