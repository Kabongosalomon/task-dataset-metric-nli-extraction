<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Two-Stream Network Fusion for Video Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<email>feichtenhofer@tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
							<email>axel.pinz@tugraz.at</email>
							<affiliation key="aff1">
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Two-Stream Network Fusion for Video Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters; (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy; finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves stateof-the-art results. Our code and models are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition in video is a highly active area of research with state of the art systems still being far from human performance. As with other areas of computer vision, recent work has concentrated on applying Convolutional Neural Networks (ConvNets) to this task, with progress over a number of strands: learning local spatiotemporal filters <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>), incorporating optical flow snippets <ref type="bibr" target="#b21">[22]</ref>, and modelling more extended temporal sequences <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>However, action recognition has not yet seen the substantial gains in performance that have been achieved in other areas by ConvNets, e.g. image classification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>, human face recognition <ref type="bibr" target="#b20">[21]</ref>, and human pose estimation <ref type="bibr" target="#b28">[29]</ref>. Indeed the current state of the art performance <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref> on standard benchmarks such as UCF-101 <ref type="bibr" target="#b23">[24]</ref> and HMDB51 <ref type="bibr" target="#b12">[13]</ref> is achieved by a combination of ConvNets and a Fisher Vector encoding <ref type="bibr" target="#b19">[20]</ref> of hand-crafted Spatial Stream Temporal Stream * <ref type="figure">Figure 1</ref>. Example outputs of the first three convolutional layers from a two-stream ConvNet model <ref type="bibr" target="#b21">[22]</ref>. The two networks separately capture spatial (appearance) and temporal information at a fine temporal scale. In this work we investigate several approaches to fuse the two networks over space and time.</p><p>features (such as HOF <ref type="bibr" target="#b13">[14]</ref> over dense trajectories <ref type="bibr" target="#b32">[33]</ref>).</p><p>Part of the reason for this lack of success is probably that current datasets used for training are either too small or too noisy (we return to this point below in related work). Compared to image classification, action classification in video has the additional challenge of variations in motion and viewpoint, and so might be expected to require more training examples than that of ImageNet (1000 per class) -yet UCF-101 has only 100 examples per class. Another important reason is that current ConvNet architectures are not able to take full advantage of temporal information and their performance is consequently often dominated by spatial (appearance) recognition.</p><p>As can be seen from <ref type="figure">Fig. 1</ref>, some actions can be identified from a still image from their appearance alone (archery in this case). For others, though, individual frames can be ambiguous, and motion cues are necessary. Consider, for example, discriminating walking from running, yawning from laughing, or in swimming, crawl from breast-stroke. The two-stream architecture <ref type="bibr" target="#b21">[22]</ref> incorporates motion information by training separate ConvNets for both appearance in still images and stacks of optical flow. Indeed, this work showed that optical flow information alone was sufficient to discriminate most of the actions in UCF101.</p><p>Nevertheless, the two-stream architecture (or any previous method) is not able to exploit two very important cues for action recognition in video: (i) recognizing what is mov-ing where, i.e. registering appearance recognition (spatial cue) with optical flow recognition (temporal cue); and (ii) how these cues evolve over time.</p><p>Our objective in this paper is to rectify this by developing an architecture that is able to fuse spatial and temporal cues at several levels of granularity in feature abstraction, and with spatial as well as temporal integration. In particular, Sec. 3 investigates three aspects of fusion: (i) in Sec. 3.1 how to fuse the two networks (spatial and temporal) taking account of spatial registration? (ii) in Sec. 3.2 where to fuse the two networks? And, finally in Sec. 3.3 (iii) how to fuse the networks temporally? In each of these investigations we select the optimum outcome (Sec. 4) and then, putting this together, propose a novel architecture (Sec. 3.4) for spatiotemporal fusion of two stream networks that achieves state of the art performance in Sec. 4.6.</p><p>We implemented our approach using the MatConvNet toolbox <ref type="bibr" target="#b30">[31]</ref> and made our code publicly available at https://github.com/feichtenhofer/twostreamfusion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Several recent work on using ConvNets for action recognition in temporal sequences have investigated the question of how to go beyond simply using the framewise appearance information, and exploit the temporal information. A natural extension is to stack consecutive video frames and extend 2D ConvNets into time <ref type="bibr" target="#b9">[10]</ref> so that the first layer learns spatiotemporal features. <ref type="bibr" target="#b10">[11]</ref> study several approaches for temporal sampling, including early fusion (letting the first layer filters operate over frames as in <ref type="bibr" target="#b9">[10]</ref>), slow fusion (consecutively increasing the temporal receptive field as the layers increase) and late fusion (merging fully connected layers of two separate networks that operate on temporally distant frames). Their architecture is not particularly sensitive to the temporal modelling, and they achieve similar levels of performance by a purely spatial network, indicating that their model is not gaining much from the temporal information.</p><p>The recently proposed C3D method <ref type="bibr" target="#b29">[30]</ref> learns 3D Con-vNets on a limited temporal support of 16 consecutive frames with all filter kernels of size 3×3×3. They report better performance than <ref type="bibr" target="#b10">[11]</ref> by letting all filters operate over space and time. However, their network is considerably deeper than <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> with a structure similar to the very deep networks in <ref type="bibr" target="#b22">[23]</ref>. Another way of learning spatiotemporal relationships is proposed in <ref type="bibr" target="#b25">[26]</ref>, where the authors factorize 3D convolution into a 2D spatial and a 1D temporal convolution. Specifically, their temporal convolution is a 2D convolution over time as well as the feature channels and is only performed at higher layers of the network.</p><p>[17] compares several temporal feature pooling architectures to combine information across longer time periods. They conclude that temporal pooling of convolutional lay-ers performs better than slow, local, or late pooling, as well as temporal convolution. They also investigate ordered sequence modelling by feeding the ConvNet features into a recurrent network with Long Short-Term Memory (LSTM) cells. Using LSTMs, however did not give an improvement over temporal pooling of convolutional features.</p><p>The most closely related work to ours, and the one we extend here, is the two-stream ConvNet architecture proposed in <ref type="bibr" target="#b21">[22]</ref>. The method first decomposes video into spatial and temporal components by using RGB and optical flow frames. These components are fed into separate deep ConvNet architectures, to learn spatial as well as temporal information about the appearance and movement of the objects in a scene. Each stream is performing video recognition on its own and for final classification, softmax scores are combined by late fusion. The authors compared several techniques to align the optical flow frames and concluded that simple stacking of L = 10 horizontal and vertical flow fields performs best. They also employed multitask learning on UCF101 and HMDB51 to increase the amount of training data and improve the performance on both. To date, this method is the most effective approach of applying deep learning to action recognition, especially with limited training data. The two-stream approach has recently been employed into several action recognition methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Also related to our work is the bilinear method <ref type="bibr" target="#b14">[15]</ref> which correlates the output of two ConvNet layers by performing an outer product at each location of the image. The resulting bilinear feature is pooled across all locations into an orderless descriptor. Note that this is closely related to second-order pooling <ref type="bibr" target="#b1">[2]</ref> of hand-crafted SIFT features.</p><p>In terms of datasets, <ref type="bibr" target="#b10">[11]</ref> introduced the Sports-1M dataset which has a large number of videos (≈1M) and classes (487). However, the videos are gathered automatically and therefore are not free of label noise. Another large scale dataset is the THUMOS dataset <ref type="bibr" target="#b7">[8]</ref> that has over 45M frames. Though, only a small fraction of these actually contain the labelled action and thus are useful for supervised feature learning. Due to the label noise, learning spatiotemporal ConvNets still largely relies on smaller, but temporally consistent datasets such as UCF101 <ref type="bibr" target="#b23">[24]</ref> or HMDB51 <ref type="bibr" target="#b12">[13]</ref> which contain short videos of actions. This facilitates learning, but comes with the risk of severe overfitting to the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We build upon the the two-stream architecture in <ref type="bibr" target="#b21">[22]</ref>. This architecture has two main drawbacks: (i) it is not able to learn the pixel-wise correspondences between spatial and temporal features (since fusion is only on the classification scores), and (ii) it is limited in temporal scale as the spatial ConvNet operates only on single frames and the temporal ConvNet only on a stack of L temporally adjacent optical flow frames (e.g. L = 10). The implementation of <ref type="bibr" target="#b21">[22]</ref> addressed the latter problem to an extent by temporal pooling across regularly spaced samples in the video, but this does not allow the modelling of temporal evolution of actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial fusion</head><p>In this section we consider different architectures for fusing the two stream networks. However, the same issues arise when spatially fusing any two networks so are not tied to this particular application.</p><p>To be clear, our intention here is to fuse the two networks (at a particular convolutional layer) such that channel responses at the same pixel position are put in correspondence. To motivate this, consider for example discriminating between the actions of brushing teeth and brushing hair. If a hand moves periodically at some spatial location then the temporal network can recognize that motion, and the spatial network can recognize the location (teeth or hair) and their combination then discriminates the action.</p><p>This spatial correspondence is easily achieved when the two networks have the same spatial resolution at the layers to be fused, simply by overlaying (stacking) layers from one network on the other (we make this precise below). However, there is also the issue of which channel (or channels) in one network corresponds to the channel (or channels) of the other network.</p><p>Suppose for the moment that different channels in the spatial network are responsible for different facial areas (mouth, hair, etc), and one channel in the temporal network is responsible for periodic motion fields of this type. Then, after the channels are stacked, the filters in the subsequent layers must learn the correspondence between these appropriate channels (e.g. as weights in a convolution filter) in order to best discriminate between these actions.</p><p>To make this more concrete, we now discuss a number of ways of fusing layers between two networks, and for each describe the consequences in terms of correspondence.</p><p>A fusion function f :</p><formula xml:id="formula_0">x a t , x b t , → y t fuses two feature maps x a t ∈ R H×W ×D and x b t ∈ R H ×W ×D , at time t,</formula><p>to produce an output map y t ∈ R H ×W ×D , where W, H and D are the width, height and number of channels of the respective feature maps. When applied to feedforward ConvNet architectures, consisting of convolutional, fullyconnected, pooling and nonlinearity layers, f can be applied at different points in the network to implement e.g. early-fusion, late-fusion or multiple layer fusion. Various fusion functions f can be used. We investigate the following ones in this paper, and, for simplicity, assume that H = H = H , W = W = W , D = D , and also drop the t subscript.</p><p>Sum fusion. y sum = f sum (x a , x b ) computes the sum of the two feature maps at the same spatial locations i, j and feature channels d:</p><formula xml:id="formula_1">y sum i,j,d = x a i,j,d + x b i,j,d ,<label>(1)</label></formula><formula xml:id="formula_2">where 1 ≤ i ≤ H, 1 ≤ j ≤ W, 1 ≤ d ≤ D and x a , x b , y ∈ R H×W ×D</formula><p>Since the channel numbering is arbitrary, sum fusion simply defines an arbitrary correspondence between the networks. Of course, subsequent learning can employ this arbitrary correspondence to its best effect, optimizing over the filters of each network to make this correspondence useful.</p><p>Max fusion. y max = f max (x a , x b ) similarly takes the maximum of the two feature map:</p><formula xml:id="formula_3">y max i,j,d = max{x a i,j,d , x b i,j,d },<label>(2)</label></formula><p>where all other variables are defined as above <ref type="formula" target="#formula_1">(1)</ref>. Similarly to sum fusion, the correspondence between network channels is again arbitrary.</p><p>Concatenation fusion. y cat = f cat (x a , x b ) stacks the two feature maps at the same spatial locations i, j across the feature channels d:</p><formula xml:id="formula_4">y cat i,j,2d = x a i,j,d y cat i,j,2d−1 = x b i,j,d ,<label>(3)</label></formula><p>where y ∈ R H×W ×2D . Concatenation does not define a correspondence, but leaves this to subsequent layers to define (by learning suitable filters that weight the layers), as we illustrate next.</p><p>Conv fusion. y conv = f conv (x a , x b ) first stacks the two feature maps at the same spatial locations i, j across the feature channels d as above <ref type="formula" target="#formula_4">(3)</ref> and subsequently convolves the stacked data with a bank of filters f ∈ R 1×1×2D×D and bi-</p><formula xml:id="formula_5">ases b ∈ R D y conv = y cat * f + b,<label>(4)</label></formula><p>where the number of output channels is D, and the filter has</p><formula xml:id="formula_6">dimensions 1 × 1 × 2D.</formula><p>Here, the filter f is used to reduce the dimensionality by a factor of two and is able to model weighted combinations of the two feature maps x a , x b at the same spatial (pixel) location. When used as a trainable filter kernel in the network, f is able to learn correspondences of the two feature maps that minimize a joint loss function.</p><p>For example, if f is learnt to be the concatenation of two permuted identity matrices 1 ∈ R 1×1×D×D , then the ith channel of the one network is only combined with the ith channel of the other (via summation). Note that if there is no dimensionality reducing convlayer injected after concatenation, the number of input channels of the upcoming layer is 2D.</p><p>Bilinear fusion. y bil = f bil (x a , x b ) computes a matrix outer product of the two features at each pixel location, followed by a summation over the locations:</p><formula xml:id="formula_7">y bil = H i=1 W j=1 x a i,j x b i,j .<label>(5)</label></formula><p>The resulting feature y bil ∈ R D 2 captures multiplicative interactions at corresponding spatial locations. The main drawback of this feature is its high dimensionality. To make bilinear features usable in practice, it is usually applied at ReLU5, the fully-connected layers are removed <ref type="bibr" target="#b14">[15]</ref> and power-and L2-normalisation is applied for effective classification with linear SVMs.</p><p>The advantage of bilinear fusion is that every channel of one network is combined (as a product) with every channel of the other network. However, the disadvantage is that all spatial information is marginalized out at this point.</p><p>Discussion: These operations illustrate a range of possible fusion methods. Others could be considered, for example: taking the pixel wise product of channels (instead of their sum or max), or the (factorized) outer product without sum pooling across locations <ref type="bibr" target="#b17">[18]</ref>.</p><p>Injecting fusion layers can have significant impact on the number of parameters and layers in a two-stream network, especially if only the network which is fused into is kept and the other network tower is truncated, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref> (left). <ref type="table">Table 1</ref> shows how the number of layers and parameters are affected by different fusion methods for the case of two VGG-M-2048 models (used in <ref type="bibr" target="#b21">[22]</ref>) containing five convolution layers followed by three fullyconnected layers each. Max-, Sum and Conv-fusion at ReLU5 (after the last convolutional layer) removes nearly half of the parameters in the architecture as only one tower of fully-connected layers is used after fusion. Conv fusion has slightly more parameters (97.58M) compared to sum and max fusion (97.31M) due to the additional filter that is used for channel-wise fusion and dimensionality reduction. Many more parameters are involved in concatenation fusion, which does not involve dimensionality reduction after fusion and therefore doubles the number of parameters in the first fully connected layer. In comparison, sum-fusion at the softmax layer requires all layers <ref type="bibr" target="#b15">(16)</ref> and parameters (181.4M) of the two towers.</p><p>In the experimental section (Sec. 4.2) we evaluate and compare the performance of each of these possible fusion methods in terms of their classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Where to fuse the networks</head><p>As noted above, fusion can be applied at any point in the two networks, with the only constraint that the two input maps x a t ∈ R H×W ×D and x b t ∈ R H ×W ×D , at time t, have the same spatial dimensions; i.e. H = H , W = W . This can be achieved by using an "upconvolutional" layer <ref type="bibr" target="#b37">[38]</ref>, or if the dimensions are similar, upsampling can be achieved by padding the smaller map with zeros. <ref type="table" target="#tab_1">Table 2</ref> compares the number of parameters for fusion at different layers in the two networks for the case of a VGG-M model. Fusing after different conv-layers has roughly the same impact on the number of parameters, as most of these are stored in the fully-connected layers. Two networks can also be fused at two layers, as illustrated in <ref type="figure" target="#fig_0">Fig. 2 (right)</ref>. This achieves the original objective of pixel-wise registration of the channels from each network (at conv5) but does not lead to a reduction in the number of parameters (by half if fused only at conv5, for example). In the experimental section (Sec. 4.3) we evaluate and compare both the performance of fusing at different levels, and fusing at multiple layers simultaneously.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal fusion</head><p>We now consider techniques to combine feature maps x t over time t, to produce an output map y t . One way of processing temporal inputs is by averaging the network predictions over time (as used in <ref type="bibr" target="#b21">[22]</ref>). In that case the architecture only pools in 2D (xy); see <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. Now consider the input of a temporal pooling layer as feature maps x ∈ R H×W ×T ×D which are generated by stacking spatial maps across time t = 1 . . . T .</p><p>3D Pooling: applies max-pooling to the stacked data within a 3D pooling cube of size W × H × T . This is a straightforward extension of 2D pooling to the temporal domain, as illustrated in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. For example, if three  2 ), to temporally adjacent inputs at a coarse temporal scale (t + T τ ). The two streams are fused by a 3D filter that is able to learn correspondences between highly abstract features of the spatial stream (blue) and temporal stream (green), as well as local weighted combinations in x, y, t. The resulting features from the fusion stream and the temporal stream are 3D-pooled in space and time to learn spatiotemporal (top left) and purely temporal (top right) features for recognising the input video. temporal samples are pooled, then a 3 × 3 × 3 max pooling could be used across the three stacked corresponding channels. Note, there is no pooling across different channels.</p><formula xml:id="formula_8">… Time t t + τ t − τ</formula><p>3D Conv + Pooling: first convolves the four dimensional input x with a bank of D filters f ∈ R W ×H ×T ×D×D and biases</p><formula xml:id="formula_9">b ∈ R D y = x t * f + b,<label>(6)</label></formula><p>as e.g. in <ref type="bibr" target="#b29">[30]</ref>, followed by 3D pooling as described above. This method is illustrated in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>. The filters f are able to model weighted combinations of the features in a local spatio-temporal neighborhood using kernels of size W × H × T × D. Typically the neighborhood is 3 × 3 × 3 (spatial × temporal).</p><p>Discussion. The authors of <ref type="bibr" target="#b16">[17]</ref> evaluate several additional methods to combine two-stream ConvNets over time. They find temporal max-pooling of convolutional layers among the top performers. We generalize max-pooling here to 3D pooling that provides invariance to small changes of the features' position over time. Further, 3D conv allows spatio-temporal filters to be learnt <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>. For example, the filter could learn to center weight the central temporal sample, or to differentiate in time or space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Proposed architecture</head><p>We now bring together the ideas from the previous sections to propose a new spatio-temporal fusion architecture and motivate our choices based on our empirical evaluation in Sec. 4. The choice of the spatial fusion method, layer and temporal fusion is based on the experiments in sections 4.2, 4.3 and 4.5, respectively.</p><p>Our proposed architecture (shown in <ref type="figure" target="#fig_4">Fig. 4</ref>) can be viewed as an extension of the architecture in <ref type="figure" target="#fig_0">Fig. 2</ref> (left) over time. We fuse the two networks, at the last convolutional layer (after ReLU) into the spatial stream to convert it into a spatiotemporal stream by using 3D Conv fusion followed by 3D pooling (see <ref type="figure" target="#fig_4">Fig. 4, left)</ref>. Moreover, we do not truncate the temporal stream and also perform 3D Pooling in the temporal network (see <ref type="figure" target="#fig_4">Fig. 4, right)</ref>. The losses of both streams are used for training and during testing we average the predictions of the two streams. In our empirical evaluation (Sec. 4.6) we show that keeping both streams performs slightly better than truncating the temporal stream after fusion.</p><p>Having discussed how to fuse networks over time, we discuss here the issue of how often to sample the temporal sequence. The temporal fusion layer receives T temporal chunks that are τ frames apart; i.e. the two stream towers are applied to the input video at time t, t + τ, . . . t + T τ . As shown in <ref type="figure" target="#fig_4">Fig. 4</ref> this enables us to capture short scale (t ± L 2 ) temporal features at the input of the temporal network (e.g. the drawing of an arrow) and put them into context over a longer temporal scale (t + T τ ) at a higher layer of the network (e.g. drawing an arrow, bending a bow, and shooting an arrow).</p><p>Since the optical flow stream has a temporal receptive field of L = 10 frames, the architecture operates on a total temporal receptive field of T ×L. Note that τ &lt; L results in overlapping inputs for the temporal stream, whereas τ ≥ L produces temporally non-overlapping features.</p><p>After fusion, we let the 3D pooling operate on T spatial feature maps that are τ frames apart. As features may change their spatial position over time, combining spatial and temporal pooling to 3D pooling makes sense. For example, the output of a VGG-M network at conv5 has an input stride of 16 pixels and captures high level features from a receptive field of 139 × 139 pixels. Spatiotemporal pooling of conv5 maps that are τ frames distant in time can therefore capture features of the same object, even if they slightly move.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation details</head><p>Two-Stream architecture. We employ two pre-trained ImageNet models. First, for sake of comparison to the original two-stream approach <ref type="bibr" target="#b21">[22]</ref>, the VGG-M-2048 model <ref type="bibr" target="#b2">[3]</ref> with 5 convolutional and 3 fully-connected layers. Second, the very deep VGG-16 model <ref type="bibr" target="#b22">[23]</ref> that has 13 convolutional and 3 fully-connected layers. We first separately train the two streams as described in <ref type="bibr" target="#b21">[22]</ref>, but with some subtle differences: We do not use RGB colour jittering; Instead of decreasing the learning rate according to a fixed schedule, we lower it after the validation error saturates; For training the spatial network we use lower dropout ratios of 0.85 for the first two fully-connected layers. Even lower dropout ratios (up to 0.5) did not decrease performance significantly.</p><p>For the temporal net, we use optical flow <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref> stacking with L = 10 frames <ref type="bibr" target="#b21">[22]</ref>. We also initialised the temporal net with a model pre-trained on ImageNet, since this generally facilitates training speed without a decrease in performance compared to our model trained from scratch. The network input is rescaled beforehand, so that the smallest side of the frame equals 256. We also pre-compute the optical flow before training and store the flow fields as JPEG images (with clipping of displacement vectors larger than 20 pixels). We do not use batch normalization <ref type="bibr" target="#b8">[9]</ref>.</p><p>Two-Stream ConvNet fusion. For fusion, these networks are finetuned with a batch size of 96 and a learning rate starting from 10 −3 which is reduced by a factor of 10 as soon as the validation accuracy saturates. We only propagate back to the injected fusion layer, since full backpropagation did not result in an improvement. In our experiments we only fuse between layers with the same output resolution; except for fusing a VGG-16 model at ReLU5 3 with a VGG-M model at ReLU5, where we pad the slightly smaller output of VGG-M (13 × 13, compared to 14 × 14) with a row and a column of zeros. For Conv fusion, we found that careful initialisation of the injected fusion layer (as in (4)) is very important. We compared several methods and found that initialisation by identity matrices (to sum the two networks) performs as well as random initialisation.</p><p>Spatiotemporal architecture. For our final architecture described in Sec. 3.4, the 3D Conv fusion kernel f has dimension 3 × 3 × 3 × 1024 × 512 and T = 5, i.e. the spatiotemporal filter has dimension H × W × T = 3 × 3 × 3, the D = 1024 results from concatenating the ReLU5 from the spatial and temporal streams, and the D = 512 matches the number of input channels of the following FC6 layer.</p><p>The 3D Conv filters are also initialised by stacking two identity matrices for mapping the 1024 feature channels to 512. Since the activations of the temporal ConvNet at the last convolutional layer are roughly 3 times lower than its appearance counterpart, we initialise the temporal identity matrix of f by a factor of 3 higher. The spatiotemporal part of f is initialised using a Gaussian of size 3 × 3 × 3 and σ = 1. Further, we do not fuse at the prediction layer during training, as this would bias the loss towards the temporal architecture, because the spatiotemporal architecture requires longer to adapt to the fused features.</p><p>Training 3D ConvNets is even more prone to overfitting than the two-stream ConvNet fusion, and requires additional augmentation as follows. During finetuning, at each training iteration we sample the T = 5 frames from each of the 96 videos in a batch by randomly sampling the starting frame, and then randomly sampling the temporal stride (τ ) ∈ [1, 10] (so operating over a total of between 15 and 50 frames). Instead of cropping a fixed sized 224 × 224 input patch, we randomly jitter its width and height by ±25% and rescale it to 224 × 224. The rescaling is chosen randomly and may change the aspect-ratio. Patches are only cropped at a maximum of 25% distance from the image borders (relative to the width and height). Note, the position (and size, scale, horizontal flipping) of the crop is randomly selected in the first frame (of a multiple-frame-stack) and then the same spatial crop is applied to all frames in the stack.</p><p>Testing. Unless otherwise specified, only the T = 5 frames (and their horizontal flips) are sampled, compared to the 25 frames in <ref type="bibr" target="#b21">[22]</ref>, to foster fast empirical evaluation. In addition we employ fully convolutional testing where the entire frame is used (rather than spatial crops).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and experimental protocols</head><p>We evaluate our approach on two popular action recognition datasets. First, UCF101 <ref type="bibr" target="#b23">[24]</ref>, which consists of 13320 action videos in 101 categories. The second dataset is HMDB51 <ref type="bibr" target="#b12">[13]</ref>, which contains 6766 videos that have been annotated for 51 actions. For both datasets, we use the provided evaluation protocol and report the mean average accuracy over the three splits into training and test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">How to fuse the two streams spatially?</head><p>For these experiments we use the same network architecture as in <ref type="bibr" target="#b21">[22]</ref>; i.e. two VGG-M-2048 nets <ref type="bibr" target="#b2">[3]</ref>. The fusion layer is injected at the last convolutional layer, after rectification, i.e. its input is the output of ReLU5 from the two streams. This is chosen because, in preliminary experiments, it provided better results than alternatives such as the non-rectified output of conv5. At that point the features are already highly informative while still providing coarse location information. After the fusion layer a single processing stream is used. We compare different fusion strategies in <ref type="table">Table 1</ref> where we report the average accuracy on the first split of UCF101. We first observe that our performance for softmax averaging (85.94%) compares favourably to the one reported in <ref type="bibr" target="#b21">[22]</ref>. Second we see that Max and Concatenation perform considerably lower than Sum and Conv fusion. Conv fusion performs best and is slightly better than Bilinear fusion and simple fusion via summation. For the reported Conv-fusion result, the convolution kernel f is initialised by identity matrices that perform summation of the two feature maps. Initialisation via random Gaussian noise ends up at a similar performance 85.59% compared to identity matrices (85.96%), however, at a much longer training time. This is interesting, since this, as well as the high result of Sum-fusion, suggest that simply summing the feature maps is already a good fusion technique and learning a randomly initialised combination does not lead to significantly different/better results.</p><p>For all the fusion methods shown in <ref type="table">Table 1</ref>, fusion at FC layers results in lower performance compared to ReLU5, with the ordering of the methods being the same as in Table 1, except for bilinear fusion which is not possible at FC layers. Among all FC layers, FC8 performs better than FC7 and FC6, with Conv fusion at 85.9%, followed by Sum fusion at 85.1%. We think the reason for ReLU5 performing slightly better is that at this layer spatial correspondences between appearance and motion are fused, which would have already been collapsed at the FC layers <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Where to fuse the two streams spatially?</head><p>Fusion from different layers is compared in by an identity matrix that sums the activations from previous layers. Interestingly, fusing and truncating one net at ReLU5 achieves around the same classification accuracy on the first split of UCF101 (85.96% vs 86.04%) as an additional fusion at the prediction layer (FC8), but at a much lower number of total parameters (97.57M vs 181.68M). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Going from deep to very deep models</head><p>For computational complexity reasons, all previous experiments were performed with two VGG-M-2048 networks (as in <ref type="bibr" target="#b21">[22]</ref>). Using deeper models, such as the very deep networks in <ref type="bibr" target="#b22">[23]</ref> can, however, lead to even better performance in image recognition tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>. Following that, we train a 16 layer network, VGG-16, [23] on UCF101 and HMDB51. All models are pretrained on ImageNet and separately trained for the target dataset, except for the temporal HMDB51 networks which are initialised from the temporal UCF101 models. For VGG-16, we use TV-L1 optical flow <ref type="bibr" target="#b36">[37]</ref> and apply a similar augmentation technique as for 3D ConvNet training (described in Sec. 3.5) that samples from the image corners and its centre <ref type="bibr" target="#b34">[35]</ref>. The learning rate is set to 50 −4 and decreased by a factor of 10 as soon as the validation objective saturates. <ref type="table">Table 3</ref>. On both datasets, one observes that going to a deeper spatial model boosts performance significantly (8.11% and 10.29%), whereas a deeper temporal network yields a lower accuracy gain (3.91% and 3.73%).  <ref type="table">Table 4</ref>. Spatiotemporal two-stream fusion on UCF101 (split1) and HMDB51 (split1). The models used are VGG-16 (spatial net) and VGG-M (temporal net). The "+" after a fusion layer indicates that both networks and their loss are kept after fusing, as this performs better than truncating one network. Specifically, at ReLU5 we fuse from the temporal net into the spatial network, then perform either 2D or 3D pooling at Pool5 and compute a loss for each tower. During testing, we average the FC8 predictions for both towers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The comparison between deep and very deep models is shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">How to fuse the two streams temporally?</head><p>Different temporal fusion strategies are shown in <ref type="table">Table 4</ref>. In the first row of <ref type="table">Table 4</ref> we observe that conv fusion performs better than averaging the softmax output (cf. <ref type="table">Table  3</ref>). Next, we find that applying 3D pooling instead of using 2D pooling after the fusion layer increases performance on both datasets, with larger gains on HMDB51. Finally, the last row of <ref type="table">Table 4</ref> lists results for applying a 3D filter for fusion which further boosts recognition rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Comparison with the state-of-the-art</head><p>Finally, we compare against the state-of-the-art over all three splits of UCF101 and HMDB51 in <ref type="table">Table 5</ref>. We use the same method as shown above, i.e. fusion by 3D Conv and 3D Pooling (illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>). For testing we average 20 temporal predictions from each network by densely sampling the input-frame-stacks and their horizontal flips. One interesting comparison is to the original two-stream approach <ref type="bibr" target="#b21">[22]</ref>, we improve by 3% on UCF101 and HMDB51 by using a VGG-16 spatial (S) network and a VGG-M temporal (T) model, as well as by 4.5% (UCF) and 6% (HMDB) when using VGG-16 for both streams. Another interesting comparison is against the two-stream network in <ref type="bibr" target="#b16">[17]</ref>, which employs temporal conv-pooling after the last dimensionality reduction layer of a GoogLeNet <ref type="bibr" target="#b26">[27]</ref> architecture. They report 88.2% on UCF101 when pooling over 120 frames and 88.6% when using an LSTM for pooling. Here, our result of 92.5% clearly underlines the importance of our proposed approach. Note also that using a single stream after temporal fusion achieves 91.8%, compared to maintaining two streams and achieving 92.5%, but with far fewer parameters and a simpler architecture.</p><p>As a final experiment, we explore what benefit results from a late fusion of hand-crafted IDT features <ref type="bibr" target="#b32">[33]</ref> with our representation. We simply average the SVM scores of the FV-encoded IDT descriptors (i.e. HOG, HOF, MBH) with the predictions (taken before softmax) of our ConvNet representations. The resulting performance is shown in Table 6. We achieve 93.5% on UCF101 and 69.2% HMDB51. This state-of-the-art result illustrates that there is still a de-Method UCF101 HMDB51 Spatiotemporal ConvNet <ref type="bibr" target="#b10">[11]</ref> 65.4% -LRCN <ref type="bibr" target="#b5">[6]</ref> 82.9% -Composite LSTM Model <ref type="bibr" target="#b24">[25]</ref> 84.3% 44.0 C3D <ref type="bibr" target="#b29">[30]</ref> 85.2% -Two-Stream ConvNet (VGG-M) <ref type="bibr" target="#b21">[22]</ref> 88.0% 59.4% Factorized ConvNet <ref type="bibr" target="#b25">[26]</ref> 88.1% 59.1% Two-Stream Conv Pooling <ref type="bibr" target="#b16">[17]</ref> 88.2% -Two-Stream ConvNet (VGG-16, <ref type="bibr">[</ref>  <ref type="table">Table 5</ref>. Mean classification accuracy of best performing ConvNet approaches over three train/test splits on HMDB51 and UCF101. For our method we list the models used for the spatial (S) and temporal (T) stream.</p><p>IDT+higher dimensional FV <ref type="bibr" target="#b18">[19]</ref> 87.9% 61.1% C3D+IDT <ref type="bibr" target="#b29">[30]</ref> 90.4% -TDD+IDT <ref type="bibr" target="#b33">[34]</ref> 91.5% 65.9% Ours+IDT (S:VGG-16, T:VGG-M) 92.5% 67.3% Ours+IDT (S:VGG-16, T:VGG-16) 93.5% 69.2% <ref type="table">Table 6</ref>. Mean classification accuracy on HMDB51 and UCF101 for approaches that use IDT features <ref type="bibr" target="#b32">[33]</ref>. gree of complementary between hand-crafted representations and our end-to-end learned ConvNet approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a new spatiotemporal architecture for two stream networks with a novel convolutional fusion layer between the networks, and a novel temporal fusion layer (incorporating 3D convolutions and pooling). The new architecture does not increase the number of parameters significantly over previous methods, yet exceeds the state of the art on two standard benchmark datasets. Our results suggest the importance of learning correspondences between highly abstract ConvNet features both spatially and temporally. One intriguing finding is that there is still such an improvement by combining ConvNet predictions with FVencoded IDT features. We suspect that this difference may vanish in time given far more training data, but otherwise it certainly indicates where future research should attend.</p><p>Finally, we return to the point that current datasets are either too small or too noisy. For this reason, some of the conclusions in this paper should be treated with caution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Two examples of where a fusion layer can be placed. The left example shows fusion after the fourth conv-layer. Only a single network tower is used from the point of fusion. The right figure shows fusion at two layers (after conv5 and after fc8) where both network towers are kept, one as a hybrid spatiotemporal net and one as a purely spatial network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Different ways of fusing temporal information. (a) 2D pooling ignores time and simply pools over spatial neighbourhoods to individually shrink the size of the feature maps for each temporal sample. (b) 3D pooling pools from local spatiotemporal neighbourhoods by first stacking the feature maps across time and then shrinking this spatiotemporal cube. (c) 3D conv + 3D pooling additionally performs a convolution with a fusion kernel that spans the feature channels, space and time before 3D pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Our spatiotemporal fusion ConvNet applies two-stream ConvNets, that capture short-term information at a fine temporal scale (t ± L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2</head><label>2</label><figDesc>shows how these two examples are implemented.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Conv fusion is used and the fusion layers are initialised</figDesc><table><row><cell cols="2">Fusion Layers</cell><cell cols="3">Accuracy #layers #parameters</cell></row><row><cell>ReLU2</cell><cell></cell><cell>82.25%</cell><cell>11</cell><cell>91.90M</cell></row><row><cell>ReLU3</cell><cell></cell><cell>83.43%</cell><cell>12</cell><cell>93.08M</cell></row><row><cell>ReLU4</cell><cell></cell><cell>82.55%</cell><cell>13</cell><cell>95.48M</cell></row><row><cell>ReLU5</cell><cell></cell><cell>85.96%</cell><cell>14</cell><cell>97.57M</cell></row><row><cell cols="2">ReLU5 + FC8</cell><cell>86.04%</cell><cell>17</cell><cell>181,68M</cell></row><row><cell cols="2">ReLU3 + ReLU5 + FC6</cell><cell>81.55%</cell><cell>17</cell><cell>190,06M</cell></row><row><cell cols="5">Table 2. Performance comparison for Conv fusion (4) at different</cell></row><row><cell cols="5">fusion layers. An earlier fusion (than after conv5) results in weaker</cell></row><row><cell cols="5">performance. Multiple fusions also lower performance if early lay-</cell></row><row><cell cols="5">ers are incorporated (last row). Best performance is achieved for</cell></row><row><cell cols="5">fusing at ReLU5 or at ReLU5+FC8 (but with nearly double the</cell></row><row><cell cols="2">parameters involved).</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">UCF101 (split 1)</cell><cell cols="2">HMDB51 (split 1)</cell></row><row><cell>Model</cell><cell cols="4">VGG-M-2048 VGG-16 VGG-M-2048 VGG-16</cell></row><row><cell>Spatial</cell><cell>74.22%</cell><cell>82.61%</cell><cell>36.77%</cell><cell>47.06%</cell></row><row><cell>Temporal</cell><cell>82.34%</cell><cell>86.25%</cell><cell>51.50%</cell><cell>55.23%</cell></row><row><cell>Late Fusion</cell><cell>85.94%</cell><cell>90.62%</cell><cell>54.90%</cell><cell>58.17%</cell></row><row><cell cols="5">Table 3. Performance comparison of deep (VGG-M-2048) vs. very</cell></row><row><cell cols="5">deep (VGG-16) Two-Stream ConvNets on the UCF101 (split1)</cell></row><row><cell cols="5">and HMDB51 (split1). Late fusion is implemented by averaging</cell></row><row><cell cols="5">the prediction layer outputs. Using deeper networks boosts perfor-</cell></row><row><cell cols="3">mance at the cost of computation time.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We are grateful for discussions with Karen Simonyan. Christoph Feichtenhofer is a recipient of a DOC Fellowship of the Austrian Academy of Sciences. This work was supported by the Austrian Science Fund (FWF) under project P27076, and also by EPSRC Programme Grant Seebibyte EP/M013774/1. The GPUs used for this research were donated by NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">P-CNN: Pose-based CNN features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Indrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://wwwthumos.info/" />
		<title level="m">Thumos challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale video classication with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Actionconditional video prediction using deep networks in atari game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice. CoRR, abs/1405</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4506</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions calsses from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>UCF Center for Research in Computer Vision</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MatConvNet -convolutional neural networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Int. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime TV-L1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DAGM</title>
		<meeting>DAGM</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
