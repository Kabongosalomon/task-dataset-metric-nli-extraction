<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LCA-Net: Light Convolutional Autoencoder for Image Dehazing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><forename type="middle">A</forename></persName>
							<email>pavana@pesu.pes.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Bennur</surname></persName>
							<email>aditya.bennur@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Gaggar</surname></persName>
							<email>gaggarmohit@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">CSE PESU</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">CSE PESU</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">CSE PESU</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Shylaja S S CSE PESU</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LCA-Net: Light Convolutional Autoencoder for Image Dehazing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image dehazing</term>
					<term>image pre-processing</term>
					<term>light convolutional encoder-decoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image dehazing is a crucial image pre-processing task aimed at removing the incoherent noise generated by haze to improve the visual appeal of the image. The existing models use sophisticated networks and custom loss functions which are computationally inefficient and requires heavy hardware to run. Time is of the essence in image pre-processing since real time outputs can be obtained instantly. To overcome these problems, our proposed generic model uses a very light convolutional encoder-decoder network which does not depend on any atmospheric models. The network complexity-image quality trade off is handled well in this neural network and the performance of this network is not limited by low-spec systems. This network achieves optimum dehazing performance at a much faster rate, on several standard datasets, comparable to the state-of-the-art methods in terms of image quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Haze is an atmospheric phenomenon wherein the clarity of the sky is obscured due to the presence of smoke, dust and other dry particulate matter. This causes light to scatter and thus, deteriorates the image quality which in turn may affect the performance of several computer vision applications such as detection, classification and tracking. The atmospheric scattering model combines the haze-free input with the global atmospheric illumination, blended together with a transmission coefficient that models the haze effects. It is closely represented by the equation <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_0">I(x)=J(x)t(x) + A(x)(1 -t(x))<label>(1)</label></formula><p>where x is the 2D pixel spatial location, I(x) is the observed haze image,J(x) is the haze-free scene radiance to be recovered, while A(x) and t(x) are two critical parameters. The first A(x), denotes the global atmospheric illumination, and t(x) is the transmission matrix. Using the atmospheric model to build a neural network is also referred to as 'plug-in approach'. However, this approach may lack the features to detect intricate differences between a clean image and a hazy image. The viability of converting the problem of image reconstruction to a problem of estimation of A and t(x) is very low. Besides, we cannot relate hazy and clear images with ease, reason being the relation may be too compound to be explained just by an equation. Thus, the plugin approach may not be able to generalize the equation over natural images. Our model simply builds a clear image from the hazy image by applying pooling and convolutions and does not refer to the above atmospheric model or the transmission map of the image for the process of dehazing of the image.</p><p>Image Dehazing is used in several image analyzing applications and other applications such as tracking, control systems, and intelligent surveillance where good quality images or videos are crucial for obtaining accurate results and optimum performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE PROPOSED NETWORK</head><p>We shall propose a convolutional autoencoder in this section which is light both in terms of the architecture as well as its speed in dehazing an image. As the name suggests, this neural network has no special consideration towards image dehazing in its design. The following subsections outlines overall architecture, important features, main building blocks and the loss function used of the LCA-net.</p><p>A. Network Architecture <ref type="figure">Figure 1</ref> illustrates the overall architecture of LCA-Net. Our neural network consists of an encoder-decoder type of architecture. The input layer takes an image (512 x 512 x 3) which is padded and pushed to the first ReLU <ref type="bibr" target="#b1">[2]</ref> activated convolutional layer consisting of 50 filters of size 3. This layer returns an output of original dimensions which acts as input for the average pooling layer which has a downsampling factor of 2, followed by the next convolutional layer where the cycle is repeated. This results in an encoded input of size 128 x 128 x 50. This stage is referred to as 'bottleneck'. The encoded input is then passed through 2 completely connected ReLU </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss function and Training setting</head><p>The loss function used in this model is mean squared error(MSE). There are high chances of encountering unexpected changes in intensity of pixel values. MSE helps average the variation of extreme values, supporting a smooth gradient descent making it an optimal loss function.</p><formula xml:id="formula_1">L = 1 N N x=1 3 i=1 Ĵ i (x) − J i (x) 2<label>(2)</label></formula><p>where J i (x) is the intensity of the i th color channel of pixel x in the output image (the ground truth), and N is the total number of pixels. <ref type="figure" target="#fig_2">Figure 2</ref> shows the loss function decreasing with increase in the number of epochs. The optimizer used is ADAM <ref type="bibr" target="#b2">[3]</ref> in its default settings which are: learning rate = 0.001 , β 1 = 0.9, β 2 = 0.999.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Datasets</head><p>Like most of the previous CNN based dehazing techniques our autoencoder is also trained on a custom dataset. Our dataset consists a total of 2061 clear images, we then select 35 varying haze levels on each image to constitute a total of 72135 haze images borrowed from the RESIDE dataset <ref type="bibr" target="#b3">[4]</ref>. There are 3 test sets borrowed from RESIDE-standard <ref type="bibr" target="#b3">[4]</ref> dataset. The test sets, known as the Synthetic Objective Testing Set (SOTS-outdoor and SOTS-indoor) contain 500 synthetic outdoor and 500 synthetic indoor hazy images respectively. They are generated using images in NYU2 and are different from the training set images. The third test set is known as the Hybrid Subjective Testing Set (HSTS), which contains 10 synthetic outdoor and 10 real-world outdoor hazy images to evaluate qualitative visual performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BENCHMARKING AND RESULTS</head><p>The performance of the neural network is determined by evaluation metrics such as PSNR(Peak Signal to Noise Ratio) and SSIM(Structural Similarity Index). PSNR is essentially the ratio between maximum intensity and MSE. High PSNR value implicates low noise, the vice-versa is also true. SSIM as the name says measures the similarity between 2 images. It also improves on methods like PSNR and MSE. <ref type="table" target="#tab_1">Table I, Table II and Table III</ref> shows the PSNR and SSIM scores of our LCA-Net compared to nine other stateof-the-art methods <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b12">[13]</ref> on the HSTS, SOTS-indoor and SOTS-outdoor test sets respectively. The results of previous dehazing methods for <ref type="table" target="#tab_1">Table I, Table II and Table III</ref> are from, <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b12">[13]</ref>. Nonetheless, since our model benchmarks on the same dataset, the comparison is fair. <ref type="table" target="#tab_1">Table I</ref> shows the PSNR, SSIM scores and average time taken to dehaze an image. Our model when compared to other state-of-the-art method <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b12">[13]</ref> comes in first and the SSIM only comes second to DehazeNet <ref type="bibr" target="#b9">[10]</ref> in outdoor HSTS. Due to the light nature of our model it outperforms all other models in the run-time per image, making it one of the fastest methods for image dehazing. <ref type="table" target="#tab_1">Table II</ref> shows the dehaze results on SOTS-indoor dataset. Our model performs inadequately when bench-marked on indoor SOTS and low-light images as they were trained on outdoor images. This phenomenon can also be explained due to the drastically varying feature vectors of indoor and outdoor images. Deficient performance on low-light images is also explained by the fact that low-light images have contrasted feature vectors. <ref type="figure">Figure 3</ref> illustrates the comparison DCP <ref type="bibr" target="#b4">[5]</ref> FVR <ref type="bibr" target="#b5">[6]</ref> BCCR <ref type="bibr" target="#b6">[7]</ref> GRM <ref type="bibr" target="#b7">[8]</ref> NLD <ref type="bibr" target="#b8">[9]</ref> DehazeNet <ref type="bibr" target="#b9">[10]</ref> MSCNN <ref type="bibr" target="#b10">[11]</ref>       <ref type="bibr" target="#b9">[10]</ref> and AOD-Net <ref type="bibr" target="#b11">[12]</ref> in PSNR and first in the SSIM metric. Therefore, our model is efficient in dehazing outdoor real world and synthetic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION AND FUTURE WORK</head><p>This paper presented a light convolutional autoencoder which quickly helps remove haze in images. Further it performs better than other state-of-the-art image dehazing methods with regard to PSNR and the time taken to dehaze an image. The image quality is comparable to the state of the art methods. Since it is a light weighted network, it is also compatible with low-spec computers. The downside of this network is its inefficient performance on low-light images and dense-haze images and our future work would be centered on eliminating this flaw to provide fine performance on all categories of images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[ 2 ]Fig. 1 :</head><label>21</label><figDesc>activated dense layers consisting of 10 neurons each. Dense layers add an interesting non-linearity property, and can model any mathematical function. Thus, they help in extracting essential features efficiently. This output is passed to the decoder which consists of 2 pairs of deconvolutional and upsampling layers. The ReLU [2] activated deconvolutional layer consists of 50 filters of size 3 and the upsampling layer utilises a scale factor of 2. The resulting output of the decoder has a dimension of 512 x 512 x 50. The final output layer is a deconvolutional layer consisting of 3 filters of size 3 which arXiv:2008.10325v1 [cs.CV] 24 Aug 2020 The architecture of LCA-Net are linearly activated producing a dehazed image whose size is identical to the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Epoch loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Dehaze results on synthetic hazy images from HSTS</figDesc><table><row><cell></cell><cell>DCP [5]</cell><cell>FVR [6]</cell><cell cols="2">BCCR [7] GRM [8]</cell><cell cols="3">NLD [9] DehazeNet [10] MSCNN [11]</cell><cell cols="3">AOD-Net [12] CAE [13] LCA-Net</cell></row><row><cell>PSNR</cell><cell>16.62</cell><cell>15.72</cell><cell>16.88</cell><cell>18.86</cell><cell>17.29</cell><cell>21.14</cell><cell>17.57</cell><cell>19.06</cell><cell>24.56</cell><cell>18.23</cell></row><row><cell>SSIM</cell><cell>0.8179</cell><cell>0.7483</cell><cell>0.7913</cell><cell>0.8553</cell><cell>0.7489</cell><cell>0.8472</cell><cell>0.8102</cell><cell>0.8504</cell><cell>0.9126</cell><cell>0.7808</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Dehaze results on Indoor hazy images from SOTS</figDesc><table><row><cell></cell><cell cols="2">DCP [5] FVR [6]</cell><cell cols="3">BCCR [7] GRM [8] NLD [9]</cell><cell cols="3">DehazeNet [10] MSCNN [11] AOD-Net [12]</cell><cell>LCA-Net</cell></row><row><cell>PSNR</cell><cell>18.54</cell><cell>16.61</cell><cell>17.71</cell><cell>20.77</cell><cell>19.52</cell><cell>26.84</cell><cell>21.73</cell><cell>24.08</cell><cell>23.37</cell></row><row><cell>SSIM</cell><cell>0.71</cell><cell>0.7236</cell><cell>0.7409</cell><cell>0.7617</cell><cell>0.7328</cell><cell>0.8264</cell><cell>0.8313</cell><cell>0.8726</cell><cell>0.8763</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Dehaze results on Outdoor hazy images from SOTS</figDesc><table><row><cell>(a) hazy</cell><cell>(b) AOD-Net</cell><cell>(c) dehazeNet</cell><cell>(d) MSCNN</cell><cell>(e) NLD</cell><cell>(f) LCA-Net</cell></row><row><cell></cell><cell cols="3">Fig. 3: Comparison of various model benchmarks</cell><cell></cell><cell></cell></row><row><cell cols="3">of different models' performances on standard hazy images</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">from the RESIDE dataset [4].</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III</head><label>III</label><figDesc>shows the PSNR and SSIM scores of models<ref type="bibr" target="#b4">[5]</ref>-<ref type="bibr" target="#b11">[12]</ref> on SOTS-outdoor images. Our model comes third behind DehazeNet</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision and the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Benchmarking single-image dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="492" to="505" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast visibility restoration from a single color or gray level image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="2201" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyong</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust image and video dehazing with visual artifact suppression via gradient residual minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="576" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Tali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional autoencoder for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="4464" to="4468" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
