<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Copy Mechanism and Tailored Training for Character-based Data-to-text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Roberti</surname></persName>
							<email>m.roberti@unito.it</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Turin</orgName>
								<address>
									<addrLine>Via Pessinetto</addrLine>
									<postCode>12 -12149</postCode>
									<settlement>Torino</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Bonetta</surname></persName>
							<email>giovanni.bonetta@unito.it</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Turin</orgName>
								<address>
									<addrLine>Via Pessinetto</addrLine>
									<postCode>12 -12149</postCode>
									<settlement>Torino</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rossella</forename><surname>Cancelliere</surname></persName>
							<email>rossella.cancelliere@unito.it</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Turin</orgName>
								<address>
									<addrLine>Via Pessinetto</addrLine>
									<postCode>12 -12149</postCode>
									<settlement>Torino</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
							<email>patrick.gallinari@lip6.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<addrLine>4 Place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Criteo AI Lab</orgName>
								<address>
									<addrLine>32 Rue Blanche</addrLine>
									<postCode>75009</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Copy Mechanism and Tailored Training for Character-based Data-to-text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Natural Language Processing · Data-to-text Generation · Deep Learning · Sequence-to-sequence · Dataset</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the last few years, many different methods have been focusing on using deep recurrent neural networks for natural language generation. The most widely used sequence-to-sequence neural methods are word-based: as such, they need a pre-processing step called delexicalization (conversely, relexicalization) to deal with uncommon or unknown words. These forms of processing, however, give rise to models that depend on the vocabulary used and are not completely neural.</p><p>In this work, we present an end-to-end sequence-to-sequence model with attention mechanism which reads and generates at a character level, no longer requiring delexicalization, tokenization, nor even lowercasing. Moreover, since characters constitute the common "building blocks" of every text, it also allows a more general approach to text generation, enabling the possibility to exploit transfer learning for training. These skills are obtained thanks to two major features: (i) the possibility to alternate between the standard generation mechanism and a copy one, which allows to directly copy input facts to produce outputs, and (ii) the use of an original training pipeline that further improves the quality of the generated texts. We also introduce a new dataset called E2E+, designed to highlight the copying capabilities of character-based models, that is a modified version of the well-known E2E dataset used in the E2E Challenge. We tested our model according to five broadly accepted metrics (including the widely used bleu), showing that it yields competitive performance with respect to both character-based and word-based approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability of recurrent neural networks (RNNs) to model sequential data stimulated interest towards deep learning models which face data-to-text generation.</p><p>An interesting application is the generation of descriptions for factual tables that consist of a set of field-value pairs; an example is shown in <ref type="table" target="#tab_3">Table 4</ref>. We present in this paper an effective end-to-end approach to this task.</p><p>Sequence-to-sequence frameworks <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b2">2]</ref> have proved to be very effective in natural language generation (NLG) tasks <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b16">16]</ref>, as well as in machine translation <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b22">22]</ref> and in language modeling <ref type="bibr" target="#b20">[20]</ref>. Usually, data are represented word-by-word both in input and output sequences; anyways, such schemes can't be effective without a special, non-neural delexicalization phase that handles unknown words, such as proper names or foreign words (see <ref type="bibr" target="#b26">[26]</ref>). The delexicalization step has the benefit of reducing the dictionary size and, consequently, the data sparsity, but it is affected by various shortcomings. In particular, according to <ref type="bibr" target="#b8">[8]</ref> -it needs some reliable mechanism for entity identification, i.e. the recognition of named entities inside text; -it requires a subsequent "re-lexicalization" phase, where the original named entities take back placeholders' place; -it cannot account for lexical or morphological variations due to the specific entity, such as gender and number agreements, that can't be achieved without a clear context awareness.</p><p>Recently, some strategies have been proposed to solve these issues: <ref type="bibr" target="#b9">[9]</ref> and <ref type="bibr" target="#b21">[21]</ref> face this problem using a special neural copying mechanism that is quite effective in alleviating the out-of-vocabulary words problem, while <ref type="bibr" target="#b15">[15]</ref> tries to extend neural networks with a post-processing phase that copies words as indicated by the model's output sequence. Some character-level aspects appear as a solution of the issue as well, either as a fallback for rare words <ref type="bibr" target="#b14">[14]</ref>, or as subword units <ref type="bibr" target="#b22">[22]</ref>.</p><p>A significantly different approach consists in employing characters instead of words, for input slot-value pairs tokenization as well as for the generation of the final utterances, as done for instance in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">20]</ref>.</p><p>In order to give an original contribution to the field, in this paper we present a character-level sequence-to-sequence model with attention mechanism that results in a completely neural end-to-end architecture. In contrast to traditional word-based ones, it does not require delexicalization, tokenization nor lowercasing; besides, according to our experiments it never hallucinates words, nor duplicates them. As we will see, such an approach achieves rather interesting performance results and produces a vocabulary-free model that is inherently more general, as it does not depend on a specific domain's set of terms, but rather on a general alphabet. Because of this, it opens up the possibility, not viable when using words, to adapt already trained networks to deal with different datasets.</p><p>More specifically, our model shows two important features, with respect to the state-of-art architecture proposed by <ref type="bibr" target="#b3">[3]</ref>: (i) a character-wise copy mechanism, consisting in a soft switch between generation and copy mode, that disengages the model to learn rare and unhelpful self-correspondences, and (ii) a peculiar training procedure, which improves the internal representation capabilities, enhancing recall; it consists in the exchange of encoder and decoder RNNs, (GRUs <ref type="bibr" target="#b5">[5]</ref> in our specific case), depending on whether the input is a tabular Meaning Representation (MR) or a natural language sentence. As a further original contribution, we also introduce a new dataset, described in section 3.1, whose particular structure allows to better highlight improvements in copying/recalling abilities with respect to character-based state-of-art approaches.</p><p>In section 2, after resuming the main ideas on encoder-decoder methods with attention, we detail our model: section 2.2 is devoted to explaining the copy mechanism while in section 2.3 our peculiar training procedure is presented. Section 3 includes the datasets descriptions, some implementation specifications, the experimental framework and the analysis and evaluation of the achieved results. Finally, in section 4 some conclusions are drawn, outlining future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Summary on Encoder-decoder Architectures with Attention</head><p>The sequence-to-sequence encoder-decoder architecture with attention <ref type="bibr" target="#b3">[3]</ref> is represented in figure 1: on the left, the encoder, a bi-directional RNN, outputs one annotation h j for each input token x j . Each vector h j corresponds to the concatenation of the hidden states produced by the backward and forward RNNs. On the right side of the figure, we find the decoder, which produces one state s i for each time step; on the center of the figure the attention mechanism is shown. The main components of the attention mechanism are: (i) the alignment model e ij</p><formula xml:id="formula_0">e ij = att(s i−1 , h j ), 1 ≤ j ≤ T x , 1 ≤ i ≤ T y<label>(1)</label></formula><p>which is parameterized as a feedforward neural network and scores how well input in position j-th and output observed in the i-th time instant match; T x and T y are the length of the input and output sequences, respectively.</p><p>(ii) the attention probability distribution α ij</p><formula xml:id="formula_1">α ij = exp(e ij ) Tx k=1 exp(e ik ) ≡ [sof tmax(e i )] j , 1 ≤ j ≤ T x , 1 ≤ i ≤ T y<label>(2)</label></formula><p>(e i is the vector whose j-th element is e ij ) (iii) the context vector C i</p><formula xml:id="formula_2">C i = Tx j=1 α ij h j , 1 ≤ i ≤ T y ,<label>(3)</label></formula><p>weighted sum of the encoder annotations h j . According to <ref type="bibr" target="#b3">[3]</ref>, the context vector C i is the key element for evaluating the conditional probability P (y i |y 1 , . . . , y i−1 , x) to output a target token y i , given the previously outputted tokens y 1 , . . . , y i−1 and the input x. They in fact express this probability as:</p><formula xml:id="formula_3">P (y i |y 1 , . . . , y i−1 , x) = g(y i−1 , s i , C i ),<label>(4)</label></formula><p>where g is a non-linear, potentially multi-layered, function. So doing, the explicit information about y 1 , . . . , y i−1 and x is replaced with the knowledge of the context C i and the decoder state s i . The model we present in this paper incorporates two additional mechanisms, detailed in the next sections: a character-wise copy mechanism and a peculiar training procedure based on GRUs switch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning to Copy</head><p>On top of the just recalled model, we build a character-based copy mechanism inspired by the Pointer-Generator Network <ref type="bibr" target="#b21">[21]</ref>, a word-based model that hybridizes the Bahdanau traditional model and a Pointer Network <ref type="bibr" target="#b25">[25]</ref>. Basing on these ideas, in our model we identify two probability distributions that, differently from what done by <ref type="bibr" target="#b21">[21]</ref> and <ref type="bibr" target="#b28">[28]</ref>, act now on characters rather than on words: the alphabet distribution P alph and the attention distribution P att .</p><p>The former is the network's generative probability of sampling a given character at time i, recalled in eq. (4):</p><formula xml:id="formula_4">P i alph = sof tmax(V [s i ; C i ] + b),<label>(5)</label></formula><p>where V and b are trainable parameters. The latter is the distribution reminded in eq. (2), created by the attention mechanism over the input tokens, i.e. in our case, over input characters:</p><formula xml:id="formula_5">P ij att ≡ α ij<label>(6)</label></formula><p>In our method this distribution is used for directly copying characters from the input to the output, pointing their input positions, while in <ref type="bibr" target="#b3">[3]</ref> P att is used only internally to weigh the input annotations and create the context vector C i . The final probability of outputting a specific character c is obtained combining P alph and P att through the quantity p gen , defined later, which acts as a soft switch between generating c or copying it:</p><formula xml:id="formula_6">P i (c) = p i gen · P i alph [c] + (1 − p i gen ) j|xi=c P ij att (c),<label>(7)</label></formula><p>where P i alph [c] is the component of P i alph corresponding to that character c. The backpropagation training algorithm, therefore, brings p gen close to 1 when it is necessary to generate the output as in a standard encoder-decoder with attention (P i (c) P i alph [c]); conversely, p gen will be close to 0 (i.e. P i (c) j|xi=c P j att (c)) when a copying step is needed. The model we propose therefore learns when to sample from P alph for selecting the character to be generated, and when to sample from P att for selecting the character that has to be copied directly from the input.</p><p>This copy mechanism is fundamental to output all the unknown words present in the input, i.e. words which never occur in the training set. In fact, generating characters in the right order to reproduce unknown words is a sub-task not "solvable" by a naive sequence-to-sequence model, which learns to output only known words.</p><p>The generation probability p gen ∈ [0, 1] is computed as follows:</p><formula xml:id="formula_7">p i gen = σ(W y ·ỹ i−1 + W s · s i + W p · p i−1 gen + W c · C i )<label>(8)</label></formula><p>where σ is the sigmoid function,ỹ i−1 is the last output character's embedding, s i is the current decoder's cell state and C i is the current context vector. W y , W s , W c and W p are the parameters whose training allows p gen to have the convenient value.</p><p>We highlight that in our formulation p i−1 gen , i.e. the value of p gen at time i − 1, contributes to the determination of p i gen . In fact, in a character-based model it is desirable that this probability remains unchanged for a fair number of time steps, and knowing its last value helps this behavior. This never happens in word-based models (such as <ref type="bibr" target="#b21">[21]</ref>), in which copying for a single time step is usually enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Switching GRUs</head><p>Aiming at improving performance, we enrich our model' training pipeline with an additional phase, which forces an appropriate language representation inside the recurrent components of the model. In order to achieve this goal, the encoder and the decoder do not own a fixed GRU, differently from what happens in classical end-to-end approaches. The recurrent module is passed each time as a parameter, depending on which one of the two training phases is actually performed.</p><p>In the first phase, similar to the usual one, the GRU assigned to the encoder deals with a tabular representation x as input, the GRU assigned to the decoder has to cope with natural language, and the model generates an output utterancẽ y = F (x). Conversely, in the second phase GRUs are switched and we use as input the just obtained natural language utteranceỹ to generate a new tablẽ x = G(ỹ) = G(F (x)). Therefore, the same model can build both F and G, thanks to the switch of GRUs.</p><p>In other words, the learning iteration is performed as follows.</p><p>-A dataset example (x, y) is given.</p><p>x is a tabular meaning representation and y is the corresponding reference sentence. -We generate an output utteranceỹ = F (x) -We perform an optimization step on the model's parameters, aiming at minimizing L f orward = loss(ỹ, y) -We reconstruct the meaning representationx back from the previously generated output:x = G(ỹ) = G(F (x)) -We perform a further optimization step on the model's parameters, this time aiming at minimizing L backward = loss(x, x)</p><p>The higher training time, direct consequence of the just described technique, is a convenient investment, as it brings an appreciable improvement of the model's performance (see section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We tested our model on four datasets, whose main descriptive statistics are given in table 1: among them, the most known and frequently used in literature is the E2E dataset <ref type="bibr" target="#b17">[17]</ref>, used as benchmark for the E2E Challenge organized by the Heriot-Watt University in 2017. It is a crowdsourced collection of roughly 50,000 instances, in which every input is a list of slot-value pairs and every expected output is the corresponding natural language sentence. The dataset has been partitioned by the challenge organizers in predefined training, validation and test sets, conceived for training data-driven, end-to-end Natural Language Generation models in the restaurant domain.</p><p>However, during our experiments, we noticed that the values contained in the E2E dataset are a little naive in terms of variability. In other words, a slot like name, that could virtually contain a very broad range of different values, is filled alternating between 19 fixed possibilities. Moreover, values are partitioned among training, validation and test set, in such a way that test set always contains values that are also present in the training set. Consequently, we created a modified version of the E2E dataset, called E2E+, as follows: we selected the slots that represent more copy-susceptible attributes, i.e. name, near and food, and conveniently replaced their values, in both meaning representations and reference sentences. New values for food are picked from Wikipedia's list of adjectival forms of countries and nations 1 , while both name and near are filled with New York restaurants' names contained in the Entree dataset presented in <ref type="bibr" target="#b4">[4]</ref>. It is worth noting that none of the values of name are found in near ; likewise, values that belong to the training set are not found in the validation set nor in the test one, and vice versa. This value partitioning shall ensure the absence of generation bias in the copy mechanism, stimulating the models to copy attribute values, regardless of their presence in the training set. The MR and 1st reference fields in table 4 are instances of this new dataset.</p><p>Finally, we decided to test our model also on two datasets, Hotel and Restaurant, frequently used in literature (for instance in <ref type="bibr" target="#b26">[26]</ref> and <ref type="bibr" target="#b8">[8]</ref>). They are built on a 12 attributes ontology: some attributes are common to both domains, while others are domain specific. Every MR is a list of key-value pairs enclosed in a dialogue act type, such as inform, used to present information about restaurants, confirm, to check that a slot value has been recognized correctly, and reject, to advise that the user's constraints cannot be met. For the sake of compatibility, we filtered out from Hotel and Restaurant all inputs whose dialogue act type was not inform, and removed the dialogue act type. Besides, we changed the format of the key-value pairs to E2E-like ones.</p><p>Tables are encoded simply converting all characters to ASCII and feeding every corresponding index to the encoder, sequentially. The resulting model's vocabulary is independent of the input, allowing the application of the transfer learning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We developed our system using the PyTorch framework 2 , release 0.4.1 <ref type="bibr" target="#b3">3</ref> . The training has been carried out as described in subsection 2.3: this training procedure needs the two GRUs to have the same dimensions, in terms of input size, hidden size, number of layers and presence of a bias term. Moreover, they both have to be bidirectional, even if the decoder ignores the backward part of its current GRU.</p><p>We minimize the negative log-likelihood loss using teacher forcing <ref type="bibr" target="#b27">[27]</ref> and Adam <ref type="bibr" target="#b11">[11]</ref>, the latter being an optimizer that computes individual adaptive learning rates. As a consequence of the length of the input sequences, a characterbased model is often subject to the exploding gradient problem, that we solved via the well-known technique of gradient norm clipping <ref type="bibr" target="#b19">[19]</ref>.</p><p>We also propose a new formulation of P (c) that helps the model to learn when it is necessary to start a copying phase:</p><formula xml:id="formula_8">P i (c) = p i gen · P i alph (c) + (1 − p i gen ) j|xi=c P i,j−1 att (c)<label>(9)</label></formula><p>Sometimes, our model has difficulty in focusing on the first letter it has to copy. This may be caused by the variety of characters it could be attending on; instead, it seems easier to learn to focus on the most largely seen characters, as for instance ' ' and '['. As these special characters are very often the prefix of the words we need to copy, when this focus is achieved, we would like the attention distribution to be translated one step to the right, over the first letter that must be copied. Therefore, the final probability of outputting a specific character c, introduced in eq. <ref type="formula" target="#formula_6">(7)</ref>, is modified to P i,j−1 att , i.e. the attention distribution shifted one step to the right and normalized.</p><p>Notice that P i,j−1 att is the only shifted probability, while P i alph remains unchanged. Therefore, if the network is generating the next token (i.e. p i gen 1 ), the shift trick does not involve P i (c) and the network samples the next character from P i alph , as usual. This means that the shift operation is not degrading the generation ability of the model, whilst improving the copying one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Discussion</head><p>In order to show that our model represents an effective and relevant improvement, we carry out two different experimentations: an ablation study and a comparison with two well-known models. The first model is the encoder-decoder architecture with attention mechanism by <ref type="bibr" target="#b3">[3]</ref> (hereafter "EDA"), used character-by-character. The second one is TGen <ref type="bibr" target="#b7">[7]</ref>, a word-based model, still derived from <ref type="bibr" target="#b3">[3]</ref>, but integrating a beam search mechanism and a reranker over the top k outputs, in order to disadvantage utterances that do not verbalize all the information contained in the MR. We chose it because it has been adopted as baseline in the E2E NLG Challenge 4 .</p><p>We used the official code provided in the E2E NLG Challenge website for TGen, and we developed our models and EDA in PyTorch, training them on NVIDIA GPUs. Hyperparameter tuning is done through 10-fold cross-validation, using the bleu metric <ref type="bibr" target="#b18">[18]</ref> for evaluating each model. The training stopping criterion was based on the absence of models' performance improvements (see <ref type="bibr" target="#b7">[7]</ref>).</p><p>We evaluated the models' performance on test sets' output utterances using the Evaluation metrics script 5 provided by the E2E NLG Challenge organizers. It rates quality according to five different metrics: bleu <ref type="bibr" target="#b18">[18]</ref>, nist <ref type="bibr" target="#b6">[6]</ref>, meteor <ref type="bibr" target="#b12">[12]</ref>, rouge_l <ref type="bibr" target="#b13">[13]</ref> and cider <ref type="bibr" target="#b24">[24]</ref>.</p><p>Our first experimentation, the ablation study, refers to the E2E dataset because of its wide diffusion, and is shown in table 2; "EDA_CS" identifies our model, and 'C' and 'S' stand for "Copy" and "Switch", the two major improvements presented in this work. It is evident that the partially-improved networks are able to provide independent benefits to the performance. Those components cooperate positively, as EDA_CS further enhances those results. Furthermore, the obtained bleu metric value on the E2E test set would allow our model to be ranked fourth in the E2E NLG Challenge, while its baseline TGen was ranked tenth. Our second experimentation, the comparison study, is shown in table 3. The character-based design of EDA_CS led us to explore in this context also a possible behavior as a transfer learning capable model: in order to test this hypothesis, we used the weights learned during training on the E2E+ dataset as the starting point for a fine-tuning phase on all the other datasets. We chose E2E+ because it reduces the generation bias, as discussed in subsection 3.1. We named this approach EDA_CS TL .</p><p>A first interesting result is that our model EDA_CS always obtains higher metric values with respect to TGen on the Hotel and Restaurant datasets, and three out of five higher metrics values on the E2E dataset. However, in the case of E2E+, TGen achieves three out of five higher metrics values. These results suggest that EDA_CS and TGen are comparable, at least from the point of view of automatic metrics' evaluation.</p><p>A more surprising result is that the approach EDA_CS TL allows to obtain better performance with respect to training EDA_CS in the standard way on the Hotel and Restaurant datasets (for the majority of metrics); on E2E, EDA_CS TL outperforms EDA_CS only in one case (i.e. meteor metric).</p><p>Moreover, EDA_CS TL shows a bleu increment of at least 14% with respect to TGen's score when compared to both Hotel and Restaurant datasets.</p><p>Finally, the baseline model, EDA, is largely outperformed by all other examined methods.</p><p>Therefore, we can claim that our model exploits its transfer learning capabilities effectively, showing very good performances in a context like data-to-text generation in which the portability of features learned from different datasets, in the extent of our knowledge, has not yet been explored. We highlight that EDA_CS's model's good results are achieved even if it consists in a fully end-to-end model which does not benefit from the delexicalizationrelexicalization procedure, differently from TGen. Most importantly, the latter represents a word-based system: as such, it is bound to a specific, limited vocabulary, in contrast to the general-purpose character one used in our work. <ref type="table" target="#tab_3">Table 4</ref> reports the output of the analyzed models for a couple of MR, taken from the E2E+ test set. The EDA's inability to copy is clear, as it tends, in its output, to substitute those values of name, food and near that do not appear in the training set with known ones, guided by the first few characters of the input slot's content. Besides, it shows serious coverage issues, frequently 'forgetting' to report information, and/or repeating more times the same ones.</p><p>These troubles are not present in EDA_CS output utterances: the model nearly always renders all of the input slots, still without duplicating any of them. This goal is achieved even in absence of explicit coverage techniques thanks to our peculiar training procedure, detailed in section 2.3, that for each input sample minimizes also the loss on the reconstructed tabular input. It is worth noting that the performance of TGen and EDA_CS are overall comparable, especially when they deal with names or other expressions not present in training.    The joint analysis of the matrix of the attention distribution P ij att and the vector p gen allows a deeper understanding of how our model works.</p><p>In figure 2 every row shows the attention probability distribution "seen" when an output character is produced at the i-th time instant (i.e. the vector P ij att , 1 ≤ j ≤ T x ), while every column shows values of the attention distribution corresponding to a specific input position j (i.e. the vector P ij att , 1 ≤ i ≤ T y ). We can therefore follow the white spots, corresponding to higher values of attention, to understand the flow of the model's attention during the generation of the output utterance.</p><p>Moreover, p gen values, which lie in the numeric interval [0, 1], help us in the interpretation of the attention: they are represented as a grayscale vector from zero (black) to one (white) under the matrices. Values close to 0 mean copying and those near 1 mean generating.</p><p>We can note that our model's behavior varies significantly depending on the dataset it has been trained on. <ref type="figure" target="#fig_1">Figure 2a</ref> shows the attention probability distribution matrix of EDA_CS (together with p gen vector) trained on the E2E dataset: as observed before, attribute values in this dataset have a very low variability (and are already present in the training set), so that they can be individually represented and easily generated by the decoder. In this case, a <ref type="figure">Fig. 3</ref>. Copying common words leads the model to "uncertain" values of pgen typical pattern is the copy of only the first, discriminating character, clearly noticeable in the graphical representation of the p gen vector, and the subsequent generation of the others. Notice that the attention tends to remain improperly focused on the same character for more than one output time step, as in the first letter of "high".</p><p>On the other hand, the copy mechanism shows its full potential when the system must learn to copy attribute values, as in the E2E+ dataset. In figure 2b the diagonal attention pattern is pervasive: (i) it occurs when the model actually copies, as in "Harley Davidson" and "Coco Pazzo", and (ii) as a soft track for the generation, as in "customer rating", where the copy-first-generate-rest behavior emerges again.</p><p>A surprising effect is shown in <ref type="figure">figure 3</ref>, when the model is expected to copy words that, instead, are usually generated: an initial difficulty in copying the word "The", that is usually a substring of a slot value, is ingeniously overcome as follows. The first character is purely generated, as shown by the white color in the underlying vector, and the sequence of the following characters, "he_", is half-generated and half-copied. Then, the value of p gen gets suddenly but correctly close to 0 (black) until the closing square bracket is met. The network's output is not affected negatively by this confusion and the attention matrix remains quite well-formed.</p><p>As a final remark, the metrics used, while being useful, well-known and broadly accepted, do not reflect the ability to directly copy input facts to produce outputs, so settling the rare word problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We showed in this paper an effective character-based end-to-end model that faces data-to-text generation tasks. It takes advantage of a copy mechanism, that deals successfully with the rare word problem, and of a specific training procedure, characterized by the switching GRUs mechanism. These innovative contributions to state-of-art further improve the quality of the generated texts.</p><p>We highlight that our formulation of the copy mechanism is an original character-based adaptation of <ref type="bibr" target="#b21">[21]</ref>, because of the use of p i−1 gen to determine the value of p i gen , at the following time step. This helps the model in choosing whether to maintain the same value for a fair number of time steps or not.</p><p>Besides, the use of characters allows the creation of more general models, which do not depend on a specific vocabulary; it also enables a very effective straightforward transfer learning procedure, which in addition eases training on small datasets. Moreover, outputs are obtained in a completely end-to-end fashion, in contrast to what happens for the chosen baseline word-based model, whose performances are comparable or even worse.</p><p>One future improvement of our model could be the "reinforcement" of the learning iteration described in section 2.3: for each dataset example (x, y), we could consider, as an ulterior example, the reverse instance (y, x). The network obtained this way should be completely reversible, and the interchangeability of input and output languages could open up new opportunities in neural machine translation, such as two-way neural translators.</p><p>New metrics that give greater importance to rare words might be needed in the future, with the purpose of better assess performances of able-to-copy NLG models on datasets such as the E2E+ one.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Encoder-decoder with attention model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( a )</head><label>a</label><figDesc>On an E2E instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( b )</head><label>b</label><figDesc>On an E2E+ instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Attention distribution (white means more attention) and pgen (white: generating, black: copying), as calculated by the model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Descriptive statistics: on the left, sizes of training, validation and test sets are shown. On the right, the average number of characters, respectively for Meaning Representations and natural language sentences, are presented</figDesc><table><row><cell>Dataset</cell><cell cols="3">Number of instances</cell><cell cols="2">Avg. number of characters</cell></row><row><cell></cell><cell cols="3">training validation test</cell><cell>MRs</cell><cell>NL sentences</cell></row><row><cell>E2E</cell><cell>42061</cell><cell cols="2">4672 4693</cell><cell>112.11</cell><cell>115.07</cell></row><row><cell>E2E+</cell><cell>42061</cell><cell cols="2">4672 4693</cell><cell>112.91</cell><cell>115.65</cell></row><row><cell>Hotel</cell><cell>2210</cell><cell>275</cell><cell>275</cell><cell>52.74</cell><cell>61.31</cell></row><row><cell cols="2">Restaurant 2874</cell><cell>358</cell><cell>358</cell><cell>53.89</cell><cell>63.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The ablation study on the E2E dataset evidences the final performance improvement reached by our model. Best values for each metric are highlighted (the higher the better)</figDesc><table><row><cell></cell><cell>bleu</cell><cell>0.4999</cell><cell></cell><cell>bleu</cell><cell>0.6538</cell></row><row><cell></cell><cell>nist</cell><cell>7.1146</cell><cell></cell><cell>nist</cell><cell>8.4601</cell></row><row><cell>EDA</cell><cell cols="2">meteor 0.3369</cell><cell>EDA_S</cell><cell>meteor 0.4337</cell></row><row><cell></cell><cell cols="2">rouge_l 0.5634</cell><cell></cell><cell>rouge_l 0.6646</cell></row><row><cell></cell><cell>cider</cell><cell>1.3176</cell><cell></cell><cell>cider</cell><cell>1.9944</cell></row><row><cell></cell><cell>bleu</cell><cell>0.6255</cell><cell></cell><cell>bleu</cell><cell>0.6705</cell></row><row><cell></cell><cell>nist</cell><cell>7.7934</cell><cell></cell><cell>nist</cell><cell>8.5150</cell></row><row><cell>EDA_C</cell><cell cols="2">meteor 0.4401</cell><cell>EDA_CS</cell><cell>meteor 0.4449</cell></row><row><cell></cell><cell cols="2">rouge_l 0.6582</cell><cell></cell><cell>rouge_l 0.6894</cell></row><row><cell></cell><cell>cider</cell><cell>1.7286</cell><cell></cell><cell>cider</cell><cell>2.2355</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison. Note the absence of transfer learning on dataset E2E+ because in this case the training and fine-tuning datasets are the same. Best values for each metric are highlighted (the higher the better)</figDesc><table><row><cell></cell><cell></cell><cell cols="3">E2E+ E2E Hotel Restaurant</cell></row><row><cell></cell><cell>bleu</cell><cell cols="2">0.3773 0.4999 0.4316</cell><cell>0.3599</cell></row><row><cell></cell><cell>nist</cell><cell cols="2">5.7835 7.1146 5.9708</cell><cell>5.5104</cell></row><row><cell>EDA</cell><cell cols="3">meteor 0.2672 0.3369 0.3552</cell><cell>0.3367</cell></row><row><cell></cell><cell cols="3">rouge_l 0.4638 0.5634 0.6609</cell><cell>0.5892</cell></row><row><cell></cell><cell>cider</cell><cell cols="2">0.2689 1.3176 3.9213</cell><cell>3.3792</cell></row><row><cell></cell><cell>bleu</cell><cell cols="2">0.6292 0.6593 0.5059</cell><cell>0.4074</cell></row><row><cell></cell><cell>nist</cell><cell cols="2">9.4070 8.6094 7.0913</cell><cell>6.4304</cell></row><row><cell>TGen</cell><cell cols="3">meteor 0.4367 0.4483 0.4246</cell><cell>0.3760</cell></row><row><cell></cell><cell cols="3">rouge_l 0.6724 0.6850 0.7277</cell><cell>0.6395</cell></row><row><cell></cell><cell>cider</cell><cell cols="2">2.8004 2.2338 5.0404</cell><cell>4.1650</cell></row><row><cell></cell><cell>bleu</cell><cell cols="2">0.6197 0.6705 0.5515</cell><cell>0.4925</cell></row><row><cell></cell><cell>nist</cell><cell cols="3">9.2103 8.5150 7.4447 6.9813</cell></row><row><cell>EDA_CS</cell><cell cols="3">meteor 0.4428 0.4449 0.4379</cell><cell>0.4191</cell></row><row><cell></cell><cell cols="3">rouge_l 0.6610 0.6894 0.7499</cell><cell>0.7002</cell></row><row><cell></cell><cell>cider</cell><cell cols="2">2.8118 2.2355 5.1376</cell><cell>4.7821</cell></row><row><cell></cell><cell>bleu</cell><cell>-</cell><cell cols="2">0.6580 0.5769 0.5099</cell></row><row><cell></cell><cell>nist</cell><cell>-</cell><cell cols="2">8.5615 7.4286 7.3359</cell></row><row><cell>EDA_CS TL</cell><cell>meteor</cell><cell>-</cell><cell cols="2">0.4516 0.4439 0.4340</cell></row><row><cell></cell><cell>rouge_l</cell><cell>-</cell><cell cols="2">0.6740 0.7616 0.7131</cell></row><row><cell></cell><cell>cider</cell><cell>-</cell><cell cols="2">2.1803 5.3456 4.9915</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>A comparison of the three models' output on some MR of the E2E+ test set. The first reference utterance is reported for convenience</figDesc><table><row><cell>MR</cell><cell>name[New Viet Huong], eatType[pub], customer rating[1 out of 5], near[Ecco]</cell></row><row><cell>1st reference</cell><cell>The New Viet Huong is a pub near Ecco that has a customer rating of 1 out of 5.</cell></row><row><cell>EDA_CS</cell><cell>New Viet Huong is a pub near Ecco with a customer rating of 1 out of 5.</cell></row><row><cell>TGen</cell><cell>New Viet Huong is a pub near Ecco with a customer rating of 1 out of 5.</cell></row><row><cell>EDA</cell><cell>Near the riverside near the ERNick Restaurant is a pub near the ERNicker's.</cell></row><row><cell></cell><cell>name[La Mirabelle], eatType[restaurant], food[Iraqi],</cell></row><row><cell>MR</cell><cell>priceRange[high], area[riverside], familyFriendly[yes],</cell></row><row><cell></cell><cell>near[Mi Cocina]</cell></row><row><cell></cell><cell>La Mirabelle is a children friendly restaurant located in the Riverside</cell></row><row><cell>1st reference</cell><cell>area near to the Mi Cocina. It serves Iraqi food and is in the high</cell></row><row><cell></cell><cell>price range.</cell></row><row><cell>EDA_CS</cell><cell>La Mirabelle is a high priced Iraqi restaurant located in the riverside area near Mi Cocina. It is children friendly.</cell></row><row><cell>TGen</cell><cell>La Mirabelle is a high priced Iraqi restaurant in the riverside area near Mi Cocina. It is child friendly.</cell></row></table><note>EDA La Memaini is a high priced restaurant that serves Iranian food in the high price range. It is located in the riverside area near Manganaro's Restaurant.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://en.wikipedia.org/wiki/List_of_adjectival_and_demonymic_forms_ for_countries_and_nations, consulted on August 30, 2018 2 Code and datasets are publicly available at https://github.com/marco-roberti/ char-data-to-text-gen 3 https://pytorch.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">www.macs.hw.ac.uk/InteractionLab/E2E/ 5 https://github.com/tuetschek/E2E-metrics</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The activity has been partially carried on in the context of the Visiting Professor Program of the Italian Istituto Nazionale di Alta Matematica (INdAM).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A surprisingly effective out-of-the-box char2char model on the E2E NLG challenge dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th</title>
		<meeting>the 18th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annual SIGdial Meeting on Discourse and Dialogue</title>
		<imprint>
			<biblScope unit="page" from="158" to="163" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving sequence to sequence learning for morphological inflection generation: The BIU-MIT systems for the SIGMORPHON 2016 shared task for morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</title>
		<meeting>the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The findme approach to assisted browsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">D</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><forename type="middle">J</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">C</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="32" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using n-gram co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Human Language Technology Research, HLT &apos;02</title>
		<meeting>the Second International Conference on Human Language Technology Research, HLT &apos;02</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence generation for spoken dialogue via deep syntax trees and strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurcícek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language generation through character-based rnns with finite-state prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">METEOR: an automatic metric for MT evaluation with high levels of correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06-23" />
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Achieving open vocabulary neural machine translation with hybrid word-character models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The E2E dataset: New challenges for end-to-end generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-02-01" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="3159" to="3166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cider: Consensusbased image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zipser</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Challenges in data-todocument generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
