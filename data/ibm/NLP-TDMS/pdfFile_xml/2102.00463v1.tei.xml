<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D object detection</term>
					<term>point clouds</term>
					<term>LiDAR</term>
					<term>convolutional neural network</term>
					<term>autonomous driving</term>
					<term>sparse convolution</term>
					<term>local feature aggregation</term>
					<term>point sampling</term>
					<term>Waymo Open Dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object detection is receiving increasing attention from both industry and academia thanks to its wide applications in various fields. In this paper, we propose the Point-Voxel Region based Convolution Neural Networks (PV-RCNNs) for accurate 3D detection from point clouds. First, we propose a novel 3D object detector, PV-RCNN-v1, which employs the voxel-to-keypoint scene encoding and keypoint-to-grid RoI feature abstraction two novel steps. These two steps deeply incorporate both 3D voxel CNN and PointNet-based set abstraction for learning discriminative point-cloud features. Second, we propose a more advanced framework, PV-RCNN-v2, for more efficient and accurate 3D detection. It consists of two major improvements, where the first one is the sectorized proposal-centric strategy for efficiently producing more representative and uniformly distributed keypoints, and the second one is the VectorPool aggregation to replace set abstraction for better aggregating local point-cloud features with much less resource consumption. With these two major modifications, our PV-RCNN-v2 runs more than twice as fast as the v1 version while still achieving better performance on the large-scale Waymo Open Dataset with 150m × 150m detection range. Extensive experiments demonstrate that our proposed PV-RCNNs significantly outperform previous state-of-the-art 3D detection methods on both the Waymo Open Dataset and the highly-competitive KITTI benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>3 D object detection is critical and indispensable to lots of real-world applications, such as autonomous driving, intelligent traffic system and robotics, etc. The most common used data representation for 3D object detection is point clouds, which could be generated by the depth sensors (e.g., LiDAR sensors and RGB-D cameras) for capturing 3D scene information. The sparse property and irregular data format of point clouds have brought great challenges for extending 2D detection methods <ref type="bibr" target="#b5">[1]</ref>, <ref type="bibr" target="#b6">[2]</ref>, <ref type="bibr" target="#b7">[3]</ref>, <ref type="bibr" target="#b8">[4]</ref>, <ref type="bibr" target="#b9">[5]</ref>, <ref type="bibr" target="#b10">[6]</ref>, <ref type="bibr" target="#b11">[7]</ref> to 3D object detection from point clouds.</p><p>To learn discriminative features from these sparse and spatially-irregular points, several methods transform them to regular grid representations by voxelization <ref type="bibr" target="#b12">[8]</ref>, <ref type="bibr" target="#b13">[9]</ref>, <ref type="bibr" target="#b14">[10]</ref>, <ref type="bibr" target="#b15">[11]</ref>, <ref type="bibr" target="#b16">[12]</ref>, <ref type="bibr" target="#b17">[13]</ref>, <ref type="bibr" target="#b18">[14]</ref>, <ref type="bibr" target="#b19">[15]</ref>, <ref type="bibr" target="#b20">[16]</ref>, <ref type="bibr" target="#b21">[17]</ref>, which could be efficiently processed by conventional Convolution Neural Networks (CNNs) and well-studied 2D detection heads <ref type="bibr" target="#b6">[2]</ref>, <ref type="bibr" target="#b7">[3]</ref> for 3D object detection. But the inevitable information loss from quantization degrades the fine-grained localization accuracy of these voxel-based methods. In contrast, powered by the pioneer works PointNet <ref type="bibr" target="#b22">[18]</ref>, <ref type="bibr" target="#b23">[19]</ref>, some other methods directly learn effective features from raw point clouds and predict 3D boxes around the foreground points <ref type="bibr" target="#b24">[20]</ref>, <ref type="bibr" target="#b25">[21]</ref>, <ref type="bibr" target="#b26">[22]</ref>, <ref type="bibr" target="#b27">[23]</ref>. These point-based methods naturally preserve accurate point location and own flexible receptive filed with radius-based local feature aggregation (like set abstraction <ref type="bibr" target="#b23">[19]</ref>), but are generally computationally intensive.</p><formula xml:id="formula_0">• Shaoshuai Shi,</formula><p>For the sake of high-quality 3D box prediction, we observe that, the voxel-based representation with intensive predefined anchors could generate better 3D box proposals with higher recall rate <ref type="bibr" target="#b28">[24]</ref>, while performing the proposal refinement on the 3D point-wise features could naturally benefit from more fine-grained point locations. Motivated by these observations, we argue that the performance of 3D object detection can be boosted by intertwining diverse local feature learning strategies from both voxel-based and pointbased representations. Thus, we propose the Point-Voxel Region based Convolutional Neural Networks (PV-RCNNs) to incorporate the best from both point-based and voxelbased strategies for 3D object detection from point clouds.</p><p>First, we propose a novel two-stage detection network, PV-RCNN-v1, for accurate 3D object detection through a two-step strategy of point-voxel feature aggregation. In the step-I where voxel-to-keypoint scene encoding is performed, a voxel CNN with sparse convolution is adopted for voxel-wise feature learning and accurate proposal generation. The voxel-wise features of the whole scene are then summarized into a small set of keypoints by PointNetbased set abstraction <ref type="bibr" target="#b23">[19]</ref>, where the keypoints with accurate point locations are sampled by furthest point sampling (FPS) from raw point clouds. The keypoint-to-grid RoI feature abstraction is conducted in step-II, where we propose the RoI-grid pooling module to aggregate the above keypoint features to the RoI-grids of each proposal. It encodes multi-scale context information and forms regular grid features for the following accurate proposal refinement.</p><p>Second, we propose an advanced two-stage detection network, PV-RCNN-v2, on top of PV-RCNN-v1, for achieving more accurate, efficient and practical 3D object detection. Compared with PV-RCNN-v1, the improvements of PV-RCNN-v2 mainly lies in two aspects.</p><p>We first propose a sectorized proposal-centric keypoint sampling strategy, that concentrates the limited keypoints to be around 3D proposals to encode more effective features for scene encoding and proposal refinement. Meanwhile, the sectorized FPS is conducted to parallelly sample keypoints in different sectors to keep the uniformly distributed property of keypoints while accelerating the keypoint sampling process. Our proposed keypoint sampling strategy is much faster and effective than the naive FPS (which has quadratic complexity), which makes the whole network much more efficient and is particularly important for large-scale 3D scenes with millions of points. Moreover, we propose a novel local feature aggregation operation, VectorPool aggregation, for more effective and efficient local feature encoding from local neighborhoods. We argue that the relative point locations of local point clouds are robust, effective and discriminative information for describing local geometry. We propose to split the 3D local space into regular, compact and dense voxels, which are sequentially assigned to form a hyper feature vector for encoding local geometric information, while the voxel-wise features in different locations are encoded with different kernel weights to generate positionsensitive local features. Hence, different resulted feature channels encode the local features of specific local locations.</p><p>Compared with the previous set abstraction operation for local feature aggregation, our proposed VectorPool aggregation can efficiently handle very large numbers of centric points for local feature aggregation due to our compact local feature representation. Equipped with the VectorPool aggregation in both the voxel-based backbone and RoI-grid pooling module, our proposed PV-RCNN-v2 framework could be much more memory-efficient and faster than previous counterparts while keeping comparable or even better performance to establish a more practical 3D detector for resource-limited devices.</p><p>In summary, our proposed Point-Voxel Region based Convolution Neural Networks (PV-RCNNs) have three major contributions. (1) Our proposed PV-RCNN-v1 adopts two novel operations, voxel-to-keypoint scene encoding and keypoint-to-grid RoI feature abstraction, to deeply incorporate the advantages of both point-based and voxelbased features learning strategies into a unified 3D detection framework. <ref type="bibr" target="#b6">(2)</ref> Our proposed PV-RCNN-v2 takes a step further to more practical 3D detection with better performance, less resource consumption and faster running speed, which is enabled by our proposed sectorized proposalcentric keypoint sampling strategy to obtain more representative keypoints with faster speed, and also powered by our novel VectorPool aggregation for achieving local aggregation on a very large number of central points with less resource consumption and more effective representation. (3) Our proposed 3D detectors surpass all published methods with remarkable margins on multiple challenging 3D detection benchmarks, and our PV-RCNN-v2 could achieve new state-of-the-art results on the Waymo Open dataset with 10 FPS inference speed for the 150m × 150m detection region. The source code will be available at https://github.com/open-mmlab/OpenPCDet <ref type="bibr" target="#b29">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">2D/3D Object Detection with RGB images</head><p>2D Object Detectors. We summarize the 2D object detectors into anchor-based and anchor-free directions. The approaches following the anchor-based paradigm advocate the empirically pre-defined anchor boxes to perform detection. In this direction, object detectors are further divided into two-stage <ref type="bibr" target="#b6">[2]</ref>, <ref type="bibr" target="#b30">[26]</ref>, <ref type="bibr" target="#b31">[27]</ref>, <ref type="bibr" target="#b32">[28]</ref>, <ref type="bibr" target="#b33">[29]</ref> and one-stage <ref type="bibr" target="#b7">[3]</ref>, <ref type="bibr" target="#b11">[7]</ref>, <ref type="bibr" target="#b34">[30]</ref>, <ref type="bibr" target="#b35">[31]</ref> categories. Two-stage approaches are characterized by suggesting a series of region proposals as candidates to make per-region refinement, while one-stage ones directly perform detection on feature maps in a fully convolutional manner. On the other hand, studies of the anchor-free direction mainly fall into keypoints-based <ref type="bibr" target="#b36">[32]</ref>, <ref type="bibr" target="#b37">[33]</ref>, <ref type="bibr" target="#b38">[34]</ref>, <ref type="bibr" target="#b39">[35]</ref> and center-based <ref type="bibr" target="#b40">[36]</ref>, <ref type="bibr" target="#b41">[37]</ref>, <ref type="bibr" target="#b42">[38]</ref>, <ref type="bibr" target="#b43">[39]</ref> paradigms. The keypoints-based methods represent bounding boxes as keypoints, i.e., corner/extreme points, grid points and a set of bounded points, while the center-based approaches predict the bounding box from foreground points inside objects. Besides, the recently proposed DETR <ref type="bibr" target="#b44">[40]</ref> leverages widely adopted transformers from natural language processing to detect objects with attention mechanism and self-learnt object queries, which also gets rid of anchor boxes. 3D Object Detectors with RGB images. Image-based 3D Object Detection aims to estimate 3D bounding boxes from a monocular image or stereo images. The early work Mono3D <ref type="bibr" target="#b45">[41]</ref> first generates 3D region proposals with ground-plane assumption, and then exploits semantic knowledge from monocular images for box scoring. The following works <ref type="bibr" target="#b46">[42]</ref>, <ref type="bibr" target="#b47">[43]</ref> incorporate the relations between 2D and 3D boxes as geometric constraint. M3D-RPN <ref type="bibr" target="#b48">[44]</ref> introduces an end-to-end 3D region proposal network with depth-aware convolutions. <ref type="bibr" target="#b49">[45]</ref>, <ref type="bibr" target="#b50">[46]</ref>, <ref type="bibr" target="#b51">[47]</ref>, <ref type="bibr" target="#b52">[48]</ref>, <ref type="bibr" target="#b53">[49]</ref> develop monocular 3D object detectors based on a wire-frame template obtained from CAD models. RTM3D <ref type="bibr" target="#b54">[50]</ref> extends the CAD based methods and performs coarse keypoints detection to localize 3d objects in real-time. On the stereo side, Stereo R-CNN <ref type="bibr" target="#b55">[51]</ref>, <ref type="bibr" target="#b56">[52]</ref> capitalizes on a Stereo RPN to associate proposals from left and right images and further refines 3D region of interest by a region-based photometric alignment. DSGN <ref type="bibr" target="#b57">[53]</ref> introduces the differentiable 3D geometric volume to simultaneously learn depth information and semantic cues in an end-to-end optimized pipelines. Another cut-in point for detecting 3D objects from RGB images is to convert the image pixels with estimated depth maps into artificial point clouds, i.e., pseudo-LiDAR <ref type="bibr" target="#b56">[52]</ref>, <ref type="bibr" target="#b58">[54]</ref>, <ref type="bibr" target="#b59">[55]</ref>, where the LiDARbased detection models can operate on pseudo-LiDAR for 3D box estimation. These image-based 3D detection methods suffer from inaccurate depth estimation and can only generate coarse 3D bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">3D Object Detection with Point Clouds</head><p>Grid-based 3D Object Detectors. To tackle the irregular data format of point clouds, most existing works project the point clouds to regular grids to be processed by 2D or 3D CNN. The pioneer work MV3D <ref type="bibr" target="#b13">[9]</ref> projects the point clouds to 2D bird view grids and places lots of predefined 3D anchors for generating 3D bounding boxes, and the following works <ref type="bibr" target="#b15">[11]</ref>, <ref type="bibr" target="#b17">[13]</ref>, <ref type="bibr" target="#b21">[17]</ref>, <ref type="bibr" target="#b60">[56]</ref>, <ref type="bibr" target="#b61">[57]</ref>, <ref type="bibr" target="#b62">[58]</ref> develop better strategies for multi-sensor fusion. <ref type="bibr" target="#b16">[12]</ref>, <ref type="bibr" target="#b18">[14]</ref>, <ref type="bibr" target="#b19">[15]</ref> introduce more efficient frameworks with bird-eye view representation while <ref type="bibr" target="#b63">[59]</ref> proposes to fuse grid features of multiple scales. MVF <ref type="bibr" target="#b64">[60]</ref> integrates 2D features from bird-eye view and perspective view before projecting points into pillar representations <ref type="bibr" target="#b19">[15]</ref>. Some other works <ref type="bibr" target="#b12">[8]</ref>, <ref type="bibr" target="#b14">[10]</ref> divide the point clouds into 3D voxels to be processed by 3D CNN, and 3D sparse convolution <ref type="bibr" target="#b65">[61]</ref> is introduced <ref type="bibr" target="#b20">[16]</ref> for efficient 3D voxel processing. <ref type="bibr" target="#b66">[62]</ref>, <ref type="bibr" target="#b67">[63]</ref> utilize multiple detection heads while <ref type="bibr" target="#b28">[24]</ref> explores the object part locations for improving the performance. In addition, <ref type="bibr" target="#b68">[64]</ref>, <ref type="bibr" target="#b69">[65]</ref> predicts bounding box parameters following the anchor-free paradigm. These grid-based methods are generally efficient for accurate 3D proposal generation but the receptive fields are constraint by the kernel size of 2D/3D convolutions. Point-based 3D Object Detectors. F-PointNet <ref type="bibr" target="#b24">[20]</ref> first proposes to apply PointNet <ref type="bibr" target="#b22">[18]</ref>, <ref type="bibr" target="#b23">[19]</ref> for 3D detection from the cropped point clouds based on the 2D image bounding boxes. PointRCNN <ref type="bibr" target="#b25">[21]</ref> generates 3D proposals directly from the whole point clouds instead of 2D images for 3D detection with point clouds only, and the following work STD <ref type="bibr" target="#b27">[23]</ref> proposes the sparse to dense strategy for better proposal refinement. <ref type="bibr" target="#b70">[66]</ref> proposes the hough voting strategy for better object feature grouping. 3DSSD <ref type="bibr" target="#b71">[67]</ref> introduces F-FPS as a complement of D-FAS and develops the first one stage object detector operating on raw point clouds. These pointbased methods are mostly based on the PointNet series, especially the set abstraction operation <ref type="bibr" target="#b23">[19]</ref>, which enables flexible receptive fields for point cloud feature learning. Representation Learning on Point Clouds. Recently representation learning on point clouds has drawn lots of attention for improving the performance of point cloud classification and segmentation <ref type="bibr" target="#b14">[10]</ref>, <ref type="bibr" target="#b22">[18]</ref>, <ref type="bibr" target="#b23">[19]</ref>, <ref type="bibr" target="#b72">[68]</ref>, <ref type="bibr" target="#b73">[69]</ref>, <ref type="bibr" target="#b74">[70]</ref>, <ref type="bibr" target="#b75">[71]</ref>, <ref type="bibr" target="#b76">[72]</ref>, <ref type="bibr" target="#b77">[73]</ref>, <ref type="bibr" target="#b78">[74]</ref>, <ref type="bibr" target="#b79">[75]</ref>, <ref type="bibr" target="#b80">[76]</ref>, <ref type="bibr" target="#b81">[77]</ref>, <ref type="bibr" target="#b82">[78]</ref>, <ref type="bibr" target="#b83">[79]</ref>. In terms of 3D detection, previous methods generally project the point clouds to regular bird view grids <ref type="bibr" target="#b13">[9]</ref>, <ref type="bibr" target="#b16">[12]</ref> or 3D voxels <ref type="bibr" target="#b14">[10]</ref>, <ref type="bibr" target="#b84">[80]</ref> for processing point clouds with 2D/3D CNN. 3D sparse convolution <ref type="bibr" target="#b65">[61]</ref>, <ref type="bibr" target="#b85">[81]</ref> are adopted in <ref type="bibr" target="#b20">[16]</ref>, <ref type="bibr" target="#b28">[24]</ref> to effectively learn sparse voxel-wise features from the point clouds. Qi et al. <ref type="bibr" target="#b22">[18]</ref>, <ref type="bibr" target="#b23">[19]</ref> proposes the PointNet to directly learn point-wise features from the raw point clouds, where set abstraction operation enables flexible receptive fields by setting different search radii. <ref type="bibr" target="#b86">[82]</ref> combines both voxelbased CNN and point-based shared-parameter multi-layer percetron (MLP) network for efficient point cloud feature learning. In comparison, our PV-RCNN-v1 takes advantages from both the voxel-based feature learning (i.e., 3D sparse convolution) and PointNet-based feature learning (i.e., set abstraction operation) to enable both high-quality 3D proposal generation and flexible receptive fields for improving the 3D detection performance. In this paper, we propose the VectorPool aggregation operation to aggregate more effective local features from point clouds with much less resource consumption and faster running speed than the commonly used set abstraction, and it is adopted in both the VSA module and RoI-grid pooling module of our new  proposed PV-RCNN-v2 framework for much more efficient and accurate 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>State-of-the-art 3D object detectors mostly adopt two-stage frameworks that generally achieve higher performance by splitting the complex detection problem into the region proposal generation and per-proposal refinement two stages.</p><p>In this section, we briefly introduce our chosen strategy for the fundamental feature extraction and proposal generation stage, and then discuss the challenges of straightforward methods for the second 3D proposal refinement stage. 3D voxel CNN and proposal generation. Voxel CNN with 3D sparse convolution <ref type="bibr" target="#b65">[61]</ref>, <ref type="bibr" target="#b85">[81]</ref> is a popular choice of stateof-the-art 3D detectors <ref type="bibr" target="#b20">[16]</ref>, <ref type="bibr" target="#b28">[24]</ref>, <ref type="bibr" target="#b87">[83]</ref> for its efficiency of converting irregular point clouds into 3D sparse feature volumes. The input points P are first divided into small voxels with spatial resolution of L × W × H, where nonempty voxel-wise features are directly calculated by averaging the features of all within points (the commonly used features are 3D coordinates and reflectance intensities). The network utilizes a series of 3 × 3 × 3 3D sparse convolution to gradually convert the point clouds into feature volumes with 1×, 2×, 4×, 8× downsampled sizes. Such sparse feature volumes at each level could be viewed as a set of sparse voxel-wise feature vectors. The Voxel CNN backbone could be naturally combined with the detection heads of 2D detectors <ref type="bibr" target="#b6">[2]</ref>, <ref type="bibr" target="#b7">[3]</ref> by converting the encoded 8× downsampled 3D feature volumes into 2D bird-view feature maps. Specifically, we follow <ref type="bibr" target="#b20">[16]</ref> to stack the 3D feature volumes along the Z axis to obtain the L 8 × W 8 bird-view feature maps. The anchor-based SSD head <ref type="bibr" target="#b7">[3]</ref> is appended to this bird-view feature maps for high quality 3D proposal generation. As shown in <ref type="table" target="#tab_2">Table 1</ref>, our adopted 3D voxel CNN backbone with anchor-based scheme achieves higher recall performance than the PointNet-based approaches, which establishes a strong backbone network and generates robust proposal boxes for the following proposal refinement stage. Discussions on 3D proposal refinement. In the proposal refinement stage, the proposal-specific features are required to be extracted from the resulting 3D feature volumes or 2D maps. Intuitively, the proposal feature extraction should be conducted in the 3D space instead of the 2D feature maps to learn more fine-grained features for 3D proposal refinement. However, these 3D feature volumes from the 3D voxel CNN have major limitations in the following aspects. (i) These feature volumes are generally of low spatial resolution as they are downsampled by up to 8 times, which hinders accurate localization of objects in the input scene. (ii) Even if one can upsample to obtain feature volumes/maps with larger spatial sizes, they are generally still quite sparse. The commonly used trilinear or bilinear interpolation in the RoIPool/RoIAlign operations can only extract features from very small neighborhoods (i.e., 4 and 8 nearest neighbors for bilinear and trilinear interpolation respectively), that would therefore obtain features with mostly zeros and waste much computation and memory for proposal refinement.</p><p>On the other hand, the point-based local feature aggregation methods <ref type="bibr" target="#b23">[19]</ref> have shown strong capability of encoding sparse features from local neighborhoods with arbitrary scales. We therefore propose to incorporate a 3D voxel CNN with the point-based local feature aggregation operation for conducting accurate and robust proposal refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PV-RCNN-V1: POINT-VOXEL FEATURE SET ABSTRACTION FOR 3D OBJECT DETECTION</head><p>To learn effective features from sparse point clouds, state-ofthe-art 3D detection approaches are based on either 3D voxel CNNs with sparse convolution or PointNet-based operators. Generally, the 3D voxel CNNs with sparse convolutional layers are more efficient and are able to generate highquality 3D proposals, while the PointNet-based operators naturally preserve accurate point locations and can capture rich context information with flexible receptive fields.</p><p>We propose a novel two-stage 3D detection framework, PV-RCNN-v1, to deeply integrate the advantages of two types of operators for more accurate 3D object detection from point clouds. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, PV-RCNN-v1 consists of a 3D voxel CNN with sparse convolution as the backbone for efficient feature encoding and proposal generation. Given each 3D proposal, we propose to encode proposal specific features in two novel steps: the voxel-to-keypoint scene encoding, which summarizes all the voxel feature volumes of the overall scene into a small number of feature keypoints, and the keypoint-to-grid RoI feature abstraction, which aggregates the scene keypoint features to RoI grids to generate proposal-aligned features for confidence estimation and location refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Voxel-to-keypoint scene encoding via voxel set abstraction</head><p>Our proposed PV-RCNN-v1 first aggregates the voxel-wise scene features at multiple neural layers of 3D voxel CNN into a small number of keypoints, which bridge the 3D voxel CNN feature encoder and the proposal refinement network. Keypoints Sampling. In PV-RCNN-v1, we simply adopt the Furtherest Point Sampling (FPS) algorithm to sample a small number of keypoints K = {p 1 , · · · , p n } from the point clouds P, where n is a hyper-parameter and is about 10% of the point number of P. Such a strategy encourages that the keypoints are uniformly distributed around non-empty voxels and can be representative to the overall scene. Voxel Set Abstraction Module. We propose the Voxel Set Abtraction (VSA) module to encode multi-scale semantic features from 3D feature volumes to the keypoints. The set abstraction <ref type="bibr" target="#b23">[19]</ref> is adopted for aggregating voxel-wise feature volumes. Different with the original set abstraction, the surrounding local points are now regular voxel-wise semantic features from 3D voxel CNN, instead of the neighboring raw points with features learned by PointNet.</p><p>Specifically, denote F (l k ) = {f</p><formula xml:id="formula_1">(l k ) 1 , . . . , f (l k ) N k } as the set of voxel-wise feature vectors in the k-th level of 3D voxel CNN, V (l k ) = {v (l k ) 1 , . . . , v (l k )</formula><p>N k } as their corresponding 3D coordinates in the uniform 3D metric space, where N k is the number of non-empty voxels in the k-th level. For each keypoint p i , we first identify its neighboring non-empty voxels at the k-th level within a radius r k to retrieve the set of neighboring voxel-wise feature vectors as</p><formula xml:id="formula_2">S (l k ) i =        f (l k ) j ; v (l k ) j − p i T v (l k ) j − p i 2 &lt; r k , ∀v (l k ) j ∈ V (l k ) , ∀f (l k ) j ∈ F (l k )        ,<label>(1)</label></formula><p>where we concatenate the local relative position v are then aggregated by a PointNet-block <ref type="bibr" target="#b22">[18]</ref> to generate the keypoint feature as</p><formula xml:id="formula_3">f (pv k ) i = max G M S (l k ) i ,<label>(2)</label></formula><p>where M(·) denotes randomly sampling at most T k voxels from the neighboring set S</p><formula xml:id="formula_4">(l k ) i for saving computations, G(·)</formula><p>denotes a multi-layer perceptron network to encode voxelwise features and relative locations. The operation max(·) maps diverse number of neighboring voxel features to a single keypoint feature f</p><formula xml:id="formula_5">(pv k ) i .</formula><p>Here multiple radii at each layer are utilized to capture richer contextual information.</p><p>The above voxel feature aggregation is performed at different scales of the 3D voxel CNN, and the aggregated features from different scales are concatenated to obtain the multi-scale semantic feature for keypoint p i as</p><formula xml:id="formula_6">f (pv) i = f (pv1) i , f (pv2) i , f (pv3) i , f (pv4) i , for i = 1, . . . , n,<label>(3)</label></formula><p>where the generated feature f</p><formula xml:id="formula_7">(pv) i incorporates both 3D CNN-based voxel-wise feature f (l k ) j</formula><p>and the PointNet-based features as Eq. (2). Moreover, the 3D coordinates of p i also naturally preserves accurate location information. Further Enriching Keypoint Features. We further enriching the keypoint features with the raw point cloud P and with the 8× downsampled 2D bird-view feature maps, where the raw point cloud can partially make up the quantization loss of the initial point-cloud voxelization while the 2D birdview maps have larger receptive fields along the Z axis. Specifically, the raw point-cloud feature f (raw) i is also aggregated as that in Eq. (2), while the bird-view features f (bev) i are obtained by performing by bilinear interpolation with projected 2D keypoints on the 2D feature maps. Hence, the keypoint features for p i is further enriched by concatenating all its associated features as</p><formula xml:id="formula_8">f (p) i = f (pv) i , f (raw) i , f (bev) i , for i = 1, . . . , n,<label>(4)</label></formula><p>which have the strong capacity of preserving 3D structural information of the entire scene for the following fine-grained proposal refinement step. Predicted Keypoint Weighting. As mentioned before, the keypoints are chosen by FPS algorithm and some of them might only represent the background regions. Intuitively, keypoints belonging to the foreground objects should contribute more to the accurate refinement of the proposals, while the ones from the background regions should contribute less. Hence, we propose a Predicted Keypoint Weighting (PKW) module to re-weight the keypoint features with extra supervisions from point-cloud segmentation. The segmentation labels are free-of-charge and can be directly generated from the 3D box annotations, i.e., by checking whether each keypoint is inside or outside of a ground-truth 3D box since the 3D objects in autonomous driving scenes are naturally separated in the 3D space. The predicted keypoint feature weighting can be formulated as</p><formula xml:id="formula_9">f (p) i = A(f (p) i ) · f (p) i ,<label>(5)</label></formula><p>where A(·) is a three-layer multi-layer perceptron (MLP) network with a sigmoid function to predict foreground confidence. The PKW module is trained with focal loss <ref type="bibr" target="#b11">[7]</ref> with its default hyper-parameters to handle the imbalanced foreground/background points of the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Keypoint-to-grid RoI feature abstraction for proposal refinement</head><p>With the above voxel-to-keypoint scene encoding step, we can summarize the multi-scale semantic features into a small number of keypoints. In this step, we propose keypoint-to-grid RoI feature abstraction to generate accurate proposal-aligned features from the keypoint features</p><formula xml:id="formula_10">F = {f (p) 1 , · · · ,f (p)</formula><p>n } for fine-grained proposal refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoI-grid Pooling via Set Abstraction. Given each 3D</head><p>proposal, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, we propose an RoI-grid pooling operation to aggregate the keypoint features to the RoI-grid points with multiple receptive fields. We uniformly sample 6×6×6 grid points within each 3D proposal, which are then flattend and denoted as G = {g 1 , · · · , g 216 }. We utilize set abstraction to obtain features of grid points via aggregating the keypoint features. Specifically, we firstly identify the neighboring keypoints of grid point g i as</p><formula xml:id="formula_11">Ψ = f (p) j ; p j − g i T p j − g i 2 &lt;r, ∀p j ∈ K, ∀f (p) j ∈F ,<label>(6)</label></formula><p>where p j − g i is appended to indicate the local relative location within the ball of radiusr. A PointNet-block <ref type="bibr" target="#b22">[18]</ref> is then adopted to aggregate the neighboring keypoint feature setΨ to obtain the feature for grid point g i as</p><formula xml:id="formula_12">f (g) i = max G M Ψ ,<label>(7)</label></formula><p>where M(·) and G(·) are defined in the same way as Eq. <ref type="formula" target="#formula_3">(2)</ref>. We set multiple radiir and aggregate keypoint features with different receptive fields, which are concatenated together for capturing richer multi-scale contextual information.</p><p>After obtaining each grid's aggregated features from its surrounding keypoints, all RoI-grid features of the same RoI can be vectorized and transformed by a two-layer MLP with 256 feature dimensions to represent the overall proposal.</p><p>Our proposed RoI-grid pooling operation could aggregate much richer contextual information than the previous RoI-pooling/RoI-align operation <ref type="bibr" target="#b25">[21]</ref>, <ref type="bibr" target="#b27">[23]</ref>, <ref type="bibr" target="#b28">[24]</ref>. This is because a single keypoint could contribute to multiple RoI-grid points due to the overlapped neighboring balls of RoI-grid points, and their receptive fields are even beyond the RoI boundaries by capturing the contextual keypoint features outside the 3D RoI. In contrast, the previous stateof-the-art methods either simply average all point-wise features within the proposal as the RoI feature <ref type="bibr" target="#b25">[21]</ref>, or pool many uninformative zeros as the RoI features because of the very sparse point-wise features <ref type="bibr" target="#b27">[23]</ref>, <ref type="bibr" target="#b28">[24]</ref>. Proposal Refinement and Confidence Prediction. Given the RoI feature extracted by the above RoI-grid pooling module, the refinement network learns to predict the size and location (i.e. center, size and orientation) residuals relative to the 3D proposal box. Separate two-layer MLP sub-networks are employed for confidence prediction and proposal refinement respectively. We follow <ref type="bibr" target="#b28">[24]</ref> to conduct the IoU-based confidence prediction, where the confidence training target y k is normalized to be between [0, 1] as</p><formula xml:id="formula_13">y k = min (1, max (0, 2IoU k − 0.5)) ,<label>(8)</label></formula><p>where IoU k is the Intersection-over-Union (IoU) of the kth RoI w.r.t. its ground-truth box. The binary cross-entropy loss is adopted to optimize the IoU branch while the box residuals are optimized with smooth-L1 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training losses</head><p>The proposed PV-RCNN framework is trained end-to-end with the region proposal loss L rpn , keypoint segmentation loss L seg and the proposal refinement loss L rcnn . (i) We adopt the same region proposal loss L rpn as that in <ref type="bibr" target="#b20">[16]</ref>,</p><formula xml:id="formula_14">L rpn = L cls + β r∈O L smooth-L1 ( ∆r a , ∆r a ) + αL dir ,<label>(9)</label></formula><p>where O = {x, y, z, l, h, w, θ}, the anchor classification loss L cls is calculated with the focal loss (default hyperparameters) <ref type="bibr" target="#b11">[7]</ref>, L dir is a binary classification loss for orientation to eliminate the ambiguity of ∆θ a as in <ref type="bibr" target="#b20">[16]</ref>, and smooth-L1 loss is for anchor box regression with the predicted residual ∆r a and regression target ∆r a . Loss weights are set as β = 2.0 and α = 0.2 in the training process. (ii) The keypoint segmentation loss L seg is also formulated by the focal loss as mentioned in Sec. 4.1. (iii) The proposal refinement loss L rcnn includes the IoU-guided confidence prediction loss L iou and the box refinement loss as</p><formula xml:id="formula_15">L rcnn = L iou + r∈{x,y,z,l,h,w,θ} L smooth-L1 ( ∆r p , ∆r p ),<label>(10)</label></formula><p>where ∆r p is the predicted box residual and ∆r p is the proposal regression target that are encoded same with ∆r a . The overall training loss are then the sum of these three losses with equal weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation details</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PV-RCNN-V2: MORE ACCURATE AND EFFI-CIENT 3D DETECTION WITH VECTORPOOL AGGRE-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GATION</head><p>As discussed above, our PV-RCNN-v1 deeply incorporates point-based and voxel-based feature encoding operations in voxel-to-keypoint and keypoint-to-grid feature set abstraction, which significantly boost the performance of 3D object detection from point clouds. To make the framework more practical for real-world applications, we propose a new detection framework, PV-RCNN-v2, for more accurate and efficient 3D object detection with less resources. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, Sectorized Proposal-Centric keypoint sampling and VectorPool aggregation are presented to replace their counterparts in the v1 framework. The Sectorized Proposal-Centric keypoint sampling strategy is much faster than the Furtherest Point Sampling (FPS) used in PV-RCNN-v1 and achieves better performance with more representative keypoint distribution. Then, to handle large-scale local feature aggregation from point clouds, we propose a more effective and efficient local feature aggregation operation, VectorPool aggregation, to explicitly encode the relative point positions with spatially vectorized representation. It is integrated into our PV-RCNN-v2 framework in both the VSA and the RoI-grid pooling to significantly reduce the memory/computation consumptions while also achieving comparable or even better detection performance. In this section, the above two novel operations are introduced in Sec. 5.1 and Sec. 5.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sectorized Proposal-Centric Sampling for Efficient and Representative Keypoint Sampling</head><p>The keypoint sampling is critical for our proposed detection framework since keypoints bridge the point-voxel representations and influence the performance of proposal refinement network. As mentioned before, in our PV-RCNN-v1 framework, we subsample a small number of keypoints from raw point clouds with the Furthest Point Sampling (FPS) algorithm, which mainly has two drawbacks. (i) The FPS algorithm is time-consuming due to its quadratic complexity, which hinders the training and inference speed of our framework, especially for keypoint sampling of largescale point clouds. (ii) The FPS keypoint sampling algorithm would generate a large number of background keypoints that do not contribute to the proposal refinement step since only the keypoints around the proposals could be retrieved by the RoI-grid pooling operation. To mitigate these drawbacks, we propose a more efficient and effective keypoint sampling algorithm for 3D object detection.</p><p>Sectorized Proposal-Centric (SPC) keypoint sampling. As discussed above, the number of keypoints is limited and FPS keypoint sampling algorithm would generate wasteful keypoints in the background regions, which decrease the capability of keypoints to well representing objects for proposal refinement. Hence, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, we propose the Sectorized Proposal-Centric (SPC) keypoint sampling operation to uniformly sample keypoints from more concentrated neighboring regions of proposals while also being much faster than the FPS sampling algorithm, Specifically, denote the raw point clouds as P, and denote the centers and sizes of 3D proposal as C and D, respectively.  </p><formula xml:id="formula_16">p i − c j &lt; max(dxj ,dyj ,dzj ) 2 + r (s) , (dx j , dy j , dz j ) ∈ D ⊂ R 3 , p i ∈ P ⊂ R 3 , c j ∈ C ⊂ R 3 ,    ,<label>(11)</label></formula><p>where r (s) is a hyperparameter indicating the maximum extended radius of the proposals, and dx j , dy j , dz j are the sizes of the 3D proposal. Through this proposal-centric filtering for keypoints, the candidate number of keypoint sampling is greatly reduced from |P| to |P |, which not only reduces the complexity of the follow-up keypoint sampling, but also concentrates the limited number of keypoints to encode the neighboring regions of proposals.</p><p>To further parallelize the keypoint sampling process for acceleration, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, we distribute the proposalcentric point set P into s sectors centered at the scene center, and the point set of the k-th sector could be represented as</p><formula xml:id="formula_17">S k = p i (arctan(py i , px i ) + π) × s 2π = k − 1, p i = (px i , py i , pz i ) ∈ P ⊂ R 3 ,<label>(12)</label></formula><p>where k ∈ {1, . . . , s}, and arctan(py i , px i ) ∈ (−π, π] indicate the angle between the positive X axis and the ray ended with (px i , py i ). Through this process, we divide sampling n keypoints into s subtasks of local keypoint sampling, where the k-th sector samples |S k | |P | × n keypoints from S k . These subtasks are eligible to be executed in parallel on GPUs, while the scale of keypoint sampling is further reduced from |P | to max k∈{1,...,s} |S k |.</p><p>Hence, our proposed SPC keypoint sampling greatly reduces the scale of keypoint sampling from |P| to the much smaller max k∈{1,...,s} |S k |, which not only effectively accelerates the keypoint sampling process, but also increases the capability of keypoint feature representation by concentrating the keypoints to the neighborhoods of 3D proposals.</p><p>Note that as shown in <ref type="figure">Fig. 6</ref>, our proposed keypoint sampling with sector-based parallelization is different from the FPS point sampling with random division as that in <ref type="bibr" target="#b27">[23]</ref>, since our strategy reserves the uniform distribution of sampled keypoints while the random division may destroy the uniform distribution property of FPS algorithm. Comparison of local keypoint sampling strategies. To sample a specific number of keypoints from each S k , there are several alternative options, including FPS, random parallel FPS, random sampling, voxelized sampling, coverage sampling <ref type="bibr" target="#b83">[79]</ref>, which result in very different keypoint distributions (see <ref type="figure">Fig. 6</ref>). We observe that FPS algorithm generates uniformly distributed keypoints covering the whole regions while other algorithms cannot generate similar keypoint  distribution patterns. <ref type="table" target="#tab_14">Table 7</ref> shows that the uniformly distribution of FPS achieves obviously better performance than other keypoint sampling algorithms and is critical for the final detection performance. Note that even though we utilize FPS for local keypoint sampling, it is still much faster than FPS-based keypoint sampling on the whole point cloud, since our SPC operation significantly reduces the candidates for keypoint sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Local vector representation for structurepreserved local feature learning from point clouds</head><p>How to aggregate informative features from local point clouds is critical in our proposed point-voxel-based object detection system. As discussed in Sec. 4, PV-RCNN-v1 adopts the set abstraction for local feature aggregation in both VSA and RoI grid-pooling. However, we observe that the set abstraction operation can be extremely time-and resource-consuming for large-scale local feature aggregation of point clouds. Hence, in this section, we first analyze the limitations of set abstraction for local feature aggregation, and then propose the VectorPool aggregation operation for local feature aggregation from point clouds, which is integrated into our PV-RCNN-v2 framework for more accurate and efficient 3D object detection. Limitations of set abstraction for RoI grid pooling. Specifically, as shown in Eqs.</p><p>(2) and <ref type="formula" target="#formula_12">(7)</ref>, the set abstraction operation samples T k point-wise features from each local neighborhood, which are encoded separately by a sharedparameter MLP for local feature encoding. Suppose that there are a total of N local point-cloud neighborhoods and the feature dimensions of each point is C in , then N × T k point-wise features with C in channels should be encoded by the shared-parameter MLP G(·) to generate point-wise features of size N × T k × C out . Both the space complexity and computations would be significant when the number of local neighborhoods are quite large. For instance, in our RoI-grid pooling module, the number of RoI-grid points could be very large (N = 100×6×6× 6 = 21, 600) with 100 proposals and grid size 6. This module is therefore slow and also consumes much GPU memory in our PV-RCNN-v1, which restricts its capability to be run on lightweight devices with limited computation and memory resources. Moreover, the max-pooling operation in set abstraction abandons the spatial distribution information of local point clouds and harms the representation capability of locally aggregated features from point clouds. Local vector representation for structure-preserved local feature learning. To extract more informative features from local point-cloud neighborhoods, we propose a novel local feature aggregation operation, VectorPool aggregation, which preserves spatial point distributions of local neighborhoods and also costs less memory/computation resources than the commonly used set abstraction. We propose to generate position-sensitive features in different local neighborhoods by encoding them with separate kernel weights and separate feature channels, which are then concatenated to explicitly represent the spatial structures of local point features.</p><p>Specifically, denote the input point coordinates and fea-</p><formula xml:id="formula_18">tures as P = {(p i , f i ) | p i ∈ R 3 , f i ∈ R Cin , i ∈ {1, .</formula><p>. . , M }}, and the centers of N local neighborhoods as Q = {q i | q i ∈ R 3 , i ∈ {1, . . . , N }}. We are going to extract N local pointwise features with C out channels for each point of Q.</p><p>To reduce the parameter size, computational and memory consumption of our VectorPool aggregation, motivated by <ref type="bibr" target="#b88">[84]</ref>, we first summarize the point-wise features f i ∈ R Cin to more compact representationsf i ∈ R Cr 1 with a parameter-free scheme aŝ</p><formula xml:id="formula_19">f i (k) = nr−1 j=0 f i (jC r1 + k), k ∈ {0, . . . , C r1 − 1},<label>(13)</label></formula><p>where C r1 is the number of output feature channels and C in = n r × C r1 . Eq. (13) sums up every n r input feature channels into one output feature channel to reduce the feature channels by n r times, which could effectively reduce the resource consumptions of the follow-up processing.</p><p>To generate position-sensitive features for a local cubic neighborhood centered at q k , we split its spatial neighboring space into n x × n y × n z dense voxels, and the above pointwise features are grouped to different voxels as follows:</p><formula xml:id="formula_20">W AvgPool ! ! ! ! ×! " ⨂ Local Vector Representation ! "</formula><formula xml:id="formula_21">V (i x , i y , i z ) =    ( p j − q k ,f j ) 2 max(|r x |, |r y |, |r z |) &lt; l), rt l + 1 2 × n t = i t − 1, t ∈ {x, y, z}    , i x ∈ {1, . . . , n x }, i y ∈ {1, . . . , n y }, i z ∈ {1, . . . , n z },<label>(14)</label></formula><p>where (r x , r y , r z ) = (p j − q k ) ∈ R 3 indicates the relative positions of point p j in this local cubic neighborhood, l is the side length of the neighboring cubic space centered at q k , and i x , i y , i z are the voxel indices along the X, Y , Z axes respectively, to indicates a specific voxel of this local cubic space. Then the relative coordinates and point-wise features of the points within each voxel are averaged as the positionspecific features of this local voxel as follows:</p><formula xml:id="formula_22">V (i x , i y , i z ) = [r v ,f v ] = 1 n v nv u=1 [p u − q k ,f u ],<label>(15)</label></formula><p>whereV (i x , i y , i z ) ∈ R 3+Cr 1 , v = {1, 2, . . . , n x × n y × n z }, and n v indicates the number of inside points in position V (i x , i y , i z ) of this local neighborhood. The resulted fea-turesV (i x , i y , i z ) encodes the relative coordinates and local features of the specific voxel (i x , i y , i z ).</p><p>Those features in different positions may represent very different local features due to the different point distributions of different local voxels. Hence, instead of encoding the local features with a shared-parameter MLP as in set abstraction <ref type="bibr" target="#b23">[19]</ref>, we propose to encode different local voxel features with separate local kernel weights for capturing position-sensitive features aŝ</p><formula xml:id="formula_23">U (i x , i y , i z ) = E r v ,f v × W v ,<label>(16)</label></formula><formula xml:id="formula_24">whereÛ (i x , i y , i z ) ∈ R Cr 2 , (r v ,f v ) ∈V (i x , i y , i z )</formula><p>, and E(·) is an operation combining the relative positionr i and featuresf i , which have several choices (including concatenation (our default setting), PosPool (xyz or cos/sin) <ref type="bibr" target="#b82">[78]</ref>) to be explored (as listed in <ref type="table" target="#tab_2">Table 10</ref>). W v ∈ R (3+Cr 1 )×Cr 2 is the learnable kernel weights for encoding the specific features of local voxel (i x , i y , i z ) with channel C r2 , and different positions have different kernel weights for encoding positionsensitive local features. Finally, we directly sort the local voxel featureŝ U (i x , i y , i z ) according to their spatial order (i x , i y , i z ), and their features are sequentially concatenated to generate the final local vector representation as U = <ref type="figure" target="#fig_0">[Û (1, 1, 1)</ref>, . . . ,Û (i x , i y , i z ), . . . ,Û (n x , n y , n z )],</p><p>where U ∈ R nx×ny×nz×Cr 2 encodes the structure-preserved local features by simply assigning the features of different locations to their corresponding feature channels, which naturally preserves the spatial structures of local features in the neighboring space centered at q k , This local vector representation would be finally processed with another MLP to encode the local features to C out feature channels for follow-up processing. Note that our feature channel summation and local volume feature encoding of VectorPool aggregation could reduce the feature dimensions from C in to C r2 , that greatly saves the needed computations and memory resources of our VectorPool operation. Morever, instead of conducting max-pooling on local point-wise features as in the set abstraction, our proposed spatial-structure-preserved local vector representation could encode the position-sensitive local features with different feature channels. PV-RCNN-v2 with local vector representation for local feature aggregation. Our proposed VectorPool aggregation is integrated in our PV-RCNN-v2 detection framework, to replace the set abstraction operation in both the VSA layer and the RoI-grid pooling module. Thanks to our VectorPool local feature aggregation operation, compared with PV-RCNN-v1 framework, our PV-RCNN-v2 not only consumes much less memory and computation resources, but also achieves better 3D detection performance with faster running speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation details</head><p>Training losses. The overall training loss of PV-RCNN-v2 is exactly the same with PV-RCNN-v1, which has already been discussed in Sec. 4.3. Network Architecture. For the SPC keypoint sampling of PV-RCNN-v2, we set the maximum extended radius r (s) = 1.6m, and each scene is split into 6 sectors for parallel keypoint sampling. Two VectorPool aggregation operations are adopted to the 4× and 8× feature volumes of the VSA module with the side lengths l = 4.8m and l = 9.6m respectively, and both of them have local voxels n x = n y = n z = 3 and channel reduced factor n r = 2. One VectorPool aggregation operation is adopted to the raw points with local voxels n x = n y = n z = 2 and without Setting   channel reduction, and the side length l = 1.6m. All Vector-Pool aggregation utilize the concatenation operation as E(·) for encoding relative positions and point-wise features. For RoI-grid pooling, we adopt a single VectorPool aggregation with local voxels n x = n y = n z = 3, channel reduced factor n r = 3 and side length l = 4.0m. We increase the number of sampled RoI-grid points to 7 × 7 × 7 for each proposal. Since our VectorPool aggregation consumes much less computation and memory resources, we can afford such a large number of RoI-grid points while still being faster than previous set abstraction (see <ref type="table">Table 8</ref>).</p><note type="other">Keypoint Sampling Algorithm Point Feature Extraction RoI Feature Aggregation LEVEL</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we evaluate our proposed framework in the large-scale Waymo Open Dataset <ref type="bibr" target="#b89">[85]</ref> and the highlycompetitive KITTI dataset <ref type="bibr" target="#b90">[86]</ref>. In Sec. 6.1, we first introduce our experimental setup and implementation details. In Sec. 6.2, we conduct extensive ablation experiments and analysis to investigate the individual components of both our PV-RCNN-v1 and PV-RCNN-v2 frameworks. In Sec. 6.3, we present the main results of our PV-RCNN framework and compare our performance with previous state-of-the-art methods on both the Waymo dataset and the KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Datasets and evaluation metrics. Our PV-RCNN frameworks are evaluated on the following two datasets.</p><p>Waymo Open Dataset <ref type="bibr" target="#b89">[85]</ref> is currently the largest dataset with LiDAR point clouds for autonomous driving. There are totally 798 training sequences with around 160k LiDAR samples, and 202 validation sequences with 40k LiDAR samples. It annotated the objects in the full 360 • field instead of 90 • as in KITTI dataset. The evaluation metrics are calculated by the official evaluation tools, where the mean average precision (mAP) and the mean average precision weighted by heading (mAPH) are used for evaluation. The 3D IoU threshold is set as 0.7 for vehicle detection and 0.5 for pedestrian/cyclist detection. We present the comparison in terms of two ways. The first way is based on objects' different distances to the sensor: 0 − 30m, 30 − 50m and &gt; 50m. The second way is to split the data into two difficulty levels, where the LEVEL 1 denotes the groundtruth objects with at least 5 inside points while the LEVEL 2 denotes the ground-truth objects with at least 1 inside points. As utilized by the official Waymo evaluation server, the mAPH of LEVEL 2 difficulty is the most important evaluate metric for all experiments.</p><p>KITTI Dataset <ref type="bibr" target="#b90">[86]</ref> is one of the most popular datasets of 3D detection for autonomous driving. There are 7, 481 training samples and 7, 518 test samples, where the training samples are generally divided into the train split (3, 712 samples) and the val split (3, 769 samples). We compare PV-RCNNs with state-of-the-art methods on this highlycompetitive 3D detection learderboard <ref type="bibr" target="#b91">[87]</ref>. The evaluation metrics are calculated by the official evaluation tools, where the mean average precision (mAP) is calculated with 40 recall positions on three difficulty levels. As utilized by the KITTI evaluation server, the 3D mAP of moderate difficulty level is the most important metric for all experiments. Training and inference details. Both PV-RCNN-v1 and PV-RCNN-v2 frameworks are trained from scratch in an endto-end manner with ADAM optimizer, learning rate 0.01 and cosine annealing learning rate decay strategy. To train the proposal refinement stage, we randomly sample 128 proposals with 1:1 ratio for positive and negative proposals, where a proposal is considered as positive sample if it has at least 0.55 3D IoU with the ground-truth boxes, otherwise it is treated as a negative sample.</p><p>During training, we adopt the widely used data augmentation strategies for 3D object detection, including the random scene flipping, global scaling with a random scaling factor sampled from [0.95, 1.05], global rotation around Z axis with a random angle sampled from [− π 4 , π 4 ], and the ground-truth sampling augmentation to randomly "paste" some new objects from other scenes to current training scene for simulating objects in various environments.</p><p>For the Waymo Open dataset, the detection range is set as  Dataset, while achieving state-of-the-art performance with 16 FPS for 70m×80m detection region on the KITTI dataset. Both of them are profiling on a single TITAN RTX GPU card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation studies for PV-RCNN framework</head><p>In this section, we investigate the individual components of our proposed PV-RCNN 3D detection frameworks with extensive ablation experiments. Unless mentioned otherwise, we conduct most experiments on the Waymo Open Dataset with detection range 150m × 150m for more comprehensive evaluation, and the input point clouds are generated by the first return from the Waymo LiDAR sensor. For efficiently conducting the ablation experiments on the Waymo Open dataset, we generate a small representative training set by uniformly sampling 20% frames (about 32k frames) from the training set, and all results are evaluated on the full validation set (about 40k frames) with the official evaluation tool of Waymo dataset. All models are trained with 30 epochs on a single GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">The component analysis of PV-RCNN-v1</head><p>Effects of voxel-to-keypoint scene encoding. In Sec. 4.1, we propose the voxel-to-keypoint scene encoding strategy to encode the global scene features to a small set of keypoints, which serves as a bridge between the backbone network and the proposal refinement network. As shown in 3 rd and 4 th rows of <ref type="table">Table 2</ref>, our proposed voxel-to-keypoint encoding strategy achieves slightly better performance than the UNetbased decoder network. This might benefit from the fact that the keypoint features are aggregated from multi-scale feature volumes and raw point clouds with large receptive fields while also keeping the accurate point-wise location information. Note that our method achieves such a performance by using much less point-wise features than the UNet-based decoder network. For instance, in the setting for Waymo dataset, our VSA module encodes the whole scene to around 4k keypoints for feeding into the RoI-grid pooling module, while the UNet-based decoder network summarizes the scene features to around 80k point-wise features, which further validates the effectiveness of our proposed voxel-to-keypoint scene encoding strategy. Effects of different features for VSA module. As mentioned in Sec. 4.1, our proposed VSA module incorporates multiple feature components, and their effects are explored in  refinement. (ii) As shown in 5 th row of <ref type="table" target="#tab_10">Table 4</ref>, f  <ref type="table" target="#tab_9">Table 3</ref>) and Waymo dataset ( <ref type="table">Table 5)</ref> show that the performance drops a bit after removing the PKW module drops, which demonstrates that the PKW module enables better multi-scale feature aggregation by focusing more on the foreground keypoints, since they are more important for the succeeding proposal refinement network. Effects of RoI-grid pooling module. RoI-grid pooling module is proposed in Sec. 4.2 for aggregating RoI features from very sparse keypoints. Here we investigate the effects of RoI-grid pooling module by replacing it with the RoIaware pooling <ref type="bibr" target="#b28">[24]</ref> and keeping other modules consistent. The experiments on both KITTI dataset (2 st and 4 th rows of <ref type="table" target="#tab_9">Table 3</ref>) and Waymo dataset ( <ref type="table" target="#tab_13">Table 6)</ref> show that the performance drops significantly when replacing RoI-grid pooling module. It validates that our proposed RoI-grid pooling module could aggregate much richer contextual information to generate more discriminative RoI features, which benefits from the large receptive field of our set abstraction based RoI-grid pooling module.</p><p>Compared with the previous RoI-aware pooling module, our proposed RoI-grid pooling module could generate denser grid-wise feature representation by supporting different overlapped ball areas among different grid points, while RoI-aware pooling module may generate lots of zeros due to the sparse inside points of RoIs. That means our proposed RoI-grid pooling module is especially effective for aggregating local features from very sparse point-wise features, such as in our PV-RCNN framework to aggregate features from very small number of keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Sampling</head><p>Coverage Sampling Voxelized-FPS RandomParallel-FPS Sectorized-FPS (Ours) <ref type="figure">Fig. 6</ref>. Illustration of the keypoint distributions from different keypoint sampling strategies. We use the dashed circles to indicate the missing parts and the clustered keypoints after using these keypoint sampling strategies. We could see that our proposed Sectorized-FPS could generate uniformly distributed keypoints that could encode more informative features for better proposal refinement, while still being much faster than the original FPS keypoint sampling strategy. Note that the coverage sampling strategy generates less keypoints in the shown cases since these keypoints already cover the neighboring space of the proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Feature Extraction</head><p>RoI   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">The component analysis of PV-RCNN-v2</head><p>Analysis of keypoint sampling strategies. In Sec. 5.1, we propose the SPC keypoint sampling strategy that is composed of proposal-centric keypoint filtering and sectorized FPS keypoint filtering. In the 1 st and 2 nd rows of <ref type="table" target="#tab_14">Table 7</ref>, we first investigate the effectiveness of our proposed proposalcentric keypoint filtering, where we could see that compared with the strong baseline PV-RCNN-v1, our proposal-centric keypoint filtering could further improve the detection performance by about 1.0 mAP/mAPH in both LEVEL 1 and LEVEL 2 difficulties. It validates our argument that our proposed SPC keypoint sampling strategy could generate more representative keypoints by concentrating the small number of keypoints to the more informative neighboring regions of proposals. Moreover, improved by our proposalcentric keypoint filtering, the keypoint sampling algorithm could be about twice faster than the baseline FPS algorithm by reducing the number of potential keypoints. Besides the original FPS strategy and our proposed Sectorized-FPS algorithm, we further explore four alternative strategies for accelerating the keypoint sampling strategy, which are as follows: (i) Random sampling indicates randomly choosing the keypoints from raw points. (ii) Proposal coverage sampling is inspired by <ref type="bibr" target="#b83">[79]</ref>, which first randomly chooses the required number of keypoints, then the chosen keypoints are randomly replaced with other raw points based on the coverage cost to cover more neighboring space of proposals. (iii) Voxelized-FPS first voxelizes the raw points to reduce the number of raw points then applies the FPS for keypoint sampling. (iv) RandomParallel-FPS first randomly split the raw points into several groups, then FPS algorithm is utilized to these groups in parallel for faster keypoint sampling. As shown in <ref type="table" target="#tab_14">Table 7</ref>, compared with the FPS algorithm (2 nd row) for keypoint sampling, the detection performances of all four alternative strategies drop a lot. In contrast, the performance of our proposed Sectorized-FPS is on par with the FPS algorithm while being more than 10 times faster than the FPS algorithm.</p><p>We argue that the uniformly distributed keypoints are important for the following proposal refinement. As shown in <ref type="figure">Fig. 6</ref>, only our proposed Sectorized-FPS could sample uniformly distributed keypoints like FPS while all other strategies have shown their own problems. The random sampling and proposal coverage sampling strategies generate messy keypoint distribution randomly, even though the coverage cost based relacement is adopted. The voxelized-FPS generates regularly scattered keypoints that lose accurate point locations due to the quantization. The RandomParallel-FPS generates small clustered keypoints since the nearby raw points could be divided into different groups, and all of them could be sampled as keypoints from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 8</head><p>The comparison of actual computations between PV-RCNN-v1 and PV-RCNN-v2 frameworks. Ncenter is the number of local regions to be aggregated features, and the evaluation metric is the number of FLOPs (multiply-adds). Our proposed VectorPool aggregation greatly reduces the computations for local feature aggregation in both the VSA module and the RoI-grid pooling module. different groups. In contrast, our proposed Sectorized-FPS could generate uniformly distributed keypoints by splitting raw points into different groups based on the structure relationship. There may still exist a very small number of clustered keypoints in the margins of different groups, but the experiments show that they have negligible effect on the performance. Hence, as shown in <ref type="table" target="#tab_14">Table 7</ref>, our proposed SPC keypoint sampling strategy outperforms previous FPS method significantly while still being more than 10 times faster than it. Effects of VectorPool aggregation. In Sec. 5.2, we propose the VectorPool aggregation operation to effectively and efficiently summarize the structure-preserved local features from point clouds. As shown in Tabel 2, our proposed Vec-torPool aggregation operation is utilized for local feature aggreagtion in both the VSA module and the RoI-grid pooling module, where the performance is improved consistently by adopting our VectorPool aggregation. The final PV-RCNN-v2 framework benefits from the structure-preserved spatial features from our VectorPool aggregation, which is critical for the following fine-grained proposal refinement.</p><p>Moreover, our VectorPool aggregation consumes much less computations and GPU memory than the original set abstraction operation. As shown in <ref type="table">Table 8</ref>, we record the actual computations of our PV-RCNN-v1 and PV-RCNN-v2 frameworks in both the VSA module and the RoI-grid pooling module. It shows that with our proposed VectorPool aggregation, the VSA module only consumes as low as 37.5% computations of the original set abstraction, which could greatly speed up the local feature aggregation, especially for the large-scale local feature aggregation. Meanwhile, the computations in the RoI-grid pooling module decrease as much as 56.6% of the counterpart of PV-RCNN-v1, even though our PV-RCNN-v2 utilizes large RoI-grid size 7×7×7. The lower computational consumption also indicates lower GPU memory consumption, which facilitates more practical usage of our proposed PV-RCNN-v2 framework. Effects of separate local kernel weights in VectorPool aggregation. We have demonstrated the fact in Eq. (16) that our proposed VectorPool aggregation generates positionsensitive features by encoding relative position features with separate local kernel weights. The 1 st and 3 rd rows of <ref type="table" target="#tab_17">Table 9</ref> show that the performance drops a lot if we remove the separate local kernel weights and adopt shared kernel weights for relative position encoding. It validates that the separate local kernel weights are better than previous shared-parameter MLP based local feature encoding, and it is important in our VectorPool aggregation operation. Effects of dense voxel numbers in VectorPool aggregation.   We investigate the number of dense voxels n x × n y × n z (see Eq. <ref type="formula" target="#formula_2">(14)</ref>) in VectorPool aggregation for VSA module and RoI-grid pooling module, where we could see that the setting of 3 × 3 × 3 achieves best performance, and both larger or smaller numbers of voxels lead to slightly worse performance. We consider that the required number of dense voxels depend on the sparsity of the point clouds, since smaller number of voxels could not well preserve the spatial local features while larger number of voxels may contain too many empty voxels in the resulted local vector representation. We empirically choose the setting of 3×3×3 dense voxel representation in both VSA module and RoIgrid pooling module of our PV-RCNN-v2 framework. Effects of different strategies for encoding relative positions. As shown in Eq. (16) of VectorPool aggregation, we utilize the operator E(·) to encode the relative position and features to generate the position-sensitive features. As shown in    operation with two variants of PosPool operations proposed by <ref type="bibr" target="#b82">[78]</ref>. <ref type="table" target="#tab_2">Table 10</ref> shows that all three operations achieve similar performance while concatenation method is slightly better than the PosPool operations. Thus, we finally utilize the simple concatenation operation in our VectorPool aggregation for generating position-sensitive features.</p><p>Effects of the number of keypoints. In <ref type="table" target="#tab_2">Table 12</ref>, we investigate the effects of the number of keypoints for encoding the scene features. <ref type="table" target="#tab_2">Table 12</ref> shows that our proposed method achieves similar performance when using more than 4,096 keypoints, while the performance drops significantly along with smaller number of keypoints (2,048 keypoints and 1,024 keypoints), especially on the LEVEL 1 difficulty level. Hence, to balance the performance and computation cost, we empirically choose to encode the whole scene to 4,096 keypoints for the Waymo dataset (2,048 keypoints for the KITTI dataset since it only needs to detect the frontal-view areas). The above experiments show that our method could effectively encode the whole scene to a small number of keypoints while keeping similar performance with a large number of keypoints, which demonstrates the effectiveness of the keypoint feature encoding strategy of our proposed PV-RCNN detection framework. Effects of the grid size in RoI-grid pooling module. <ref type="table" target="#tab_2">Table 12</ref> shows the performance of adopting different RoI-grid sizes for RoI-grid pooling module. We could see that the performance increases along with the RoI-grid sizes from 3 × ×3 to 7 × 7 × 7, but larger RoI-grid sizes (8 × 8 × 8 and 9 × 9 × 9) harm the performance slightly. We consider the reason may be that models with larger RoI-grid sizes contain much more learnable parameters which may result in over-fitting on the training set. Hence we finally adopt RoI-grid size 7 × 7 × 7 for the RoI-grid pooling module.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Main results of PV-RCNN framework and comparison with state-of-the-art methods</head><p>In this section, we demonstrate the main results of our proposed PV-RCNN frameworks, and make the comparison with state-of-the-art methods on both the large-scale Waymo Open Dataset and the highly-competitive KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">3D detection on the Waymo Open Dataset</head><p>To validate the effectiveness of our proposed PV-RCNN frameworks, we compare our proposed PV-RCNN-v1 and PV-RCNN-v2 frameworks with state-of-the-art methods on the Waymo Open Dataset, which currently is the largest 3D detection benchmark of autonomous driving. Comparison with state-of-the-art methods. As shown in <ref type="table" target="#tab_2">Table 13</ref>, our proposed PV-RCNN-v2 framework outperforms previous state-of-the-art method <ref type="bibr" target="#b28">[24]</ref> significantly with +1.74% mAP LEVEL 1 gain and +1.79% mAP LEVEL 2 gain for the most important vehicle detection. <ref type="table" target="#tab_2">Table 13</ref> also demonstrates that our proposed PV-RCNN-v2 framework also consistently outperforms all previous methods in terms of pedestrian and cyclist detection, including very recent Pillar-based method <ref type="bibr" target="#b68">[64]</ref> and Part-A2-Net <ref type="bibr" target="#b28">[24]</ref>. Compared with our preliminary work PV-RCNN-v1, our  latest PV-RCNN-v2 framework achieves remarkably better mAP/mAPH on all difficulty levels for the detection of all three categories, while also increasing the processing speed from 3.3 FPS to 10 FPS for the 3D detection of 150m × 150m such a large area, which validates the effectiveness and efficiency of our proposed PV-RCNN-v2.</p><p>To better demonstrate the performance at different distance ranges, we also present the distance-based detection performance in <ref type="table" target="#tab_2">Table 14</ref>, <ref type="table" target="#tab_2">Table 15</ref>, <ref type="table" target="#tab_2">Table 16</ref> for vehicle, pedestrian, cyclist three categories respectively. We follow StarNet <ref type="bibr" target="#b92">[88]</ref> and MVF <ref type="bibr" target="#b64">[60]</ref> to evaluate the models on the LEVEL 1 difficulty for comparing with previous methods. We could see that our PV-RCNN-v2 achieves best performance on all distance ranges of vehicle detection and pedestrian detection, and on most distance ranges for cyclist 3D detection. It is worth noting that <ref type="table" target="#tab_2">Table 14</ref>, <ref type="table" target="#tab_2">Table 15</ref>, <ref type="table" target="#tab_2">Table 16</ref> show that our proposed PV-RCNN-v2 framework achieves much better performance than previous methods in terms of the furthest area (&gt; 50m), where PV-RCNN-v2 outperforms previous state-of-the-art method with a +3.32% 3D mAP gain for vehicle detection, a +3.67% 3D mAP for pedestrian detection and a +2.39% for cyclist detection. It may benefit from several aspects of our PV-RCNN-v2 framework, such as the designing of our two-step point-voxel interaction to extract richer contextual information in the furthest area, and our SPC keypoint sampling strategy to produce enough keypoints for these furthest proposals, and our VectorPool aggregation to preserve accurate spatial structures for the fine-grained proposal refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">3D detection on the KITTI dataset</head><p>To evaluate our PV-RCNN frameworks on the highlycompetitive 3D detection learderboard of KITTI dataset, we train our models with 80% of train + val data and the remaining 20% data is used for validation. All results are evaluated by submitting to the official evaluation server. Comparison with state-of-the-art methods. As shown in <ref type="table" target="#tab_2">Table 17</ref>, both of our PV-RCNN-v1 and PV-RCNN-v2 outperform all published methods with remarkable margins on the most important moderate difficulty level. Specifically, compared with previous LiDAR-only state-of-the-art methods on the 3D detection benchmark of car category, our PV-RCNN-v2 increases the mAP by +1.78%, +2.17%, +2.06% on easy, moderate and hard difficulty levels, respectively. For the 3D detection and bird-view detection of cyclist, our methods outperforms all previous methods with large margins on the moderate and hard difficulty level, where the maximum gain is +4.24% mAP on the moderate difficulty level of bird-view detection for cyclist.</p><p>Compared with our preliminary work PV-RCNN-v1, our PV-RCNN-v2 achieves better performance on the moderate and hard levels of 3D detection over car and cyclist categories, while also greatly reducing the GPU-memory consumption and increasing the running speed from 10 FPS to 16 FPS in the KITTI dataset. The significant improvements on both the performance and the efficiency manifest the effectiveness of our PV-RCNN-v2 framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we present two novel frameworks, named PV-RCNN-v1 and PV-RCNN-v2, for accurate 3D object detection from point clouds. Our PV-RCNN-v1 framework adopts a novel Voxel Set Abstraction module to deeply integrates both the multi-scale 3D voxel CNN features and the PointNet-based features to a small set of keypoints, and the learned discriminative keypoint features are then aggregated to the RoI-grid points through our proposed RoI-grid pooling module to capture much richer contextual information for proposal refinement. Our PV-RCNN-v2 further improves the PV-RCNN-v1 framework by efficiently generating more representative keypoints with our novel SPC keypoint sampling strategy, and also by equipping with our proposed VectorPool aggregation operation to learn structure-preserved local features in both the VSA module and RoI-grid pooling module. Thus, our PV-RCNN-v2 finally achieves better performance with much faster running speed than the v1 version.</p><p>Both of our proposed two PV-RCNN frameworks significantly outperform previous 3D detection methods and achieve new state-of-the-art performance on both the Waymo Open Dataset and the KITTI 3D detection benchmark, and extensive experiments are designed and conducted to deeply investigate the individual components of our proposed frameworks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The overall architecture of our proposed PV-RCNN-v1. The raw point clouds are first voxelized to feed into the 3D sparse convolution based encoder to learn multi-scale semantic features and generate 3D object proposals. Then the learned voxel-wise feature volumes at multiple neural layers are summarized into a small set of key points via the novel voxel set abstraction module. Finally the keypoint features are aggregated to the RoI-grid points to learn proposal specific features for fine-grained proposal refinement and confidence prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>l k ) j −p i to indicate the relative location of voxel feature f (l k ) j . The features within neighboring set S (l k ) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of RoI-grid pooling module. Rich context information of each 3D RoI is aggregated by the set abstraction operation with multiple receptive fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The overall architecture of our proposed PV-RCNN-v2. We propose the Sectorized-Proposal-Centric keypoint sampling module to concentrate keypoints to the neighborhoods of 3D proposals while also accelerating the process with sectorized FPS. Moreover, our proposed VectorPool module is utilized in both the voxel set abstraction module and the RoI-grid pooling module to improve the local feature aggregation and save memory/computation resources point sets of all proposals as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of Sectorized Proposal-Centric (SPC) keypoint sampling. It contains two steps, where the first proposal filter step concentrates the limited number of keypoints to the neighborhoods of proposals, and the following sectorized-FPS step divides the whole scene into several sectors for accelerating the keypoint sampling process while also keeping the keypoints uniformly distributed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Illustration of VectorPool aggregation for local feature aggregation from point clouds. The local space around a center point is divided into dense voxels, where the inside point-wise features of each voxel are averaged as the volume features. The features of each volume are encoded with position-specific kernels to generate position-sensitive features, that are sequentially concatenated to generate the local vector representation to explicitly encode the spatial structure information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2</head><label>2</label><figDesc>Effects of different components in our proposed PV-RCNN-v1 and PV-RCNN-v2 frameworks. All models are trained with 20% frames from the training set and are evaluated on the full validation set of the Waymo Open Dataset. Here SPC-FPS denotes our proposed Sectorized Proposal-Centric keypoint sampling strategy, VSA (SA) denotes the Voxel Set Abstraction module with commonly used set abstraction operation, VSA (VP) denotes the Voxel Set Abstraction module with our proposed VectorPool aggregation, and RoI-grid pool (SA) / (VP) are defined in the same way as the VSA counterparts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>i contain both 3D structure information and high level semantic features, which could improve the perofrmance a lot by combining with the bird-view semantic features f (bev) i and the raw point locations f (raw) i . (iii) The shallow semantic features f the performance and the best performance is achieved with all the feature components as the keypoint features. Effects of PKW module. We propose the predicted keypoint weighting (PKW) module in Sec. 4.1 to re-weight the pointwise features of keypoint with extra keypoint segmentation supervision. The experiments on both KITTI dataset (1 st and 4 th rows of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Recall of different proposal generation networks on the car class at moderate difficulty level of the KITTI val split set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, the 3D voxel CNN has four levels with feature dimensions 16, 32, 64, 64, respectively. Their two neighboring radii r k of each level in the VSA module are set as (0.4m, 0.8m), (0.8m, 1.2m), (1.2m, 2.4m), (2.4m, 4.8m), and the neighborhood radii of set abstraction for raw points are (0.4m, 0.8m). For the proposed RoI-grid pooling operation, we uniformly sample 6 × 6 × 6 grid points in each 3D proposal and the two neighboring radiir of each grid point are (0.8m, 1.6m).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>To better generate the set of restricted keypoints, we first restrict the keypoint candidates P to the neighboring</figDesc><table><row><cell>z</cell><cell>Raw Point Cloud y</cell><cell>Voxelization</cell><cell cols="3">3D Sparse Convolution Network</cell><cell>To BEV</cell><cell>RPN …</cell><cell>Classification</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Regression</cell></row><row><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Raw Points</cell><cell></cell><cell cols="3">Sparse Voxel Features</cell><cell></cell><cell>BEV Features</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Keypoints</cell><cell>Raw Points</cell><cell cols="2">Sparse Voxel Features …</cell><cell>BEV Features</cell></row><row><cell>Proposal Boxes</cell><cell cols="3">Sectorized Proposal-Centric Keypoint Sampling</cell><cell></cell><cell>…</cell><cell cols="2">Predicted Keypoint Weighting Voxel Set Abstraction Module …</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Confidence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Box Refinement</cell></row><row><cell></cell><cell cols="2">RoI-grid Pooling Module</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>VectorPool Module</cell><cell cols="2">Bilinear Feature Interpolation</cell><cell cols="2">Feature Channel Concatenation</cell><cell></cell><cell>… … Keypoint Features</cell><cell>RoI-grid Point</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>−75.2, 75.2]m for both the X and Y axes, and [−2, 4]m for the Z axis, while the voxel size is set as (0.1m, 0.1m, 0.15m). We train the PV-RCNN-v1 with batch size 64 for 50 epochs on 32 GPUs, while training the PV-RCNN-v2 with batch size 64 for 50 epochs on 16 GPUs since our v2 version consumes much less GPU memory. For the KITTI dataset, the detection range is set as [0, 70.4]m for X axis, [−40, 40]m for Y axis and [−3, 1]m for the Z axis, which is voxelized with voxel size (0.05m, 0.05m, 0.1m) in each axis. We train PV-RCNN-v1 with batch size 16 for 80 epochs, and train PV-RCNN-v2 with batch size 32 for 80 epochs on 8 GPUs.</figDesc><table><row><cell>PKW</cell><cell>RoI Pooling</cell><cell>Confidence Prediction</cell><cell cols="3">Easy Moderate Hard</cell></row><row><cell></cell><cell cols="3">RoI-grid Pooling IoU-guided scoring 92.09</cell><cell>82.95</cell><cell>81.93</cell></row><row><cell></cell><cell cols="3">RoI-aware Pooling IoU-guided scoring 92.54</cell><cell>82.97</cell><cell>80.30</cell></row><row><cell></cell><cell>RoI-grid Pooling</cell><cell>Classification</cell><cell>91.71</cell><cell>82.50</cell><cell>81.41</cell></row><row><cell></cell><cell cols="3">RoI-grid Pooling IoU-guided Scoring 92.57</cell><cell>84.83</cell><cell>82.69</cell></row></table><note>For the inference speed, our final PV-RCNN-v2 frame- work can achieve state-of-the-art performance with 10 FPS for 150m × 150m detection region on the Waymo Open</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 3</head><label>3</label><figDesc>Effects of different components in our PV-RCNN-v1 framework, and all models are trained on the KITTI training set and are evaluated on the car category (3D mAP) of KITTI validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4</head><label>4</label><figDesc>70.94 64.04 63.46 71.97 71.36 64.44 63.86 72.15 71.54 64.66 64.07 73.77 73.09 64.72 64.11 74.06 73.38 64.99 64.38 74.10 73.42 65.04 64.43</figDesc><table><row><cell>(bev) i</cell><cell>) or accurate point locations (f</cell><cell>(raw) i</cell><cell>), since neither 2D-</cell></row><row><cell cols="4">semantic-only nor point-only are enough for the proposal</cell></row></table><note>. We could summarize the observations as fol- lows: (i) The performance drops a lot if we only aggre- gate features from high level bird-view semantic features (f</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 4</head><label>4</label><figDesc>Effects of different feature components for VSA module. All experiments are based on our PV-RCNN-v1 framework.</figDesc><table><row><cell>Use PKW</cell><cell cols="2">LEVEL 1 (3D) mAP mAPH</cell><cell cols="2">LEVEL 2 (3D) mAP mAPH</cell></row><row><cell></cell><cell>73.90</cell><cell>73.29</cell><cell>64.75</cell><cell>64.23</cell></row><row><cell></cell><cell>74.06</cell><cell>73.38</cell><cell>64.99</cell><cell>64.38</cell></row><row><cell></cell><cell></cell><cell>TABLE 5</cell><cell></cell></row><row><cell cols="5">Effects of predicted keypoint weighting (PKW) module. All experiments</cell></row><row><cell cols="5">are based on our PV-RCNN-v1 framework.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 6</head><label>6</label><figDesc>Effects of RoI-grid pooling module. In these two groups of experiments, we only replace the RoI pooling modules while keeping other modules consistent. All experiments are based on our PV-RCNN-v1 framework.</figDesc><table><row><cell>Keypoint Sampling</cell><cell>Running</cell><cell>LEVEL 1 (3D) LEVEL 2 (3D)</cell></row><row><cell>Algorithm</cell><cell>Time</cell><cell>mAP mAPH mAP mAPH</cell></row><row><cell>FPS</cell><cell cols="2">133ms 74.06 73.38 64.99 64.38</cell></row><row><cell>PC-Filter + FPS</cell><cell cols="2">27ms 75.05 74.41 65.98 65.40</cell></row><row><cell>PC-Filter + Random Sampling</cell><cell cols="2">&lt;1ms 69.85 69.23 60.97 60.42</cell></row><row><cell>PC-Filter + Coverage Sampling</cell><cell cols="2">21ms 73.97 73.30 64.94 64.34</cell></row><row><cell>PC-Filter + Voxelized-FPS</cell><cell cols="2">17ms 74.12 73.46 65.06 64.47</cell></row><row><cell>PC-Filter + RandomParallel-FPS</cell><cell>2ms</cell><cell>73.94 73.28 64.90 64.31</cell></row><row><cell>PC-Filter + Sectorized-FPS</cell><cell>9ms</cell><cell>74.94 74.27 65.81 65.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 7</head><label>7</label><figDesc>Effects of keypoint sampling algorithm. The running time is the average running time of keypoint sampling process on the validation set of the Waymo Open Dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>× 3 73.97 73.28 64.92 64.29 2 × 2 × 2 74.68 74.04 65.62 65.04 3 × 3 × 3 75.37 74.73 66.29 65.72 4 × 4 × 4 74.70 74.04 65.60 65.01</figDesc><table><row><cell>Separate Local</cell><cell>Number of</cell><cell>LEVEL 1 (3D) LEVEL 2 (3D)</cell></row><row><cell>Kernel Weights</cell><cell>Dense Voxels</cell><cell>mAP mAPH mAP mAPH</cell></row><row><cell></cell><cell>3 × 3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 9</head><label>9</label><figDesc>The effects of separate local kernel weights and number of dense voxels in our proposed VectorPool aggregation in VSA module and RoI-grid pooling module. All experiments are based on our PV-RCNN-v2 framework.</figDesc><table><row><cell>Method of Relative</cell><cell cols="2">LEVEL 1 (3D)</cell><cell cols="2">LEVEL 2 (3D)</cell></row><row><cell>Position Encoding</cell><cell cols="2">mAP mAPH</cell><cell cols="2">mAP mAPH</cell></row><row><cell>PosPool (xyz) [78]</cell><cell>75.15</cell><cell>74.68</cell><cell>66.21</cell><cell>65.56</cell></row><row><cell>PosPool (cos/sin) [78]</cell><cell>75.13</cell><cell>74.65</cell><cell>66.19</cell><cell>65.53</cell></row><row><cell>concat</cell><cell>75.37</cell><cell>74.73</cell><cell>66.29</cell><cell>65.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 10 The</head><label>10</label><figDesc></figDesc><table><row><cell>effects of different operators for encoding the relative positions in</cell></row><row><cell>VectorPool aggregation. All experiments are based on our</cell></row><row><cell>PV-RCNN-v2 framework.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 ,</head><label>10</label><figDesc>we compare the simple concatenation</figDesc><table><row><cell>Number of Keypoints</cell><cell cols="2">LEVEL 1 (3D) mAP mAPH</cell><cell cols="2">LEVEL 2 (3D) mAP mAPH</cell></row><row><cell>8192</cell><cell>75.20</cell><cell>74.65</cell><cell>66.13</cell><cell>65.64</cell></row><row><cell>4096</cell><cell>75.37</cell><cell>74.73</cell><cell>66.29</cell><cell>65.72</cell></row><row><cell>2048</cell><cell>74.00</cell><cell>73.32</cell><cell>64.95</cell><cell>64.34</cell></row><row><cell>1024</cell><cell>71.39</cell><cell>70.76</cell><cell>63.95</cell><cell>63.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE 11 TABLE 12</head><label>1112</label><figDesc>Effects of the number of keypoints for encoding the global scene. All experiments are based on our PV-RCNN-v2 framework. Effects of the grid size in RoI-grid pooling module. All experiments are based on our PV-RCNN-v2 framework.</figDesc><table><row><cell>RoI-grid Size</cell><cell cols="2">LEVEL 1 (3D) mAP mAPH</cell><cell cols="2">LEVEL 2 (3D) mAP mAPH</cell></row><row><cell>9 × 9 × 9</cell><cell>75.05</cell><cell>74.42</cell><cell>65.75</cell><cell>65.21</cell></row><row><cell>8 × 8 × 8</cell><cell>75.10</cell><cell>74.56</cell><cell>65.80</cell><cell>65.23</cell></row><row><cell>7 × 7 × 7</cell><cell>75.37</cell><cell>74.73</cell><cell>66.29</cell><cell>65.72</cell></row><row><cell>6 × 6 × 6</cell><cell>74.46</cell><cell>73.80</cell><cell>65.40</cell><cell>64.81</cell></row><row><cell>5 × 5 × 5</cell><cell>74.16</cell><cell>73.48</cell><cell>65.11</cell><cell>64.49</cell></row><row><cell>4 × 4 × 4</cell><cell>73.84</cell><cell>73.19</cell><cell>64.81</cell><cell>64.23</cell></row><row><cell>3 × 3 × 3</cell><cell>71.70</cell><cell>71.07</cell><cell>64.24</cell><cell>63.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 13 78.79 93.05 77.70 57.38 91.85 97.98 91.21 82.56</head><label>13</label><figDesc>Performance comparison on the Waymo Open Dataset with 202 validation sequences. * : re-implemented by<ref type="bibr" target="#b64">[60]</ref>. †: re-implemented by ourselves with their open source code. ‡: only the first return of the Waymo LiDAR sensor is used for training and testing.<ref type="bibr" target="#b88">84</ref>.90 53.11 23.92 71.57 92.94 74.92 48.87 † SECOND [16] 72.26 90.66 70.03 47.55 89.18 96.84 88.39 78.37 PointPillar [15] 56.62 81.01 51.75 27.94 75.57 92.10 74.06 55.47 MVF [60] 62.93 86.30 60.02 36.02 80.40 93.59 79.21 63.09 Pillar-based [64] 69.80 88.53 66.50 42.93 87.11 95.78 84.74 72.12 † Part-A2-Net [24] 77.05 92.35 75.91 54.06 90.81 97.52 90.35 80.12 PV-RCNN-v1 77.51 92.44 76.11 55.55 91.39 97.62 90.72 81.84 PV-RCNN-v2 Improvement +1.74 +0.70 +1.79 +3.32 +1.04 +0.46 +0.86 +2.44</figDesc><table><row><cell>Method</cell><cell>Vehicle 3D mAP (IoU=0.7) Overall 0-30m 30-50m 50m-Inf Overall 0-30m 30-50m 50m-Inf Vehicle mAP (IoU=0.7)</cell></row><row><cell>LaserNet [89]</cell><cell>55.10</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE 14</head><label>14</label><figDesc>Performance comparison on the Waymo Open Dataset with 202 validation sequences for the vehicle detection.</figDesc><table /><note>* : re-implemented by [60].†: re-implemented by ourselves with their open source code.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>TABLE 15 71.00 81.96 66.38 48</head><label>15</label><figDesc>Performance comparison on the Waymo Open Dataset with 202 validation sequences for the pedestrian detection. * : re-implemented by [60]. †: re-implemented by ourselves with their open source code. SECOND [16] 60.61 73.33 55.51 41.98 63.55 74.58 59.31 46.75 † Part-A2-Net [24] 68.60 80.87 62.57 45.04 .15 PV-RCNN-v1 67.81 79.54 62.25 46.19 69.85 80.53 64.91 49.81 PV-RCNN-v2 68.98 80.76 63.10 47.40 70.74 81.40 65.31 50.64 Improvement +0.38 -0.11 +0.53 +2.39 -0.26 -0.56 -1.07 +2.49</figDesc><table><row><cell>Method</cell><cell>Cyclist 3D mAP (IoU=0.7) Overall 0-30m 30-50m 50m-Inf Overall 0-30m 30-50m 50m-Inf Cyclist BEV mAP (IoU=0.7)</cell></row><row><cell></cell><cell></cell></row></table><note>†</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>TABLE 16</head><label>16</label><figDesc>Performance comparison on the Waymo Open Dataset with 202 validation sequences for the cyclist detection. †: re-implemented by ourselves with their open source code.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>33 60.04 84.60 71.86 63.84</head><label></label><figDesc>LiDAR 83.68 68.78 61.67 94.07 85.35 75.88 LiDAR 83.07 71.76 65.73 90.99 84.82 79.62 63.76 50.55 44.93 69.39 57.12 51.09 F-PointNet [20] CVPR 2018 RGB + LiDAR 82.19 69.79 60.59 91.17 84.67 74.77 72.27 56.12 49.01 77.26 61.37 53.78 UberATG-MMF [17] CVPR 2019 RGB + LiDAR 88.40 77.43 70.22 93.67 88.21 81.99 LiDAR 89.20 80.05 73.11 93.52 89.56 82.45 LiDAR 88.94 80.67 77.15 93.05 89.80 86.57 72.55 65.82 89.39 83.77 78.59 71.33 52.08 45.83 76.50 56.05 49.45 PointPillars [15] CVPR 2019 LiDAR only 82.58 74.31 68.99 90.07 86.56 82.81 77.10 58.65 51.92 79.90 62.73 55.58 PointRCNN [21] CVPR 2019 LiDAR only 86.96 75.64 70.70 92.13 87.39 82.72 74.96 58.82 52.53 82.56 67.24 60.28 79.71 75.09 94.74 89.19 86.42 78.69 61.59 55.30 81.36 67.23 59.35 79.57 74.55 92.66 89.02 85.86 82.48 64.10 56.90 85.04 67.62 61.14 Point-GNN [92] CVPR 2020 LiDAR only 88.33 79.47 72.29 93.11 89.17 83.90 78.60 63.48 57.08 81.17 67.28 59.67 PV-RCNN-v1 (Ours) -LiDAR only 90.25 81.43 76.82 94.98 90.65 86.14 78.60 63.71 57.65 82.49 68.89 62.41 PV-RCNN-v2 (Ours) -LiDAR only 90.14 81.88 77.15 92.66 88.74 85.97 82.22 67.</figDesc><table><row><cell>Method</cell><cell>Reference</cell><cell>Modality</cell><cell cols="7">Car -3D Detection Easy Mod. Hard Easy Mod. Hard Easy Mod. Hard Car -BEV Detection Cyclist -3D Detection Cyclist -BEV Detection Easy Mod. Hard</cell></row><row><cell>MV3D [9]</cell><cell>CVPR 2017</cell><cell>RGB + LiDAR</cell><cell>74.97 63.63 54.00 86.62 78.93 69.80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ContFuse [13]</cell><cell>ECCV 2018</cell><cell cols="3">RGB + -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AVOD-FPN [11]</cell><cell>IROS 2018</cell><cell cols="3">RGB + -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3D-CVF at SPA [57]</cell><cell>ECCV 2020</cell><cell cols="3">RGB + -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLOCs [90]</cell><cell>IROS 2020</cell><cell cols="3">RGB + -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">SECOND [16] 83.34 3D IoU Loss [91] Sensors 2018 LiDAR only 3DV 2019 LiDAR only 86.16 76.50 71.39 91.36 86.22 81.20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">STD [23] 87.95 Part-A2-Net [24] ICCV 2019 LiDAR only TPAMI 2020 LiDAR only 87.81 78.49 73.51 91.70 87.79 84.61</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3DSSD [67]</cell><cell>CVPR 2020</cell><cell>LiDAR only</cell><cell>88.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>TABLE 17</head><label>17</label><figDesc>Performance comparison on the KITTI test set. The results are evaluated by the mean Average Precision with 40 recall positions by summiting to the official KITTI evaluation server.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">† ‡</forename><surname>Part</surname></persName>
		</author>
		<idno>A2-Net [24] TPAMI 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename><surname>Pv-Rcnn-V1</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Ours</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename><surname>Pv-Rcnn-V2</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Ours</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Method Pedestrian 3D mAP (IoU=0.7) Pedestrian BEV mAP (IoU=0.7) Overall 0-30m 30-50m 50m-Inf Overall 0-30m 30-50m 50m-Inf LaserNet</title>
		<idno>89] 63.40 73.47 61.55 42.69 70.01 78.24 69.47</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Part</surname></persName>
		</author>
		<idno>A2-Net [24] 75.24 81.87 73.65 62.34 80.25 84.49 79.22 70.34</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hdnet: Exploiting hd maps for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">STD: sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ICCV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and partaggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Openpcdet: An open-source toolbox for 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenPCDet" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gradient harmonized single-stage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchor-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Are cars just 3d boxes?-jointly estimating the 3d shape of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A coarse-to-fine model for 3d pose estimation and sub-category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Reconstructing vehicles from a single image: Shape priors for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">End-to-end pseudo-lidar for image-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06310</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pointpainting: Sequential fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">3d-cvf: Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ECCV</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Epnet: Enhancing point features with image semantics for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Hvnet: Hybrid voxel network for lidar based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Voxel-fpn: multi-scale voxel feature aggregation in 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.05286" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Classbalanced grouping and sampling for point cloud 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1908.09492</idno>
		<ptr target="http://arxiv.org/abs/1908.09492" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Pillar-based object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10323</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Object as hotspots: An anchor-free 3d object detection approach via firing of hotspots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">3dssd: Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Multi-view pointnet for 3d scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">441</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01294</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Grid-gcn for fast and scalable point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno>abs/1706.01307</idno>
		<ptr target="http://arxiv.org/abs/1706.01307" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Structure aware single-stage 3d object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Fishnet: A versatile backbone for image, region, and pixel level prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="754" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
				<ptr target="http://www.cvlibs.net/datasets/kitti/evalobject.php?objbenchmark=3d" />
		<title level="m">KITTI leader board of 3D object detection benchmark</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Starnet: Targeted computation for object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11069</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Lasernet: An efficient probabilistic 3d object detector for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Clocs: Camera-lidar object candidates fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Radha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Iou loss for 2d/3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
