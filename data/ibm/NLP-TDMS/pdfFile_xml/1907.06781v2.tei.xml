<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<title level="a" type="main">Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks</title>
					</analytic>
					<monogr>
						<title level="j" type="main">TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</title>
						<imprint>
							<biblScope unit="volume">X</biblScope>
							<biblScope unit="page">1</biblScope>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Benchmark</term>
					<term>SIP Dataset</term>
					<term>Salient Object Detec- tion</term>
					<term>Saliency</term>
					<term>RGB-D</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The use of RGB-D information for salient object detection has been extensively explored in recent years. However, relatively few efforts have been put towards modelling salient object detection in real-world human activity scenes with RGB-D. In this work, we fill the gap by making the following contributions to RGB-D salient object detection. (1) We carefully collect a new SIP (salient person) dataset, which consists of ∼1K high-resolution images that cover diverse real-world scenes from various viewpoints, poses, occlusions, illuminations, and backgrounds. <ref type="formula">(2)</ref> We conduct a large-scale (and, so far, the most comprehensive) benchmark comparing contemporary methods, which has long been missing in the field and can serve as a baseline for future research. We systematically summarize 32 popular models, and evaluate 18 parts of 32 models on seven datasets containing a total of about 97K images. <ref type="formula">(3)</ref> We propose a simple general architecture, called Deep Depth-Depurator Network (D 3 Net). It consists of a depth depurator unit (DDU) and a three-stream feature learning module (FLM), which performs low-quality depth map filtering and cross-modal feature learning respectively. These components form a nested structure and are elaborately designed to be learned jointly. D 3 Net exceeds the performance of any prior contenders across all five metrics under consideration, thus serving as a strong model to advance research in this field. We also demonstrate that D 3 Net can be used to efficiently extract salient object masks from real scenes, enabling effective background changing application with a speed of 65fps on a single GPU. All the saliency maps, our new SIP dataset, the D 3 Net model, and the evaluation tools are publicly available at https://github.com/DengPingFan/D3NetBenchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H OW to take high-quality photos has become one of the most important competition points among mobile phone manufacturers. Salient object detection (SOD) methods <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b17">[18]</ref> have been incorporated into mobile phones and been widely used for creating perfect portraits by automatically adding large aperture and other enhancement effects. While existing SOD methods <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b34">[35]</ref> have achieved remarkable success, most of them only rely on RGB images and ignore the important depth information, which is widely available in modern smartphones (e.g., iPhone X, Huawei Mate10, and Samsung Galaxy S10). Thus, fully utilizing RGB-D information for SOD detection has recently attracted significant research attention <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b50">[51]</ref>.</p><p>One of the primary goals of existing smartphone cameras is to identify humans in visual scenes, through either coarse, D.-P. <ref type="bibr">Fan</ref>  . Left to right: input image, ground truth, and the corresponding depth map. The quality of the depth map from low (1 st row), mid (2 nd row) to high (last row). As shown in the 2 nd row, it is difficult to recognize the boundary of the human's arm in the boundary box region. However, it is clearly visible in the depth map. The highquality depth maps benefit the RGB-D based salient object detection task. These three examples are from the NJU2K <ref type="bibr" target="#b36">[37]</ref>, our SIP and NLPR <ref type="bibr" target="#b38">[39]</ref> datasets respectively.</p><p>bounding-box-level, or instance-level; segmentation. To this end, intelligence solutions, such as RGB-D saliency detecting techniques have gained considerable attention. However, most existing RGB-D based SOD methods are tested on RGB-D images taken by Kinect <ref type="bibr" target="#b51">[52]</ref> or a light field camera <ref type="bibr" target="#b52">[53]</ref>, or estimated by optical flow <ref type="bibr" target="#b53">[54]</ref>, which have different characteristics from actual smartphone cameras. Since humans are the key subjects of photographs taken with smartphones, a human-oriented RGB-D dataset featuring realistic, in-the-wild images would be more useful for mobile manufacturers. Despite the effort of some authors <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref> to augment their scenes with additional objects, a human-centered RGB-D dataset for salient object detection does not yet exist.</p><p>Furthermore, although depth maps provide important complementary information for identifying salient objects, the lowquality versions often cause wrong detections <ref type="bibr" target="#b54">[55]</ref>. While existing RGB-D based SOD models typically fuse RGB and depth features by different strategies <ref type="bibr" target="#b50">[51]</ref>. There is no model that explicitly/automatically discard the low-quality depth map in the RGB-D SOD field. We believe such models have a high potential for driving this field forward.</p><p>In addition to the limitations of current RGB-D datasets and models already mentioned, most RGB-D studies also suffer from several other common constraints, including:</p><p>Sufficiency. Only a limited number of datasets (1∼4) have been benchmarked in recent papers <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b55">[56]</ref>  <ref type="table" target="#tab_2">(Table II)</ref>  <ref type="bibr">Figure 2</ref>. Representative subsets in our SIP. The images in SIP are grouped into eight subsets according to background objects (i.e., grass, car, barrier, road, sign, tree, flower, and other), different lighting conditions (i.e., low-light, sunny with clear object boundary) and various number of objects (i.e., 1, 2, ≥3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB RGB Gray Gray Depth</head><p>Depth Object-level Instance-level Instance-level Object-level <ref type="bibr">Figure 3</ref>. Examples of images, depth maps and annotations (i.e., object-level, instance-level) in our SIP dataset with different numbers of salient objects, object sizes, object positions, scene complexities, and lighting conditions. Note that the "RGB" and "Gray" images are captured by two different monocular cameras from short distances. Thus, the "Gray" images are slightly different from the grayscale images obtained from colorful (RGB) image. Our SIP dataset provides a new direction such as depth estimating from "RGB" and "Gray" images, and instance-level RGB-D salient object detection. generalizability of models cannot be properly accessed with such a small number of datasets.</p><p>Completeness. F-measure <ref type="bibr" target="#b56">[57]</ref>, MAE, and PR (precision &amp; recall) Curve are the three most widely-used metrics in existing works. However, as suggested by <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, these metrics essentially act at a pixel-level. It is thus difficult to draw thorough and reliable conclusions from quantitative evaluations <ref type="bibr" target="#b59">[60]</ref>.</p><p>Fairness. Some works <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b60">[61]</ref> use the same Fmeasure metric, but do not explicitly describe which statistic (e.g., mean or max) was used, easily resulting in unfair comparison and inconsistent performance. Meanwhile, the different threshold strategies for F-measure (e.g., 255 varied thresholds <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, adaptive saliency threshold <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>, and self-adaptive threshold <ref type="bibr" target="#b42">[43]</ref>) will result in different performance. It is thus of crucial need to provide a fair comparison of RGB-D based SOD models by extensively evaluating them with same metrics on a standard leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contribution</head><p>To address the above-mentioned problems, we provide three distinct contributions.</p><p>(1) We have built a new Salient Person (SIP) dataset (see <ref type="figure">Fig. 2, Fig. 3</ref>). It consists of 929 accurately annotated highresolution images which are designed to contain multiple salient persons per image. It is worth mentioning that the depth maps are captured by a real smartphone. We believe such a dataset is highly valuable and will facilitate the application of RGB-D models to mobile devices. Besides, the dataset is carefully designed to cover diverse scenes, various challenging situations (e.g., occlusion, appearance change), and elaborately annotated with pixel-level ground truths (GT). Another discriminative feature of our SIP dataset is the availability of both RGB and grayscale images captured by a binocular camera, which can benefit a broad number of research directions, such as, stereo matching, depth estimation, human-centered detection, etc.</p><p>(2) With the proposed SIP and six existing RGB-D datasets <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref>, we provide a more comprehensive comparison of 32 classical RGB-D salient object detection models and present the large-scale (∼97K images) fair evaluation of 18 state-of-the-art (SOTA) algorithms <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b66">[67]</ref>- <ref type="bibr" target="#b68">[69]</ref>, making our study a good all-around RGB-D benchmark. To further promote the development of this field, we additionally provide an online evaluation platform with the preserved test set.</p><p>(3) We propose a simple general model called Deep Depth-Depurator Network (D 3 Net), which learns to automatically discard low-quality depth maps using a novel depth depurator unit (DDU). Thanks to the gate connection mechanism, our D 3 Net can predict salient objects accurately. Extensive experiments demonstrate that our D 3 Net remarkably outperforms prior work on many challenging datasets. Such a general framework design helps to learn cross-modality features from RGB images and depth maps.</p><p>Our contributions offer a systematic benchmark equipped with the basic tools for comprehensive assessment of RGB-D models, offering deep insight into the task of RGB-D based modelling and encouraging future research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Organization</head><p>In § II, we first review current datasets for RGB-D salient object detection, as well as representative models for this task. Then, we present details on the proposed salient person dataset SIP in § III. In § IV, we describe our D 3 Net model for RGB-D salient object detection by explicitly filtering out the low-quality depth maps.</p><p>In § V, we provide both a quantitative and qualitative experimental analysis of the proposed algorithm. Specifically, in § V-A, we offer more details on our experimental settings, including the benchmarked models, datasets and runtime. In § V-B, five evaluation metrics (E-measure <ref type="bibr" target="#b58">[59]</ref>, S-measure <ref type="bibr" target="#b57">[58]</ref>, MAE, PR Curve, and F-measure <ref type="bibr" target="#b56">[57]</ref>) are described in detail. In § V-C, we provide the mean statistics over different datasets and summarize them in <ref type="table" target="#tab_2">Table IV</ref>. comparison results of 18 SOTA RGB-D based SOD models over seven datasets, namely STERE <ref type="bibr" target="#b62">[63]</ref>, LFSD <ref type="bibr" target="#b63">[64]</ref>, DES <ref type="bibr" target="#b37">[38]</ref>, NLPR <ref type="bibr" target="#b38">[39]</ref>, NJU2K <ref type="bibr" target="#b36">[37]</ref>, SSD <ref type="bibr" target="#b65">[66]</ref>, and SIP (Ours) clearly demonstrate the robustness and efficiency of our D 3 Net model. Further, in § V-D, we provide a performance comparison between traditional and deep models. We also discuss the experimental results in more depth. In § V-E, we provide visualizations of the results and present saliency maps generated for various challenging scenes. In § VI, we discuss some potential applications about human activities and provide an interesting and realistic use scenario of D 3 Net in a background changing application. To better understand the contributions of DDU in the proposed D 3 Net, in § VII, we present the upper and lower bound of the DDU. All in all, the extensive experimental results clearly demonstrate that our D 3 Net model exceeds the performance of any prior competitors across five different metrics. In § VII-B, we discuss the limitations of this work. Finally, § VIII concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RGB-D Datasets</head><p>Over the past few years, several RGB-D datasets have been constructed for SOD. Some statistics of these datasets are shown in <ref type="table" target="#tab_2">Table I</ref>. Specifically, the STERE [63] dataset was the first collection of stereoscopic photos in this field. GIT <ref type="bibr" target="#b35">[36]</ref>, LFSD <ref type="bibr" target="#b63">[64]</ref> and DES <ref type="bibr" target="#b63">[64]</ref> are three small-sized datasets. GIT and LFSD were designed with specific purposes in mind, e.g., saliency-based segmentation of generic objects, and saliency detection on the light field. DES has 135 indoor images captured by Microsoft Kinect <ref type="bibr" target="#b51">[52]</ref>. Although these datasets have advanced the field to various degrees, they are severely restricted by their small scale or low resolution. To overcome these barriers, Peng et al. created NLPR <ref type="bibr" target="#b38">[39]</ref>, a large-scale RGB-D dataset with a resolution of 640×480. Later, Ju et al. built NJU2K <ref type="bibr" target="#b36">[37]</ref>, which has become one of the most popular RGB-D datasets. The recent SSD <ref type="bibr" target="#b65">[66]</ref> dataset partially remedied the resolution restriction of NLPR and NJU2K. However, it only contains 80 images. Despite the progress made by existing RGB-D datasets, they still suffer from the common limitation of not capturing depth maps in the real smartphones, making them unsuitable for reflecting real environmental conditions (e.g., lighting or distance to object).</p><p>Compared to previous datasets, the proposed SIP dataset has three fundamental differences:</p><p>• It includes 929 images with many challenging situations <ref type="bibr" target="#b82">[83]</ref> (e.g., dark background, occlusion, appearance change, and out-of-view) from various outdoor scenarios. • The RGB, grayscale images, and estimated depth maps are captured by a smartphone with a dual-camera. Due to the predominant application of SOD to human subjects on mobile phones, we also focus on this and thus and thus, for the first time, emphasize the salient persons in the real-world scenes. • A detailed quantitative analysis is presented for the quality of the dataset (e.g., center bias, object size distribution, etc.), which was not carefully investigated in previous RGB-D based studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RGB-D Models</head><p>Traditional models rely heavily on hand-crafted features (e.g., contrast <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b74">[75]</ref>, shape <ref type="bibr" target="#b35">[36]</ref>). By embedding the classical principles (e.g., spatial bias <ref type="bibr" target="#b37">[38]</ref>, center-dark channel <ref type="bibr" target="#b45">[46]</ref>, 3D <ref type="bibr" target="#b76">[77]</ref>, background <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b46">[47]</ref>), difference of Gaussian <ref type="bibr" target="#b36">[37]</ref>, region classification <ref type="bibr" target="#b61">[62]</ref>, SVM <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b72">[73]</ref>, graph knowledge <ref type="bibr" target="#b54">[55]</ref>, cellular automata <ref type="bibr" target="#b41">[42]</ref>, and Markov random field <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b74">[75]</ref>, these models show that specific handcrafted features can lead to decent performance. Several studies have also explored methods of integrating RGB and depth features via various combination strategies, using, for instance, angular densities <ref type="bibr" target="#b40">[41]</ref>, random forest regressors <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b61">[62]</ref>, and minimum barrier distances <ref type="bibr" target="#b76">[77]</ref>. More details are shown in <ref type="table" target="#tab_2">Table II</ref>.</p><p>To overcome the limited expression ability of hand-crafted features, recent works <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b79">[80]</ref>- <ref type="bibr" target="#b81">[82]</ref> have proposed to introduce CNNs to infer salient objects from RGB-D data. BED <ref type="bibr" target="#b75">[76]</ref> and DF <ref type="bibr" target="#b43">[44]</ref> are two pioneering works for this, which introduced deep learning technology into the RGB-D based SOD task. More recently, Huang et al. developed a more efficient end-to-end model <ref type="bibr" target="#b77">[78]</ref> with a modified loss function. To address the shortage of training data, Zhu et al. <ref type="bibr" target="#b47">[48]</ref> presented a robust prior model with a guided depth-enhancement module for SOD. In addition, Chen et al. developed a series of novel approaches for this field, such as hidden structure transfer <ref type="bibr" target="#b42">[43]</ref>, a complementarity fusion module <ref type="bibr" target="#b48">[49]</ref>, an attention-aware component <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b81">[82]</ref>, and dilated convolutions <ref type="bibr" target="#b80">[81]</ref>. Nevertheless, these works, to the best of our knowledge, are dedicated to extracting general depth features/information.</p><p>We argue that not all information in a depth map is informative for SOD, and low-quality depth maps often introduce significant noise (1 st row in <ref type="figure" target="#fig_0">Fig. 1</ref>). Thus, we instead design a simple general framework D 3 Net, which is equipped with a depth-depurator unit to explicitly exclude low-quality depth maps when learning complementary feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED DATASET A. Dataset Overview</head><p>We introduce SIP, the first human activities oriented salient person detection dataset. Our dataset contains 929 RGB-D images belonging to eight different background scenes, under two different objecy boundary conditions, which portray multiple actors. Each of them wears different clothes in different images. Following <ref type="bibr" target="#b82">[83]</ref>, the images are carefully selected to cover diverse challenging cases (e.g., appearance change, occlusion, and shape complexity). Examples can be found in <ref type="figure">Fig. 2</ref> and <ref type="figure">Fig. 3</ref>. The overall dataset can be downloaded from our website http://dpfan.net/SIPDataset/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sensors and Data Acquisition</head><p>Image Collection: We used a Huawei Mate 10 to collect our images. The Mate 10's rear cameras feature high-grade Leica SUMMILUX-H lenses with bright f/1.6 apertures and combine 12MP RGB and 20MP Monochrome (grayscale) sensors. The depth map is automatically estimated by the Mate10. We asked nine people, all dressed in different colors, to perform specific actions in real-world daily scenes. Instructions on how to perform the action to cover different challenging situations (e.g., occlusion, out-of-view) were given, but no instructions on style, angle, or speed were provided, in order to record realistic data.</p><p>Data Annotation: After capturing 5,269 images and the corresponding depth maps, we first manually selected about 2,500 images, each of which included one or multiple salient people. Following many famous SOD datasets <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b83">[84]</ref>- <ref type="bibr" target="#b89">[90]</ref>, six viewers were further instructed to draw the bounding boxes (bboxes) around the most attention-grabbing person, according to their first instinct. We adopted the voting scheme described in <ref type="bibr" target="#b38">[39]</ref> to discard images with low voting consistency and chose top 1,000 most satisfactory images. Another five annotators were then introduced to label accurate silhouettes of the salient objects according to the bboxes. We discard some images with low-quality annotations and finally obtained the 929 images with high-quality ground-truth annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dataset Statistics</head><p>Center Bias: Center bias has been identified as one of the most significant biases of saliency detection datasets <ref type="bibr" target="#b90">[91]</ref>. It occurs because subjects tend to look at the center of a screen <ref type="bibr" target="#b91">[92]</ref>. As noted in <ref type="bibr" target="#b82">[83]</ref>, simply overlapping all of the maps in the dataset cannot well describe the degree of center bias.</p><p>Following <ref type="bibr" target="#b82">[83]</ref>, we present the statistics of two distance R o and R m in <ref type="figure">Fig. 4</ref> (a &amp; b), where R o and R m indicate how far an object center and margin (farthest) point in an object are from the image center, respectively. The center biases of our SIP and existing <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref> datasets are shown in <ref type="figure">Fig. 4 (a &amp; b)</ref>. Except for our SIP and two small-scale datasets (GIT and SSD), most datasets present a high degree of center bias, i.e. the center of the object is close to the image center.</p><p>Size of Objects: We define object size as the ratio of salient object pixels to the total number of pixels in the image. The <ref type="table" target="#tab_2">Table II  COMPARISON</ref>     distribution ( <ref type="figure">Fig. 4 (c)</ref>) of normalized object size in SIP are 0.48%∼66.85% (avg.: 20.43%).</p><p>Background Objects: As summarized in <ref type="table" target="#tab_2">Table III</ref>, SIP includes diverse background objects (e.g., cars, trees, and grass).</p><p>Models tested on such a dataset would likely be able to handle realistic scenes better and thus be more practical.</p><p>Object boundary Conditions: In   <ref type="figure">Figure 5</ref>. Illustration of the proposed D 3 Net. In the training stage (Left), the input RGB and depth images are processed with three parallel sub-networks, e.g., RgbNet, RgbdNet, and DepthNet. The three sub-networks are based on a same modified structure of Feature Pyramid Networks (FPN) (see § IV-A for details). We introduced these sub-networks to obtain three saliency maps (i.e., S rgb , S rgbd , and S depth ) which considered both coarse and fine details of the input. In the test phase (Right), a novel depth depurator unit (DDU) ( § IV-B) is utilized for the first time in this work to explicitly discard (i.e., S rgbd ) or keep (i.e., S rgbd ) the saliency map introduced by the depth map. In the training/test phase, these components form a nested structure and are elaborately designed (e.g., gate connection in DDU) to automatically learn the salient object from the RGB image and Depth image jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>our SIP dataset. One example of a dark condition , which often occurs in daily scenes, can be found in <ref type="figure">Fig. 3</ref>. The depth maps obtained in low-light conditions inevitably introduce more challenges for detecting salient objects.</p><p>Number of Salient Object: From <ref type="table" target="#tab_2">Table I</ref>, we note that existing datasets fall short in their numbers of salient objects (e.g., they often only have one). Previous studies <ref type="bibr" target="#b92">[93]</ref>, however, have shown that humans can accurately enumerate up to at least five objects without counting. Thus, our SIP is designed to contain up to five salient objects per-image. The statistics of labelled objects in each image are shown in <ref type="table" target="#tab_2">Table III (#  Object).</ref> IV. PROPOSED MODEL According to motivation described in <ref type="figure" target="#fig_0">Fig. 1</ref>, cross-modality feature extraction and depth filter unit are highly desired; therefore we proposed the simple general D 3 Net model (illustrated in <ref type="figure">Fig. 5</ref>) which contains two components, e.g., a three-stream feature learning module ( § IV-A) and a depth depurator unit ( § IV-B). The FLM (feature learning module) is utilized to extract the features from different modality. While the DDU (depth depurator unit) is acting as a gate to explicitly filter out the low-quality depth maps. If DDU decides to filter out this depth map, the data flow will pass along with the RgbNet. These components form a nested structure, and are elaborately designed to achieve robust performance and high generalization ability on various challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Learning Module</head><p>Most existing models <ref type="bibr" target="#b93">[94]</ref>- <ref type="bibr" target="#b95">[96]</ref> have shown significant improvement for object detectors in several applications. These models typically share a common structure of Feature Pyramid Networks (FPN) <ref type="bibr" target="#b96">[97]</ref>. Based on this motivation, we decide to introduce this component like FPN in our D 3 Net baseline to efficiently extract the features in a pyramid manner. The entire D 3 Net model is divided into the training phase and test phase due to the DDU has opted to use only in test phase.</p><p>As shown in <ref type="figure">Fig. 5</ref>, the designed FLM appears in training and test phases. The FLM consists of three sub-networks, i.e., RgbNet, RgbdNet, and DepthNet. Note that the three subnetworks have the same structure while fed with different input channel. Specifically, each sub-network receives a re-scaled image I ∈ {I rgb , I rgbd , I depth } with 224×224 resolution. The goal of FLM is to obtain the corresponding predicted map S ∈ {S rgb , S rgbd , S depth }.</p><p>As in <ref type="bibr" target="#b96">[97]</ref>, we also use bottom-up, top-down pathway, and lateral connections to extract the features. Then the outputs will be proportionally organized at multiple levels. The FPN is independent of the backbone, thus for simplicity, we adopt the VGG-16 <ref type="bibr" target="#b97">[98]</ref> architecture as our basic convolutional network to extract spatial features, while utilizing more powerful backbone <ref type="bibr" target="#b98">[99]</ref> feature extractor could be explored in future. Some studies like <ref type="bibr" target="#b99">[100]</ref> have shown that deeper layers retain more semantic information for locating objects. Based on this observation, we introduce a layer containing two 3×3 convolution kernels on the basis of the 5 layers VGG-16 structure to achieve this goal.</p><p>As shown in <ref type="figure">Fig. 6</ref>, our top-down features are built. For a specific layer (e.g., coarser layer), we first conduct a 2 × upsampling using nearest neighbor operation. Then, the upsampled feature is concatenated with the finer feature map to obtain rich features. Before concatenated with coarse map, the finer map undergoes a 1×1 Conv operation to reduce the channel. For example, let I rgbd ∈ R W×H×4 denotes the fourdimensional feature tensor of the input of RgbdNet. Then we define a set of anchors on different layers so that we can obtain a set of pyramid feature tensors with   <ref type="figure">Figure 6</ref>. The Feature Pyramid Network (FPN) is introduced to extract the context-aware information. Different from <ref type="bibr" target="#b96">[97]</ref>, we further add the sixth layer on the base of VGG-16 and the information merge strategy is concatenation rather than addition. More details can be found in § IV-A. </p><formula xml:id="formula_0">C i ×W i × H i , i.e.,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth Depurator Unit (DDU)</head><p>In the test phase, we further adopt a new gate connection strategy to obtain the optimal predicted map. Low-quality depth maps introduce more noise than informative cues to the prediction. The goal of gate connection is to classify depth maps into reasonable and low-quality ones and not use the poor ones in the pipeline.</p><p>As illustrated in <ref type="figure" target="#fig_5">Fig. 7 (b)</ref>, a stand-alone salient object in a high-quality depth map is typically characterized by welldefined closed boundaries and shows clear double peaks in its depth distribution. The statistics of the depth maps in existing datasets <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref> also support the fact that "high quality depth maps usually contain clear objects, while the elements in low-quality depth maps are cluttered (2 nd row in <ref type="figure" target="#fig_5">Fig. 7</ref>)". In order to reject the low-quality depth maps, we propose DDU as follows:</p><p>More specifically, in the test phase, the RGB and depth map is firstly re-sized to a fixed size (e.g., same as the training phase 224×224) to reduce the computational complexity. As shown in <ref type="figure">Fig. 5 (right)</ref>, the DDU is implemented with a gate connection. Denote the input images with three predicted maps S ∈ {S rgb , S rgbd , S depth }, then the goal of DDU is to decide which predicted map P ∈ [0, 1] W ×H is optimal.</p><formula xml:id="formula_1">P = F ddu ({S rgb , S rgbd , S depth }).<label>(1)</label></formula><p>Intuitively, there are two ways to achieve this goal, e.g., postprocessing and pre-processing. We propose a simple but general post-processing scheme for DDU. The DDU is considered in the test phase rather than in the training phase. Specially, a comparison unit F cu is leveraged to assess the similarity between the S depth and S rgbd generated from DepthNet and RgbdNet, respectively. where the δ (·) represents distance function, and t indicates a fixed threshold. Note that the comparison unit F cu is act as an index to decide which sub-network (RgbNet or RgbdNet) should be utilized.</p><formula xml:id="formula_2">F cu = 1, δ (S rgbd , S depth ) ≤ t 0, otherwise,<label>(2)</label></formula><p>The key of our comparison unit is the DDU. We utilize the comparison unit F cu as a gate connection to decide the final/optimal predicted map P. Thus, our F ddu module can be formulated as:</p><formula xml:id="formula_3">P = F cu · S rgbd +F cu · S rgb ,<label>(3)</label></formula><p>whereF cu = 1 − F cu . The F cu can be viewed as a fixed weight. A more elegant formulation (adaptive weight) would be a part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DDU.</head><p>The key component of our D <ref type="bibr" target="#b2">3</ref> Net is the DDU. In this work, we show a simple yet powerful distance function formulated in (Eq. 2). We leverage the mean absolute error (MAE) metric (same as (Eq. 5)) to assess the distance between two maps. The basic motivation is that if the high-quality depth contains clear objects the DepthNet will easily detect these objects in S depth (see first row in <ref type="figure" target="#fig_5">Fig. 7)</ref>. The higher the quality of depth map in I depth , the more similarity between the S rgbd and the S depth . In other words, the predicted map S rgbd from RgbdNet have considered the feature from I depth . If the quality of the depth map is low, then the predicted map from RgbdNet will quite different from the generated map from DepthNet. We have tested a set of values of the fixed threshold t in (Eq. 2) such as, 0.01, 0.02, 0.05, 0.10, 0.15, 0.20, but have found t = 0.15 achieve the best performance.</p><p>Loss Function. We adopt the widely-used cross entropy loss function L to train our model:</p><formula xml:id="formula_4">L(S, G) = − 1 N ∑ N i=1 g i log(s i ) + (1 − g i ) log(1 − s i ) ,<label>(4)</label></formula><p>where S ∈ [0, 1] 224×224 and G ∈ {0, 1} 224×224 indicate the estimated saliency map (i.e., S rgb , S rgbd , or S depth ) and the GT map, respectively. g i ∈ G, s i ∈ S, and N denotes the total number of pixels.</p><p>Training Settings. For fair comparisons, we follow the same training settings described in <ref type="bibr" target="#b50">[51]</ref>. We select 1485 image pairs from the NJU2K <ref type="bibr" target="#b36">[37]</ref> and 700 image pairs from NLPR <ref type="bibr" target="#b38">[39]</ref> dataset, respectively, as the training data (Please refer to our website for the Trainlist.txt). The proposed D 3 Net is implemented using Python, with the Pytorch toolbox. We adopt Adam as the optimizer and the initial learning rate is 1e-4 and batchsize is set to 8. The total training is 30 epoch on a GTX TITAN X GPU with 12G of memory.</p><p>Data Augmentation. Due to the limited scale of existing datasets, we augment the training samples by flipped the images horizontally to overcome the risk of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. BENCHMARKING EVALUATION RESULTS</head><p>We benchmark about 97K images (5,398 images × 18 models) in this study, making it the largest and most comprehensive RGB-D based SOD benchmark to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Models. We benchmark 18 SOTA models (see <ref type="table" target="#tab_2">Table IV</ref>), including 10 traditional and 8 CNN based models.</p><p>Datasets. We conduct our experiments on seven datasets (see <ref type="table" target="#tab_2">Table IV</ref>). The test sets of NJU2K <ref type="bibr" target="#b36">[37]</ref> and NLPR <ref type="bibr" target="#b38">[39]</ref> datasets, and the whole STERE <ref type="bibr" target="#b62">[63]</ref>, DES <ref type="bibr" target="#b37">[38]</ref>, SSD <ref type="bibr" target="#b65">[66]</ref>, LFSD <ref type="bibr" target="#b63">[64]</ref>, and SIP datasets are used for testing.</p><p>Runtime. In <ref type="table" target="#tab_2">Table IV</ref>, we summarize the runtime of existing approaches. The timings are tested on the same platform: Intel Xeon(R) E5-2676v3 2.4GHz×24 and GTX TITAN X. Since <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b66">[67]</ref>- <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b79">[80]</ref>- <ref type="bibr" target="#b81">[82]</ref> have not released their codes, the timings are borrowed from the original papers or provided by the authors. Our D 3 Net does not apply postprocessing (e.g., CRF), thus the computation only takes about 0.015s for a 224 × 224 image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>MAE M. We follow Perazzi et al. <ref type="bibr" target="#b100">[101]</ref> and evaluate the mean absolute error (MAE) between a real-valued saliency map Sal and a binary ground truth G for all image pixels:</p><formula xml:id="formula_5">MAE = 1 N |Sal − G|,<label>(5)</label></formula><p>where N is the total number of pixels. The MAE estimates the approximation degree between the saliency map and the ground truth map, and it is normalized to [0, 1]. The MAE provides a direct estimate of conformity between estimated and ground truth maps. However, for the MAE metric, small objects are naturally assigned smaller errors, while larger objects are given larger errors. The metric is also unable to tell where the error occurs <ref type="bibr" target="#b101">[102]</ref>. PR Curve. We also follow Borji et al. <ref type="bibr" target="#b4">[5]</ref> and provide the PR Curve. We divide a saliency map S using a fixed threshold which changes from 0 to 255. For each threshold, a pair of recall &amp; precision scores are computed, and then combined to form a precision-recall curve that describes the model performance in different situations. The overall evaluation results for PR Curves are shown in <ref type="figure" target="#fig_6">Fig. 8 (Top)</ref> and <ref type="figure" target="#fig_7">Fig. 9 (Left)</ref>.</p><p>F-measure F β . F-measure is essentially a region-based similarity metric. Following the works by Cheng and Zhang et al. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b102">[103]</ref>, we also provide the max F-measure using various fixed (0-255) thresholds. The overall F-measure evaluation results under different thresholds on each dataset are shown in <ref type="figure" target="#fig_6">Fig. 8 (Bottom)</ref> and <ref type="figure" target="#fig_7">Fig. 9 (Right)</ref>.</p><p>S-measure S α . Both the MAE and F-measure metrics ignore important structural information. However, behavioral vision studies have shown that the human visual system is highly sensitive to structures in scenes <ref type="bibr" target="#b57">[58]</ref>. Thus, we additionally include the structure measure (S-measure <ref type="bibr" target="#b57">[58]</ref>).The S-measure combines the region-aware (S r ) and object-aware (S o ) structural similarity as the final structure metric:</p><formula xml:id="formula_6">S α = α * S o + (1 − α) * S r ,<label>(6)</label></formula><p>where α ∈ [0, 1] is the balance parameter and set to 0.5. E-measure E ξ . E-measure is the recently proposed Enhanced alignment measure <ref type="bibr" target="#b58">[59]</ref> from the binary map evaluation field. This measure is based on cognitive vision studies, and combines local pixel values with the image-level mean value in one term, jointly capturing image-level statistics and local pixel matching information. Here, we introduce max/maximal E-measure to provide a more comprehensive evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Metric Statistics</head><p>For a given metric ζ ∈ {S α , F β , E ξ , M} we consider different statistics. I i j denote an image from a specific dataset D i . Thus, D i = {I i 1 , I i 2 , . . . , I i j }. Let ζ (I i j ) be the metric score on image I i j . The mean is the average dataset statistic defined as</p><formula xml:id="formula_7">M ζ (D i ) = 1 |D i | ∑ ζ (I i j ),</formula><p>where |D i | is the total number of images on the D i dataset. The mean statistics over different datasets are summarized in <ref type="table" target="#tab_2">Table IV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Comparison and Analysis</head><p>Performance of Traditional Models. Based on the overall performances listed in <ref type="table" target="#tab_2">Table IV</ref>, we observe that "SE <ref type="bibr" target="#b41">[42]</ref>, MDSF <ref type="bibr" target="#b44">[45]</ref>, and DCMC <ref type="bibr" target="#b54">[55]</ref> are the top-3 traditional algorithms." Utilizing superpixel technology, both SE and DCMC explicitly extract the region contrast features from an RGB image. In contrast, MDSF formulates SOD as a pixelwise binary labelling problem, which is solved by SVM.</p><p>Performance of Deep Models. Our D 3 Net, CPFP <ref type="bibr" target="#b50">[51]</ref> and TANet <ref type="bibr" target="#b81">[82]</ref> are the top-3 deep models out of all leading methods, showing the strong feature representation ability of deep learning for this task.</p><p>Traditional vs Deep Models. From <ref type="table" target="#tab_2">Table IV</ref>, we observe that most of the deep models perform better than the traditional algorithms. Interestingly, MDSF <ref type="bibr" target="#b65">[66]</ref> outperforms two deep models (i.e., DF <ref type="bibr" target="#b43">[44]</ref> and AFNet <ref type="bibr" target="#b60">[61]</ref>) on the NLPR dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with SOTAs</head><p>We compare our D 3 Net with 17 SOTA models in <ref type="table" target="#tab_2">Table IV.</ref> In general, our model outperforms the best published result (CPFP [51]-CVPR'19) by large margins of 1.0% ∼ 5.8% on six datasets. Notably, we also achieve a significant improvement of 1.4% on the proposed real-world SIP dataset.</p><p>We also report saliency maps generated on various challenging scenes to show the visual superiority of our D 3 Net. Some representative examples are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, such as when the structure of the salient object in the depth map is partially (e.g., the 1 st , 4 th , and 5 th rows) or dramatically (i.e., the 2 nd -3 rd rows) damaged. Specifically, in the 3 rd and 5 th rows, the depth DF <ref type="bibr" target="#b43">[44]</ref> AFNet <ref type="bibr" target="#b60">[61]</ref> CTMF <ref type="bibr" target="#b42">[43]</ref> MMCI <ref type="bibr" target="#b80">[81]</ref> PCF <ref type="bibr" target="#b48">[49]</ref> TANet <ref type="bibr" target="#b81">[82]</ref> CPFP <ref type="bibr" target="#b50">[51]</ref> D3Net</p><p>LHM <ref type="bibr" target="#b38">[39]</ref> CDB <ref type="bibr" target="#b46">[47]</ref> DESM <ref type="bibr" target="#b37">[38]</ref> GP <ref type="bibr" target="#b39">[40]</ref> CDCP <ref type="bibr" target="#b45">[46]</ref> ACSD <ref type="bibr" target="#b36">[37]</ref> LBE <ref type="bibr" target="#b40">[41]</ref> DCMC <ref type="bibr" target="#b54">[55]</ref> MDSF <ref type="bibr" target="#b44">[45]</ref>  of the salient object is locally connected with background scenes. Also, the 4 th row contains multiple isolated salient objects. For these challenging situations, most of the existing top competitors are unlikely to locate the salient objects due to their poor depth maps or insufficient multi-modal fusion schemes. Although CPFP <ref type="bibr" target="#b50">[51]</ref>, TANet <ref type="bibr" target="#b81">[82]</ref>, and PCF <ref type="bibr" target="#b48">[49]</ref> can generate more correct saliency maps than others, the salient object often introduces noticeable distinct backgrounds (3 rd -5 th rows) or the fine details of the salient object are lost(1 st row) due to the lack of a cross-modality learning ability. In contrast, our D 3 Net can eliminate low-quality depth maps and adaptively select complementary cues from RGB and depth images to infer the real salient object and highlight its details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. APPLICATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Human Activities</head><p>Nowadays, mobile phones generally have deep sensing cameras. With RGB-D salient object detection, users can better achieve the following functions: object extraction, a bokeh effect, mobile user recognition, etc. Many monitoring probes also have depth sensors, and RGB-D SOD can be helpful to the discovery of suspicious objects. For example, there is a lidar probe in autonomous vehicles designed to obtain depth information. RGB-D SOD is thus helpful for detecting basic objects such as pedestrians and signboards in these vehicles. There are also depth sensors in most industrial robots, so RGBD-SOD can help them better perceive the environment <ref type="table" target="#tab_2">Table IV</ref> BENCHMARKING RESULTS OF 18 LEADING RGB-D APPROACHES ON OUR SIP AND FDPSIX CLASSICAL <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref> DATASETS. ↑ &amp; ↓ DENOTE LARGER AND SMALLER IS BETTER, RESPECTIVELY. <ref type="table" target="#tab_2">"-T" INDICATES THE TEST SET OF THE CORRESPONDING  DATASET. FOR TRADITIONAL MODELS, THE STATISTICS ARE BASED ON OVERALL DATASETS RATHER ON THE TEST SET. THE "RANK"  DENOTES THE RANKING OF EACH MODEL IN A SPECIFIC MEASURE. THE "ALL RANK" INDICATES THE</ref>  and take certain actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Background Changing Application</head><p>Background changing techniques have become vital for art designers to leverage the increasing volumes of available image database. Traditional designers utilize photoshop to design their products. This is quite a time-consuming task and requires significant technical knowledge. A large majority of potential users fail to grasp the high-skilled technique in the art design. Thus, an easy-to-use application is needed.</p><formula xml:id="formula_8">(a) RGB (b) Depth (c) GT (d) D 3</formula><p>Net (e) CPFP <ref type="bibr" target="#b50">[51]</ref> (f) TANet <ref type="bibr" target="#b81">[82]</ref> (g) PCF <ref type="bibr" target="#b48">[49]</ref> (h) MDSF <ref type="bibr" target="#b44">[45]</ref> (i) SE <ref type="bibr" target="#b41">[42]</ref> (j) DCMC <ref type="bibr" target="#b54">[55]</ref> LFSD <ref type="bibr" target="#b63">[64]</ref> NJU2K <ref type="bibr" target="#b36">[37]</ref> STERE <ref type="bibr" target="#b62">[63]</ref> SSD <ref type="bibr" target="#b65">[66]</ref> SIP (Ours) <ref type="figure" target="#fig_0">Figure 10</ref>. Visual comparisons with the top-3 CNN-based models (CPFP <ref type="bibr" target="#b50">[51]</ref>, TANet <ref type="bibr" target="#b81">[82]</ref>, and PCF <ref type="bibr" target="#b48">[49]</ref>) and three classical non-deep methods (MDSF <ref type="bibr" target="#b44">[45]</ref>, SE <ref type="bibr" target="#b44">[45]</ref> and DCMC <ref type="bibr" target="#b54">[55]</ref>), on five datasets. Further results can be found in http://dpfan.net/D3NetBenchmark. To overcome the above-mentioned drawbacks, salient object detection technology could be a potential solution. Previous similar works, such as the automatic generation of visual-textual applications <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b104">[105]</ref> motive us to create a background changing application for book cover layouts. We provide a prototype demo, as shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. First, the user can upload an image as a candidate design image ((a) Input Image). Then, content-based image features, such as an RGB-D based saliency map, are considered in order to automatically generate salient objects. Finally, the system allows us to choose from our library of professionally designed book cover layouts ((b) Template). By combining high-level template constraints and low-level image features, we obtain the background changed book cover ((d) Results).</p><p>Since designing a complete software system is not our main focus in this article, Future researchers can follow yang et al. <ref type="bibr" target="#b103">[104]</ref> and set our visual background image with a specified topic <ref type="bibr" target="#b104">[105]</ref>. In stage two, the input image is resized to match the target style size and preserve the salient region according to the inference of our D 3 Net model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION</head><p>Based on our comprehensive benchmarking results, we present our conclusions to the most important questions that may benefit the research community to rethink the RGB-D image for salient object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Study.</head><p>We now provide a detailed analysis on the proposed baseline D 3 Net model. To verify the effectiveness of the depth map filter mechanism (the DDU), we derive two ablation studies: w/o DDU and DDU, which refer to our D 3 Net without utilizing DDU or include the DDU. For w/o DDU, we further test the performance of the three sub-network in the test phase of D 3 Net. In <ref type="table" target="#tab_9">Table V</ref>, we observe that RgbdNet performs better than RgbNet on the SIP, STERE, DES, LFSD, SSD, NJU2K datasets. It indicates that the cross-modality (RGB and depth) features show strong promise for RGB-D image representation learning. In most cases, however, DepthNet has lower performance than DepthNet and RgbNet. It shows that only based on a single modality, it is difficult for the model to construct the structure of the geometry in an image.</p><p>From <ref type="table" target="#tab_9">Table V</ref>, we also observed that the use of the DDU improves the performance (compared to RgbdNet) to a certain extent on the STERE, DES, NJU2K, and NLPR datasets. We attribute the improvement to the DDU being able to discard low-quality depth maps and select one optimal path (RgbNet or RgbdNet). For the SSD dataset, however, the DDU achieves comparable performance to the single stream network (i.e., RgbdNet). It is worth mentioning that D 3 Net outperforms any prior approach intended for SOD, without any post-processing techniques, such as CRF, which are typically used to boost scores. In order to know the lower and upper bound of our D 3 Net, we additionally select the optimal path (RgbdNet or RgbNet) of the D 3 Net. For example, for a specific RGB (I rgb ) and depth map (I depth ), the two predicted maps i.e., S rgb and S rgbd , can be assessed separately. Thus, for each input we know the best output in existing network. We aggregate all the best and worst results and achieve the upper bound and lower bound of our D 3 Net. From existing results listed in <ref type="table" target="#tab_9">Table V, D 3</ref> Net still has a ∼1.6% performance gap on average related to the upper bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Limitations</head><p>First, it is worth pointing out that the number of images in the SIP dataset is relatively small compared with most datasets for RGB salient object detection. Our goal behind building this dataset is to explore the potential direction of smartphone based applications. As can be seen from the benchmark results and the demo application described in § VI, salient object detection over real human activity scenes is a promising direction. We plan to keep growing the dataset with more challenging situations and various kinds of foreground persons.</p><p>Second, our simple general framework D 3 Net consists of three sub-networks, which may increase the memory on a lightweight device. In a real environment, several strategies can be considered to avoid this, such as replacing the backbone with MobileNet V2 <ref type="bibr" target="#b105">[106]</ref>, dimension reduction <ref type="bibr" target="#b106">[107]</ref>, or using the recently released ESPNet V2 <ref type="bibr" target="#b107">[108]</ref> models. Third, we present the lower and upper bounds of the DDU. The optimal upper bound is obtained by feeding the input into RgbdNet or RgbNet so that the predicted map is optimal. As shown in <ref type="table" target="#tab_9">Table V</ref>, our DDU module does not achieve the best upper bound on the current training subset. There is thus still an opportunity to design a better DDU to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>We present systematic studies on RGB-D based salient object detection by: (1) Introducing a new human-oriented SIP dataset reflecting the realistic in-the-wild mobile use scenarios. (2) Designing a novel D 3 Net. (3) Conducting so far the largest-scale (∼97K) benchmark. Compared with existing datasets, SIP covers several challenges (e.g., background diversity, occlusion, etc) of human in the real environments. Moreover, the proposed baseline achieves promising results. It is among the fastest methods, making it a practical solution to RGB-D salient object detection. The comprehensive benchmarking results include 32 summarized SOTAs and 18 evaluated traditional/deep models. We hope this benchmark will accelerate not only the development of this area but also others (e.g., stereo estimating/matching <ref type="bibr" target="#b108">[109]</ref>, multiple salient person detection, salient instance detection <ref type="bibr" target="#b18">[19]</ref>, sensitive object detection <ref type="bibr" target="#b109">[110]</ref>, image segmentation <ref type="bibr" target="#b110">[111]</ref>). Note that the methods utilized in our D 3 Net baseline are simple and more complex components (e.g., PDC in <ref type="bibr" target="#b111">[112]</ref>) or training strategy <ref type="bibr" target="#b112">[113]</ref> are promising to increase the performance. In the future, we plan to incorporate recently proposed techniques e.g., the weighted triplet loss <ref type="bibr" target="#b113">[114]</ref>, hierarchical deep features <ref type="bibr" target="#b114">[115]</ref>, visual question-driven saliency <ref type="bibr" target="#b115">[116]</ref>, into our D 3 Net to further boost the performance. After this submission, there are many interesting models, such as UCNet <ref type="bibr" target="#b116">[117]</ref>, JL-DCF <ref type="bibr" target="#b117">[118]</ref>, GFNet <ref type="bibr" target="#b118">[119]</ref>, DMRA <ref type="bibr" target="#b119">[120]</ref>, ERNet <ref type="bibr" target="#b120">[121]</ref>, BiANet <ref type="bibr" target="#b121">[122]</ref>, etc, have been released. Please refer to our online leaderboard (http://dpfan.net/d3netbenchmark/) for more details. This website will be updated continually. We foresee this study driving salient object detection towards real-world application scenarios with multiple salient persons and complex interactions through the mobile device (e.g., smartphone or tablets). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1. Left to right: input image, ground truth, and the corresponding depth map. The quality of the depth map from low (1 st row), mid (2 nd row) to high (last row). As shown in the 2 nd row, it is difficult to recognize the boundary of the human's arm in the boundary box region. However, it is clearly visible in the depth map. The highquality depth maps benefit the RGB-D based salient object detection task. These three examples are from the NJU2K [37], our SIP and NLPR [39] datasets respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>OF 31 CLASSICAL RGB-D BASED SOD ALGORITHMS AND THE PROPOSED BASELINE (D 3 NET). TRAIN/VAL SET. (#) = TRAINING OR VALIDATION SET: NLR = NLPR [39]. NJU = NJU2K [37]. MK = MSRA10K [70]. O = MK + DUTS [71]. BASIC: 4PRIORS = 4 PRIORS, e.g., REGION, BACKGROUND, DEPTH, AND SURFACE ORIENTATION PRIOR. IPT = INITIALIZATION PARAMETERS TRANSFER. LGBS PRIORS = LOCAL CONTRAST, GLOBAL CONTRAST, BACKGROUND, AND SPATIAL PRIOR. RFR [72] = RANDOM FOREST REGRESSOR. MCFM = MULTI-CONSTRAINT FEATURE MATCHING. CLP = CROSS LABEL PROPAGATION. TYPE: T = TRADITIONAL. D = DEEP LEARNING. SP. = SUPERPIXEL: WHETHER OR NOT USE THE SUPERPIXEL ALGORITHM. E-MEASURE: THE RANGE OF SCORES OVER THE SEVEN DATASETS IN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Object center to image center distance Probability (b) Margin to image center distance Probability (c) Normalized object size Probability Figure 4. (a) Distribution of normalized object center distance from image center. (b) Distribution of normalized object margin (farthest point in an object) distance from image center. (c) Distribution of normalized object size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The smoothed histogram (c) of high-quality (1 st row), lowquality (2 nd row) depth map, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>PR Curve (top) and F-measures (bottom) for 18 methods on NJU2K, STERE, and NLPR datasets, using various fixed thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>PR Curve (Left) and F-measures (Right) under different thresholds on the proposed SIP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>(a) Input Image (b) Template (c) Salient object (d) Results Examples of book cover maker. See § VI for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Zheng</head><label></label><figDesc>Lin is currently a Ph.D. candidate with College of Computer Science, Nankai University, under the supervision of Prof. Ming-Ming Cheng. His research interests include deep learning, computer graphics and computer vision. Zhao Zhang received the B.Eng degree from Yangzhou University in 2019. Currently, he is a master student in Nankai University under the supervision of Prof. Ming-Ming Cheng. His research interests includes computer vision and image processing. Menglong Zhu is a Computer Vision Software Engineer at Google. He obtained a Bachelors degree in Computer Science from Fudan University, in 2010, and a Masters degree in Robotics and a PhD degree in Computer and Information Science from University of Pennsylvania, in 2012 and 2016, respectively. His research interests are on object recognition, 3D object/human pose estimation, human action recognition, visual SLAM and text recognition. Ming-Ming Cheng received his PhD degree from Tsinghua University in 2012. Then he did 2 years research fellow, with Prof. Philip Torr in Oxford. He is now a professor at Nankai University, leading the Media Computing Lab. His research interests includes computer graphics, computer vision, and image processing. He received research awards including ACM China Rising Star Award, IBM Global SUR Award, CCF-Intel Young Faculty Researcher Program, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, Z. Lin, Z. Zhang, and M.-M. Cheng are with the College of Computer Science, Nankai University, Tianjin, China M. Zhu is with the Google AI, USA.</figDesc><table><row><cell>low low low low low low low low low low low low low low low low low</cell></row><row><cell>mid mid mid mid mid mid mid mid mid mid mid mid mid mid mid mid mid</cell></row><row><cell>high high high high high high high high high high high high high high high high high</cell></row></table><note>M.-M. Cheng is the corresponding author (email: cmm@nankai.edu.cn). Manuscript received July 16, 2019; revised March 10, 2020.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Grass Barrier Road Other Flower Tree Sign Low-light Sunny One Two Three Four Lighting Conditions # Object</head><label></label><figDesc>. The arXiv:1907.06781v2 [cs.CV] 11 Jul 2020</figDesc><table><row><cell>Car</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table I COMPARISON</head><label>I</label><figDesc>OF CURRENT RGB-D DATASETS IN TERMS OF YEAR (YEAR), PUBLICATION (PUB.), DATASET SIZE (DS.), NUMBER OF OBJECTS IN THE IMAGES (#OBJ.), TYPE OF SCENE (TYPES.), DEPTH SENSOR (SENSOR.), DEPTH QUALITY (DQ., e.g., HIGH-QUALITY DEPTH MAP SUFFERS FROM LESS RANDOM NOISE. SEE LAST ROW IN FIG. 1), ANNOTATION QUALITY (AQ., SEE FIG. 12), WHETHER OR NOT PROVIDE GRAYSCALE IMAGE FROM MONOCULAR CAMERA (GI.), CENTER BIAS (CB., SEE FIG. 4 (A)-(B)), AND RESOLUTION (IN PIXEL). H &amp; W DENOTE THE HEIGHT AND WIDTH OF THE IMAGE, RESPECTIVELY.</figDesc><table><row><cell>No.</cell><cell>Dataset Year Pub.</cell><cell>DS.</cell><cell>#Obj.</cell><cell>Types.</cell><cell cols="2">Sensor. DQ. AQ. GI. CB.</cell><cell>Resolution (H×W)</cell></row><row><cell cols="2">1 STERE [63] 2012 CVPR</cell><cell>1K</cell><cell>∼one</cell><cell>internet</cell><cell>Stereo camera+sift flow [54]</cell><cell cols="2">High No High [251∼1200]×[222∼900]</cell></row><row><cell>2</cell><cell>GIT [36] 2013 BMVC</cell><cell cols="2">0.08K multiple</cell><cell>home environment</cell><cell>Microsoft Kinect [52]</cell><cell>High No Low</cell><cell>640 × 480</cell></row><row><cell>3</cell><cell>LFSD [64] 2014 CVPR</cell><cell>0.1K</cell><cell>one</cell><cell>60 indoor/40 outdoor</cell><cell>Lytro Illum camera [53]</cell><cell>High No High</cell><cell>360 × 360</cell></row><row><cell>4</cell><cell cols="2">DES [38] 2014 ICIMCS 0.135K</cell><cell>one</cell><cell>135 indoor</cell><cell>Microsoft Kinect [52] High</cell><cell>No High</cell><cell>640 × 480</cell></row><row><cell>5</cell><cell>NLPR [39] 2014 ECCV</cell><cell cols="2">1K multiple</cell><cell>indoor/outdoor</cell><cell>Microsoft Kinect [52] High</cell><cell cols="2">No High 640 × 480, 480 × 640</cell></row><row><cell cols="2">6 NJU2K [37] 2014 ICIP</cell><cell>1.985K</cell><cell>∼one</cell><cell cols="2">3D movie/internet/photo FujiW3 camera+optical flow [65]</cell><cell cols="2">High No High [231∼1213]×[274∼828]</cell></row><row><cell>7</cell><cell>SSD [66] 2017 ICCVW</cell><cell cols="2">0.08K multiple</cell><cell>three stereo movies</cell><cell>Sun's optical flow [65]</cell><cell>No Low</cell><cell>960 × 1080</cell></row><row><cell>8</cell><cell cols="3">SIP (Ours) 2020 TNNLS 0.929K multiple</cell><cell>person in the wild</cell><cell cols="2">Huawei Mate10 High High Yes Low</cell><cell>992×744</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV .</head><label>IV</label><figDesc>EVALUATION TOOLS:HTTPS://GITHUB.COM/DENGPINGFAN/E-MEASURE.</figDesc><table><row><cell>No.</cell><cell>Model Year Pub.</cell><cell>Train/Val Set. (#)</cell><cell cols="2">Test (#) Basic</cell><cell cols="2">Type SP. E-measure↑ [59]</cell></row><row><cell>1</cell><cell>LS [36] 2013 BMVC</cell><cell>Without training dataset</cell><cell>One</cell><cell>Markov Random Field</cell><cell>T</cell><cell>Not Available</cell></row><row><cell>2</cell><cell>RC [73] 2013 BMVC</cell><cell>Without training dataset</cell><cell>One</cell><cell>Region Contrast, SVM [74]</cell><cell>T</cell><cell>Not available</cell></row><row><cell>3</cell><cell>LHM [39] 2014 ECCV</cell><cell>Without training dataset</cell><cell>One</cell><cell>Multi-Context Contrast</cell><cell>T</cell><cell>0.653∼0.771</cell></row><row><cell>4</cell><cell>DESM [38] 2014 ICIMCS</cell><cell>Without training dataset</cell><cell>One</cell><cell>Color/Depth Contrast, Spatial Bias Prior</cell><cell>T</cell><cell>0.770∼0.868</cell></row><row><cell>5</cell><cell>ACSD [37] 2014 ICIP</cell><cell>Without training dataset</cell><cell>One</cell><cell>Difference of Gaussian</cell><cell>T</cell><cell>0.780∼0.850</cell></row><row><cell>6</cell><cell>SRDS [75] 2014 DSP</cell><cell>Without training dataset</cell><cell>One</cell><cell>Weighted Color Contrast</cell><cell>T</cell><cell>Not available</cell></row><row><cell>7</cell><cell>GP [40] 2015 CVPRW</cell><cell>Without training dataset</cell><cell>Two</cell><cell>Markov Random Field, 4Priors</cell><cell>T</cell><cell>0.670∼0.824</cell></row><row><cell>8</cell><cell>PRC [62] 2016 Access</cell><cell>Without training dataset</cell><cell>Two</cell><cell>Region Classification, RFR</cell><cell>T</cell><cell>Not available</cell></row><row><cell>9</cell><cell>LBE [41] 2016 CVPR</cell><cell>Without training dataset</cell><cell>Two</cell><cell>Angular Density Component</cell><cell>T</cell><cell>0.736∼0.890</cell></row><row><cell>10</cell><cell>DCMC [55] 2016 SPL</cell><cell>Without training dataset</cell><cell>Two</cell><cell>Depth Confidence, Compactness, Graph</cell><cell>T</cell><cell>0.743∼0.856</cell></row><row><cell>11</cell><cell>SE [42] 2016 ICME</cell><cell>Without training dataset</cell><cell>Two</cell><cell>Cellular Automata</cell><cell>T</cell><cell>0.771∼0.856</cell></row><row><cell>12</cell><cell cols="2">MCLP [67] 2017 Cybernetic Without training dataset</cell><cell>Two</cell><cell>Addition, Deletion and Iteration Scheme</cell><cell>T</cell><cell>Not available</cell></row><row><cell>13</cell><cell>TPF [66] 2017 ICCVW</cell><cell>Without training dataset</cell><cell>Four</cell><cell>Cellular Automata, Optical Flow</cell><cell>T</cell><cell>Not available</cell></row><row><cell>14</cell><cell>CDCP [46] 2017 ICCVW</cell><cell>Without training dataset</cell><cell>Two</cell><cell>Center-dark Channel Prior</cell><cell>T</cell><cell>0.700∼0.820</cell></row><row><cell>15</cell><cell>DF [44] 2017 TIP</cell><cell>NLR (0.75K) + NJU (1.0K)</cell><cell cols="2">Three Laplacian Propagation, LGBS Priors</cell><cell>D</cell><cell>0.759∼0.880</cell></row><row><cell>16</cell><cell>BED [76] 2017 ICCVW</cell><cell>NLR (0.80K) + NJU (1.6K) + MK (9K)</cell><cell>Two</cell><cell>Background Enclosure Distribution</cell><cell>D</cell><cell>Not available</cell></row><row><cell>17</cell><cell>MDSF [45] 2017 TIP</cell><cell>NLR (0.50K) + NJU (0.5K)</cell><cell>Two</cell><cell>SVM [74], RFR, Ultrametric Contour Map</cell><cell>T</cell><cell>0.779∼0.885</cell></row><row><cell>18</cell><cell>MFF [77] 2017 SPL</cell><cell>Without training dataset</cell><cell>One</cell><cell>Minimum Barrier Distance, 3D prior</cell><cell>T</cell><cell>Not available</cell></row><row><cell>19</cell><cell>Review [56] 2018 TCSVT</cell><cell>Without training dataset</cell><cell>Two</cell><cell>Without model introduced</cell><cell>T</cell><cell>Not available</cell></row><row><cell>20</cell><cell>HSCS [68] 2018 TMM</cell><cell>Without training dataset</cell><cell>Two</cell><cell>Hierarchical Sparsity, Energy Function</cell><cell>T</cell><cell>Not available</cell></row><row><cell>21</cell><cell>ICS [69] 2018 TIP</cell><cell>Without training dataset</cell><cell>One</cell><cell>MCFM, CLP</cell><cell>T</cell><cell>Not available</cell></row><row><cell>22</cell><cell>CDB [47] 2018 NC</cell><cell>Without training dataset</cell><cell>One</cell><cell>Background Prior</cell><cell>T</cell><cell>0.698∼0.830</cell></row><row><cell>23</cell><cell>SCDL [78] 2018 DSP</cell><cell>NLR (0.75K) + NJU (1.0K)</cell><cell>Two</cell><cell>Silhouette Feature, Spatial Coherence Loss</cell><cell>D</cell><cell>Not available</cell></row><row><cell>24</cell><cell>PCF [49] 2018 CVPR</cell><cell>NLR (0.70K) + NJU (1.5K)</cell><cell cols="2">Three Complementarity-Aware Fusion module [49]</cell><cell>D</cell><cell>0.827∼0.925</cell></row><row><cell>25</cell><cell cols="2">CTMF [43] 2018 Cybernetic NLR (0.65K) + NJU (1.4K)</cell><cell>Four</cell><cell>HHA [79], IPT, Hidden Structure Transfer</cell><cell>D</cell><cell>0.829∼0.932</cell></row><row><cell>26</cell><cell>ACCF [80] 2018 IROS</cell><cell>NLR (0.65K) + NJU (1.4K)</cell><cell cols="2">Three Attention-Aware</cell><cell>D</cell><cell>Not available</cell></row><row><cell>27</cell><cell>PDNet [48] 2019 ICME</cell><cell>NLR (0.50K) + NJU (1.5K) + O (21K)</cell><cell>Five</cell><cell>Depth-Enhanced Net [48]</cell><cell>D</cell><cell>Not available</cell></row><row><cell>28</cell><cell>AFNet [61] 2019 Access</cell><cell>NLR (0.70K) + NJU (1.5K)</cell><cell cols="2">Three Switch map, Edge-Aware loss</cell><cell>D</cell><cell>0.807∼0.887</cell></row><row><cell>29</cell><cell>MMCI [81] 2019 PR</cell><cell>NLR (0.70K) + NJU (1.5K)</cell><cell cols="2">Three HHA [79], Dilated Convolutional</cell><cell>D</cell><cell>0.839∼0.928</cell></row><row><cell>30</cell><cell>TANet [82] 2019 TIP</cell><cell>NLR (0.70K) + NJU (1.5K)</cell><cell cols="2">Three Attention-Aware Multi-Modal Fusion</cell><cell>D</cell><cell>0.847∼0.941</cell></row><row><cell>31</cell><cell>CPFP [51] 2019 CVPR</cell><cell>NLR (0.70K) + NJU (1.5K)</cell><cell>Five</cell><cell>Contrast Prior, Fluid Pyramid</cell><cell>D</cell><cell>0.852∼0.932</cell></row><row><cell cols="2">32 D 3 Net (Ours) 2020</cell><cell>NLR (0.70K) + NJU (1.5K)</cell><cell cols="2">Seven Depth Depurator Unit</cell><cell>D</cell><cell>0.862∼0.953</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III STATISTICS</head><label>III</label><figDesc>REGARDING CAMERA/OBJECT MOTIONS AND SALIENT OBJECT INSTANCE NUMBERS IN SIP DATASET.</figDesc><table><row><cell>SIP (Ours)</cell><cell>car</cell><cell>flower</cell><cell>grass</cell><cell cols="3">Background Objects road tree signs</cell><cell>barrier</cell><cell>other</cell><cell cols="2">Object Boundary dark clear</cell><cell>1</cell><cell># Object 2</cell><cell>≥3</cell></row><row><cell>#Img</cell><cell>107</cell><cell>9</cell><cell>154</cell><cell>140</cell><cell>97</cell><cell>25</cell><cell>366</cell><cell>32</cell><cell>162</cell><cell>767</cell><cell>591</cell><cell>159</cell><cell>179</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>OVERALL RANKING (AVERAGE OF EACH RANK) IN A SPECIFIC DATASET. THE BEST PERFORMANCE IS HIGHLIGHTED IN BOLD. ↑ .514 .624 .665 .527 .669 .699 .695 .686 .748 .664 .763 .772 .849 .858 .877 .878 .879 .900 F β ↑ .632 .648 .717 .647 .621 .711 .748 .715 .775 .748 .804 .775 .845 .852 .872 .874 .877 .900 E ξ ↑ .724 .742 .791 .703 .741 .803 .803 .799 .838 .813 .864 .853 .913 .915 .924 .925 .926 .950 M ↓ .205 .203 .283 .211 .180 .202 .153 .172 .157 .169 .141 .100 .085 .079 .059 .060 .053 .041 ↑ .562 .615 .642 .588 .713 .692 .660 .731 .728 .708 .757 .825 .848 .873 .875 .871 .879 .899 F β ↑ .683 .717 .700 .671 .664 .669 .633 .740 .719 .755 .757 .823 .831 .863 .860 .861 .874 .891 E ξ ↑ .771 .823 .811 .743 .786 .806 .787 .819 .809 .846 .847 .887 .912 .927 .925 .923 .925 .938 M ↓ .172 .166 .295 .182 .149 .200 .250 .148 .176 .143 .141 .075 .086 .068 .064 .060 .051 .046 ↑ .578 .645 .622 .636 .709 .728 .703 .707 .741 .741 .752 .770 .863 .848 .842 .858 .872 .898 F β ↑ .511 .723 .765 .597 .631 .756 .788 .666 .746 .741 .766 .728 .844 .822 .804 .827 .846 .885 E ξ ↑ .653 .830 .868 .670 .811 .850 .890 .773 .851 .856 .870 .881 .932 .928 .893 .910 .923 .946 M ↓ .114 .100 .299 .168 .115 .169 .208 .111 .122 .090 .093 .068 .055 .065 .049 .046 .038 .031 ↑ .630 .629 .572 .654 .727 .673 .762 .724 .805 .756 .802 .799 .860 .856 .874 .886 .888 .912 F β ↑ .622 .618 .640 .611 .645 .607 .745 .648 .793 .713 .778 .771 .825 .815 .841 .863 .867 .897 E ξ ↑ .766 .791 .805 .723 .820 .780 .855 .793 .885 .847 .880 .879 .929 .913 .925 .941 .932 .953 M ↓ .108 .114 .312 .146 .112 .179 .081 .117 .095 .091 .085 .058 .056 .059 .044 .041 .036 .030 ↑ .566 .562 .602 .615 .603 .675 .621 .704 .673 .675 .747 .714 .776 .813 .841 .839 .807 .857 F β ↑ .568 .592 .680 .740 .535 .682 .619 .711 .703 .710 .735 .687 .729 .781 .807 .810 .766 .834 E ξ ↑ .717 .698 .769 .782 .700 .785 .736 .786 .779 .800 .828 .807 .865 .882 .894 .897 .852 .910 M ↓ .195 .196 .308 .180 .214 .203 .278 .169 .192 .165 .142 .118 .099 .082 .062 .063 .082 .058 ↑ .553 .515 .716 .635 .712 .727 .729 .753 .694 .692 .783 .738 .788 .787 .786 .801 .828 .825 F β ↑ .708 .677 .762 .783 .702 .763 .722 .817 .779 .786 .813 .744 .787 .771 .775 .796 .826 .810 E ξ ↑ .763 .766 .811 .824 .780 .829 .797 .856 .819 .832 .857 .815 .857 .839 .827 .847 .872 .862 M ↓ .218 .225 .253 .190 .172 .195 .214 .155 .197 .174 .145 .133 .127 .132 .119 .111 .088 .095 ↑ .511 .557 .616 .588 .595 .732 .727 .683 .717 .628 .653 .720 .716 .833 .842 .835 .850 .860 F β ↑ .574 .620 .669 .687 .505 .763 .751 .618 .698 .661 .657 .712 .694 .818 .838 .830 .851 .861 E ξ ↑ .716 .737 .770 .768 .721 .838 .853 .743 .798 .771 .759 .819 .829 .897 .901 .895 .903 .909 M ↓ .184 .192 .298 .173 .224 .172 .200 .186 .167 .164 .185 .118 .139 .086 .071 .075 .064 .063</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2014-2017</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2018-2019</cell><cell></cell><cell></cell></row><row><cell>*</cell><cell cols="18">Model LHM CDB DESM GP CDCP ACSD LBE DCMC MDSF SE DF AFNet CTMF MMCI PCF TANet CPFP D 3 Net</cell></row><row><cell></cell><cell cols="18">[39] [47] [38] [40] [46] [37] [41] [55] [45] [42] [44]  † [61]  † [43]  † [81]  † [49]  † [82]  † [51]  † Ours  †</cell></row><row><cell></cell><cell cols="18">Time (s) 2.130 -7.790 12.98 &gt;60.0 0.718 3.110 1.200 &gt;60.0 1.570 10.36 0.030 0.630 0.050 0.060 0.070 0.170 0.015</cell></row><row><cell></cell><cell>Code M</cell><cell>-</cell><cell cols="6">M M&amp;C M&amp;C C M&amp;C M</cell><cell cols="10">C M&amp;C M&amp;C Tf Caffe Caffe Caffe Caffe Caffe Pytorch</cell></row><row><cell>NJU-T [37]</cell><cell cols="2">S α Rank 17 16</cell><cell>14</cell><cell>17</cell><cell>15</cell><cell>12</cell><cell>10</cell><cell>13</cell><cell>9</cell><cell>11</cell><cell>7</cell><cell>7</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell></row><row><cell>STERE [63]</cell><cell cols="2">S α Rank 16 12</cell><cell>14</cell><cell>18</cell><cell>13</cell><cell>15</cell><cell>17</cell><cell>10</cell><cell>11</cell><cell>9</cell><cell>8</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>2</cell><cell>1</cell></row><row><cell>DES [38]</cell><cell cols="2">S α Rank 18 13</cell><cell>14</cell><cell>17</cell><cell>16</cell><cell>12</cell><cell>10</cell><cell>15</cell><cell>11</cell><cell>9</cell><cell>7</cell><cell>8</cell><cell>3</cell><cell>5</cell><cell>6</cell><cell>4</cell><cell>2</cell><cell>1</cell></row><row><cell>NLR-T [39]</cell><cell cols="2">S α Rank 14 15</cell><cell>16</cell><cell>18</cell><cell>12</cell><cell>17</cell><cell>10</cell><cell>13</cell><cell>7</cell><cell>11</cell><cell>8</cell><cell>8</cell><cell>5</cell><cell>6</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell></row><row><cell>SSD [66]</cell><cell cols="2">S α Rank 16 17</cell><cell>15</cell><cell>11</cell><cell>17</cell><cell>13</cell><cell>14</cell><cell>9</cell><cell>12</cell><cell>9</cell><cell>7</cell><cell>8</cell><cell>6</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>5</cell><cell>1</cell></row><row><cell>LFSD [64]</cell><cell cols="2">S α Rank 17 18</cell><cell>16</cell><cell>12</cell><cell>15</cell><cell>11</cell><cell>14</cell><cell>6</cell><cell>13</cell><cell>9</cell><cell>5</cell><cell>10</cell><cell>4</cell><cell>7</cell><cell>8</cell><cell>3</cell><cell>1</cell><cell>2</cell></row><row><cell>SIP (Ours)</cell><cell cols="2">S α Rank 17 16</cell><cell>14</cell><cell>12</cell><cell>18</cell><cell>6</cell><cell>9</cell><cell>14</cell><cell>10</cell><cell>11</cell><cell>13</cell><cell>7</cell><cell>8</cell><cell>5</cell><cell>3</cell><cell>4</cell><cell>2</cell><cell>1</cell></row><row><cell></cell><cell cols="2">All Rank 18 17</cell><cell>15</cell><cell>14</cell><cell>16</cell><cell>13</cell><cell>12</cell><cell>11</cell><cell>10</cell><cell>9</cell><cell>7</cell><cell>8</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table V S</head><label>V</label><figDesc>-MEASURE↑ SCORE ON OUR SIP AND THE STERE DATASET. THE SYMBOL ↑ INDICATES THAT THE HIGHER THE SCORE IS, THE BETTER THE MODEL PERFORMS AND VICE VERSA. SEE DETAILS IN § VII. Comparison with previous object-level datasets, which are labeled with polygons (the foot pad in NLPR [39]), coarse boundaries (i.e., the chair in DES<ref type="bibr" target="#b37">[38]</ref>), and missed object parts (e.g., the person in SSD<ref type="bibr" target="#b65">[66]</ref>). In contrast, the proposed object-/instance-level SIP dataset is labeled with smooth, fine boundaries. More specifically, occlusions are also considered (e.g., the barrier region).</figDesc><table><row><cell>Aspects</cell><cell>Model</cell><cell>SIP (Ours)</cell><cell>STERE [63]</cell><cell>DES [38]</cell><cell>LFSD [64]</cell><cell>SSD [66]</cell><cell>NJU2K [37]</cell><cell>NLPR [39]</cell></row><row><cell></cell><cell>RgbNet</cell><cell>0.831</cell><cell>0.893</cell><cell>0.881</cell><cell>0.810</cell><cell>0.839</cell><cell>0.888</cell><cell>0.911</cell></row><row><cell>w/o DDU</cell><cell>RgbdNet</cell><cell>0.862</cell><cell>0.898</cell><cell>0.896</cell><cell>0.836</cell><cell>0.857</cell><cell>0.898</cell><cell>0.910</cell></row><row><cell></cell><cell>DepthNet</cell><cell>0.862</cell><cell>0.713</cell><cell>0.911</cell><cell>0.724</cell><cell>0.811</cell><cell>0.857</cell><cell>0.864</cell></row><row><cell></cell><cell>Lower Bound</cell><cell>0.822</cell><cell>0.881</cell><cell>0.870</cell><cell>0.788</cell><cell>0.817</cell><cell>0.875</cell><cell>0.897</cell></row><row><cell>DDU</cell><cell>D 3 Net (Ours)</cell><cell>0.860</cell><cell>0.899</cell><cell>0.898</cell><cell>0.825</cell><cell>0.857</cell><cell>0.900</cell><cell>0.912</cell></row><row><cell></cell><cell>Upper Bound</cell><cell>0.872</cell><cell>0.910</cell><cell>0.907</cell><cell>0.858</cell><cell>0.879</cell><cell>0.912</cell><cell>0.924</cell></row><row><cell cols="2">Polygons labels &amp;</cell><cell cols="2">Coarse boundaries &amp;</cell><cell></cell><cell>Object missing &amp;</cell><cell></cell><cell cols="2">Fine boundaries &amp;</cell></row><row><cell></cell><cell>Object-level</cell><cell></cell><cell>Object-level</cell><cell></cell><cell>Object-level</cell><cell></cell><cell cols="2">Instance-level</cell></row><row><cell></cell><cell>NLPR</cell><cell></cell><cell>DES</cell><cell></cell><cell>SSD</cell><cell></cell><cell>SIP</cell><cell></cell></row><row><cell>Figure 12.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment.</head><p>We thank Jia-Xing Zhao, Yun Liu, and Qibin Hou for insightful feedback. This research was supported by Major Project for New Generation of AI under Grant No. 2018AAA0100400, NSFC (61922046), and Tianjin Natural Science Foundation (17JCJQJC43700).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="150" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3127" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object-based rgbd image cosegmentation with mutex constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4428" to="4436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Salient object detection with lossless feature reflection and weighted structural loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE T. Image Process</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Salient Object Detection: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Revisiting video saliency prediction in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-source weak supervision for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A mutual learning method for salient object detection with intertwined multi-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Capsal: Leveraging captioning to boost semantics for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jzhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrently aggregating deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Salient object detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Saliency integration: An arbitrator model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PiCANet: Learning Pixel-wise Contextual Attention for Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">R3Net: recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Manifold ranking-based matrix factorization for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neur. Net. Lear</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1122" to="1134" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Instance-level salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Salient object detection using a context-aware refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlocal deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6609" to="6617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DISC: Deep Image Saliency Computing via Progressive Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neur. Net. Lear</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1135" to="1149" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Saliency detection by multicontext deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep embedding features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Selectivity or invariance: Boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10066</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Super diffusion for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09038</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep reasoning with multi-scale context for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08362</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Richer and deeper supervision network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02425</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">300-fps salient object detection via minimum directional contrast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4243" to="4254" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contour knowledge transfer for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="355" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Saliency benchmarking made easy: Separating models, maps and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kummerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S A</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Look, perceive and segment: Finding the salient objects in images via two-stream fixation-semantic cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Revisiting salient object detection: Simultaneous detection, ranking, and subitizing of multiple salient objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7142" to="7150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">An in depth view of saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1115" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Internet Multi</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploiting Global Priors for RGB-D Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Local background enclosure for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2343" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Salient object detection for rgb-d image via saliency evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">CNNs-based RGB-D saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Cybern</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">RGBD salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An innovative salient object detection using center-dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Worksh</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stereoscopic saliency model using contrast and depth-guided-background prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="2227" to="2238" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pdnet: Prior-model guided depth-enhanced network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Progressively Complementarity-Aware Fusion Network for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stereoscopic thumbnail creation via efficient stereo saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Vis. Comput. Gr</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2014" to="2027" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y.</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Light field photography with a hand-held plenoptic camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brédif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Technical Report (CSTR)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Review of visual saliency detection with comprehensive information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Circuit Syst. Video Technol</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Enhanced-alignment Measure for Binary Foreground Map Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adaptive Fusion for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="55" to="277" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Improving rgbd saliency detection using progressive region classification and saliency fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="8987" to="8994" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2806" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2432" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A Three-pathway Psychobiological Framework of Salient Object Detection Using Stereoscopic Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Worksh</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3008" to="3014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An iterative co-saliency framework for rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Cybern</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Hscs: Hierarchical sparsity based co-saliency detection for rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE T. Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Co-saliency detection for RGBD images based on multi-constraint feature matching and cross label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="568" to="579" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Accurate regression procedures for active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Depth really matters: Improving visual salient region detection with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desingh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brit. Mach. Vis. Conf</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM T. Intel. Syst. Tec</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Salient region detection for stereoscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Dig. Sig. Process</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="454" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning RGB-D salient object detection using background enclosure, depth contrast, and top-down features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shigematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Worksh</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2749" to="2757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">RGB-D salient object detection via minimum barrier distance transform and saliency fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="663" to="667" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection using spatially coherent deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Dig. Sig</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Attention-aware cross-modal cross-level fusion network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Intell. Rob. Syst</title>
		<imprint>
			<biblScope unit="page" from="6821" to="6826" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multiscale multi-path and cross-modal interactions for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Three-stream attention-aware network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE T. Image Process</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Image segmentation by probabilistic bottom-up aggregation and cue integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Joint Salient Object Detection and Existence Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Comput. Sci</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">What is and what is not a salient object? learning salient object detector by ensembling linear exemplar regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Visual correlates of fixation selection: Effects of scale and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Tatler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Baddeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Gilchrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="643" to="659" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">The discrimination of visual number</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Reese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Volkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="525" />
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2061" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8818" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3085" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Motion coherent tracking with multilabel mrf optimization, algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Automatic generation of visual-textual presentation layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Recommendation system for automatic design of magazine covers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tretter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>O&amp;apos;brien-Strain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allebach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 international conference on Intelligent user interfaces</title>
		<meeting>the 2013 international conference on Intelligent user interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="95" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>IEEE Conf. Comput. Vis. Pattern Recog.</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Local deep-feature alignment for unsupervised dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2420" to="2432" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Multi-level context ultra-aggregation for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">iprivacy: image privacy protection by identifying sensitive objects via deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Info. Foren. Secur</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1005" to="1016" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Submodular function optimization for motion clustering and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neur. Net. Lear</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Camouflaged object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Inf-net: Automatic covid-19 lung infection segmentation from ct scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Spatial pyramidenhanced netvlad with weighted triplet loss for place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neur. Net. Lear</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Hierarchical deep click feature prediction for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Exploring duality in visual questiondriven top-down saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neur. Net. Lear</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A Cross-modal Adaptive Gated Fusion Generative Adversarial Network for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7254" to="7263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Exploit and Replace: An Asymmetrical Two-Stream Architecture for Versatile Light Field Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Bilateral attention network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14582</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">He joined Inception Institute of Artificial Intelligence (IIAI), UAE in 2019</title>
		<editor>Prof. Ming-Ming Cheng</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Deng-Ping Fan received his PhD degree from Nankai University of Tianjin ; Department of Computer Science, University of Nankai</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include computer vision, image processing and deep learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
