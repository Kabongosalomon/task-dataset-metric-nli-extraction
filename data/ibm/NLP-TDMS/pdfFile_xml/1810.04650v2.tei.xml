<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Learning as Multi-Objective Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Task Learning as Multi-Objective Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of pertask losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multiobjective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multilabel classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the most surprising results in statistics is Stein's paradox. <ref type="bibr" target="#b48">Stein (1956)</ref> showed that it is better to estimate the means of three or more Gaussian random variables using samples from all of them rather than estimating them separately, even when the Gaussians are independent. Stein's paradox was an early motivation for multi-task learning (MTL) <ref type="bibr">(Caruana, 1997)</ref>, a learning paradigm in which data from multiple tasks is used with the hope to obtain superior performance over learning each task independently. Potential advantages of MTL go beyond the direct implications of Stein's paradox, since even seemingly unrelated real world tasks have strong dependencies due to the shared processes that give rise to the data. For example, although autonomous driving and object manipulation are seemingly unrelated, the underlying data is governed by the same laws of optics, material properties, and dynamics. This motivates the use of multiple tasks as an inductive bias in learning systems. A typical MTL system is given a collection of input points and sets of targets for various tasks per point. A common way to set up the inductive bias across tasks is to design a parametrized hypothesis class that shares some parameters across tasks. Typically, these parameters are learned by solving an optimization problem that minimizes a weighted sum of the empirical risk for each task. However, the linear-combination formulation is only sensible when there is a parameter set that is effective across all tasks. In other words, minimization of a weighted sum of empirical risk is only valid if tasks are not competing, which is rarely the case. MTL with conflicting objectives requires modeling of the trade-off between tasks, which is beyond what a linear combination achieves.</p><p>An alternative objective for MTL is finding solutions that are not dominated by any others. Such solutions are said to be Pareto optimal. In this paper, we cast the objective of MTL in terms of finding Pareto optimal solutions. The problem of finding Pareto optimal solutions given multiple criteria is called multi-objective optimization. A variety of algorithms for multi-objective optimization exist. One such approach is the multiple-gradient descent algorithm (MGDA), which uses gradient-based optimization and provably converges to a point on the Pareto set <ref type="bibr" target="#b10">(Désidéri, 2012)</ref>. MGDA is well-suited for multi-task learning with deep networks. It can use the gradients of each task and solve an optimization problem to decide on an update over the shared parameters. However, there are two technical problems that hinder the applicability of MGDA on a large scale. (i) The underlying optimization problem does not scale gracefully to high-dimensional gradients, which arise naturally in deep networks. (ii) The algorithm requires explicit computation of gradients per task, which results in linear scaling of the number of backward passes and roughly multiplies the training time by the number of tasks.</p><p>In this paper, we develop a Frank-Wolfe-based optimizer that scales to high-dimensional problems. Furthermore, we provide an upper bound for the MGDA optimization objective and show that it can be computed via a single backward pass without explicit task-specific gradients, thus making the computational overhead of the method negligible. We prove that using our upper bound yields a Pareto optimal solution under realistic assumptions. The result is an exact algorithm for multi-objective optimization of deep networks with negligible computational overhead.</p><p>We empirically evaluate the presented method on three different problems. First, we perform an extensive evaluation on multi-digit classification with MultiMNIST <ref type="bibr" target="#b43">(Sabour et al., 2017)</ref>. Second, we cast multi-label classification as MTL and conduct experiments with the CelebA dataset <ref type="bibr" target="#b28">(Liu et al., 2015b)</ref>. Lastly, we apply the presented method to scene understanding; specifically, we perform joint semantic segmentation, instance segmentation, and depth estimation on the Cityscapes dataset <ref type="bibr" target="#b7">(Cordts et al., 2016)</ref>. The number of tasks in our evaluation varies from 2 to 40. Our method clearly outperforms all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-task learning. We summarize the work most closely related to ours and refer the interested reader to reviews by <ref type="bibr" target="#b42">Ruder (2017)</ref> and <ref type="bibr" target="#b58">Zhou et al. (2011b)</ref> for additional background. Multi-task learning (MTL) is typically conducted via hard or soft parameter sharing. In hard parameter sharing, a subset of the parameters is shared between tasks while other parameters are task-specific. In soft parameter sharing, all parameters are task-specific but they are jointly constrained via Bayesian priors <ref type="bibr" target="#b50">(Xue et al., 2007;</ref><ref type="bibr" target="#b2">Bakker and Heskes, 2003)</ref> or a joint dictionary <ref type="bibr" target="#b0">(Argyriou et al., 2007;</ref><ref type="bibr" target="#b29">Long and Wang, 2015;</ref><ref type="bibr" target="#b51">Yang and Hospedales, 2016;</ref><ref type="bibr" target="#b42">Ruder, 2017)</ref>. We focus on hard parameter sharing with gradient-based optimization, following the success of deep MTL in computer vision <ref type="bibr">(Bilen and Vedaldi, 2016;</ref><ref type="bibr" target="#b33">Misra et al., 2016;</ref><ref type="bibr" target="#b41">Rudd et al., 2016;</ref><ref type="bibr" target="#b51">Yang and Hospedales, 2016;</ref><ref type="bibr" target="#b23">Kokkinos, 2017;</ref><ref type="bibr" target="#b52">Zamir et al., 2018)</ref>, natural language processing <ref type="bibr" target="#b6">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b11">Dong et al., 2015;</ref><ref type="bibr" target="#b27">Liu et al., 2015a;</ref><ref type="bibr" target="#b30">Luong et al., 2015;</ref><ref type="bibr" target="#b15">Hashimoto et al., 2017)</ref>, speech processing <ref type="bibr" target="#b18">(Huang et al., 2013;</ref><ref type="bibr" target="#b46">Seltzer and Droppo, 2013;</ref><ref type="bibr" target="#b19">Huang et al., 2015)</ref>, and even seemingly unrelated domains over multiple modalities <ref type="bibr" target="#b21">(Kaiser et al., 2017)</ref>. <ref type="bibr" target="#b3">Baxter (2000)</ref> theoretically analyze the MTL problem as interaction between individual learners and a meta-algorithm. Each learner is responsible for one task and a meta-algorithm decides how the shared parameters are updated. All aforementioned MTL algorithms use weighted summation as the meta-algorithm. Meta-algorithms that go beyond weighted summation have also been explored. <ref type="bibr" target="#b26">Li et al. (2014)</ref> consider the case where each individual learner is based on kernel learning and utilize multi-objective optimization. <ref type="bibr" target="#b53">Zhang and Yeung (2010)</ref> consider the case where each learner is a linear model and use a task affinity matrix. <ref type="bibr" target="#b57">Zhou et al. (2011a)</ref> and <ref type="bibr" target="#b1">Bagherjeiran et al. (2005)</ref> use the assumption that tasks share a dictionary and develop an expectation-maximization-like metaalgorithm. <ref type="bibr" target="#b8">de Miranda et al. (2012)</ref> and <ref type="bibr" target="#b56">Zhou et al. (2017b)</ref> use swarm optimization. None of these methods apply to gradient-based learning of high-capacity models such as modern deep networks. <ref type="bibr" target="#b22">Kendall et al. (2018)</ref> and <ref type="bibr" target="#b5">Chen et al. (2018)</ref> propose heuristics based on uncertainty and gradient magnitudes, respectively, and apply their methods to convolutional neural networks. Another recent work uses multi-agent reinforcement learning <ref type="bibr" target="#b40">(Rosenbaum et al., 2017)</ref>.</p><p>Multi-objective optimization. Multi-objective optimization addresses the problem of optimizing a set of possibly contrasting objectives. We recommend <ref type="bibr" target="#b32">Miettinen (1998)</ref> and <ref type="bibr" target="#b12">Ehrgott (2005)</ref> for surveys of this field. Of particular relevance to our work is gradient-based multi-objective optimization, as developed by <ref type="bibr" target="#b13">Fliege and Svaiter (2000)</ref>, <ref type="bibr" target="#b44">Schäffler et al. (2002)</ref>, and <ref type="bibr" target="#b10">Désidéri (2012)</ref>. These methods use multi-objective Karush-Kuhn-Tucker (KKT) conditions <ref type="bibr" target="#b24">(Kuhn and Tucker, 1951)</ref> and find a descent direction that decreases all objectives. This approach was extended to stochastic gradient descent by <ref type="bibr" target="#b36">Peitz and Dellnitz (2018)</ref> and <ref type="bibr" target="#b38">Poirion et al. (2017)</ref>. In machine learning, these methods have been applied to multi-agent learning <ref type="bibr" target="#b14">(Ghosh et al., 2013;</ref><ref type="bibr" target="#b37">Pirotta and Restelli, 2016;</ref><ref type="bibr" target="#b34">Parisi et al., 2014)</ref>, kernel learning <ref type="bibr" target="#b26">(Li et al., 2014)</ref>, sequential decision making <ref type="bibr" target="#b39">(Roijers et al., 2013)</ref>, and Bayesian optimization <ref type="bibr" target="#b47">(Shah and Ghahramani, 2016;</ref><ref type="bibr" target="#b17">Hernández-Lobato et al., 2016)</ref>. Our work applies gradient-based multi-objective optimization to multi-task learning.</p><p>3 Multi-Task Learning as Multi-Objective Optimization Consider a multi-task learning (MTL) problem over an input space X and a collection of task spaces {Y t } t∈ <ref type="bibr">[T ]</ref> , such that a large dataset of i.i.d. data points {x i , y 1 i , . . . , y T i } i∈[N ] is given where T is the number of tasks, N is the number of data points, and y t i is the label of the t th task for the i th data point. <ref type="bibr">1</ref> We further consider a parametric hypothesis class per task as f t (x; θ sh , θ t ) : X → Y t , such that some parameters (θ sh ) are shared between tasks and some (θ t ) are task-specific. We also consider task-specific loss functions</p><formula xml:id="formula_0">L t (·, ·) : Y t × Y t → R + .</formula><p>Although many hypothesis classes and loss functions have been proposed in the MTL literature, they generally yield the following empirical risk minimization formulation:</p><formula xml:id="formula_1">min θ sh , θ 1 ,...,θ T T t=1 c tLt (θ sh , θ t )<label>(1)</label></formula><p>for some static or dynamically computed weights c t per task, whereL t (θ sh , θ t ) is the empirical loss of the task t, defined asL t (θ sh , θ t )</p><formula xml:id="formula_2">1 N i L f t (x i ; θ sh , θ t ), y t i .</formula><p>Although the weighted summation formulation (1) is intuitively appealing, it typically either requires an expensive grid search over various scalings or the use of a heuristic <ref type="bibr" target="#b22">(Kendall et al., 2018;</ref><ref type="bibr" target="#b5">Chen et al., 2018)</ref>. A basic justification for scaling is that it is not possible to define global optimality in the MTL setting. Consider two sets of solutions θ andθ such thatL t1 (θ sh , θ t1 ) &lt;L t1 (θ sh ,θ t1 ) and L t2 (θ sh , θ t2 ) &gt;L t2 (θ sh ,θ t2 ), for some tasks t 1 and t 2 . In other words, solution θ is better for task t 1 whereasθ is better for t 2 . It is not possible to compare these two solutions without a pairwise importance of tasks, which is typically not available.</p><p>Alternatively, MTL can be formulated as multi-objective optimization: optimizing a collection of possibly conflicting objectives. This is the approach we take. We specify the multi-objective optimization formulation of MTL using a vector-valued loss L:</p><formula xml:id="formula_3">min θ sh , θ 1 ,...,θ T L(θ sh , θ 1 , . . . , θ T ) = min θ sh , θ 1 ,...,θ T L 1 (θ sh , θ 1 ), . . . ,L T (θ sh , θ T ) .<label>(2)</label></formula><p>The goal of multi-objective optimization is achieving Pareto optimality. Definition 1 (Pareto optimality for MTL)</p><formula xml:id="formula_4">(a) A solution θ dominates a solutionθ ifL t (θ sh , θ t ) ≤L t (θ sh ,θ t ) for all tasks t and L(θ sh , θ 1 , . . . , θ T ) = L(θ sh ,θ 1 , . . . ,θ T ). (b) A solution θ is called Pareto optimal if there exists no solution θ that dominates θ .</formula><p>The set of Pareto optimal solutions is called the Pareto set (P θ ) and its image is called the Pareto front (P L = {L(θ)} θ∈P θ ). In this paper, we focus on gradient-based multi-objective optimization due to its direct relevance to gradient-based MTL.</p><p>In the rest of this section, we first summarize in Section 3.1 how multi-objective optimization can be performed with gradient descent. Then, we suggest in Section 3.2 a practical algorithm for performing multi-objective optimization over very large parameter spaces. Finally, in Section 3.3 we propose an efficient solution for multi-objective optimization designed directly for high-capacity deep networks. Our method scales to very large models and a high number of tasks with negligible overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multiple Gradient Descent Algorithm</head><p>As in the single-objective case, multi-objective optimization can be solved to local optimality via gradient descent. In this section, we summarize one such approach, called the multiple gradient descent algorithm (MGDA) <ref type="bibr" target="#b10">(Désidéri, 2012)</ref>. MGDA leverages the Karush-Kuhn-Tucker (KKT) conditions, which are necessary for optimality <ref type="bibr" target="#b13">(Fliege and Svaiter, 2000;</ref><ref type="bibr" target="#b44">Schäffler et al., 2002;</ref><ref type="bibr" target="#b10">Désidéri, 2012)</ref>. We now state the KKT conditions for both task-specific and shared parameters:</p><formula xml:id="formula_5">• There exist α 1 , . . . , α T ≥ 0 such that T t=1 α t = 1 and T t=1 α t ∇ θ shL t (θ sh , θ t ) = 0 • For all tasks t, ∇ θ tL t (θ sh , θ t ) = 0</formula><p>Any solution that satisfies these conditions is called a Pareto stationary point. Although every Pareto optimal point is Pareto stationary, the reverse may not be true. Consider the optimization problem</p><formula xml:id="formula_6">min α 1 ,...,α T T t=1 α t ∇ θ shL t (θ sh , θ t ) 2 2 T t=1 α t = 1, α t ≥ 0 ∀t<label>(3)</label></formula><p>Désidéri <ref type="formula" target="#formula_1">(2012)</ref> showed that either the solution to this optimization problem is 0 and the resulting point satisfies the KKT conditions, or the solution gives a descent direction that improves all tasks. Hence, the resulting MTL algorithm would be gradient descent on the task-specific parameters followed by solving <ref type="formula" target="#formula_6">(3)</ref> and applying the solution ( T t=1 α t ∇ θ sh ) as a gradient update to shared parameters. We discuss how to solve (3) for an arbitrary model in Section 3.2 and present an efficient solution when the underlying model is an encoder-decoder in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Solving the Optimization Problem</head><p>The optimization problem defined in (3) is equivalent to finding a minimum-norm point in the convex hull of the set of input points. This problem arises naturally in computational geometry: it is equivalent to finding the closest point within a convex hull to a given query point. It has been studied extensively <ref type="bibr" target="#b31">(Makimoto et al., 1994;</ref><ref type="bibr" target="#b49">Wolfe, 1976;</ref><ref type="bibr" target="#b45">Sekitani and Yamamoto, 1993)</ref>. Although many algorithms have been proposed, they do not apply in our setting because the assumptions they make do not hold. Algorithms proposed in the computational geometry literature address the problem of finding minimum-norm points in the convex hull of a large number of points in a low-dimensional space (typically of dimensionality 2 or 3). In our setting, the number of points is the number of tasks and is typically low; in contrast, the dimensionality is the number of shared parameters and can be in the millions. We therefore use a different approach based on convex optimization, since (3) is a convex quadratic problem with linear constraints.</p><p>Before we tackle the general case, let's consider the case of two tasks. The optimization problem can be defined as min</p><formula xml:id="formula_7">α∈[0,1] α∇ θ shL 1 (θ sh , θ 1 ) + (1 − α)∇ θ shL 2 (θ sh , θ 2 ) 2</formula><p>2 , which is a onedimensional quadratic function of α with an analytical solution:</p><formula xml:id="formula_8">α = ∇ θ shL 2 (θ sh , θ 2 ) − ∇ θ shL 1 (θ sh , θ 1 ) ∇ θ shL 2 (θ sh , θ 2 ) ∇ θ shL 1 (θ sh , θ 1 ) − ∇ θ shL 2 (θ sh , θ 2 ) 2 2 +, 1<label>(4)</label></formula><p>where [·] +, 1 represents clipping to [0, 1] as [a] +, 1 = max(min(a, 1), 0). We further visualize this solution in <ref type="figure">Figure 1</ref>. Although this is only applicable when T = 2, this enables efficient application of the Frank-Wolfe algorithm (Jaggi, 2013) since the line search can be solved analytically. Hence, we use Frank-Wolfe to solve the constrained optimization problem, using (4) as a subroutine for the line search. We give all the update equations for the Frank-Wolfe solver in Algorithm 2.</p><p>Figure 1: Visualisation of the min-norm point in the convex hull of two points (min γ∈[0,1] γθ + (1 − γ)θ 2 2 ). As the geometry suggests, the solution is either an edge case or a perpendicular vector.</p><formula xml:id="formula_9">Algorithm 1 min γ∈[0,1] γθ + (1 − γ)θ 2 2 1: if θ θ ≥ θ θ then 2: γ = 1 3: else if θ θ ≥θ θ then 4: γ = 0 5: else 6: γ = (θ−θ) θ θ−θ 2 2 7: end if Algorithm 2 Update Equations for MTL 1: for t = 1 to T do 2: θ t = θ t − η∇ θ tL t (θ sh , θ t )</formula><p>Gradient descent on task-specific parameters 3: end for 4: α 1 , . . . , α T = FRANKWOLFESOLVER(θ)</p><p>Solve <ref type="formula" target="#formula_6">(3)</ref> to find a common descent direction</p><formula xml:id="formula_10">5: θ sh = θ sh − η T t=1 α t ∇ θ shL t (θ sh , θ t )</formula><p>Gradient descent on shared parameters 6: procedure FRANKWOLFESOLVER(θ) 7:</p><formula xml:id="formula_11">Initialize α = (α 1 , . . . , α T ) = ( 1 T , . . . , 1 T ) 8: Precompute M st. M i,j = ∇ θ shL i (θ sh , θ i ) ∇ θ shL j (θ sh , θ j ) 9: repeat 10:t = arg min r t α t M rt 11:γ = arg min γ (1 − γ)α + γet M (1 − γ)α + γet</formula><p>Using Algorithm 1 12:</p><formula xml:id="formula_12">α = (1 −γ)α +γet 13:</formula><p>untilγ ∼ 0 or Number of Iterations Limit 14:</p><p>return α 1 , . . . , α T 15: end procedure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient Optimization for Encoder-Decoder Architectures</head><p>The MTL update described in Algorithm 2 is applicable to any problem that uses optimization based on gradient descent. Our experiments also suggest that the Frank-Wolfe solver is efficient and accurate as it typically converges in a modest number of iterations with negligible effect on training time. However, the algorithm we described needs to compute ∇ θ shL t (θ sh , θ t ) for each task t, which requires a backward pass over the shared parameters for each task. Hence, the resulting gradient computation would be the forward pass followed by T backward passes. Considering the fact that computation of the backward pass is typically more expensive than the forward pass, this results in linear scaling of the training time and can be prohibitive for problems with more than a few tasks.</p><p>We now propose an efficient method that optimizes an upper bound of the objective and requires only a single backward pass. We further show that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. The architectures we address conjoin a shared representation function with task-specific decision functions. This class of architectures covers most of the existing deep MTL models and can be formally defined by constraining the hypothesis class as</p><formula xml:id="formula_13">f t (x; θ sh , θ t ) = (f t (·; θ t ) • g(·; θ sh ))(x) = f t (g(x; θ sh ); θ t )<label>(5)</label></formula><p>where g is the representation function shared by all tasks and f t are the task-specific functions that take this representation as input. If we denote the representations as Z = z 1 , . . . , z N , where z i = g(x i ; θ sh ), we can state the following upper bound as a direct consequence of the chain rule:</p><formula xml:id="formula_14">T t=1 α t ∇ θ shL t (θ sh , θ t ) 2 2 ≤ ∂Z ∂θ sh 2 2 T t=1 α t ∇ ZL t (θ sh , θ t ) 2 2<label>(6)</label></formula><p>where ∂Z ∂θ sh 2 is the matrix norm of the Jacobian of Z with respect to θ sh . Two desirable properties of this upper bound are that (i) ∇ ZL t (θ sh , θ t ) can be computed in a single backward pass for all tasks and (ii) ∂Z ∂θ sh 2 2 is not a function of α 1 , . . . , α T , hence it can be removed when it is used as an optimization objective. We replace the T t=1 α t ∇ θ shL t (θ sh , θ t ) 2 2 term with the upper bound we have just derived in order to obtain the approximate optimization problem and drop the ∂Z ∂θ sh 2 2 term since it does not affect the optimization. The resulting optimization problem is</p><formula xml:id="formula_15">min α 1 ,...,α T T t=1 α t ∇ ZL t (θ sh , θ t ) 2 2 T t=1 α t = 1, α t ≥ 0 ∀t (MGDA-UB)</formula><p>We refer to this problem as MGDA-UB (Multiple Gradient Descent Algorithm -Upper Bound).</p><p>In practice, MGDA-UB corresponds to using the gradients of the task losses with respect to the representations instead of the shared parameters. We use Algorithm 2 with only this change as the final method.</p><p>Although MGDA-UB is an approximation of the original optimization problem, we now state a theorem that shows that our method produces a Pareto optimal solution under mild assumptions. The proof is given in the supplement.</p><p>Theorem 1 Assume ∂Z ∂θ sh is full-rank. If α 1,...,T is the solution of MGDA-UB, one of the following is true:</p><p>(a) T t=1 α t ∇ θ shL t (θ sh , θ t ) = 0 and the current parameters are Pareto stationary.</p><formula xml:id="formula_16">(b) T t=1 α t ∇ θ shL t (θ sh , θ t ) is a descent direction that decreases all objectives.</formula><p>This result follows from the fact that as long as ∂Z ∂θ sh is full rank, optimizing the upper bound corresponds to minimizing the norm of the convex combination of the gradients using the Mahalonobis norm defined by ∂Z ∂θ sh ∂Z ∂θ sh . The non-singularity assumption is reasonable as singularity implies that tasks are linearly related and a trade-off is not necessary. In summary, our method provably finds a Pareto stationary point with negligible computational overhead and can be applied to any deep multi-objective problem with an encoder-decoder model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the presented MTL method on a number of problems. First, we use MultiMNIST <ref type="bibr" target="#b43">(Sabour et al., 2017)</ref>, an MTL adaptation of <ref type="bibr">MNIST (LeCun et al., 1998)</ref>. Next, we tackle multi-label classification on the CelebA dataset <ref type="bibr" target="#b28">(Liu et al., 2015b)</ref> by considering each label as a distinct binary classification task. These problems include both classification and regression, with the number of tasks ranging from 2 to 40. Finally, we experiment with scene understanding, jointly tackling the tasks of semantic segmentation, instance segmentation, and depth estimation on the Cityscapes dataset <ref type="bibr" target="#b7">(Cordts et al., 2016)</ref>. We discuss each experiment separately in the following subsections.</p><p>The baselines we consider are (i) uniform scaling: minimizing a uniformly weighted sum of loss functions 1 T t L t , (ii) single task: solving tasks independently, (iii) grid search: exhaustively trying various values from {c t ∈ [0, 1]| t c t = 1} and optimizing for 1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MultiMNIST</head><p>Our initial experiments are on MultiMNIST, an MTL version of the MNIST dataset <ref type="bibr" target="#b43">(Sabour et al., 2017)</ref>. In order to convert digit classification into a multi-task problem, <ref type="bibr" target="#b43">Sabour et al. (2017)</ref> overlaid multiple images together. We use a similar construction. For each image, a different one is chosen uniformly in random. Then one of these images is put at the top-left and the other one is at the bottom-right. The resulting tasks are: classifying the digit on the top-left (task-L) and classifying the digit on the bottom-right (task-R). We use 60K examples and directly apply existing single-task MNIST models. The MultiMNIST dataset is illustrated in the supplement.</p><p>We use the LeNet architecture <ref type="bibr" target="#b25">(LeCun et al., 1998)</ref>. We treat all layers except the last as the representation function g and put two fully-connected layers as task-specific functions (see the  <ref type="bibr" target="#b28">(Liu et al., 2015b)</ref>. Lower is better. We divide attributes into two sets for legibility: easy on the left, hard on the right. Zoom in for details. supplement for details). We visualize the performance profile as a scatter plot of accuracies on task-L and task-R in <ref type="figure">Figure 3</ref>, and list the results in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>In this setup, any static scaling results in lower accuracy than solving each task separately (the singletask baseline). The two tasks appear to compete for model capacity, since increase in the accuracy of one task results in decrease in the accuracy of the other. Uncertainty weighting <ref type="bibr" target="#b22">(Kendall et al., 2018)</ref> and GradNorm <ref type="bibr" target="#b5">(Chen et al., 2018)</ref> find solutions that are slightly better than grid search but distinctly worse than the single-task baseline. In contrast, our method finds a solution that efficiently utilizes the model capacity and yields accuracies that are as good as the single-task solutions. This experiment demonstrates the effectiveness of our method as well as the necessity of treating MTL as multi-objective optimization. Even after a large hyper-parameter search, any scaling of tasks does not approach the effectiveness of our method. Next, we tackle multi-label classification. Given a set of attributes, multi-label classification calls for deciding whether each attribute holds for the input. We use the CelebA dataset <ref type="bibr" target="#b28">(Liu et al., 2015b)</ref>, which includes 200K face images annotated with 40 attributes. Each attribute gives rise to a binary classification task and we cast this as a 40-way MTL problem. We use ResNet-18 <ref type="bibr" target="#b16">(He et al., 2016)</ref> without the final layer as a shared representation function, and attach a linear layer for each attribute (see the supplement for further details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Label Classification</head><p>We plot the resulting error for each binary classification task as a radar chart in <ref type="figure" target="#fig_1">Figure 2</ref>. The average over them is listed in <ref type="table" target="#tab_0">Table 1</ref>. We skip grid search since it is not feasible over 40 tasks. Although uniform scaling is the norm in the multi-label classification literature, single-task performance is significantly better. Our method outperforms baselines for significant majority of tasks and achieves comparable performance in rest. This experiment also shows that our method remains effective when the number of tasks is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scene Understanding</head><p>To evaluate our method in a more realistic setting, we use scene understanding. Given an RGB image, we solve three tasks: semantic segmentation (assigning pixel-level class labels), instance segmentation (assigning pixel-level instance labels), and monocular depth estimation (estimating continuous disparity per pixel). We follow the experimental procedure of <ref type="bibr" target="#b22">Kendall et al. (2018)</ref> and use an encoder-decoder architecture. The encoder is based on ResNet-50 <ref type="bibr" target="#b16">(He et al., 2016)</ref> and is shared by all three tasks. The decoders are task-specific and are based on the pyramid pooling module <ref type="bibr" target="#b54">(Zhao et al., 2017</ref>) (see the supplement for further implementation details).</p><p>Since the output space of instance segmentation is unconstrained (the number of instances is not known in advance), we use a proxy problem as in <ref type="bibr" target="#b22">Kendall et al. (2018)</ref>. For each pixel, we estimate the location of the center of mass of the instance that encompasses the pixel. These center votes can then be clustered to extract the instances. In our experiments, we directly report the MSE in the proxy task. <ref type="figure">Figure 4</ref> shows the performance profile for each pair of tasks, although we perform all experiments on all three tasks jointly. The pairwise performance profiles shown in <ref type="figure">Figure 4</ref> are simply 2D projections of the three-dimensional profile, presented this way for legibility. The results are also listed in <ref type="table" target="#tab_4">Table 4</ref>. MTL outperforms single-task accuracy, indicating that the tasks cooperate and help each other. Our method outperforms all baselines on all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Role of the Approximation</head><p>In order to understand the role of the approximation proposed in Section 3.3, we compare the final performance and training time of our algorithm with and without the presented approximation in <ref type="table" target="#tab_1">Table 2</ref> (runtime measured on a single Titan Xp GPU). For a small number of tasks (3 for scene understanding), training time is reduced by 40%. For the multi-label classification experiment (40 tasks), the presented approximation accelerates learning by a factor of 25.</p><p>On the accuracy side, we expect both methods to perform similarly as long as the full-rank assumption is satisfied. As expected, the accuracy of both methods is very similar. Somewhat surprisingly, our approximation results in slightly improved accuracy in all experiments. While counter-intuitive at first, we hypothesize that this is related to the use of SGD in the learning algorithm. Stability analysis in convex optimization suggests that if gradients are computed with an error∇ θ L t = ∇ θ L t + e t (θ corresponds to θ sh in (3)), as opposed to Z in the approximate problem in (MGDA-UB), the error in the solution is bounded as α − α 2 ≤ O(max t e t 2 ). Considering the fact that the gradients are computed over the full parameter set (millions of dimensions) for the original problem and over a smaller space for the approximation (batch size times representation which is in the thousands), the dimension of the error vector is significantly higher in the original problem. We expect the l 2 norm of such a random vector to depend on the dimension.</p><p>In summary, our quantitative analysis of the approximation suggests that (i) the approximation does not cause an accuracy drop and (ii) by solving an equivalent problem in a lower-dimensional space, our method achieves both better computational efficiency and higher stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We described an approach to multi-task learning. Our approach is based on multi-objective optimization. In order to apply multi-objective optimization to MTL, we described an efficient algorithm as well as specific approximations that yielded a deep MTL algorithm with almost no computational overhead. Our experiments indicate that the resulting algorithm is effective for a wide range of multi-task scenarios.  <ref type="figure">Figure 3</ref>: MultiMNIST accuracy profile. We plot the obtained accuracy in detecting the left and right digits for all baselines. The grid-search results suggest that the tasks compete for model capacity. Our method is the only one that finds a solution that is as good as training a dedicated model for each task. Top-right is better.   <ref type="figure">Figure 4</ref>: Cityscapes performance profile. We plot the performance of all baselines for the tasks of semantic segmentation, instance segmentation, and depth estimation. We use mIoU for semantic segmentation, error of per-pixel regression (normalized to image size) for instance segmentation, and disparity error for depth estimation. To convert errors to performance measures, we use 1 − instance error and 1/disparity error. We plot 2D projections of the performance profile for each pair of tasks. Although we plot pairwise projections for visualization, each point in the plots solves all tasks. Top-right is better.</p><p>The KKT condition for this Lagrangian yields the desired result as</p><formula xml:id="formula_17">T t=1 α t ∇ ZL t ∇ ZL t = λ 2 ≥ 0 (12)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Results on Multi-label Classification</head><p>In this section, we present the experimental results we did not include in the main text.</p><p>In the main text, we plotted a radar chart of the binary attribute classification errors. However, we did not include the tabulated results due to the space limitations. Here we list the binary classification error of each attribute for each algorithm in <ref type="table" target="#tab_5">Table 5</ref>. C Implementation Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 MultiMNIST</head><p>We use the MultiMNIST dataset, which overlays multiple images together <ref type="bibr" target="#b43">(Sabour et al., 2017)</ref>. For each image, a different one is chosen uniformly in random. One of these images is placed at the top-left and the other at the bottom-right. We show sample MultiMNIST images in <ref type="figure">Figure 6</ref>. For the MultiMNIST experiments, we use an architecture based on <ref type="bibr">LeNet (LeCun et al., 1998)</ref>. We use all layers except the final one as a shared encoder. We use the fully-connected layer as a task-specific function for the left and right tasks by simply adding two independent fully-connected layers, each taking the output of the shared encoder as input. As a task-specific loss function, we use the cross-entropy loss with a softmax for both tasks. The architecture is visualized in <ref type="figure" target="#fig_2">Figure 5</ref>.</p><p>The implementation uses PyTorch <ref type="bibr" target="#b35">(Paszke et al., 2017)</ref>. For all baselines, we searched over the set LR = {1e−4, 5e−4, 1e−3, 5e−3, 1e−2, 5e−2} of learning rates and chose the model with the highest validation accuracy. We used SGD with momentum, halving the learning rate every 30 epochs. We use batch size 256 and train for 100 epochs. We report test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Multi-label classification</head><p>For multi-label classification experiments, we use ResNet-18 <ref type="bibr" target="#b16">(He et al., 2016)</ref> without the final layer as a shared representation function. Since there are 40 attributes, we add 40 separate 2048 × 2 dimensional fully-connected layers as task-specific functions. The final two-dimensional output is passed through a 2-class softmax to get binary attribute classification probabilities. We use cross-entropy as a task-specific loss. The architecture is visualized in <ref type="figure" target="#fig_3">Figure 7</ref>.</p><p>The implementation uses PyTorch <ref type="bibr" target="#b35">(Paszke et al., 2017)</ref>. We resize each CelebA image <ref type="bibr" target="#b28">(Liu et al., 2015b)</ref> to 64 × 64 × 3. For all experiments, we searched over the set LR = {1e−4, 5e−4, 1e−3, 5e−3, 1e−2, 5e−2} of learning rates and chose the model with the highest validation accuracy. We used SGD with momentum, halving the learning rate every 30 epochs. We use batch size 256 and train for 100 epochs. We report attribute-wise binary accuracies on the test set as well as the average accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Scene understanding</head><p>For scene understanding experiments, we use the Cityscapes dataset <ref type="bibr" target="#b7">(Cordts et al., 2016)</ref>. We resize all images to resolution 256 × 512 for computational efficiency. As a shared representation function (encoder), we use the ResNet-50 architecture <ref type="bibr" target="#b16">(He et al., 2016)</ref> in fully-convolutional fashion. We take the ResNet-50 architecture and only use layers prior to average pooling that are fully convolutional. As a decoder, we use the pyramid pooling module <ref type="bibr" target="#b54">(Zhao et al., 2017)</ref> and set the output sizes to 256 × 512 × 19 for semantic segmentation (19 classes), 256 × 512 × 2 for instance segmentation (one output channel for the x-offset of the center location and another channel for the y-offset), and 256 × 512 × 1 for monocular depth estimation. For instance segmentation, we use the proxy task of estimating the offset for the center location of the instance that encompasses the pixel. We directly estimate disparity instead of depth and later convert it to depth using the provided camera intrinsics. As a loss function, we use cross-entropy with a softmax for semantic segmentation, and MSE for depth and instance segmentation. We visualize the architecture in <ref type="figure" target="#fig_5">Figure 8</ref>.</p><p>We initialize the encoder with a model pretrained on ImageNet <ref type="bibr" target="#b9">(Deng et al., 2009)</ref>. We use the implementation of the pyramidal pooling network with bilinear interpolation shared by <ref type="bibr" target="#b55">Zhou et al. (2017a)</ref>. Ground-truth results for the Cityscapes test set are not publicly available. Therefore, we report numbers on the validation set. As a validation set for hyperparameter search, we randomly choose 275 images from the training set. After the best hyperparameters are chosen, we retrain with the full training set and report the metrics on the Cityscapes validation set, which our algorithm never sees during training or hyperparameter search. As metrics, we use mean intersection over union (mIoU) for semantic segmentation, MSE for instance segmentation, and MSE for disparities (depth estimation). We directly report the metric in the proxy task for instance segmentation instead of performing a further clustering operation. For all experiments, we searched over the set LR = {1e−4, 5e−4, 1e−3, 5e−3, 1e−2, 5e−2} of learning rates and chose the model with the highest <ref type="figure">Figure 6</ref>: Sample MultiMNIST images. In each image, one task (task-L) is classifying the digit on the top-left and the second task (task-R) is classifying the digit on the bottom-right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet 18:</head><p>ResNet 18 without the final fully connected layer. validation accuracy. We used SGD with momentum, halving the learning rate every 30 epochs. We use batch size 8 and train for 250 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional ResNet 50:</head><p>ResNet 50 without final average poling and fully connected layer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pyramid Pooling Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pyramid Pooling Network</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Tt</head><label></label><figDesc>c t L t , (iv)<ref type="bibr" target="#b22">Kendall et al. (2018)</ref>: using the uncertainty weighting proposed by<ref type="bibr" target="#b22">Kendall et al. (2018)</ref>, and (v) GradNorm: using the normalization proposed by<ref type="bibr" target="#b5">Chen et al. (2018)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Radar charts of percentage error per attribute on CelebA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Architecture used for MultiMNIST experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Architecture used for multi-label classification experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Architecture used for scene understanding experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Mean of error per</cell></row><row><cell>category of MTL algorithms</cell></row><row><cell>in multi-label classification on</cell></row><row><cell>CelebA (Liu et al., 2015b).</cell></row><row><cell>Average</cell></row><row><cell>error</cell></row><row><cell>Single task 8.77</cell></row><row><cell>Uniform scaling 9.62</cell></row><row><cell>Kendall et al. 2018 9.53</cell></row><row><cell>GradNorm 8.44</cell></row><row><cell>Ours 8.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effect of the MGDA-UB approximation. We report the final accuracies as well as training times for our method with and without the approximation.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Scene understanding (3 tasks)</cell><cell></cell><cell cols="2">Multi-label (40 tasks)</cell></row><row><cell></cell><cell cols="4">Training Segmentation Instance Disparity</cell><cell>Training</cell><cell>Average</cell></row><row><cell></cell><cell>time</cell><cell cols="4">mIoU [%] error [px] error [px] time (hour)</cell><cell>error</cell></row><row><cell>Ours (w/o approx.)</cell><cell>38.6</cell><cell>66.13</cell><cell>10.28</cell><cell>2.59</cell><cell>429.9</cell><cell>8.33</cell></row><row><cell>Ours</cell><cell>23.3</cell><cell>66.63</cell><cell>10.25</cell><cell>2.54</cell><cell>16.1</cell><cell>8.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of MTL algorithms on MultiMNIST. Single-task baselines solve tasks separately, with dedicated models, but are shown in the same row for clarity.</figDesc><table><row><cell></cell><cell>Left digit</cell><cell>Right digit</cell></row><row><cell></cell><cell cols="2">accuracy [%] accuracy [%]</cell></row><row><cell>Single task</cell><cell>97.23</cell><cell>95.90</cell></row><row><cell>Uniform scaling</cell><cell>96.46</cell><cell>94.99</cell></row><row><cell>Kendall et al. 2018</cell><cell>96.47</cell><cell>95.29</cell></row><row><cell>GradNorm</cell><cell>96.27</cell><cell>94.84</cell></row><row><cell>Ours</cell><cell>97.26</cell><cell>95.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">: Performance of MTL algorithms in</cell></row><row><cell cols="4">joint semantic segmentation, instance segmenta-</cell></row><row><cell cols="4">tion, and depth estimation on Cityscapes. Single-</cell></row><row><cell cols="4">task baselines solve tasks separately but are</cell></row><row><cell cols="3">shown in the same row for clarity.</cell><cell></cell></row><row><cell></cell><cell cols="3">Segmentation Instance Disparity</cell></row><row><cell></cell><cell cols="3">mIoU [%] error [px] error [px]</cell></row><row><cell>Single task</cell><cell>60.68</cell><cell>11.34</cell><cell>2.78</cell></row><row><cell>Uniform scaling</cell><cell>54.59</cell><cell>10.38</cell><cell>2.96</cell></row><row><cell>Kendall et al. 2018</cell><cell>64.21</cell><cell>11.54</cell><cell>2.65</cell></row><row><cell>GradNorm</cell><cell>64.81</cell><cell>11.31</cell><cell>2.57</cell></row><row><cell>Ours</cell><cell>66.63</cell><cell>10.25</cell><cell>2.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Multi-label classification error per attribute for all algorithms.</figDesc><table><row><cell></cell><cell cols="4">Uniform Single Kendall Grad</cell><cell></cell><cell></cell><cell cols="4">Uniform Single Kendall Grad</cell></row><row><cell></cell><cell>scaling</cell><cell>task</cell><cell>et al.</cell><cell cols="2">Norm Ours</cell><cell></cell><cell>scaling</cell><cell>task</cell><cell>et al.</cell><cell>Norm Ours</cell></row><row><cell>Attr. 0</cell><cell>7.11</cell><cell>7.16</cell><cell>7.18</cell><cell>6.54</cell><cell>6.17</cell><cell>Attr. 5</cell><cell>4.91</cell><cell>4.75</cell><cell>4.95</cell><cell>4.19 4.13</cell></row><row><cell>Attr. 1</cell><cell>17.30</cell><cell cols="2">14.38 16.77</cell><cell cols="2">14.80 14.87</cell><cell>Attr. 6</cell><cell>20.97</cell><cell cols="2">14.24 15.17</cell><cell>14.07 14.08</cell></row><row><cell>Attr. 2</cell><cell>20.99</cell><cell cols="2">19.25 20.56</cell><cell cols="2">18.97 18.35</cell><cell>Attr. 7</cell><cell>18.53</cell><cell cols="2">17.74 18.84</cell><cell>17.33 17.25</cell></row><row><cell>Attr. 3</cell><cell>17.82</cell><cell cols="2">16.79 18.45</cell><cell cols="2">16.47 16.06</cell><cell>Attr. 8</cell><cell>10.22</cell><cell cols="2">8.87 10.19</cell><cell>8.67 8.42</cell></row><row><cell>Attr. 4</cell><cell>1.25</cell><cell>1.20</cell><cell>1.17</cell><cell>1.13</cell><cell>1.08</cell><cell>Attr. 9</cell><cell>5.29</cell><cell>5.09</cell><cell>5.44</cell><cell>4.68 4.60</cell></row><row><cell>Attr. 10</cell><cell>4.14</cell><cell>4.02</cell><cell>4.33</cell><cell>3.77</cell><cell>3.60</cell><cell>Attr. 15</cell><cell>0.81</cell><cell>0.52</cell><cell>0.62</cell><cell>0.56 0.56</cell></row><row><cell cols="2">Attr. 11 16.22</cell><cell cols="2">15.34 16.64</cell><cell cols="2">14.73 14.56</cell><cell>Attr. 16</cell><cell>4.00</cell><cell>3.94</cell><cell>3.99</cell><cell>3.72 3.46</cell></row><row><cell>Attr. 12</cell><cell>8.42</cell><cell>7.68</cell><cell>8.85</cell><cell>7.23</cell><cell>7.41</cell><cell>Attr. 17</cell><cell>2.39</cell><cell>2.66</cell><cell>2.35</cell><cell>2.09 2.16</cell></row><row><cell>Attr. 13</cell><cell>5.17</cell><cell>5.15</cell><cell>5.26</cell><cell>4.75</cell><cell>4.52</cell><cell>Attr. 18</cell><cell>8.79</cell><cell>9.01</cell><cell>8.84</cell><cell>8.00 7.83</cell></row><row><cell>Attr. 14</cell><cell>4.14</cell><cell>4.13</cell><cell>4.17</cell><cell>3.73</cell><cell>3.54</cell><cell cols="2">Attr. 19 13.78</cell><cell cols="2">12.27 13.86</cell><cell>11.79 11.29</cell></row><row><cell>Attr. 20</cell><cell>1.61</cell><cell>1.61</cell><cell>1.58</cell><cell>1.42</cell><cell>1.43</cell><cell cols="2">Attr. 25 27.59</cell><cell cols="2">24.82 26.94</cell><cell>24.26 23.87</cell></row><row><cell>Attr. 21</cell><cell>7.18</cell><cell>6.20</cell><cell>7.73</cell><cell>6.91</cell><cell>6.26</cell><cell>Attr. 26</cell><cell>3.54</cell><cell>3.40</cell><cell>3.78</cell><cell>3.22 3.16</cell></row><row><cell>Attr. 22</cell><cell>4.38</cell><cell>4.14</cell><cell>4.08</cell><cell>3.88</cell><cell>3.81</cell><cell cols="2">Attr. 27 26.74</cell><cell cols="2">22.74 26.21</cell><cell>23.12 22.45</cell></row><row><cell>Attr. 23</cell><cell>8.32</cell><cell>6.57</cell><cell>8.80</cell><cell>6.54</cell><cell>6.47</cell><cell>Attr. 28</cell><cell>6.14</cell><cell>5.82</cell><cell>6.17</cell><cell>5.43 5.16</cell></row><row><cell>Attr. 24</cell><cell>5.01</cell><cell>5.38</cell><cell>5.12</cell><cell>4.63</cell><cell>4.23</cell><cell>Attr. 29</cell><cell>5.55</cell><cell>5.18</cell><cell>5.40</cell><cell>5.13 4.87</cell></row><row><cell>Attr. 30</cell><cell>3.29</cell><cell>3.79</cell><cell>3.24</cell><cell>2.94</cell><cell>3.03</cell><cell>Attr. 35</cell><cell>1.15</cell><cell>1.13</cell><cell>1.08</cell><cell>0.94 1.08</cell></row><row><cell>Attr. 31</cell><cell>8.05</cell><cell>7.18</cell><cell>8.40</cell><cell>7.21</cell><cell>6.92</cell><cell>Attr. 36</cell><cell>7.91</cell><cell>7.56</cell><cell>8.06</cell><cell>7.47 7.18</cell></row><row><cell cols="2">Attr. 32 18.21</cell><cell cols="2">17.25 18.15</cell><cell cols="2">15.93 15.93</cell><cell cols="2">Attr. 37 13.27</cell><cell cols="2">11.90 13.47</cell><cell>11.61 11.19</cell></row><row><cell cols="2">Attr. 33 16.53</cell><cell cols="2">15.55 16.19</cell><cell cols="2">13.93 13.80</cell><cell>Attr. 38</cell><cell>3.80</cell><cell>3.29</cell><cell>4.04</cell><cell>3.57 3.51</cell></row><row><cell cols="2">Attr. 34 11.12</cell><cell cols="2">9.76 11.46</cell><cell>10.17</cell><cell>9.73</cell><cell cols="2">Attr. 39 13.25</cell><cell cols="2">13.40 13.78</cell><cell>12.26 11.95</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This definition can be extended to the partially-labelled case by extending Y t with a null label.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theorem 1</head><p>Proof. We begin by showing that if the optimum value of (MGDA-UB) is 0, so is the optimum value of (3). This shows the first case of the theorem. Then, we will show the second part.</p><p>If the optimum value of (MGDA-UB) is 0,</p><p>Hence α 1 , . . . , α T is the solution of (3) and the optimal value of (3) is 0. This proves the first case of the theorem. Before we move to the second case, we state a straightforward corollary. Since ∂Z ∂θ sh is full rank, this equivalence is bi-directional. In other words, if α 1 , . . . , α T is the solution of (3), it is the solution of (MGDA-UB) as well. Hence, both formulations completely agree on Pareto stationarity.</p><p>In order to prove the second case, we need to show that the resulting descent direction computed by solving (MGDA-UB) does not increase any of the loss functions. Formally, we need to show that</p><p>This condition is equivalent to</p><p>where M = ∂Z ∂θ sh ∂Z ∂θ sh . Since M is positive definite (following the assumption), this is further equivalent to</p><p>We show that this follows from the optimality conditions for (MGDA-UB). The Lagrangian of (MGDA-UB) is</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Content-based image retrieval through a multi-agent meta-learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagherjeiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vilalta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Eick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Tools with Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Task clustering and gating for Bayesian multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="83" to="99" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A model of inductive bias learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="149" to="198" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Integrated perception with recurrent multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, 2016. R. Caruana. Multitask learning. Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GradNorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining a multi-objective optimization approach with meta-learning for SVM parameter selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B C</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B C</forename><surname>Prudêncio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C P L F</forename><surname>De Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple-gradient descent algorithm (MGDA) for multiobjective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-A</forename><surname>Désidéri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes Rendus Mathematique</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multicriteria Optimization (2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ehrgott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Steepest descent methods for multicriteria optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fliege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Svaiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Methods of Operations Research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="479" to="494" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards Pareto descent directions in sampling experts for multiple tasks in an on-line learning paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Lifelong Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A joint many-task model: Growing a neural network for multiple NLP tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predictive entropy search for multi-objective bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rapid adaptation for deep neural networks through multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting Frank-Wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">One model to learn them all</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05137</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">UberNet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonlinear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Second Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley, Calif</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pareto-path multi-task multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgiopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3190</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02117</idno>
		<title level="m">Learning multiple tasks with deep relationship networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06114</idno>
		<title level="m">Multi-task sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An efficient algorithm for finding the minimum norm point in the convex hull of a finite point set in the plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Makimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="40" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Nonlinear Multiobjective Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miettinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Policy gradient approaches for multi-objective sequential decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pirotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smacchia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bascetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient-based multiobjective optimization with uncertainties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dellnitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NEO</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inverse reinforcement learning through policy gradient minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pirotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Descent algorithm for nonsmooth stochastic multiobjective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Désidéri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="317" to="331" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey of multi-objective sequential decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roijers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vamplew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dazeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="67" to="113" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Routing networks: Adaptive selection of non-linear functions for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01239</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MOON: A mixed objective optimization network for the recognition of facial attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Rudd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stochastic method for the solution of unconstrained vector optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schäffler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinzierl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="222" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A recursive algorithm for finding the minimum norm point in a polytope and a pair of closest points in two polytopes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sekitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="page" from="233" to="249" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-task learning in deep neural networks for improved phoneme recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pareto frontier learning with expensive correlated objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Inadmissibility of the usual estimator for the mean of a multivariate normal distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1956" />
			<pubPlace>Stanford University, US</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Finding the nearest point in a polytope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="149" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-task learning for classification with dirichlet process priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="35" to="63" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Trace norm regularised deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04038</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A convex formulation for learning task relationships in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Multi-task multi-view learning based on cooperative multiobjective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Clustered multi-task learning via alternating structure optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">MALSAR: Multi-task learning via structural regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Arizona State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
