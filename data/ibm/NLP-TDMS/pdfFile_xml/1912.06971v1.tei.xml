<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Hanqing</forename><forename type="middle">Lu</forename></persName>
						</author>
						<title level="a" type="main">Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Skeleton-based action recognition</term>
					<term>graph convo- lutional network</term>
					<term>adaptive graph</term>
					<term>muti-stream network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional networks (GCNs), which generalize CNNs to more generic non-Euclidean structures, have achieved remarkable performance for skeleton-based action recognition. However, there still exist several issues in the previous GCN-based models. First, the topology of the graph is set heuristically and fixed over all the model layers and input data. This may not be suitable for the hierarchy of the GCN model and the diversity of the data in action recognition tasks. Second, the second-order information of the skeleton data, i.e., the length and orientation of the bones, is rarely investigated, which is naturally more informative and discriminative for the human action recognition. In this work, we propose a novel multistream attention-enhanced adaptive graph convolutional neural network (MS-AAGCN) for skeleton-based action recognition. The graph topology in our model can be either uniformly or individually learned based on the input data in an end-to-end manner. This data-driven approach increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Besides, the proposed adaptive graph convolutional layer is further enhanced by a spatial-temporalchannel attention module, which helps the model pay more attention to important joints, frames and features. Moreover, the information of both the joints and bones, together with their motion information, are simultaneously modeled in a multistream framework, which shows notable improvement for the recognition accuracy. Extensive experiments on the two largescale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Action recognition has been widely researched since it plays a significant role in many applications such as video surveillance and human-computer interaction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Recently, compared with conventional methods that use RGB videos for recognition, the skeleton-based action recognition draws increasingly more attention due to its strong adaptability to the dynamic circumstance and complicated background <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Early deep-learning-based approaches for skeleton-based action recognition manually structure the skeleton data as a sequence of joint-coordinate vectors <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> or as a pseudo-image <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which is fed into RNNs or CNNs to generate the prediction. However, representing the skeleton data as a vector sequence or a 2D grid cannot fully express the dependencies between correlated joints. The skeleton is naturally structured as a graph in a non-Euclidean space with the joints as vertexes and their natural connections in the human body as edges. Previous approaches cannot exploit the graph structure of the skeleton data and are difficult to generalize to skeletons with arbitrary forms. Recently, graph convolutional networks (GCNs), which generalize convolution from image to graph, have been successfully adopted in many applications <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. For the skeleton-based action recognition task, Yan et al. <ref type="bibr" target="#b19">[20]</ref> first use GCNs to model the skeleton data. They construct a spatial graph based on the natural connections of the joints on the human body and add temporal edges between the corresponding joints in consecutive frames. A distance-based sampling function is proposed for constructing the graph convolutional layer, which is then used as a basic block to build the final spatiotemporal graph convolutional network (ST-GCN).</p><p>However, there are three disadvantages for the process of the graph construction in ST-GCN <ref type="bibr" target="#b19">[20]</ref>: <ref type="bibr" target="#b0">(1)</ref> The skeleton graph used in ST-GCN is heuristically predefined based on the natural connectivity of the human body. Thus it is not guaranteed to be optimal for the action recognition task. For example, the relationship between the two hands is important for recognizing classes such as "clapping" and "reading." However, it is difficult for ST-GCN to capture the dependency between the two hands since they are located far away from each other in the predefined human-body-based graphs. <ref type="bibr" target="#b1">(2)</ref> The neural networks are hierarchical where different layers contain different levels of semantics. However, the topology of the graph applied in ST-GCN is fixed over all the layers, which lacks the flexibility and capacity to model the multilevel semantics contained in different layers. (3) One fixed graph structure may not be optimal for all the samples of different action classes. For classes such as "wiping face" and "touching head", the connection between the hands and head should be stronger, but it is not true for some other classes, such as "jumping up" and "sitting down". This fact suggests that the graph structure should be data dependent, which, however, is not supported in ST-GCN.</p><p>To solve the above problems, a novel adaptive graph convolutional layer is proposed in this work. It parameterizes two kinds of adaptive graphs for graph convolution. One is referred as the global graph, which is obtained by learning the graph adjacency matrix based on the knowledge distilled from the dataset. The learning process uses the task-based loss. Thus the obtained graph topology is more suitable than previous humanbody-based graph for the action recognition task. Another is referred as the individual graph, whose edges are built according to the feature similarity between graph vertexes. Since the data samples are diverse, the module can capture a unique structure for each input. The two kinds of graphs are fused using a gating mechanism, which can adaptively adjust their importance in each of the model layers. Note that both of the graphs are optimized individually across different layers, thus it can better fit the hierarchical structure of the neural networks. In a word, this data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples.</p><p>Besides, since the attention mechanism has been demonstrated the effectiveness and necessity in many tasks <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, it is necessary to investigate it for the skeleton-based action recognition. From the spatial perspective, a certain kind of action is usually associated with and characterized by a key subset of the joints. From the temporal perspective, an action flow may contains multiple stages where different substages or frames have different degrees of importance for the final recognition. From the feature perspective, multiple channels of a convolutional feature map contain multiple levels of semantics. Each channel plays different roles for different actions and data samples. These observations inspire us to design a spatial-temporal-channel (STC) attention module to adaptively recalibrate the activations of the joints, frames and channels for different data samples. The module is plugged in each graph convolutional layer, with small amount of parameters yet encouraging performance improvement.</p><p>Another notable problem in previous approaches is that the feature vector attached to each vertex only contains the 2D or 3D coordinates of joints. We referred it as the the first-order information of the skeleton data. However, the second-order information, which represents the feature of bones between two joints, is not exploited. Typically, the lengths and directions of bones are naturally more informative and discriminative for action recognition. In this work, we formulate the bone information as a vector pointing from its source joint to its target joint. Moreover, since the optical flow field have been demonstrated a usefulmodality in the temporal stream for the RGB-based action recognition <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, for skeleton data, we propose to extract the coordinate differences of the joints and the bones between two consecutive frames as the motion information to help modeling the temporal evolution of the action. Finally, both the joint information and the bone information, together with their motion information, are integrated in a multi-stream framework. All of the streams use the same architecture and the sof tmax scores of the four streams are fused to generate the final prediction.</p><p>To verify the superiority of the proposed model, namely, the multi-stream attention enhanced adaptive graph convolutional network (MS-AAGCN), extensive experiments are performed on two large-scale datasets: NTU-RGBD <ref type="bibr" target="#b8">[9]</ref> and Kinetics-Skeleton <ref type="bibr" target="#b33">[34]</ref>. Our model achieves the state-of-the-art performance on both of the datasets for skeleton-based action recognition. We also visualize the learned adaptive graphs and attention maps of the model and analyze the complementary of the four modalities.</p><p>In addition, since the RGB data contains more abundant appearance information than skeletons, we provide a compar-ison with the RGB modality and propose a skeleton-guided cropping strategy to fuse the two modalities. The fused model obtains accuracies of 99% and 96% in the CV and CS benchmarks of the NTU-RGBD dataset, respectively, which exceeds the other methods with a significant margin.</p><p>Overall, the main contributions of our work lie in four folds: (1) An adaptive graph convolutional network is proposed to adaptively learn the topology of the graph in an endto-end manner, which can better suit the action recognition task, the hierarchical structure of the GCNs and the diverse skeleton samples. (2) A STC-attention module is proposed and embedded in each graph convolutional layer, which can help model learn to selectively focus on discriminative joints, frames and channels. (3) The second-order information (bones) of the skeleton data is firstly formulated and combined with the first-order information (joints) in this work, which brings notable improvement for the recognition performance. We further extract the motion information of both joints and bones and integrate these four modalities in a multi-stream framework. (4) The effectiveness of the proposed MS-AAGCN is verified on two large-scale datasets for skeleton-based action recognition. Compared with the baseline method, i.e., ST-GCN, it obtains significant improvements of +7.9% and +8.5% on the CV and CS benchmarks of the NTU-RGBD dataset, respectively. By combing it with the skeleton-guided cropped RGB data, it obtains additional improvements of +2.8% and +6.1%. The code is released for future work and to facilitate communication <ref type="bibr" target="#b0">1</ref> .</p><p>This paper is an extended version of our previous work <ref type="bibr" target="#b34">[35]</ref> in a number of aspects. First, we optimize the composition scheme of the proposed global and individual graphs and introduce a gating mechanism to adaptively adjust the importance of the two graphs. Second, we propose a STC-attention module, which turn out to be effective to help the model paying attention to important joints, frames and features. Third, we extend the previous model to a multi-stream framework, which integrates the motion modality for both of the joints and the bones. We provide more extensive experiments and more comprehensive discussion and visualization to demonstrate the effectiveness and the necessity of the proposed modules and data modalities. The model with these designs achieves the accuracy improvements of +1.1% and +1.5% on the cross-view and cross-subject benchmarks of the NTU-RGBD dataset for skeleton-based recognition, respectively. Finally, we present an effective pose-guided cropping strategy to fuse the skeletons with the RGB modality, which exhibits excellent performance.</p><p>The rest of paper is organized as follows. Sec. II introduces the related work. Sec. III formulates the basic GCNs for the skeleton-based action recognition. The components of our proposed MS-AAGCN is introduced in detail in Sec. III. The ablation study and the comparison with the state-of-theart methods are shown in Sec. V. Sec. VI investigates the fusion of the skeleton data with the RGB modality. Sec. VII provides some quantitative results and discussions. Sec. VII-D concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Skeleton-based action recognition</head><p>Skeleton-based action recognition has been studied extensively in recent years. Here, we only review the works related to our approach. Conventional methods for skeletonbased action recognition usually design handcrafted features to model the human body <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. For example, Vemulapalli et al. <ref type="bibr" target="#b5">[6]</ref> encode the skeletons with their rotations and translations in a Lie group. Fernando et al. <ref type="bibr" target="#b6">[7]</ref> use the rank pooling method to represent the data with the parameters of the ranker.</p><p>With the development of the deep learning, data-driven methods have become the mainstream methods, where the most widely used models are RNNs and CNNs. RNN-based methods usually model the skeleton data as a sequence of the coordinate vectors along both the spatial and temporal dimensions, where each of the vectors represents a human body joint <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Du et al. <ref type="bibr" target="#b7">[8]</ref> use a hierarchical bidirectional RNN model to identify the skeleton sequence, which divides the human body into different parts and sends them to different sub-networks. Song et al. <ref type="bibr" target="#b10">[11]</ref> embed a spatiotemporal attention module in LSTM-based model, so that the network can automatically pay attention to the discriminant spatiotemporal region of the skeleton sequence. Zhang et al. <ref type="bibr" target="#b11">[12]</ref> introduce the mechanism of view transformation in an LSTM-based model, which automatically translates the skeleton data into a more advantageous angle for action recognition. Si et al. <ref type="bibr" target="#b35">[36]</ref> propose a model with spatial reasoning (SRN) and temporal stack learning (TSL), where the SRN can capture the structural information between different body parts and the TSL can model the detailed temporal dynamics.</p><p>CNN-based methods model the skeleton data as a pseudoimage based on the manually designed transformation rules <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b36">[37]</ref>. The CNN-based methods are generally more popular than the RNN-based methods because the CNNs have better parallelizability and are easier for training. Kim et al. <ref type="bibr" target="#b14">[15]</ref> use a one-dimensional residual CNN to identify skeleton sequences where the coordinates of joints are directly concatenated. Liu et al. <ref type="bibr" target="#b16">[17]</ref> propose 10 kinds of spatiotemporal images for skeleton encoding, and enhance these images using visual and motion enhancement methods. Li et al. <ref type="bibr" target="#b18">[19]</ref> use multi-scale residual networks and various data-augmentation strategies for the skeletonbased action recognition. Cao et al. <ref type="bibr" target="#b36">[37]</ref> design a permutation network to learn an optimized order for the rearrangement of the joints.</p><p>However, both the RNNs and CNNs fail to fully represent the structure of the skeleton data because the skeleton data are naturally embedded in the form of graphs rather than a vector sequence or a 2D grid. Recently, Yan et al. <ref type="bibr" target="#b19">[20]</ref> propose a spatiotemporal graph convolutional network (ST-GCN) to directly model the skeleton data as the graph structure. It eliminates the requirement for designing handcrafted transformation rules to transform the skeleton data into vector sequences or pseudoimages, thus achieves better performance. Based on this, Tang et al. <ref type="bibr" target="#b20">[21]</ref> further propose a selection strategy of the key frames with the help of reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph convolutional neural networks</head><p>The input of traditional CNNs is usually low-dimensional regular grids, such as image, video and audio. However, it is not straightforward to use the CNN to model the graph data, which always has arbitrary size and shape. The graph is more common and general in the wild, such as social network, molecule and parse tree. How to operate on graphs has been explored extensively over decade and now the most popular solution is to use the graph convolutional networks (GCNs). The GCNs is similar with the traditional CNNs, but it can generalize the convolution from image to graph of arbitrary size and shape <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p><p>The principle of constructing GCNs mainly follows two streams: spatial perspective and spectral perspective. Spatial perspective methods directly perform convolution on the graph vertexes and their neighbors. The key lies in how to construct the locally connected neighborhoods from the graph which misses the implicit order of vertexes and edges. These methods always extracts the neighbors based on the manually designed rules <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Niepert et al. <ref type="bibr" target="#b24">[25]</ref> sample the neighborhoods for each of the vertexes based on their distances in the graph. A normalization algorithm is proposed to crop excess vertexes and pad dummy vertexes. Wang and Gupta. <ref type="bibr" target="#b41">[42]</ref> represent the video as a graph containing persons and detected objects for action recognition. The neighborhood of each vertex is defined according to the feature similarity and the spatial-temporal relations.</p><p>In contrast to the spatial perspective methods, spectral perspective methods use the eigenvalues and eigen vectors of the graph Laplace matrices. These methods perform graph convolution in the frequency domain with the help of the graph Fourier transform <ref type="bibr" target="#b37">[38]</ref>, which does not need to extract locally connected regions from graphs at each convolution step <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Defferrard et al. <ref type="bibr" target="#b40">[41]</ref> propose to use recurrent Chebyshev polynomials as the filtering scheme which is more efficient than previous polynomial filter. <ref type="bibr">Kipf</ref> and Welling <ref type="bibr" target="#b22">[23]</ref> further simplified this approach using the first-order approximation of the spectral graph convolutions. This work follows the spatial perspective methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRAPH CONVOLUTIONAL NETWORKS</head><p>In this section, we introduce a basic graph convolutional network and its implementation for skeleton-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph construction</head><p>The raw skeleton data in one frame are always represented by a sequence of vectors. Each vector represents the 2D or 3D coordinates of the corresponding human joint. A complete action contains multiple frames with different lengths for different samples. We use a spatiotemporal graph to model the structured information among these joints along both the spatial and temporal dimensions. Here, the spatial dimension refers to the joints in the same frame, and the temporal dimension refers to the same joints over all of the frames. The left sub-figure in <ref type="figure" target="#fig_0">Fig. 1</ref> presents an example of the constructed spatiotemporal skeleton graph, where the joints are represented as the vertexes and their natural connections in the human body are represented as the spatial edges (the orange lines in <ref type="figure" target="#fig_0">Fig. 1</ref>, left). For the temporal dimension, the corresponding joints in two consecutive frames are connected with temporal edges the green lines in <ref type="figure" target="#fig_0">Fig. 1, left)</ref>. The coordinate vector of each joint is set as the attribute of the corresponding vertex. Since the graph is intrinsic and is built based on the natural connectivity of the human body, we refer it as the human-body-based graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph convolution</head><p>Given the graph defined above, for the spatial dimension, the graph convolution operation on vertex v i is formulated as:</p><formula xml:id="formula_0">f out (v i ) = vj ∈Bi 1 Z ij f in (v j ) · w(l i (v j ))<label>(1)</label></formula><p>where f denotes the feature map and v denotes the vertex of the graph. B i denotes the sampling area of the convolution for v i , which is defined as the 1-distance neighboring vertexes (v j ) of the target vertex (v i ). w is the weighting function similar to the traditional convolution operation, which provides a weight vector based on the given input. Note that the number of weight vectors of convolution is fixed, while the number of vertexes in B i is varied. So a mapping function l i is required to map all neighboring vertexes into a fix-numbered subsets each of which is associated with a unique weight vector. The right sub-figure in <ref type="figure" target="#fig_0">Fig. 1</ref> shows this mapping strategy, where × represents the center of gravity of the skeleton. B i is the sampling area enclosed by the curve. In detail, the strategy empirically sets the kernel size as 3 and naturally divides B i into 3 subsets: S i1 is the vertex itself (the red circle in <ref type="figure" target="#fig_0">Fig. 1</ref>, right); S i2 is the centripetal subset, which contains the neighboring vertexes that are closer to the center of gravity (the green dot); S i3 is the centrifugal subset, which contains the neighboring vertexes that are farther from the center of gravity (the blue dot). Z ij denotes the cardinality of S ik that contains v j . It aims to balance the contribution of each subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation</head><p>The implementation of the graph convolution for the spatial dimension is not straightforward. Concretely, the feature map of the network is actually a tensor f ∈ R C×T ×N , where N denotes the number of vertexes, T denotes the temporal length and C denotes the number of channels. To implement the GCN in code, Eq. 1 is transformed into</p><formula xml:id="formula_1">f out = Kv k W k (f in A k )<label>(2)</label></formula><p>where K v denotes the kernel size of the spatial dimension. With the mapping strategy designed above,</p><formula xml:id="formula_2">K v is set to 3. A k = Λ − 1 2 kĀ k Λ − 1 2</formula><p>k , whereĀ k ∈ R N ×N is similar to the adjacency matrix. Its elementĀ ij k indicates whether the vertex v j is in the subset S ik of the vertex v i . It is used to select the connected vertexes in a particular subset from f in for the corresponding weight vector. Λ ii k = j (Ā ij k ) + α is the normalized diagonal matrix. α is set to 0.001 to avoid empty rows. W k ∈ R Cout×Cin×1×1 is the weight vector of the 1 × 1 convolution operation, which represents the weighting function w in Eq. 1.</p><p>For the temporal dimension, since the number of neighbors for each vertex is fixed as 2 (corresponding joints in the two adjacent frames), it is straightforward to perform the graph convolution similar to the classical convolution operation. Concretely, we perform a K t × 1 convolution on the output feature map calculated above, where K t is the kernel size of the temporal dimension.</p><p>With above formulations, multiple layers of spatiotemporal graph convolution operations are applied on the graph to extract the high-level features. The global average pooling layer and the sof tmax classifier are then used to predict the action categories based on the extracted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MULTI-STREAM ATTENTION-ENHANCED ADAPTIVE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRAPH CONVOLUTIONAL NETWORK</head><p>In this section, we introduce the components of the proposed multi-stream attention-enhanced adaptive graph convolutional network (MS-AAGCN) in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Adaptive graph convolutional layer</head><p>The spatiotemporal graph convolution for the skeleton data described above is calculated based on a intrinsic human-bodybased graph, which may not be the best choice as explained in Sec. I. To solve this problem, we propose an adaptive graph convolutional layer. It makes the topology of the graph optimized together with other parameters of the network in an end-to-end learning manner. The graph is unique for different layers and samples, which greatly increases the flexibility of the model. Meanwhile, it is designed as a residual branch, which guarantees the stability of the original model.</p><p>In detail, according to the Eq. 2, the graph is actually determined by the adjacency matrix and the mask, i.e., A k and M k , respectively. A k determines whether there are connections between two vertexes and M k determines the strength of the connection. To make the graph topology adaptive, we modify the Eq. 2 into the following form:</p><formula xml:id="formula_3">f out = Kv k W k f in (B k + αC k )<label>(3)</label></formula><p>The main difference lies in the adjacency matrix of the graph, which is divided into two sub-graphs: B k and C k . The first sub-graph (B k ) is the global graph learned from the data. It represents the graph topology that is more suitable for the action recognition task. It is initialized with the adjacency matrix of the human-body-based graph, i.e., A k in Eq. 2. Different with A k , the elements of B k are parameterized and updated together with other parameters in the training process. There are no constraints on the value of B k , which means that the graph is completely learned according to the training data. With this data-driven method, the model can learn graphs that are fully targeted to the recognition task. B k is unique for each layer, thus is more individualized for different levels of semantics contained in different layers.</p><p>The second sub-graph (C k ) is the individual graph which learns a unique topology for each sample. To determine whether there is a connection between two vertexes and how strong is the connection, we use the normalized embedded Gaussian function to estimate the feature similarity of the two vertexes as:</p><formula xml:id="formula_4">f (v i , v j ) = e θ(vi) T φ(vj ) N j=1 e θ(vi) T φ(vj )<label>(4)</label></formula><p>where N is the number of vertexes. We use the dot product to measure the similarity of the two vertexes in an embedding space. In detail, given the input feature map f in ∈ R Cin×T ×N , we first embed it into the embedding space R Ce×T ×N with two embedding functions, i.e., θ and φ. Here, through extensive experiments, we choose the 1 × 1 convolutional layer as the embedding function. The two embedded feature maps are reshaped to matrix M θk ∈ R N ×CeT and matrix M φk ∈ R CeT ×N . They are then multiplied to obtain a similarity matrix C k ∈ R N ×N , whose element C ij k represents the similarity of the vertex v i and the vertex v j . The value of the matrix is normalized to 0 − 1, which is used as the soft edge of the two vertexes. Since the normalized Gaussian is equipped with a sof tmax operation, we can calculate C k based on Eq.4 as follows:</p><formula xml:id="formula_5">C k = Sof tM ax(f in T W T θk W φk f in )<label>(5)</label></formula><p>where W θ ∈ R Ce×Cin×1×1 and W φ ∈ R Ce×Cin×1×1 are the parameters of the embedding functions θ and φ, respectively. Gating mechanism: The global graph determines the basic graph topology for the action recognition task and the individual graph adds individuality according to the various sample features. In the experiments we found that the individual graph is required more strongly in the top layers than the bottom layers. It is reasonable since the receptive field of the bottom layer is smaller, which limits the ability of learning the graph topology from diverse samples. Furthermore, the information contained in the top layers is more semantic, which is more variable and requires more individuality of the graph topology. The individual graph is easier to meet the requirement because it is constructed based on the input features and is individual for each of the samples. Based on these observations, we use a gating mechanism to adjust the importance of the individual graph for different layers. In detail, the C k is multiplied with <ref type="figure">Fig. 2</ref>. Illustration of the adaptive graph convolutional layer (AGCL). There are two kinds of graphs in each layer, i.e., B k and C k . The orange box indicates that the part is the parameter of the network and is updated during the training process. θ and φ are two embedding functions whose kernel size is (1 × 1). Kv denotes the number of subsets. ⊕ denotes the element-wise addition. ⊗ denotes the matirx multiplication. G is the gate that controls the importance of the two kinds of graphs. The residual box (dotted line) is only needed when C in is not the same as Cout.</p><formula xml:id="formula_6">f f (1 × 1) (1 × 1) (1 × 1) × × × × × × × (1 × 1) = 3 × G</formula><p>a parameterized coefficient α that is unique for each layer and is learned and updated in the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization:</head><p>In the experiments we found that the graph topology changes dramatically in the early stage of the training process, thus causes instable and affect the convergence of the model. To stabilize the training, we tried two strategies. The first strategy is using A k + αB k + βC k as the adjacency matrix where A k is the human-body-based graph which is fixed. The B k , C k , α and β are initialized to be 0, thus the A k will dominate the early stage of the training. The second strategy is initializing the B k with A k and blocking the propagation of the gradient for B k at the early stage of the training process until the training stabilizes. The second strategy is verified slightly better in Sec. V-C. The weights of the embedding functions (θ and φ) and the fusion coefficient α are all initialized as 0.</p><p>The overall architecture of the adaptive graph convolutional layer (AGCL) is shown in <ref type="figure">Fig. 2</ref>. The kernel size of the graph convolution (K v ) is set to 3. w k is the weighting function introduced in Eq. 1, whose parameter is W k introduced in Eq. 3. A residual connection, similar to <ref type="bibr" target="#b42">[43]</ref>, is added for each layer, which allows the layer to be inserted into any existing models without breaking its initial behavior. If the number of input channels is different from the number of output channels, a 1 × 1 convolution (orange box with dashed line in <ref type="figure">Fig. 2</ref>) will be inserted in the residual path to transform the input to match the output in the channel dimension. G is the gate that controls the semantics of the two kinds of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention module</head><p>There have been many formulations for attention modules <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Here, with extensive experiments, we propose a STC-attention module which is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. It contains three sub-modules: spatial attention module, temporal attention module and channel attention module.</p><p>Spatial attention module (SAM) can help the model pay different levels of attention for each of the joints. It is computed as:</p><formula xml:id="formula_7">M s = σ(g s (AvgP ool(f in )) (6)</formula><p>where f in ∈ R Cin×T ×N is the input feature map and is averaged over all of the frames. g s is a 1-D convolutional operation. W gs ∈ R 1×Cin×Ks where K s is the kernel size. σ denotes the Sigmoid activation function. The attention map M s ∈ R 1×1×N is then multiplied to the input feature map in a residual manner for adaptive feature refinement. Temporal attention module (TAM) is similar with the SAM and is computed as:</p><formula xml:id="formula_8">M t = σ(g t (AvgP ool(f in )) (7)</formula><p>where M t ∈ R 1×T ×1 and the definitions of other symbols are similar with Eq. 6</p><p>Channel attention module (CAM) can help models strengthen the discrimitive features (channels) according to the input samples. It generates the attention maps as follows:</p><formula xml:id="formula_9">M c = σ(W 2 (δ(W 1 (AvgP ool(f in )))))<label>(8)</label></formula><p>where f in is averaged over all of the joints and frames. M c ∈ R C×1×1 . W 1 ∈ R (C× C r ) and W 2 ∈ R ( C r ×C) are the weights of two fully-connected layers. δ denotes the ReLu activation function.</p><p>Arrangement of the attention modules: the three submodules introduced above can be placed in different manners: the parallel manner or the sequential manner with different orders. Finally we found that the sequential manner is better, where the order is SAM, TAM and CAM. This will be shown in Sec. V-C</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Basic block</head><p>The convolution along the temporal dimension is the same as the ST-GCN, i.e., performing the K t × 1 convolution on the C × T × N feature maps. Both the spatial GCN and the temporal GCN are followed by a batch normalization (BN) layer and a ReLU layer. As shown in <ref type="figure">Fig. 4</ref>, one basic block is the series of one spatial GCN (Convs), one STC-attention module (STC) and one temporal GCN (Convt). To stabilize the training and ease the gradient propagation, a residual connection is added for each basic block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network architecture</head><p>As shown in <ref type="figure">Fig. 5</ref>, the overall architecture of the network is the stack of these basic blocks <ref type="figure">(Fig. 4</ref>). There are a total of 9 blocks. The numbers of output channels for each block are 64, 64, 64, 128, 128, 128, 256, 256 and 256. A data BN layer is added at the beginning to normalize the input data. A global average pooling layer is performed at the end to pool feature maps of different samples to the same size. The final output is sent to a sof tmax classifier to obtain the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Multi-stream networks</head><p>As introduced in Sec. I, both the first-order information (the coordinates of the joints) and the second-order information (the direction and length of the bones), as well as their motion information, are worth to be investigated for the skeleton-based action recognition task. In this work, we model these four modalities in a multi-stream framework.</p><p>In particular, we define that the joint closer to the center of gravity of the skeleton is the source joint and the joint farther away from the center of gravity is the target joint. Each bone is represented as a vector pointing from its source joint to its target joint. For example, given a bone in frame t with its source joint v i,t = (x i,t , y i,t , z i,t ) and its target joint v j,t = (x j,t , y j,t , z j,t ), the vector of the bone is calculated as e i,j,t = (x j,t − x i,t , y j,t − y i,t , z j,t − z i,t ). Since there are no cycles in the graph of the skeleton data, each bone can be assigned with a unique target joint. Note that the number of joints is one more than the number of bones because the root joint is not assigned to any bones. To simplify the design of the network, we assign an empty bone with its values as 0 to the root joint. Thus both the graph and the network of bones can be designed the same as that of joints. We use J-stream and B-stream to represent the networks of joints and bones, respectively.</p><p>As for the motion information, it is calculated as the difference between the same joints or bones in two consecutive frames. For example, given a joint in frame t, i.e., v i,t = (x i,t , y i,t , z i,t ) and the same joint in frame t + 1, i.e., v i,t+1 = (x i,t+1 , y i,t+1 , z i,t+1 ), the motion information between the v i,t and v i,t+1 is represented as m i,t,t+1 = (</p><formula xml:id="formula_10">x i,t+1 − x i,t , y i,t+1 − y i,t , z i,t+1 − z i,t ).</formula><p>The overall architecture (MS-AAGCN) is shown in <ref type="figure" target="#fig_2">Fig. 6</ref>. The four modalities (joints, bones and their motions) are fed into four streams. Finally, the sof tmax scores of the four streams are fused using weighted summation to obtain the action scores and predict the action label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>To perform a head-to-head comparison with ST-GCN, our experiments are conducted on the same two large-scale action recognition datasets: NTU-RGBD <ref type="bibr" target="#b8">[9]</ref> and Kinetics-Skeleton <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b19">[20]</ref>. First, since the NTU-RGBD dataset is smaller than the Kinetics-Skeleton dataset, we perform exhaustive ablation studies on it to verify the effectiveness of the proposed model components based on the recognition performance. Then, the final model is evaluated on both of the datasets to verify the generality and is compared with the other state-of-the-art approaches for skeleton-based action recognition task. Finally, we fuse the skeleton data with poseguided cropped RGB data and achieve higher accuracies in NTU-RGBD dataset. The definitions of joints and their natural connections in the two datasets are shown in <ref type="figure" target="#fig_3">Fig. 7</ref>.   <ref type="figure">Fig. 5</ref>. Illustration of the network architecture. There are a total of 9 basic blocks (B1-B9). The three numbers of each block represent the number of input channels, the number of output channels and the stride, respectively. GAP represents the global average pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>NTU-RGBD: NTU-RGBD <ref type="bibr" target="#b8">[9]</ref> is currently the largest and most widely used in-door-captured action recognition dataset, which contains 56,000 action clips in 60 action classes. The clips are performed by 40 volunteers in different age groups ranging from 10 to 35. Each action is captured by 3 cameras at the same height but from different horizontal angles: −45 • , 0 • , 45 • . This dataset provides 3D joint locations of each frame detected by Kinect depth sensors. There are 25 joints for each subject in the skeleton sequences, while each video has no more than 2 subjects. The original paper <ref type="bibr" target="#b8">[9]</ref> of the dataset recommends two benchmarks: (1) Cross-subject (CS): the dataset in this benchmark is divided into a training set (40,320 videos) and a validation set <ref type="bibr">(16,</ref> <ref type="figure" target="#fig_2">560 videos)</ref>, where the subjects in the two subsets are different. (2) Cross-view (CV): the training set in this benchmark contains 37,920 videos that are captured by cameras 2 and 3, and the validation set contains 18,960 videos that are captured by camera 1. We follow this convention and report the top-1 accuracy on both benchmarks.</p><p>Kinetics-Skeleton: Kinetics <ref type="bibr" target="#b33">[34]</ref> is a large-scale human action dataset that contains 300,000 videos clips in 400 classes. The video clips are sourced from YouTube videos and have a great variety. It only provides raw video clips without skeleton data. <ref type="bibr" target="#b19">[20]</ref> estimate the locations of 18 joints on every frame of the clips using the publicly available OpenPose toolbox <ref type="bibr" target="#b43">[44]</ref>. Two peoples are selected for multi-person clips based on the average joint confidence. We use their released data (Kinetics-Skeleton) to evaluate our model. The dataset is divided into a training set (240,000 clips) and a validation set (20,000 clips). Following the evaluation method in <ref type="bibr" target="#b19">[20]</ref>, we train the models on the training set and report the top-1 and top-5 accuracies on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training details</head><p>All experiments are conducted on the PyTorch deep learning framework <ref type="bibr" target="#b44">[45]</ref>. Stochastic gradient descent (SGD) with Nesterov momentum (0.9) is applied as the optimization strategy. The batch size is 64. Cross-entropy is selected as the loss function to back-propagate gradients. The weight decay is set to 0.0001.</p><p>For the NTU-RGBD dataset, there are at most two peoples in each sample of the dataset. If the number of bodies in the sample is less than 2, we pad the second body with 0. The max number of frames in each sample is 300. For samples with less than 300 frames, we repeat the samples until it reaches 300 frames. The learning rate is set as 0.1 and is divided by 10 at the 30 th epoch and 40 th epoch. The training process is ended at the 50 th epoch.</p><p>For the Kinetics-Skeleton dataset, the size of the input tensor of Kinetics is set the same as <ref type="bibr" target="#b19">[20]</ref>, which contains 150 frames with 2 bodies in each frame. We perform the same dataaugmentation skills as in <ref type="bibr" target="#b19">[20]</ref>. In detail, we randomly choose 150 frames from the input skeleton sequence and slightly disturb the joint coordinates with randomly chosen rotations and translations. The learning rate is also set as 0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>We verify the effectiveness of the proposed components in MS-AAGCN in this section using the NTU-RGBD dataset.</p><p>1) Learning rate scheduler and data preprocessing: In the original paper of ST-GCN, the learning rate is multiplied by 0.1 at the 10 th and 50 th epochs. The training process is ended in 80 epochs. We rearrange the learning rate scheduler from <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr">80]</ref> to <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50]</ref> and obtain the better performance (shown in Tab. I, "before preprocessing").</p><p>Moreover, we use some preprocessing strategies on the NTU-RGBD dataset. The body tracker of Kinect is prone to detecting more than 2 bodies, some of which are objects. To filter the incorrect bodies, we first select two bodies in each sample based on the body energy. The energy is defined as the average of the skeleton's standard deviation across each of the channels. Subsequently, each sample is normalized to make the distribution of the data for each channel unified. In detail, the coordinates of each joint are subtracted from the coordinates of the "spine joint" (the 2 nd joint of the left subgraph in <ref type="figure" target="#fig_3">Fig. 7)</ref>. Finally, as different samples may be captured in different viewpoints, similar to <ref type="bibr" target="#b8">[9]</ref>, we translate the original 3D location of the body joints from the camera coordinate system to body coordinates. For each sample, we perform a 3D rotation to fix the X axis parallel to the 3D vector from the "right shoulder" (5 th joint) to the "left shoulder" (9 th joint), and the Y axis toward the 3D vector from the "spine base (21 st joint) to the spine (2 nd joint). <ref type="figure" target="#fig_5">Fig. 8</ref> shows an example of the preprocessing. The performance after the preprocessing is referred as "after-preprocessing" in Tab. I.  Tab. I shows that the preprocessing considerably helps the recognition. It is because that the original data are too noisy. 2) Adaptive graph convolutional block: We use the ST-GCN as the baseline method. As introduced in Section IV-A, there are 2 kinds of sub-graphs in the our proposed AGCL, i.e., the global graph B and the individual graph C. We test the performance of using each of the graphs along and combining them together. The results are shown as AGCN-B, AGCN-C and AGCN-BC in Tab. II, respectively. It shows that both of the two designed graphs brings notable improvement for the action recognition task. With two graphs added together, the model obtains the best performance.</p><p>Besides, the performance of initial strategies introduced in Sec. IV-A is tested and shown as AGCN-ABC and AGCN-BC in Tab. II. It suggests the second strategy is slightly better.</p><p>Moreover, we verify the effectiveness of adding the gating mechanism (AGCN-BC-G), which also brings encouraging improvement. Overall, the complete AGCL brings improvements of +3.1% and +1.7% on CS and CV benchmarks, respectively. 3) Attention module: In this section, we verify the effectiveness of the proposed STC-attention module introduced in Sec. IV-B. The results are shown in Tab. III. We first separately test the contributions of three sub-modules (SAM, TAM and CAM) based on the ST-GCN , shown as ASTGCN-S, ASTGCN-T and ASTGCN-C, respectively. It shows that all of the three sub-modules can help improving the performance. Then we test the performance of adding each of the submodules as well as concatenating them sequentially, shown as STGCN-ADD and STGCN-STC, respectively. It suggests that concatenating the three sub-modules is slightly better. Finally, we embed the STC-attention module into the AGCN and obtains the similar results. Note that the improvement brought by the attention module for AGCN (+0.6% and +0.7%) is less significant than that for STGCN (+2.8% and +1.5%). We argue that it is because the AGCN is powerful and the accuracy is already very high, thus the effect of the attention module is limited. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Multi-stream framework:</head><p>Finally we test the performance of using four proposed data modalities and show the results in Tab. IV. Here, J, B, J-M and B-M denote the joint modality, the bone modality, the motion of joint and the motion of bone introduced in Sec. IV-E, respectively. Clearly, the multi-stream method outperforms the single-stream methods. For single-stream methods, the bone modality (B-AAGCN) performs slightly better than the joint modality (J-AAGCN) for CS benchmark. As for the CV benchmark, the result is reversed. This suggests the complementarity of the two modalities. By combing the joints and bones (JB-AAGCN), it brings notable improvement as expected.</p><p>The performance of motion modalities (J-M-AAGCN and B-M-AAGCN) is generally lower than the performance of the joint and bone modalities. However, adding them together still brings improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparisons with the state-of-the-art methods</head><p>We compare the final model with the state-of-the-art skeleton-based action recognition methods on both the NTU-RGBD dataset and Kinetics-Skeleton dataset. The results are shown in Tab. V and Tab. VI, respectively. The methods used for comparisons include the handcraft-feature-based methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, RNN-based methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b35">[36]</ref>, CNN-based methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> and GCN-based methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Our model achieves the state-of-the-art performance with a large margin on both of the datasets, which suggests the superiority of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>CS (%) CV (%) Lie Group <ref type="bibr" target="#b5">[6]</ref> 50.1 82.8 HBRNN <ref type="bibr" target="#b7">[8]</ref> 59.1 64.0 Deep LSTM <ref type="bibr" target="#b8">[9]</ref> 60.7 67.3 ST-LSTM <ref type="bibr" target="#b9">[10]</ref> 69.2 77.7 STA-LSTM <ref type="bibr" target="#b10">[11]</ref> 73.4 81.2 VA-LSTM <ref type="bibr" target="#b11">[12]</ref> 79.2 87.7 ARRN-LSTM <ref type="bibr" target="#b12">[13]</ref> 80.7 88.8 Ind-RNN <ref type="bibr" target="#b13">[14]</ref> 81.8 88.0 SRN+TSL <ref type="bibr" target="#b35">[36]</ref> 84 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. FUSION WITH THE RGB MODALITY</head><p>The skeleton data is robust to the dynamic circumstance and complicated background. However, it lacks the appearance information. For example, if a person is eating something, it is hard to judge whether he is eating an apple or a peal using only the skeleton data.</p><p>In this section, we investigate the necessity of fusing both the skeleton data and the RGB data for the action recognition task on the NTU-RGBD dataset. We use a two-stream framework where one of the stream models the RGB data with the 3D convolutional networks and another stream models the skeleton data with our MS-AAGCN. The details of skeletonstream is the same as Sec V-B. For RGB-stream, inspired by <ref type="bibr" target="#b45">[46]</ref>, we use the ResNeXt3D-101 model that is pre-trained on ImageNet and Kinetics. When training, we randomly crop a clip from the whole video whose length is random sampled from <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">64,</ref><ref type="bibr">128]</ref>. The crop position is randomly selected from four corners and one center. Then each image is cropped based on the crop ratio sampled form [1, 0.875, 0.75]. Note that the width and height of the cropped image can be different. Finally the cropped sequence of images are normalized and resized to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">224,</ref><ref type="bibr">224]</ref>, which denotes the length, height and width of the clip. The learning rate is initialized with 0.01 and is multiplied with 0.1 after the validation accuracy saturates. We use four TITANXP-GPUs and the batch size is set to 32. Momentum-SGD is used as the optimizer and the weight decay is set to 0.0005. When testing, we crop clips with different lengths and sizes in different positions. The result is the average of these clips.</p><p>Moreover, to get rid of the interference of the background, we propose to crop the persons from the original images and use only the person part for the recognition. In detail, we calculate the border of the persons in each image and use their union as the crop box. This is referred as the pose-guided cropping strategy.</p><p>Tab. VII shows the results of our methods as well as the previous methods that also use the RGB modality. Here, C denotes using the pose-guided cropping strategy. It shows that the pose-guided cropping strategy brings notable improvement when using only the RGB modality (RNX3D101 vs RNX3D101-C). This suggests that the model using only RGB data is easily misguided by the complicated background. However, the improvement decreases when fusing the skeleton data and the RGB data (RNX3D101+MS-AAGCN vs RNX3D101+MS-AAGCN-C). The reason is that the skeleton data can effectively avoid the interference of the circumstance, thus the contribution of the cropping strategy becomes not as obvious as before. Our best model, i.e., RNX3D101+MS-AAGCN-C, achieves 96.1% for CS benchmark and 99.0% for CV benchmark. As shown in Tab. VII, it exceeds the previous methods with a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Pose RGB CS (%) CV (%) DSSCA-SSLM <ref type="bibr" target="#b46">[47]</ref> 74.9 -Chained Network <ref type="bibr" target="#b47">[48]</ref> 80.8 -RGB + 2D Pose <ref type="bibr" target="#b48">[49]</ref> 85.5 -Glimpse Clouds <ref type="bibr" target="#b49">[50]</ref> 86 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. VISUALIZATION AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Adaptive graphs</head><p>The are two kinds of graphs in our model: the global graph and the individual graph. <ref type="figure" target="#fig_6">Fig. 9</ref> shows an example of the learned adjacency matrices of the global graph for different subsets and different layers. The first and second rows show the adjacency matrices of the centripetal subset (S i2 ) and the centrifugal subset (S i3 ) introduced in Sec III-B, respectively. The first column shows the graph structure defined based on the natural connectivity of the human body, i.e., A in Eq. 2. Others are the adjacency matrices of the global graph in different layers. The gray scale of each element in the matrix represents the strength of the connection. It shows that the topology of the learned graph is updated based on the human-body-based graph but has many changes. It confirms that the human-body-based graph is not the optimal choice for the action recognition task. Besides, it shows that the graph topology of the higher layers changes more than the lower layers. It is because the information contained in higher layers is more semantic, thus the required topology of graph is more different from the human-body-based graph.</p><p>Similarly, <ref type="figure" target="#fig_0">Fig. 10</ref> shows some examples of the learned adjacency matrices of the individual graph for two different samples. It shows that different samples and layers need different graph topologies, which confirms our motivation. The two kinds of the graphs are fused using the gating mechanism. We visualize the importance of the two kinds of graphs in each layer in <ref type="figure" target="#fig_0">Fig. 11</ref>. It shows that the individual graph is more important in top layers. This is because the individual graph is learned based on the features of the data samples. The receptive fields of the top layers are larger and their features are more informative. Thus it is easier for top layers to learn the topology of the individual graph than lower layers.  To see the learned graph topologies clearly, we draw the connections between joints on the skeleton sketches as shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. The orange lines represent the connections whose values are in the top 50. The alpha value of the lines represents the strength of the connections. It shows that the learned graph topology is accorded with the human intuition. For example, the relationships between the hand and the head should be more important to recognize the action of "take a selfie", and the learned graphs did have more connections between them as shown in the first row. This confirms the effectiveness and necessity of the proposed adaptive graph convoutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention module</head><p>There are three sub-modules for our proposed STC-attention module: the spatial attention module, the temporal attention module and the channel attention module. For the spatial attention, we show the learned attention map for different samples and different layers in <ref type="figure" target="#fig_0">Fig. 13</ref>. The size of the circle take a selfie layer3 throw layer5 layer7 layer9 represents the importance of the corresponding joint. It shows that the model focus more on the joints of hands and head. Besides, the attention is not obvious in the lower layers. It is because the receptive fields of lower layers are relatively smaller, thus is hard to learn good attention maps.</p><p>taking a selfie layer1 put on a hat layer3 layer8 layer10 For the temporal attention, we show an example of the learned attention weights for each of the frames and the corresponding skeleton sketches in <ref type="figure" target="#fig_0">Fig. 14.</ref> For the sample of taking a selfie, it shows the model focus more on the process of raising people's hand in the fifth layer, and cares more about the final posture of the selfie in the seventh layer. For the sample of throwing, although in the same fifth layer, it learns different structure and pays more attentions on the frames where the hands are in the lower position. This shows the effectiveness and the adaptability of the designed module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-modalities</head><p>To show the complementarity between different modalities, we plot the accuracy difference for some of the classes between the different modalities. We use the CV benchmark of the NTU-RGBD dataset. <ref type="figure" target="#fig_0">Fig. 15</ref> shows the accuracy differences between the skeleton modality and the RGB modality. It shows that the skeleton modality helps RGB modality a lot for "rub the hands" class and the RGB modality helps the skeleton modality a lot for "reading" and "writing" classes. We find two examples for the class "reading" and class "writing" as shown in <ref type="figure" target="#fig_0">Fig. 16</ref>.  The skeletons of these two examples are very similar, thus are hard to tell apart. But with the help of the RGB data, they can be distinguished according to whether there is a pen in the hands. This example illustrates the complementarity between the skeleton modality and the RGB modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Conclusion</head><p>In this work, we propose a novel multi-stream attentionenhanced adaptive graph convolutional neural network (MS-AAGCN) for skeleton-based action recognition. The graph topology of the skeleton data used in this model is parameterized and embedded into the network to be jointly learned and updated with the other parameters. This data-driven approach increases the flexibility and generalization capacity of the model. It is also confirmed that the adaptively learned topology of graph is more suitable for the action recognition task than the human-body-based graph. Besides, an STC-attention module is embedded in every graph convolutional layers, which helps the model paying more attention to the important joints, frames and features. Moreover, we propose to explicitly model the joints, bones and the corresponding motion information in a unified multi-stream framework, which further enhances the performance. The final model is evaluated on two largescale action recognition datasets, i.e., the NTU-RGBD and the Skeleton-Kinetics. It achieves the state-of-the-art performance on both of them. In addition, we fuse the skeleton data with the skeleton-guided cropped RGB data, which brings additional improvement. Future works can focus on how to better fuse the RGB modality and the skeleton modality. It is also worth to study the combination of the skeleton-based action recognition algorithms and the pose estimation algorithms in a unified framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Left: Illustration of the spatiotemporal graph. Right: Illustration of the mapping strategy. Different colors denote different subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the STC-attention module. Three sub-modules are arranged sequentially in the orders of SAM, TAM and CAM. ⊗ denotes the element-wise multiplication. ⊕ denotes the element-wise addition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>1 and is divided by 10 at the 45 th epoch and 55 th epoch. The training process is ended at the 65 th epoch. Illustration of the overall architecture of the MS-AAGCN. The sof tmax scores of the four streams are fused using weighted summation to obtain the final prediction. J denotes the joint information. B denotes the bone information. M denotes the motion information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>The left sub-graph shows the joint labels of the Kinetics-Skeleton dataset and the right sub-graph shows the joint labels of the NTU-RGBD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Example of the data preprocessing on the NTU-RGBD dataset. The left is the original skeleton, and the right is the preprocessed skeleton.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Example of the learned adjacency matrices of the global graph. Different rows shows different subsets. The first column is the adjacency matrices of the human-body-based graph in the NTU-RGBD dataset. Others are examples of the learned adaptive adjacency matrices of different layers learned by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Examples of the learned adjacency matrices of the individual graph. The first and second rows show different samples. Different columns represent different layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Visualization for the importance of the two kinds of graphs in each of the layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Examples of the learned graph topologies. The orange lines represent the connections whose values are in the top 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Examples of the learned spatial attention maps. The size of the circle represents the importance of the corresponding joint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Visualization of the temporal attention map. The first row shows the learned temporal attention weights for each of the frames for different layers and samples. The second and third rows show the corresponding skeleton sketches. Accuracy difference between the skeleton modality and the RGB modality, i.e., ACC(skeleton)-ACC(RGB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 .</head><label>16</label><figDesc>Two examples for class "reading" and class "writing". The white lines represent the skeletons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Illustration of the basic block. Convs represents the spatial AGCL, and Convt represents the temporal AGCL, both of which are followed by a BN layer and a ReLU layer. STC represents the STC-attention module. Moreover, a residual connection is added for each block.</figDesc><table><row><cell></cell><cell cols="4">Residual connection</cell><cell></cell></row><row><cell>Convs BN</cell><cell>Relu</cell><cell>STC</cell><cell>Convt</cell><cell>BN</cell><cell>Relu</cell></row><row><cell cols="2">Fig. 4. BN</cell><cell>B2,B3 64,64,1</cell><cell cols="2">B5,B6 128,128,1</cell><cell>B8,B9 256,256,1</cell><cell>Softmax</cell></row><row><cell>B1</cell><cell></cell><cell>B4</cell><cell>B7</cell><cell cols="2">GAP</cell></row><row><cell>3,64,1</cell><cell cols="2">64,128,2</cell><cell cols="2">128,256,2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>THE VALIDATION ACCURACIES ON THE NTU-RGBD DATASET. A DENOTES THE ADJACENCY MATRIX OF THE BODY-BASED GRAPH SHOWN IN EQ. 2. B AND C DENOTE THE GLOBAL GRAPH AND THE INDIVIDUAL GRAPH INTRODUCED IN SEC IV-A, RESPECTIVELY. G</figDesc><table><row><cell>Methods</cell><cell cols="2">CS (%) CV (%)</cell></row><row><cell>STGCN</cell><cell>84.3</cell><cell>92.7</cell></row><row><cell>AGCN-B</cell><cell>86.4</cell><cell>93.6</cell></row><row><cell>AGCN-C</cell><cell>86.1</cell><cell>93.5</cell></row><row><cell>AGCN-ABC</cell><cell>86.6</cell><cell>93.7</cell></row><row><cell>AGCN-BC</cell><cell>87.0</cell><cell>94.1</cell></row><row><cell>AGCN-BC-G</cell><cell>87.4</cell><cell>94.4</cell></row><row><cell cols="2">TABLE II</cell><cell></cell></row><row><cell>COMPARISONS OF</cell><cell></cell><cell></cell></row></table><note>DENOTES USING THE GATING MECHANISM.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>THE VALIDATION ACCURACY ON THE NTU-RGBD DATASET FOR EACH OF THE ATTENTION SUB-MODULES AND DIFFERENT ARRANGEMENT STRATEGIES. STGCN AND AGCN DENOTE THE STGCN-M AND AGCN-BC-G IN TAB. II, RESPECTIVELY. STC MEANS CONCATENATING THE THREE SUB-MODULES SEQUENTIALLY.</figDesc><table><row><cell>Methods</cell><cell cols="2">CS (%) CV (%)</cell></row><row><cell>STGCN</cell><cell>84.3</cell><cell>92.7</cell></row><row><cell>ASTGCN-S</cell><cell>86.1</cell><cell>93.1</cell></row><row><cell>ASTGCN-T</cell><cell>86.6</cell><cell>93.6</cell></row><row><cell>ASTGCN-C</cell><cell>86.5</cell><cell>93.7</cell></row><row><cell>ASTGCN-ADD</cell><cell>86.8</cell><cell>94.1</cell></row><row><cell>ASTGCN-STC</cell><cell>87.1</cell><cell>94.2</cell></row><row><cell>AGCN</cell><cell>87.4</cell><cell>94.4</cell></row><row><cell>AAGCN-ADD</cell><cell>87.7</cell><cell>94.8</cell></row><row><cell>AAGCN-STC</cell><cell>88.0</cell><cell>95.1</cell></row><row><cell cols="2">TABLE III</cell><cell></cell></row><row><cell>COMPARISONS OF</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>RGBD DATASET. JB REPRESENTS USING THE JOINT AND BONE MODALITIES. MS REPRESENTS USING ALL OF THE FOUR MODALITIES. AAGCN DENOTES THE AAGCN-STC IN TAB. III.</figDesc><table><row><cell>Methods</cell><cell cols="2">CS (%) CV (%)</cell></row><row><cell>J-AAGCN</cell><cell>88.0</cell><cell>95.1</cell></row><row><cell>B-AAGCN</cell><cell>88.4</cell><cell>94.7</cell></row><row><cell>J-M-AAGCN</cell><cell>85.9</cell><cell>93.0</cell></row><row><cell>B-M-AAGCN</cell><cell>86.0</cell><cell>93.1</cell></row><row><cell>JB-AAGCN</cell><cell>89.4</cell><cell>96.0</cell></row><row><cell>MS-AAGCN</cell><cell>90.0</cell><cell>96.2</cell></row><row><cell cols="2">TABLE IV</cell><cell></cell></row><row><cell cols="3">COMPARISONS OF THE VALIDATION ACCURACY WITH DIFFERENT INPUT</cell></row><row><cell>MODALITIES ON THE NTU-</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/lshiwjx/2s-AGCN</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action Recognition with Improved Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features With 3d Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo Vadis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>Action Recognition? A New Model and the Kinetics Dataset</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal hallucinating for action recognition with few still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5314" to="5322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
	<note>NTU RGB+D: A Large Scale Dataset for 3d Human Activity Analysis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatio-Temporal LSTM with Trust Gates for 3d Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition From Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Skeleton-Based Relational Modeling for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02556</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building A longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A New Representation of Skeleton Sequences for 3d Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4570" to="4579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia &amp; Expo Workshops</title>
		<imprint>
			<publisher>ICMEW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia &amp; Expo Workshops</title>
		<imprint>
			<publisher>ICMEW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">EgoGesture: A New Dataset and Benchmark for Egocentric Hand Gesture Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1038" to="1050" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2693" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Residual Attention Network for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9423" to="9433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The Kinetics Human Action Video Dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition with Gated Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01810</idno>
		<title level="m">Videos as Space-Time Region Graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advancesin Neural Information Processing Systems Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gesture Recognition using Spatiotemporal Deformable Convolutional Represention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hanqing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1045" to="1058" />
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Chained Multi-Stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2904" to="2913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">2d/3d Pose Estimation and Action Recognition Using Multitask Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Glimpse Clouds: Human Activity Recognition From Unstructured Feature Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recognizing Human Actions as the Evolution of Pose Estimation Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Where to Focus on for Human Action Recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019-01" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
