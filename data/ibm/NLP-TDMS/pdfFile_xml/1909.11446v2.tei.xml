<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoder Choice Network for Meta-Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-19">19 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Fei</forename><surname>Chao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Longzhi</forename><forename type="middle">Yang</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Chih-Min</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Shen</surname></persName>
						</author>
						<title level="a" type="main">Decoder Choice Network for Meta-Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-19">19 Nov 2019</date>
						</imprint>
					</monogr>
					<note>1 JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Meta-learning</term>
					<term>latent code</term>
					<term>decoder</term>
					<term>ensemble learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Meta-learning has been widely used for implementing few-shot learning and fast model adaptation. One kind of meta-learning methods attempt to learn how to control the gradient descent process in order to make the gradient-based learning have high speed and generalization. This work proposes a method that controls the gradient descent process of the model parameters of a neural network by limiting the model parameters in a low-dimensional latent space. The main challenge of this idea is that a decoder with too many parameters is required. This work designs a decoder with typical structure and shares a part of weights in the decoder to reduce the number of the required parameters. Besides, this work has introduced ensemble learning to work with the proposed approach for improving performance. The results show that the proposed approach is witnessed by the superior performance over the Omniglot classification and the miniImageNet classification tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Decoder Choice Network for Meta-Learning Jialin Liu, Fei Chao, Member, IEEE, Longzhi Yang, Senior Member, IEEE, Chih-Min Lin, Fellow, IEEE, and Qiang Shen</p><p>Abstract-Meta-learning has been widely used for implementing few-shot learning and fast model adaptation. One kind of meta-learning methods attempt to learn how to control the gradient descent process in order to make the gradient-based learning have high speed and generalization. This work proposes a method that controls the gradient descent process of the model parameters of a neural network by limiting the model parameters in a low-dimensional latent space. The main challenge of this idea is that a decoder with too many parameters is required. This work designs a decoder with typical structure and shares a part of weights in the decoder to reduce the number of the required parameters. Besides, this work has introduced ensemble learning to work with the proposed approach for improving performance. The results show that the proposed approach is witnessed by the superior performance over the Omniglot classification and the miniImageNet classification tasks.</p><p>Index Terms-Meta-learning, latent code, decoder, ensemble learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M ACHINE learning has recently demonstrated nearhuman performance in the traditionally challenging tasks of object recognition, image classification, and games and scenario generation, amongst other applications. The key to such successes is the availability or obtainability of high-quality large datasets. Collecting and labeling data or harvesting labeled data from literature and historic archives require significant human efforts, but the resulted dataset can usually only be used for one specific task. However, humans have the ability to quickly learn new conceptions and skills for novel tasks based on prior knowledge and experience; meta-learning is a branch of machine learning techniques imitating such ability by learning parameters fine-tuning from prior datasets and pre-training models. Consequently, metalearning can extend the boundary of machine learning greatly, which concerns the distributions of tasks in addition to the traditionally-used distribution of data samples. It not only enables the 'reuse' of datasets across different tasks, but also prevents from over-fitting to new, and usually small dataset for novel tasks <ref type="bibr" target="#b0">[1]</ref>. In this case, each novel task is a learning task, which is supported by training examples (or shot) and testing examples (or query) <ref type="bibr" target="#b1">[2]</ref>.</p><p>The most widely studied form of meta-learning is few-shot learning, which requires models to predict labels of instances from unseen classes during the testing phase, with the support of only a few labeled samples from each category. Many methods have recently been proposed to implement few-shot learning tasks, which can be categorized into three types <ref type="bibr" target="#b2">[3]</ref>: memory-based, optimization-based, and metric-based. The memory-based methods extend a memory space to store key training examples or model-related information, and it is often achieved by using attention model <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Optimization-based methods learn to control the process of optimization within each task, by learning the initial parameters (e.g., <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>) or the optimizer (e.g., <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>). Metric-based methods focus on learning similarity metrics which maximizing the similarity between members from the same class.</p><p>The purpose of this work is to design an optimizationbased method, which controls the optimization of model parameters by limiting those parameters in a low dimension space. This inspiration comes from neural style transfer <ref type="bibr" target="#b9">[10]</ref>, which updates the input image of a deep network rather than parameters. If we replace the input image as the latent code and the output as the model parameters, we can indirectly update the model parameters by updating the latent code. The network maps the latent code to the model parameters is called decoder in this work.</p><p>However, this idea is difficult to implement for the highdimension model parameter space. If the fully connected network is adopted as the decoder, the complexity of the decoder is usually square of the number of the model parameters. Instead of a fully connected network, we propose a new structure, named group linear transformation (GLT), with lower time and space complexity to denote the decoder network (detail in III-B).</p><p>Besides, we also draw on the idea of task-dependent adaptive metric (TADAM) <ref type="bibr" target="#b10">[11]</ref> and latent embedding optimization (LEO) <ref type="bibr" target="#b2">[3]</ref>, which enhances the correlation between the model and the task by making the model parameters depending on the task. We enhances the correlation between the decoder and the task by choosing the different decoders based on the task features by a choice network. Due to all decoders are able to share a part of their parameters and the complexity of the choice network is lower than that of the decoders, this approach makes the decoders be task-dependent with few costs.</p><p>Finally, in order to further improve generalization of the model, this work adopts the training protocol proposed in snapshot ensemble <ref type="bibr" target="#b11">[12]</ref> instead of the standard training protocol. The training protocol with snapshot ensemble <ref type="bibr" target="#b11">[12]</ref> selects the models with the highest accuracy on the validation set in a single training process to enable the ensemble process <ref type="bibr" target="#b11">[12]</ref>.</p><p>We evaluate DCN on both regression and classification fewshot learning tasks. The experiment results show the proposed optimization-based method greatly improves the accuracy of the few-shot learning model by enhancing the dependency between model and task, and even have learnable fewer parameters in several tasks. The main contribution of this work is threefold: 1) an efficient structure which enables the gradient control for the high-dimension model parameters by the lowdimension space, 2) the implementation of the tack-dependent gradient control by choosing the decoders based on the task features, and 3) the integration of snapshot ensemble in the proposed DCN for performance improvement.</p><p>The rest of the paper is organized as follows. Section II reviews the underpinning theory of the proposed work. Section III details the proposed DCN. Section IV reports the results. Section V concludes the work with a list of future work provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>The theoretical underpinning of the proposed approach is reviewed in this section, including few-shot learning and metalearning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Few-Shot Learning</head><p>In supervised learning, the training dataset is a number of labelled data instances D = {(x 1 , y 1 ), (x 2 , y 2 ), · · · , (x K , y K )}, with each (x i , y i ), 1 ≤ i ≤ K being a data instance with the features x i and the labels y i , where K is the number of data instances. Differently, few-shot learning learns between tasks, and thus the input dataset can be represented</p><formula xml:id="formula_0">as D meta = {D tr i , D test i } i , where D tr i = {x tr ij , y tr ij } j and D test i = {x test ij , y test ij } j .</formula><p>In other words, few-shot learning takes each dataset regarding tasks as instances of training.</p><p>We explain the definition of N-way, K-shot H-query tasks. It means those tasks are N classification tasks, each with K training examples and H testing examples <ref type="bibr" target="#b12">[13]</ref>. <ref type="figure">Fig. 1</ref> shows the 5-way, 1-shot, 1-query task of miniImageNet, and each class in a task has 1 training instances and 1 testing instances. In the 5-way, 5-shot, 1-query task, there are 5 training instances and 5 testing instances in each class.</p><p>The objectives of supervised learning tasks and few-shot learning tasks can be expressed by the following optimization tasks respectively:</p><formula xml:id="formula_1">θ = arg min θ i L(f θ (x i ), y i ),<label>(1)</label></formula><formula xml:id="formula_2">θ = arg min θ i j L(f {D tr i ,θ} (x test ij ), y test ij )<label>(2)</label></formula><p>Eq. (1) represents a standard empirical risk minimization task of supervised learning, in which the prediction of y i only depends on x i and θ, but when it comes to few-shot learning as shown in Eq. (2), the prediction of y test ij also depends on the training examples of D tr i , in addition to the corresponding the features x test ij and the parameters θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Meta-Learning</head><p>Few-shot learning approaches are commonly implemented by meta-learning mechanisms, which enables learn-to-learn. There are usually two hierarchies learning processes in metalearning. The low-level learning process, usually termed as the "inner loop", learns to handle general tasks; and the high-level learning process, usually termed as the "outer loop", improves the performance of low-level learning process.</p><p>Deep learning is often employed in the meta-learning process, although other machine learning methods, such as Bayesian learning <ref type="bibr" target="#b13">[14]</ref>, can also be applied. Therefore, most of the meta-learning approaches use gradient descent in the "outer loop"; hence, the gradient of the "outer loop" is termed as meta-gradient. However, the machine learning methods used in the "inner loop" are different. According to different machine learning methods meta-learning approaches can be implemented in three categories: 1) memory-based methods, 2) optimization-based methods, and 3) metric-based methods <ref type="bibr" target="#b2">[3]</ref>.</p><p>Metric-based methods can be artificially viewed as using a K-nearest neighbor (KNN) or its variation, to optimise a feature embedding space during the "outer loop", minimizing the similarity metrics between instances from same class and maximizing those from different classes, and to predict the labels of testing examples based on the similarity metrics in this embedding space during the "inner loop". A number of similarity metrics approaches have been used in meta-learning, such as cosine distance, squared Euclidean distance, or even a relationship learned by a neural network <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Optimization-based methods adopt deep learning methods during the "inner loop", and during the "outer loop" these approaches learn the hyperparameters of the deep learning methods, such as parameter initialization, learning rate, gradient direction, and et al. Among such methods MAML is the most typical one <ref type="bibr" target="#b5">[6]</ref>, which tried to learn the parameter initialization. Besides, the methods in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref> try to learn the learning rate of the "inner loop".</p><p>Memory-based methods remember and search key training examples <ref type="bibr" target="#b3">[4]</ref> or model-related information during the "inner loop". The model related information is any information related to the model of "inner loop", such as the network weights <ref type="bibr" target="#b4">[5]</ref> or the activation values <ref type="bibr" target="#b16">[17]</ref> of different layers. These methods extend the external memory, and read and write the memory by employing the attention models.</p><p>A general meta-learning model is illustrated in <ref type="figure">Fig. 2</ref>. If a parametric learning method is used during the "inner loop", the model will get the parameters θ i by training on the data of task i; otherwise, if a nonparametric learning method is used, θ i is just equal to {θ, {x tr ij , y tr ij } j }. During the "inner loop" θ is updated to θ i by an algorithm, which is able to keep differentiable between θ and θ i . Last, during the "outer loop", θ is updated by the gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED META-LEARNING MODELS</head><p>The proposed model is an optimization-based method which controls the gradient descent process during the "inner loop" by limiting the model parameters in a low-dimensional latent space. The latent code in the latent space is decoded to ... ... <ref type="figure">Fig. 1</ref>. Example of few-shot learning data. These are the instances from a 5-way, 1-shot, 1-query meta-data. Each few learning task contains ten images from different classes, each class has one training example and one testing example. In the figure, images with the same number in the upper left corner of each line are from the same class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Data</head><formula xml:id="formula_3">D tr D test D tr D test D tr D test</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inner Loop</head><p>Outer Loop the model parameters of a predict model by the non-linear decoder. Several decoders can be chosen based on a different task, so the low-dimensional latent space used in DCN is dependent on the task.</p><formula xml:id="formula_4">f θ f θ i x tr ij y tr ij x test ij y test ij i j i ∇ θ L(f ψ i (x test ij ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Decoder Choice Network</head><p>DCN consists of three parts: choice network C, decoders {d 1 , d 2 , · · · , d S } and latent code z. In the start of "inner loop", the choice network receives the task features and produces the choice of the decoder. In order to make the choice differentiable, here we adopt the idea of Neural Turing Machine (NTMs) <ref type="bibr" target="#b17">[18]</ref>. Choice network chooses every decoder with different extends {c 1 , c 2 , · · · , c S }. The difference with NTMs is that the weights of decoders are not provided by the attention model, but by fuzzy set, and we will detail this in the next subsection. Then, given the weights of decoders, we initialize the latent code of "inner loop" latent code z = z and decode it to the parameters of a neural network model:</p><formula xml:id="formula_5">θ i ← S s=1 c s · d s (z ).<label>(3)</label></formula><p>Before parameterizing the neural network model withθ i , in order to prevent vanishing gradient and accelerate convergencê θ i will be normalized by Batch Normalization <ref type="bibr" target="#b18">[19]</ref>, which is given by:</p><formula xml:id="formula_6">θ i = γ * θ i − µ √ σ 2 + + β,<label>(4)</label></formula><p>where µ and σ are mean and variance of {θ i } i respectively, γ and β are learnable parameters, is a positive value close </p><formula xml:id="formula_7">{c 1 , c 2 , · · · , c S } ← C(D tr i ) 3: for m = 1, · · · , M do 4:θ i ← S s=1 c s · d s (z ) 5: θ i ← Normalizeθ i by Eq. (4) 6: L tr i = j L(f θi (x tr ij ), y tr ij ) 7: z ← z − α∇ z L tr i 8: end for 9:θ i ← S s=1 c s · d s (z ) 10: θ i ← Normalizeθ i by Eq. (4) 11: L test i = j L(f θi (x test ij ), y test ij ) 12: return L test i</formula><p>to zero added to the denominator for numerical stability. After obtaining θ i , the model f θi is builded depending on the task. f θi is used to process each data from training examples with a general feed-forward mapping. The forward process is depicted in <ref type="figure" target="#fig_0">Fig. 3</ref>.</p><p>After getting prediction and loss of all training examples, gradient descent is used to update θ. However, we would not directly update θ, but update the latent code z instead:</p><formula xml:id="formula_8">z ← z − α∇ z L tr i (f θi ),<label>(5)</label></formula><p>where α is the learning rate of gradient descent during the "inner loop", in order to be simple we rewrite j L(f θi (x tr ij ), y tr ij ) as L tr i . In next step the model parameterized by θ i would be used to calculate the loss on training examples, and the process Finally, the adapted parameters θ i are used to calculate the testing loss j L i (f θi (x test ij ), y test ij ), and we write it as L test i . During the "outer loop" the choice network C, the decoders {d 1 , d 2 , · · · , d S } and the latent code z are updated to reduce L test i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Decoders</head><p>The main challenge for the implementation of DCN is that if we use the fully connected multi-layer network as decoders it will require too many parameters. The decoders receive the latent code and produce the model parameters of a neural network. Because the dimension of the model parameters is too large, the complexity of decoders become unacceptable. In this subsection, we detail several methods to reduce the complexity of decoders and analyze it.</p><p>1) Structure of linear transformation: Although the main challenge is the high dimension output of decoders, we should also consider the dimension of the latent code z for it can be 10 times smaller than the model parameters at most. The reason is that if the dimension of the latent code is too low it will limit the expression of the meta-learning model.</p><formula xml:id="formula_9">X = X = X = ... ... X = Concatenates Concatenates Concatenates h 1 h 2 h N h −1 h N h W 1 z W 2 z W N h −1 z W N h z z 1 z 5 z 8 Fig. 4. Where 'Concatenates' denotes concatenating {h 1 , h 2 , · · · , h N h } to h.</formula><p>The length of vector in each channel of h is increased by N h times, which is the number of the weight matrices.</p><p>In order to overcome these two challenges, we divide the input and the weight matrices of a layer in the decoders into groups, and each of the weight matrices is reused in all groups of the input. Here we use the first layer of the decoders as an example to illustrate the idea. The latent code is divided into several groups z = [z 1 , z 2 , · · · , z Ng ], and where z is a matrix and the elements in each column are in the same group. Each element of the output only depends on the latent code in one group. Then each weight matrix of {W 1 , W 2 , · · · , W N h } is used to calculate the hidden variable:</p><formula xml:id="formula_10">h n = W n z, (n = 1, 2, · · · , N h ).<label>(6)</label></formula><p>{h 1 , h 2 , · · · , h N h } are concatenated to a matrix h, and h have already been divided into N h . This process is summarized in <ref type="figure">Fig. 4</ref>. We call this structure as group linear transformation (GLT), and we will analyze this structure reduce how much parameters in the decoder network in III-B4.</p><p>2) Non-linear: In hidden layer, we choose ELU (α = 1) <ref type="bibr" target="#b19">[20]</ref> as non-linear transformation in the decoder network, which is given by:</p><formula xml:id="formula_11">ELU(x) = max(0, x) + min(0, α * (exp(x) − 1)),<label>(7)</label></formula><p>where α is equal to 1 in all our experiments. In output layer, we use double-thresholding strategy, which has equal thresholds in both positive and negative sides, called "softshrink". We use this function based on PyTorch <ref type="bibr" target="#b20">[21]</ref>. Softshrink is given by:</p><formula xml:id="formula_12">sof tshrink(x) =      x − λ, if x &gt; λ x + λ, if x &lt; −λ 0, otherwise ,<label>(8)</label></formula><p>where λ denotes the threshold. All experiments in this work use λ = 0.01.</p><p>3) Structure sharing: In order to further reduce the complexity of decoders, all decoders network share all low-level layers. Last layer of this neural network has multi heads, the output dimension of each of them is equal to each other, and each head represents a decoder. When the function of each head is related to each other, this kind structure not only reduces the time and spatial complexity of the model but also prevents vanishing gradient, and accelerates convergence greatly. The reason is the shared shallow layers of the network can obtain the gradients from all heads. This is a common method in deep learning, and a number of works have proven its efficiency by empirical evidence, such as <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>.</p><p>We also reuse the choice network and decoders in different layers. However, if the task features are the same for different layers, all layers with DCN would have the same {c 1 , c 2 , · · · , c S }. We simply use the features of training examples of the previous layer as the task features of the current layer. By this way, each layer chooses its own decoder.</p><p>In all experiments, we use 2 layers neural network as the decoder, and the second layer outputs are S vectors with the size of the model parameter.</p><p>In addition, in some case like miniImageNet, the number of the model parameters is still too large, we resizeθ by linear interpolation to enlarge the dimension ofθ. This method not only reduces the output size of the decoders, but also makes each model parameters related to others, which can be seen as a kind of regularization <ref type="bibr" target="#b9">[10]</ref>. Besides, linear interpolation allows DCN to be reused in layers with different dimension model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Complexity Analysis:</head><p>We analyze the number of the model parameters in the predict model at first. Assume the number of channels (corresponding to the convolution layer or corresponding to fully connected layer) is F . If the number of channels is equal to each other in hidden layers, it would be proportional to F 2 , and the proportion is N l K s , where N l is the number of layers and K s is kernel size of the convolutional unit. It is obvious that most of CNNs meet this condition. The deviation caused by the first and last layers can be ignored due to the lower magnitude.</p><p>We assume that each decoder is a single fully connected network. The input and the output of the decoder network are the latent code and the model parameters, respectively; thus it is sensible to assume dim(z) ∝ dim(θ i ), which leads to dim(W) ∝ F 4 N 2 l K 2 s , where W denotes the weight matrix of the decoder network. Since the time and spatial complexity are proportional to the number of the weights in a neural network, both time and spatial complexity are O(F 4 N 2 l K 2 s ). Now, we share all decoders amongst the different layers. The time and spatial complexity are reduced to O(F 4 SK 2 s ).</p><p>If we the decoders are also shared between the dimension of the kernel, the complexity would reduce to O(F 4 S). In general case there is S N 2 l K 2 s . Then we replace the fully connected network as the structure proposed in III-B1, GLT.</p><p>The number of parameters in 2 layers fully connected network and GLT are [dim(z) + dim(θ i )S]dim(h) and</p><formula xml:id="formula_13">dim(h) dim(z) N 2 g + dim(θi) dim(h) SN 2 h , respectively,</formula><p>where dim(h) and dim(z) are total dimension of the latent code and the hidden variable, respectively. Here dim(h) dim(z) and dim(θi) dim(h) should be integers, and it is obvious that 1 ≤ N g ≤ dim(z) and 1 ≤ N h ≤ dim(h). The smaller N g and N h , the less the number of parameters in GLT. Therefore we can reduce parameters number by reducing the number of the groups. If N g = N h = 1 and dim(h) 2 = dim(z)dim(θ i ), time and spatial complexity are reduced to O( F dim(z) ). On the contrary, if N g = dim(z) and N h = dim(h), GLT is equal to the fully connected network. However, smaller N g and N h mean that each element of the model parameters depends on less variables in the latent code. However, in experiments, we find it has enough flexible to fit different tasks even that it is not every element of the model parameters depending on all variables in the latent code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Choice Network</head><p>Choice network C receives the task features and produces the choice of the decoder. How to choose task features, using which kind of neural network to process the task features and how to calculate the weights for each decoder should be considered.</p><p>On the one hand, the choice network is reused in different layers. In order to choose different decoders in different layers, the choice network receives the input features of all training examples in the current layer as the task features of this layer. By this way, each layer chooses its own decoder. On the other hand, the choice network is also reused in the different dimension of the convolutional network kernel. The task features should be organized based on the dimension of kernels, which is detailed in Appendix A-A.</p><p>After obtaining the task features, capsule net <ref type="bibr" target="#b24">[25]</ref> is adopted to process the task features. We use 1 capsule layer in all our experiments. The task features which are input to the capsule layer are divided into several capsules, and each capsule only involves 1 variable. The output variables of the capsule layer are all in one capsule. Since <ref type="bibr" target="#b24">[25]</ref> has detailed the process of dynamic routing, more details are illustrated in Appendix A-B.</p><p>Finally, fuzzy set is used to calculate the weight of each decoders. The output variables of the capsule layer are in [−1, 1], which are denoted as {v 1 , v 2 , · · · , v N f }. We transform the output variables to [0, 1] to obtain the state variables, γ n = (v n + 1)/2, n = 1, 2, · · · , N f . Each state variable is represented into two fuzzy sets, so there is S = 2 N f . The relationship of two firing strengths is µ A (x) = 1 − µ B (x), and the value of µ A is calculated by,</p><formula xml:id="formula_14">µ A (x) =      1, x ≤ 0 1 − x, 0 &lt; x ≤ 1 0, x &gt; 1 .<label>(9)</label></formula><p>The firing strengths of two fuzzy set in γ n are µ A (γ n ) and 1 − µ A (γ n ). The weight of each decoder is calculated by:</p><formula xml:id="formula_15">c s = n µ A (γ n ) a n s µ B (γ n ) 1−a n s S l=1 n µ A (γ n ) a n l µ B (γ n ) 1−a n l ,<label>(10)</label></formula><p>where a n s ∈ {0, 1} denotes the s th variable has which fuzzy set. Due to S l=1 n µ A (γ n ) a n l µ B (γ n ) 1−a n l ≡ 1, Eq. (11) can be written as:</p><formula xml:id="formula_16">c s = n µ A (γ n ) a n s µ B (γ n ) 1−a n s .<label>(11)</label></formula><p>For the complexity of the choice network: First, the complexity of capsule net is O(N x N f ), where N x is the dimension of data features. On the one hand, N x would not larger than the dimension of the original feature. The reason is that we will use the feature embedding to process data in order to get low-dimensional, highly abstract features (see III-D). On the other hand, N f = log S and the largest S is equal to 16 in all experiments, N f is very small; therefore, the size of capsule net is a fairly small model relative to the decoders. Second, the complexity of calculating firing strength is O(S log S). This complexity is also can be ignored relative to the complexity of decoders. As a result, the time and spatial complexity of DCN are similar to those of the decoders with little effect from the choice network.</p><p>D. Meta-Training Strategy 1) Feature embedding: It is necessary to use a much deeper network, such as resnet <ref type="bibr" target="#b25">[26]</ref> or dense net <ref type="bibr" target="#b26">[27]</ref>, for higher classification accuracy on a dataset with complicated image content such as miniImagenet. However, it requires too a large amount of GPU memory to learn all the parameters of a very deep network by optimization-based methods or memory-based methods. Besides, some metric-based methods would be instability in this case <ref type="bibr" target="#b10">[11]</ref>. Therefore, the co-training or supervised pre-training methods have been proposed to ease such situation recently. For examples, the task-dependent adaptive metric (TADAM) <ref type="bibr" target="#b10">[11]</ref> trains resnet with auxiliary co-training of 64 classifications <ref type="bibr" target="#b10">[11]</ref> in order to improve convergence; Latent embedding optimization (LEO) <ref type="bibr" target="#b2">[3]</ref> used the whole classes from training and validation set to do 80 classification pre-training in order to get a feature embedding, and the similar approach was reported in <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>.</p><p>In contrast, the proposed method is trained end-to-end. The training process of DCN is summarised in <ref type="figure">Fig. 5</ref>. The feature embedding in <ref type="figure">Fig. 5</ref> is implemented using the standard CNN or much deep network, such as resnet <ref type="bibr" target="#b25">[26]</ref> or dense net <ref type="bibr" target="#b26">[27]</ref>. As <ref type="figure">Fig. 5</ref> shows, the parameters of the feature embedding θ f e is consistent during the "inner loop", and updated during the "outer loop". Due to the fine-tuning in the last few layers is differentiable, The feature embedding can be updated by gradient backward from the last few layers directly. This method make meta-training a very deep network become simpler and efficiency. Use the feature embedding to extract features:</p><formula xml:id="formula_17">D tr i = {x tr ij , y tr ij } j ,D test i = {x test ij , y test ij } j 6:</formula><p>Initialize z = z, θ FC = θ FC 7:</p><formula xml:id="formula_18">{c 1 , c 2 , · · · , c S } ← C(D tr i ) with θ cd 8: for m = 1, · · · , M do 9:θ i ← S s=1 c s · d s (z ) 10: θ i ← Normalizeθ i by Eq. (4) 11:x tr ij ← f θi (x tr ij ) 12: L tr i = j L(f θFC (x tr ij ), y tr ij ) 13: z ← z − α∇ z ,θ FC L tr i 14: end for 15:θ i ← S s=1 c s · d s (z ) 16: θ i ← Normalizeθ i by Eq. (4) 17:x test ij ← f θi (x test ij ) 18: L test i = j L(f θ FC (x test ij ), y test ij ) 19:</formula><p>end for 20:</p><formula xml:id="formula_19">Θ ← Θ − β∇ Θ i L test i 21: end while</formula><p>2) Fine-tuning: DCN is an optimization-based method and thus it implements gradient descent in the "inner loop". The latent parameters are updated by the method which we explain in III-A. The other parameters such as θ FC in the last fully connected layer (FC), showed in <ref type="figure">Fig. 5</ref>, are updated by gradient descent like MAML <ref type="bibr" target="#b5">[6]</ref>.</p><p>The hyperparameters which is adopted to control the "inner loop" like the learning rate α, the choice network C and the decoders {d 1 , d 2 , · · · , d s } are consistent during the "inner loop", and updated during the "outer loop" by stochastic gradient descent(SGD).</p><p>The "outer loop" process of DCN is summarised in Algorithm 2. Due to θ FC is also required to updated during the "inner loop", it would be a little different from Algorithm 1. Then forward propagation of training examples is performed within each task and the backward gradient is used to update the parameters θ FC and z to get θ FC and z (detailed in the "inner inner"). After training within each task, a unique model is generated for each task and this model is tested on the testing examples of each task. Finally, the loss of all tasks based on the testing examples is used to update Θ during the "outer loop". The structure of feature embedding can be any differentiable. </p><formula xml:id="formula_20">X tr Y tr P tr X test Y test P test ∇ z,θ FC L tr ∇ Θ L test C, {d1,d2,...,dS} θ cd C, {d1,d2,...,dS} θ cd θ f e z θ θFC θ f e z θi θ FC f θ f θ i FC FC z θFC z θ FC θ f e z θ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ensemble Learning</head><p>The standard training protocol used in most of the previous few-shot learning problems is decreasing learning rates during training and choosing a model by the validation set. However, a recent study found a training protocol with snapshot ensemble <ref type="bibr" target="#b11">[12]</ref> is more suitable for the model training on some datasets like miniImageNet. This is because it makes better use of the model obtained in a single training process. This is because when a model is trained based on miniIm-ageNet, there are not the same classes between training and validation set, and the number of classes is too small in training set, there is larger generalization gap between training and validation loss. When the training loss reduces quickly, the validation loss barely changes. In this case, picking models in a different number of iteration can obtain the models with similar validation losses and completely different training losses. It means that there are several which have similar generalization performance, but they are very different from each other, so the diversity and quality of models ensemble are fully guaranteed.</p><p>In this work, instead of using the best model in the training process, all models with strong performance sample from the training process are used. This approach has been adopted in support nets <ref type="bibr" target="#b30">[31]</ref>, and they have proved its effectiveness by experiments. Following <ref type="bibr" target="#b30">[31]</ref>, the proposed work chooses the top n models with the best performance on the validation set as an ensemble model instead of a single model.</p><p>We take the average outputs of all models involved in the ensemble model. The models are selected at certain iteration intervals, and sort based on accuracy. Those models are added to the ensemble model in order of accuracy on the validation set, and if one model improves the performance of the ensemble model, it is retained, otherwise dropped. The model with higher validation accuracy is tested for ensemble earlier.</p><p>The ensemble learning is integrated into the proposed optimization-based method DCN, and we call DCN with ensemble learning as DCN-E. This combination utilizes all the models obtained in a single training process, and the results show that this method improves the generation performance of the model greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Compare DCN with LEO</head><p>The method which is most similar to the proposed is LEO <ref type="bibr" target="#b2">[3]</ref>, which encodes the latent code depending on the task and decodes the latent code to the model parameters by the decoder. Both LEO <ref type="bibr" target="#b2">[3]</ref> and DCN perform gradient descent within a low-dimensional latent space during the "inner loop". However, there are several differences between LEO [3] and DCN.</p><p>To begin with, LEO <ref type="bibr" target="#b2">[3]</ref> and DCN build task dependence on different places. Both of LEO <ref type="bibr" target="#b2">[3]</ref> and DCN try to learn the latent code and the decoder which decodes the latent code to the model parameters. The difference between LEO <ref type="bibr" target="#b2">[3]</ref> and DCN is that LEO <ref type="bibr" target="#b2">[3]</ref> try to build task dependence on the latent code, while DCN try to build task dependence on the decoder. Besides, the decoder of LEO <ref type="bibr" target="#b2">[3]</ref> and the latent code of DCN are constant for different tasks.</p><p>In addition, LEO <ref type="bibr" target="#b2">[3]</ref> and DCN build task dependence by a different way. LEO <ref type="bibr" target="#b2">[3]</ref> uses a network to convert the task features to the latent code. DCN chooses the decoder from the set of decoders according to the task features. Secondly, the model parameters which LEO <ref type="bibr" target="#b2">[3]</ref> and DCN try to generate are different. LEO <ref type="bibr" target="#b2">[3]</ref> generates the model parameter of the last layer, while DCN generates that of the hidden layer. Higher time and space complexity are required for generating the model parameter of the hidden layer. This is because the model parameter of the last layer can be generated by classes within a task separately, and the features of each class are used for the model parameter of this class. However, the model parameter of the hidden layer cannot be separated by classes.</p><p>For this reason, the output size of decoder increases from h s to h 2 s , and the parameter size of decoder increases from h 2 s to h 4 s . This is the problem we analyzed in Section III-B4. Therefore, LEO <ref type="bibr" target="#b2">[3]</ref> only uses a linear decoder. However, since DCN can reduce the parameter size of decode to an acceptable scale, DCN can use multi-layers non-linear decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTATION</head><p>A comparative study in reference to the state-of-the-art approaches is reported in this section for model evaluation, and particularly to address these three questions: (1) Can optimization control of DCN learn useful information for metalearning? (2) Can the proposed training strategy train a much deep network without co-or pre-training? (3) Can snapshot ensemble <ref type="bibr" target="#b11">[12]</ref> improves the performance of meta-learning? In this work, all experiments were run on Pytorch <ref type="bibr" target="#b20">[21]</ref>.</p><p>Regression and classification tasks are used in the experiments. The regression task is a simple sinusoid curve fitting. The results are compared with MAML's <ref type="bibr" target="#b5">[6]</ref>. The classification tasks are based on the Omniglot <ref type="bibr" target="#b31">[32]</ref> and miniImageNet dataset <ref type="bibr" target="#b12">[13]</ref> which are common benchmark few-shot learning tasks. The code is available on www.github.com/AceChuse/ DCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Description</head><p>The Sinusoid curve fitting task has been used in the work of <ref type="bibr" target="#b5">[6]</ref>. The examples of each task are sampled from the input and output of a sine wine, where the amplitudes and phases of sine wine from p(T ) are different. The amplitudes and phases random are sampled from the uniform distribution from 0.1 to 5.0 and 0 to π, respectively. Both training and testing features are sampled uniformly from [−5.0, 5.0]. The mean-squared error between the output of the network and corresponding sine function value is used for both evaluation and loss function.</p><p>The Omniglot consists of samples from 50 international languages, each character has 20 instances, there are a total of 1,623 characters. The 20 instances of each character ward are written by a different person. Following the way of <ref type="bibr" target="#b12">[13]</ref>, the Omniglot dataset was divided into 1,200 and 423 characters for training and evaluation respectively. All images were resized to 28 × 28, and samples were augmented by being rotated 90, 180, 270 degrees. The model was evaluated on 1-shot and 5shot, 5-way and 20-way tasks. Each task contains the same number of shot and query.</p><p>The miniImageNet consists of 100 classes each of which involves 600 natural images. We resize all images into 84 × 84 in order to guarantee a fair comparison to prior work. The miniImageNet dataset was sampled from ILSVRC-12 dataset <ref type="bibr" target="#b32">[33]</ref>. This dataset was first proposed in <ref type="bibr" target="#b12">[13]</ref> by Vinyals et al. Following the split of the miniImageNet proposed by Ravi and Larochelle <ref type="bibr" target="#b7">[8]</ref> and most previous work, the dataset was split to 64, 16 and 20 class for training, validation, and testing, respectively.</p><p>1) Sinusoid Curve Fitting: Three hidden layers of size <ref type="bibr">[40,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b34">35]</ref> were used with ReLU nonlinearities, two layers in the middle are layer with DCN. This is different from <ref type="bibr" target="#b5">[6]</ref>, because if two hidden were used in the model, there would be only one layers with DCN. In this case, it is hard to reflect the advantages of DCN, since it needs to choose the decoders which are shared between layers. Except for the hidden layers with DCN, the other layers are the standard fully connected layer trained by MAML. There is not the feature embedding which does not fine-tune in sinusoid curve fitting experiment.</p><p>During training process, two step updates were applied with the "inner loop" fixed learning rate α = 0.01, and fixed the "outer loop" learning rate β = 10 −3 . Both of two models are trained for 60,000 iterations by Adam with AMSGrad <ref type="bibr" target="#b33">[34]</ref>. We did not update the inner learning and weight decay rate in sinusoid curve fitting experiment. During testing, we use 10, 20 and 30 steps update for 5-shot, 10-shot, and 20-shot respectively. The common loss function, mean-squared loss, was used for sinusoid curve fitting, the form of the loss is given by,</p><formula xml:id="formula_21">L test i = j f θ (x test ij ) − y test ij 2 2 ,<label>(12)</label></formula><p>where x test ij and y test ij are the input and output sampled from each sinusoid curve, f θ denotes a model with parameters θ.</p><p>Since the sizes of hidden layers are not equal to each other, we use linear interpolation to resize the output of the decoders. <ref type="table" target="#tab_4">Table I</ref> shows the results of both MAML and proposed method. There are 3116 parameters in MAML and 2,020 parameters in DCN, these contain all learnable parameters. Their structure builded in the "inner loop" used to forward is same. Due to the unlimited number of samples, using the same scale model to ensure fairness is required. 600 mini-batch of tasks are randomly sampled to evaluate, each batch has 25 tasks.</p><p>2) Omniglot: The layers with DCN were applied to replace two convolution layers before fully connected layer in a standard 4-layer embedding CNNs which proposed in <ref type="bibr" target="#b12">[13]</ref>, and mean-pooling used to replace max-pooling, this model is denoted by DCN4. DCN6 denote the model with four convolution layers and two layers with DCN before fully connected layer. The layers before the layers with DCN is the feature embedding which is not fine-tuned, and their parameters are updated by gradient descent during the "outer loop". The fully connected layer with size 64 and softmax was used to calculate the classification probability. As in the case of sinusoid curve fitting, our model has fewer parameters than a standard 4-layer convolution embedding with a fully connected layer and softmax. In the 5-way task, there are 112,005 parameters in a standard 4-layer convolution embedding and 76,553 parameters in DCN4, these contain all learnable parameters, of course. Models were trained for 60K iterations, and the initial learning rate is 10 −3 , and decays by 0.5 for every 10K episode. Inner learning and weight decay rate were updated in Omniglot experiment, and their learning rate is 0.1 times of other parameters. The common loss function, cross-entropy loss, was used for classification, the loss takes the form:</p><formula xml:id="formula_22">L test i = j k y test ij,k log f θ,k (x test ij )<label>(13)</label></formula><p>where x test ij is the features of a testing example, {y test ij,k } k is the labels of a testing example, and f θ denotes a model with parameters θ.</p><p>Since DCN is similar to LEO [3], we do the comparative experiments and the results are showed in <ref type="table" target="#tab_4">Table II</ref>. LEO is used to replace DCN in DCN4, DCN4-E, DCN6, and DCN6-E. The coefficients of entropy penalty and stopgrad penalty are set as 0.1 and 1e-8, which have a similar scale with those obtained by random search in <ref type="bibr" target="#b2">[3]</ref>, and we have not used orthogonality penalty because it requires too much GPU memory. Since we explain before that LEO requires too many parameters which make it unfair to compare with other model. In 5-way task, there are 5,125,525 parameters in LEO4, and the other three models are also much larger than the model of DCN. However, DCN has better performance in most case. Since DCN4 has the same size as the model used in the prior works, we compare it with other methods in <ref type="table" target="#tab_4">Table III</ref>.</p><p>3) MiniImageNet: The feature embedding was trained using DenseNet-161 <ref type="bibr" target="#b26">[27]</ref> with 96 initial channels and 16 growth rate. After DenseNet-161 one 1 × 1 convolution layer change the number of channels to 256 without pooling as feature embedding, the feature embedding is not fine-tuned during the "inner loop". We use the standard data augmentation from ImageNet:</p><p>• random horizontal flipping.</p><p>• resize image into 100 × 100 frame and crop the image to random size and aspect ratio, then resize to 84 × 84.</p><p>• randomly jitter the brightness contrast and saturation of the image.</p><p>After the feature embedding, the structure of resnet block (3, 3) include a layer with DCN, a convolution layer with batch normalization and the non-linear function ReLU. Although there is only one layer within DCN, it is still effective because the decoders can be been reused within the layer. The features after global average pooling fed into a fully connected layer with softmax. Models are trained for 40K iterations by Adam with AMSGrad <ref type="bibr" target="#b33">[34]</ref>. The initial learning rate is 10 −1 , and decays by 0.5 for every 10K episodes, after 20k episodes we use learning rate cyclic annealing, which is given by:</p><formula xml:id="formula_23">α(t) = α 0 2 cos πmod(t − 1, T ) T + 1 ,<label>(14)</label></formula><p>where α 0 denotes the initial learning rate, and T is the period of cyclic annealing. We set T = 2000, and decay α 0 by 0.5 for each 10K episodes. Same as Omniglot model was trained using cross-entropy loss, and use learnable inner learning and weight decay rate, and their learning rate is 0.1 times of other parameters. It is different to Omniglot we do clips of gradient norm of an iterable of parameters during "inner loop". After testing on the validation set, we retrain the model on dataset involving training set and validation set with the same hyper-parameter and choose a model with same number of iteration to the ensemble, then testing on the testing set. Since we have not computing power to train a WRN-28-10 <ref type="bibr" target="#b37">[38]</ref> by Population-Based Training (PBT) <ref type="bibr" target="#b38">[39]</ref> like in <ref type="bibr" target="#b2">[3]</ref>, we replace DCN by LEO in miniImageNet model same as Omniglot. There are more than 80 million parameters in minImageNet model with LEO, which is much larger than those in the DCN model, which has 4 million parameters.</p><p>We evaluate our model on 1-shot, 5-shot, and 5-way tasks. Both of two tasks have 8 tasks in a mini-batch. Each task contains 15 examples as a query. We random generate 1000 tasks through validation and testing set respectively after training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>The results of sinusoid curve fitting are summarised in <ref type="table" target="#tab_4">Table I</ref>, which shows that our model has better performance than MAML. It proves that sinusoid function can be learned quickly by DCN. The performance improvement of DCN is more significant when the number of shot is smaller. Compare 5-shot to 20-shot, while the loss of MAML has increased by 28 times, the loss of DCN has only increased by 6 times. This shows that our model has made better use of a small amount of data in the regression task.</p><p>The results of Omniglot experiment are listed in <ref type="table" target="#tab_4">Table II  and Table III</ref>. The support nets6 in <ref type="table" target="#tab_4">Table III</ref> involves 6-layer embedding CNNs, which is different from others. But we involve it here in order to make the table more obvious. Except for 5-way 1-shot task, all our models get higher accuracy than the state-of-the-art. The results of DCN4 in 20-way 1-shot and 5-shot are even better than support nets6, which have more convolutional layers. Besides, the results in <ref type="table" target="#tab_4">Table II</ref> show that DCN has better performance on Omniglot dataset than LEO. The results of miniImageNet experiment are shown in <ref type="table" target="#tab_4">Table IV</ref>. From this table, most of the large scale models with need co-training or pre-training, but our method obtains the state-of-art results on 5-way 5-shot classification, and comparable results with state-of-art on 5-way 1-shot classification without co-training or pre-training. The comparison between DCN and LEO shows DCN and LEO have similar performance on miniImageNet. However, DCN has much fewer parameters than LEO. Improvements from ensemble can be observed in <ref type="table" target="#tab_4">Table II  and Table IV</ref>. Ensemble method enhances most of the results, and it is more significant on miniImageNet, which has higher generation gaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>The proposed approach benefits from DCN, the feature embedding, and ensemble learning. First, DCN improves the performance of the model by updating parameters in the lowdimension space. However, even if we adopt DCN, it did not increase the size of the model, and even reduced it. The reasons are the model parameters of the layer with DCN is replaced by latent code which is a much lower dimension and DCN has fewer parameters itself.</p><p>Second, the feature embedding works well, it is effective and efficient. Fine-tuning is only required on last few layers, while the feature embedding can be updated during the "outer loop" to enhances the evaluation results of the "outer loop". This enables the learning of useful features to meta-learning,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE IV</head><p>The results of few-shot classification on miniImageNet dataset, which are average accuracy over 1000 tasks generated from test dataset. After ± is 95% confidence intervals over testing tasks. The first set is the results use convolutional networks, and the results of using much deeper network with resnet or dense net block are showed in the second layer. The method with best-performing of ours and prior works are highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Fine Tune</head><p>Co-or Pre-training 5-way Acc and gets higher performance than many models with the pretraining feature embedding. This method makes the training of training large-scale meta-model end-to-end. Finally, the results show the performance improvement of ensemble learning on miniImageNet is more significant than that on Omniglot. It is same as we speculate that the task with the more larger generation gap between the training set and validation set is more suitable for snapshot ensemble because it is able to get the models with higher quality and diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposes a meta-model using DCN, the results show that DCN is able to denote task-general information. In addition, ensemble learning is integrated with DCN to improve model generalization ability. The experimental results on a benchmark dataset show the effectiveness of solving few-shot learning problems.</p><p>In future work, despite of promising better experimental results, the work can be improved by replacing the decoder networks with other kinds of neural networks. In addition, DCN may be applied to reinforcement learning tasks for quick adaptation; learning environment information by the low-dimension parameter space may make deep reinforcement learning model more stable during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Task feature</head><p>Assume that the size of the input u of the network layer is (N d , C in , W in , H in ), where N d and C in are the numbers of examples and channels respectively, and W in and H in are the width and height respectively. The output size is (N d , C ou , W ou , H ou ). One example of the features choice is shown in <ref type="figure">Fig. 6</ref>. In <ref type="figure">Fig. 6</ref> kernel size is 9 × 9 and C in = 1.  <ref type="figure">6</ref>. An example of kernel size 9 × 9 and C in = 1, where U l is the output capsule. The kernel size, stride and zero-padding and a channel size of the layer are 9 × 9, 2 × 2, 1 × 1 and 1 respectively. When channel size is not equal to 1, w is not a vector but a matrix, but the number of U l remains unchanged. The lower right corner of the graph is the operation process after the generated parameters. The convolution operation is consistent with standard CNN.</p><p>The mathematical form of channel e is given by, (n = 1, 2, · · · , N d ), (e = 1, 2, · · · , C in ),</p><p>,</p><p>u e k = vec u 1e k , u 2e k , · · · , u N e k ,</p><p>where e and k are the indexes of channels and kernel dimensions, respectively; h k i = h k 0 +(i−1)str 1 , (i = 1, 2, · · · , H ou ) and w k i = w k 0 + (i − 1)str 2 , (i = 1, 2, · · · , W ou ), h k 0 and w k 0 are the starting coordinates of scanning; str 1 and str 2 are the strides in row and column respectively, and vec() is the operation of pulling the matrix into a row vector. The weights of a capsule layer are shared by the same channel cross different examples. Last, the task features are received by the capsule layer is given by:</p><formula xml:id="formula_26">u k = cat w 1 k u 1 k , w 2 k u 2 k , · · · , w Cin k u Cin k =        û 1|1û1|2 · · ·û 1|Ĵ u 2|1û2|2 · · ·û 2|J . . . . . . . . . . . . u N f |1ûN f |2 · · ·û N f |J         ,<label>(17)</label></formula><p>where cat() denotes the concatenation of matrix, N f is the number of state variables, J = H ou W ou N d C in , and w e k is a column vector with length N f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic routing</head><p>After getting the features, dynamic routing <ref type="bibr" target="#b24">[25]</ref> is adopted to calculate the output capsule. The 'prediction vectors" <ref type="bibr" target="#b24">[25]</ref> is given by:û ·|j = û 1|j ,û 2|j , · · · ,û N f |j T ,</p><p>where j = 1, 2, · · · , J. Eachû ·|j accumulates according to coupling coefficients and non-linear squashing is used to shrink the module of vector to 0 ∼ 1:</p><formula xml:id="formula_28">s = j L j •û ·|j ,<label>(19)</label></formula><formula xml:id="formula_29">v = ŝ 2 1 + ŝ 2ŝ ŝ ,<label>(20)</label></formula><p>where • denotes element-wise multiplication. In this case, the input includes J capsule the length of which is 1, and the output includes 1 capsule the length of which is N f ; the squashing operation is applied to the entire outputŝ. L j is the coupling coefficients that are calculated by iterative dynamic routing <ref type="bibr" target="#b24">[25]</ref> through:</p><formula xml:id="formula_30">L ij = exp(b ij ) k exp(b kj ) ,<label>(21)</label></formula><p>where L j = {L ij } i , and b = {b ij } ij denotes the correlation betweenû l and v. The initial value of b is a zero matrix. After getting the output v, b will be recalculated by:</p><p>b n· =û n|· v n , (n = 1, 2, · · · , N f ),</p><p>where b n· andû n|· are row vectors of b andû k respectively, and v n is the element of v. Repeat the process based on Eq. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>The typical structure of a neural network layer with DCN, where x tr i = {x tr ij } j and y tr i = {y tr ij } j , and the choice network C is denoted as the white box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 )</head><label>3</label><figDesc>→ (4) → (5) → (3) would loop several times before it being evaluated on testing examples. The "inner loop" process of DCN is summarised in Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>After model initialization, the approach randomly samples a batch of tasks {D tr i , D test i } i from task distribution T and extract features of all examples in {D tr i , D test i } i by the feature embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>cd θFC Θ Fig. 5 .</head><label>Θ5</label><figDesc>A typical training process, where Θ = {θ f e , z, θ cd , θ FC }, and θ d is the parameters of choice network, decoders; FC denotes the last layer of model; X tr = {x tr ij } j and Y tr = {y tr ij } j are the features and the labels of training examples, X test = {x test ij } j and Y test = {y test ij } j are those of testing examples, and P tr and P test are the model prediction for the training and testing samples. The z, is the latent code, is transformed into the model parameters θ by the decoders network. During inner training step get the training loss L tr i , and fine tune (z, θ FC ) to (z , θ FC ) by ∇ z,θ F C L tr i during the "inner loop". Then the choice network and the decoders receives z and product θ i . Last the testing loss L test i was calculated to update Θ during the "outer loop".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig. 6. An example of kernel size 9 × 9 and C in = 1, where U l is the output capsule. The kernel size, stride and zero-padding and a channel size of the layer are 9 × 9, 2 × 2, 1 × 1 and 1 respectively. When channel size is not equal to 1, w is not a vector but a matrix, but the number of U l remains unchanged. The lower right corner of the graph is the operation process after the generated parameters. The convolution operation is consistent with standard CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 21 )</head><label>21</label><figDesc>→ (19) → (20) → (22) r times after initialising b. Following the work of [25] r = 3 is applied in all the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1 Inner Loop of DCN. Require: Choice network C; Decoders {d 1 , d 2 , · · · , d S }; Latent code z; Training examples D tr i = {x tr ij , y tr ij } j ; Testing examples D test i = {x test ij , y test ij } j ; Learning rate α; Number of steps M ; Loss function L.</figDesc><table /><note>1: Initialize z = z 2:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Algorithm 2</head><label>2</label><figDesc>Outer Loop of DCN. Require: Parameters of choice network and decoders θ cd ; Learning rate of the "inner loop" α; Number of steps of the "inner loop" M ; Loss function L; Parameters of FC layer and latent code θ FC , z; Distribution over tasks p(T ); Parameters of the feature embedding θ f e ; Learning rate of the "outer loop" β; 1: Initialise Θ = {θ f e , z, θ cd , θ FC } 2: while Θ has not converged do</figDesc><table><row><cell>3: 4:</cell><cell>Sample batch of tasks {D tr i , D test i for all {D tr i , D test i } do \\ Inner Loop } i ∼ p(T )</cell></row><row><cell>5:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I</head><label>I</label><figDesc>The mean-squared error of sinusoid curve fitting. The 95% confidence intervals over tasks is showed after ±.</figDesc><table><row><cell>Models</cell><cell>5-shot</cell><cell>10-shot</cell><cell>20-shot</cell></row><row><cell>MAML</cell><cell>0.1564±0.0052</cell><cell>0.0360±0.0011</cell><cell>0.0055±0.00014</cell></row><row><cell>DCN</cell><cell>0.0176±0.0011</cell><cell>0.0051±0.0001</cell><cell>0.0028±0.00005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II The</head><label>II</label><figDesc>results of few-shot classification on Omniglot dataset, which are average accuracy over 1,800 tasks generated from testing set. After ± is 95% confidence intervals over testing tasks. The higher accuracies of DCN and LEO with same structure are highlighted. Omniglot dataset, which are average accuracy over 1800 tasks generated from testing set. After ± is 95% confidence intervals over testing tasks. The method with best-performing of ours and prior works are highlighted, and '-' means no report.</figDesc><table><row><cell>Model</cell><cell cols="2">5-way Acc</cell><cell></cell><cell cols="2">20-way Acc</cell></row><row><cell></cell><cell>1-shot</cell><cell></cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>LEO4 [16]</cell><cell>99.433±0.074%</cell><cell cols="2">99.727±0.011%</cell><cell>98.358±0.030%</cell><cell>99.178±0.005%</cell></row><row><cell>LEO4-E [16]</cell><cell>99.444±0.075%</cell><cell cols="2">99.727±0.011%</cell><cell>98.361±0.030%</cell><cell>99.184±0.005%</cell></row><row><cell>LEO6 [16]</cell><cell>99.422±0.073%</cell><cell cols="2">99.751±0.012%</cell><cell>98.692±0.026%</cell><cell>99.355±0.005%</cell></row><row><cell>LEO6-E [16]</cell><cell>99.478±0.070%</cell><cell cols="2">99.771±0.010%</cell><cell>98.736±0.026%</cell><cell>99.370±0.004%</cell></row><row><cell>DCN4</cell><cell>99.800±0.050%</cell><cell cols="2">99.891±0.008%</cell><cell>98.825±0.025%</cell><cell>99.505±0.004%</cell></row><row><cell>DCN4-E</cell><cell>99.833±0.042%</cell><cell cols="2">99.909±0.007%</cell><cell>98.842±0.025%</cell><cell>99.522±0.004%</cell></row><row><cell>DCN6</cell><cell>99.856±0.040%</cell><cell cols="2">99.924±0.007%</cell><cell>99.183±0.021%</cell><cell>99.593±0.004%</cell></row><row><cell>DCN6-E</cell><cell>99.922±0.032%</cell><cell cols="2">99.924±0.007%</cell><cell>99.108±0.022%</cell><cell>99.633±0.003%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell></row><row><cell cols="2">The results of few-shot classification on Model</cell><cell cols="2">5-way Acc</cell><cell cols="2">20-way Acc</cell></row><row><cell></cell><cell>1-shot</cell><cell></cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>MANN [4]</cell><cell>82.8%</cell><cell></cell><cell>94.9%</cell><cell>-</cell><cell>-</cell></row><row><cell>Siamese nets [35]</cell><cell>97.3%</cell><cell></cell><cell>98.4%</cell><cell>88.1%</cell><cell>97.0%</cell></row><row><cell>Matching nets [13]</cell><cell>98.1%</cell><cell></cell><cell>98.9%</cell><cell>93.8%</cell><cell>98.5%</cell></row><row><cell>Neural statistician [36]</cell><cell>98.1%</cell><cell></cell><cell>99.5%</cell><cell>93.2%</cell><cell>98.1%</cell></row><row><cell>Prototypical nets [37]</cell><cell>98.8%</cell><cell></cell><cell>99.7%</cell><cell>96.0%</cell><cell>98.9%</cell></row><row><cell>MAML [6]</cell><cell>98.7±0.4%</cell><cell></cell><cell>99.9±0.1%</cell><cell>95.8±0.3%</cell><cell>98.9±0.2%</cell></row><row><cell>Meta-SGD [9]</cell><cell cols="2">99.53±0.26%</cell><cell>99.93±0.09%</cell><cell>95.93±0.38%</cell><cell>98.97±0.19%</cell></row><row><cell>Relation nets [15]</cell><cell>99.6±0.2%</cell><cell></cell><cell>99.8±0.1%</cell><cell>97.6±0.2%</cell><cell>99.1±0.1%</cell></row><row><cell>SNAIL [16]</cell><cell cols="2">99.07±0.16%</cell><cell>99.78±0.09%</cell><cell>97.64±0.30%</cell><cell>99.36±0.18%</cell></row><row><cell>Support nets4 [31]</cell><cell cols="2">99.24±0.14%</cell><cell>99.75±0.15%</cell><cell>97.79±0.06%</cell><cell>99.27±0.15%</cell></row><row><cell>Support nets6 [31]</cell><cell cols="2">99.37±0.09%</cell><cell>99.80±0.03%</cell><cell>98.58±0.07%</cell><cell>99.45±0.04%</cell></row><row><cell>DCN4</cell><cell cols="2">99.800±0.050%</cell><cell>99.891±0.008%</cell><cell>98.825±0.025%</cell><cell>99.505±0.004%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Continuous adaptation via meta-learning in nonstationary and competitive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03641</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00837</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reptile: a scalable metalearning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can we gain more from orthogonality regularizations in training deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4266" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00109</idno>
		<title level="m">Snapshot ensembles: Train 1, get m for free</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3661" to="3670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introduction to pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ketkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning with Python</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="195" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Discriminative k-shot learning using probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Świątkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00326</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to support: Exploiting structure information in support sets for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osadchy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07270</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02185</idno>
		<title level="m">Towards a neural statistician</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09846</idno>
		<title level="m">Population based training of neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
