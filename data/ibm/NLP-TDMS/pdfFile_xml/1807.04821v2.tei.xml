<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CTAP: Complementary Temporal Action Proposal Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
							<email>jiyangga@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
							<email>kanchen@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
							<email>nevatia@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CTAP: Complementary Temporal Action Proposal Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Temporal Action Proposal; Temporal Action Detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action proposal generation is an important task, akin to object proposals, temporal action proposals are intended to capture "clips" or temporal intervals in videos that are likely to contain an action. Previous methods can be divided to two groups: sliding window ranking and actionness score grouping. Sliding windows uniformly cover all segments in videos, but the temporal boundaries are imprecise; grouping based method may have more precise boundaries but it may omit some proposals when the quality of actionness score is low. Based on the complementary characteristics of these two methods, we propose a novel Complementary Temporal Action Proposal (CTAP) generator. Specifically, we apply a Proposal-level Actionness Trustworthiness Estimator (PATE) on the sliding windows proposals to generate the probabilities indicating whether the actions can be correctly detected by actionness scores, the windows with high scores are collected. The collected sliding windows and actionness proposals are then processed by a temporal convolutional neural network for proposal ranking and boundary adjustment. CTAP outperforms state-of-the-art methods on average recall (AR) by a large margin on THUMOS-14 and ActivityNet 1.3 datasets. We further apply CTAP as a proposal generation method in an existing action detector, and show consistent significant improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We focus on the task of generating accurate temporal action proposals in videos; akin to object proposals for object detection <ref type="bibr" target="#b0">[1]</ref>, temporal action proposals are intended to capture "clips" or temporal intervals in videos that are likely to contain an action. There has been some previous work in this topic and it has been shown that, as expected and in analogy with object proposals, quality of temporal action proposals has a direct influence on the action detection performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. High quality action proposals should reach high Average Recall (AR) with as few number of retrieved proposals as possible.</p><p>The existing action proposal generation methods can be considered to belong to two main types. The first type is sliding-window based, which takes clips  <ref type="figure">Fig. 1</ref>. The architectures of three baseline methods are shown: (1) SW+R&amp;A: sliding windows are processed by a model for proposal ranking and boundary adjustment, e.g. TURN <ref type="bibr" target="#b1">[2]</ref>, SCNN <ref type="bibr" target="#b2">[3]</ref>; (2) TAG: TAG <ref type="bibr" target="#b3">[4]</ref> generate proposals based on unit-level actionness; (3) TAG+R&amp;A: actionness proposals are processed with proposal ranking and boundary adjustment.</p><p>from sliding windows as input, and outputs scores for proposals. SCNN-prop <ref type="bibr" target="#b2">[3]</ref> is a representative of this type; it applies a binary classifier to rank the sliding windows. TURN <ref type="bibr" target="#b1">[2]</ref> adopts temporal regression in additional to binary classification to adjust the boundary of sliding windows. The architecture of this type is outlined as "SW-R&amp;A" in <ref type="figure">Fig. 1</ref>. Sliding windows uniformly cover all segments in the videos (thus cover every ground truth segment), however the drawback is that the temporal boundaries are imprecise, in spite of the use of boundary adjustment, and thus high AR is reached at large number of retrieved of proposals, as shown in circle A in <ref type="figure">Fig. 1</ref>.</p><p>The second type of action proposal generation methods can be summarized as actionness score based. It applies binary classification on a finer level, i.e., unit or snippet (a few contiguous frames) level, to generate actionness scores for each unit. A Temporal Action Grouping (TAG) <ref type="bibr" target="#b3">[4]</ref> technique, derived from the watershed algorithm <ref type="bibr" target="#b4">[5]</ref>, is designed to group continuous high-score regions as proposals. Each proposal's score is calculated as the average of its unit actionness scores. The structure is shown as "TAG" in <ref type="figure">Fig. 1</ref>. This type of method generates high precision boundaries, as long as the quality of actionness scores is high. However, the actionness scores have two common failure cases: having high scores at background segments, and having low scores at action segments. The former case leads to generation of wrong proposals, while the latter case may omit some correct proposals. These lead to the upper bound of AR performance limited at a low value (circle B in <ref type="figure">Fig. 1</ref>).</p><p>Based on the above analysis, ranking-sliding-window and grouping-actionnessscore methods have two complementary properties: (1) The boundaries from actionness-based proposals are more precise as they are predicted on a finer level, and window-level ranking could be more discriminative as it takes more global contextual information; (2) actionness-based methods may omit some correct proposals when quality of actionness scores is low, sliding windows can uniformly cover all segments in the videos. Adopting the first complementary characteristic helps to resolve the first failure case of actionness proposals (i.e., generating wrong proposals). As shown in <ref type="figure">Fig. 1</ref>, a window-level classifier is applied after TAG to adjust boundaries and rank the proposals, which corresponds to model "TAG+R&amp;A". Such combination has higher AR at low number of retrieved proposals compared to the sliding-window-based method (circle C in <ref type="figure">Fig. 1</ref>). However, it still fails to solve the second failure case, when actionness scores are low at true action segments, TAG is unable to generate these proposal candidates. This results in the limited performance upper bound as shown in circle B, <ref type="figure">Fig. 1</ref>. To address this, we further explore the complementary characteristics, and propose to adaptively select sliding windows to fill the omitted ones in actionness proposals.</p><p>We propose a novel Complementary Temporal Action Proposal (CTAP) generator consisting of three modules. The first module is an initial proposal generator, which outputs actionness proposals and sliding-window proposals. The second module is a proposal complementary filter collects missing correct ones from sliding windows (addressing the second failure case of actionness score). Specifically, the complementary filter applies a binary classifier on the initial proposals to generate the probabilities indicating whether the proposals can be detected by actionness and TAG correctly, this classifier is called proposal-level actionness trustworthiness estimator. The third module ranks the proposals and adjusts the temporal boundaries. Specifically, we design a temporal convolutional neural network, rather than simple temporal mean pooling used in TURN <ref type="bibr" target="#b1">[2]</ref>, to preserve the temporal ordering information.</p><p>We evaluated the proposed method on THUMOS-14 and ActivityNet v1.3; experiments show that our method outperforms state-of-the-art methods by a large margin for action proposal generation. We further apply the generated temporal proposals on the action detection task with a standard detector, and show significant performance improvements consistently.</p><p>In summary, our contribution are three-fold: (1) We proposed a novel Complementary Temporal Action Proposal (CTAP) generator which uses the complementary characteristics of actionness proposals and sliding windows to generate high quality proposals. <ref type="bibr" target="#b1">(2)</ref> We designed a new boundary adjustment and proposal ranking network with temporal convolution which can effectively save the ordering information on the proposal boundaries. (3) We evaluated our method on two large scale datasets (THUMOS-14 and ActivityNet v1.3) and our model outperforms state-of-the-art methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we introduce the related work, which includes temporal action proposal, temporal action detection and online action detection.</p><p>Temporal Action Proposal. Temporal action proposal generation has been shown to be an effective step in action detection, and could be useful for many high level video understanding tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Two types of methods have been proposed, the first type of methods formulates it as a binary classification problem on sliding windows. Among them, Sparse-prop <ref type="bibr" target="#b7">[8]</ref> uses STIPs <ref type="bibr" target="#b8">[9]</ref> and dictionary learning for proposal generation. SCNN-prop <ref type="bibr" target="#b2">[3]</ref> is based on training C3D [10] network for binary classification task. TURN <ref type="bibr" target="#b1">[2]</ref> cuts the videos to units, and reuse unit-level features for proposals, which improves computational efficiency. TURN <ref type="bibr" target="#b1">[2]</ref> also proposes to apply temporal regression to adjust the action boundaries which improves the AR performance. The performance of this type of methods is limited by the imprecise temporal boundaries of sliding windows. The second type of method is based on snippet level actionness score and apply Temporal Action Grouping (TAG) <ref type="bibr" target="#b3">[4]</ref> method on the score sequence to group continuous high-score region as proposal. However, TAG may omit the correct proposals when the quality of actionness scores is low. Besides, DAPs <ref type="bibr" target="#b10">[11]</ref> and SST [12] are online proposal generators, which could run over the video in a single pass, without the use of overlapping temporal sliding windows.</p><p>Temporal Action Detection. This task <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b13">15]</ref> focuses on predicting the action categories, and also the start/end times of the action instances in untrimmed videos. S-CNN <ref type="bibr" target="#b2">[3]</ref> presents a two-stage action detection model, which first generates proposals and then classifies the proposals. Lin et al. propose a Single Shot Action Detector (SSAD) <ref type="bibr" target="#b14">[16]</ref>, which skips the proposal generation step and directly detects action instances in untrimmed video. Gao et al. <ref type="bibr" target="#b5">[6]</ref> design a Cascaded Boundary Regression (CBR) network to refine the action boundaries iteratively. SSN <ref type="bibr" target="#b3">[4]</ref> presents a mechanism to model the temporal structures of activities, and thus the capability of discriminating between complete and incomplete proposals for precisely detecting actions. R-C3D <ref type="bibr" target="#b15">[17]</ref> designs a 3D fully convolutional network, which generates candidate temporal regions and classifies selected regions into specific activities in a two-stage manner. Yuan et al. <ref type="bibr" target="#b16">[18]</ref> propose to localize actions by searching for the structured maximal sum of frame-wise classification scores. Shou et al. <ref type="bibr" target="#b17">[19]</ref> design a Convolutional-De-Convolutional (CDC) operation that makes dense predictions at a fine granularity in time to determine precise temporal boundaries. Dai et al. <ref type="bibr" target="#b18">[20]</ref> propose a temporal context network, which adopts a similar architecture to Faster-RCNN <ref type="bibr" target="#b0">[1]</ref>, for temporal action detection. Beyond the fixed category action detection, TALL <ref type="bibr" target="#b19">[21]</ref> proposes to use natural language as the query to detect the target actions in videos.</p><p>Online action detection <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b22">24]</ref> is different from temporal action detection that the whole video is not available at detection time, thus it needs the system to detect actions on the fly. Geest et al. <ref type="bibr" target="#b20">[22]</ref> built a dataset for online action detection, which consists of 16 hours (27 episodes) of TV series with temporal annotation for 30 action categories. Gao et al. <ref type="bibr" target="#b21">[23]</ref> propose a Reinforced Encoder Decoder (RED) network for online action detection and action anticipation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Complementary Temporal Action Proposal Generator</head><p>In this section, we present the details of the Complementary Temporal Action Proposal (CTAP) generator. There are three stages in the pipeline of CTAP. The architecture of Complementary Temporal Action Proposal (CTAP) generator. "BA" is short for boundary adjustment, "PR" is short for proposal ranking, "ppl" is short for proposal and "bdy" is short for boundary.</p><p>The first stage is to generate initial proposals, which come from two sources, one is actionness score and TAG <ref type="bibr" target="#b3">[4]</ref>, the other is sliding windows. The second stage is complementary filtering. As we discussed before, TAG omits some correct proposals when the quality of actionness score is low (i.e. low actionness score on action segments), but sliding windows uniformly cover all segments in videos. Thus, we design a complementary filter to collect high quality complementary proposals from sliding windows to fill the omitted actionness proposals. The third stage is boundary adjustment and proposal ranking, which is composed of a temporal convolutional neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Initial Proposal Generation</head><p>In this part, we first introduce video pre-processing, then present the actionness score generation, temporal grouping process and sliding window sampling strategy.</p><p>Video pre-processing. Following previous work <ref type="bibr" target="#b1">[2]</ref>, a long untrimmed video is first cut into video units or snippets, each unit contains n u continuous frames. A video unit u is processed by a visual encoder E v to extract the unit-level rep-</p><formula xml:id="formula_0">resentation x u = E v (u) ∈ R d f .</formula><p>In our experiments, we use the two-stream CNN model <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b24">26]</ref> as the visual encoder, details are given in Sec 4.2. Consequently, a long video is converted to a sequence of unit-level features, which are used as basic processing units later.</p><p>Actionness score. Based on unit-level features, we train a binary classifier to generate actionness score for each unit. Specifically, we design a two-layer temporal convolutional neural network, which takes a t a continuous unit features as input, x ∈ R ta×d f , and outputs a probability for each unit indicating whether it is background or action, p x ∈ R ta .</p><formula xml:id="formula_1">p x = σ(t conv (x)), t conv (x) = F(ϕ(F(x; W 1 )); W 2 )<label>(1)</label></formula><p>where F(.; W) denotes a temporal convolution operator, W is the weight of its convolution kernel. In this network,</p><formula xml:id="formula_2">W 1 ∈ R d f ×dm×k×k , W 2 ∈ R dm×1×k×k (k is</formula><p>the kernel size) are training parameters. ϕ(.) is an non-linear activation function, σ(.) is a sigmoid function. After generating the probability p x for each continuous unit features x, the loss is calculated as the cross-entropy for each input sample within the batch:</p><formula xml:id="formula_3">L act = − 1 N N i=1 y i log(p xi ) + (1 − y i ) log(1 − p xi )<label>(2)</label></formula><p>where y i ∈ R ta is a binary sequence for each input x i indicating whether each unit in x i contains action (label 1) or not (label 0). N is the batch size. Actionness proposal generation strategy. We follow <ref type="bibr" target="#b3">[4]</ref> and implement a watershed algorithm <ref type="bibr" target="#b4">[5]</ref> to segment 1-D sequence signals. Given each unit's actionness score, raw proposals are generated whose units all have scores larger than a threshold τ . For some neighbor raw proposals, if the time during ration (i.e., maximum end time minus minimum start time among these raw proposals) is larger than a ratio η of the whole video length, we group them as a proposal candidate. We iterate all possible combinations of τ and η to generate proposal candidates and apply non-maximum suppression (NMS) to eliminate redundant proposals. The output actionness proposals are denoted as {b j }.</p><p>Sliding window sampling strategy. Unlike actionness proposals which depend on actionness score distribution, sliding windows can uniformly cover all segments in the videos. The goal is to maximum the match with groundtruth segments (high recall), meanwhile maintaining the number of sliding windows as low as possible. In our experiments, different combinations of window size and overlap ratio are tested on validation set. The sliding windows are denoted as {a k }. Detail setting is given in Sec 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposal Complementary Filtering</head><p>As discussed before, actionness proposals could be more precise but less stable, but sliding windows are more stable but less precise. The goal of second stage is to collect proposals, that could be omitted by TAG, from sliding windows. The core of this stage is a binary classifier, whose input is a sequence of unit features (i.e. a proposal), and output is the probability that indicates whether this proposal can be correctly detected by the unit-level actionness scores and TAG. This classifier is called Proposal-level Actionness Trustworthiness Estimator (PATE).</p><p>PATE training. The training samples are collected as follows: Given a video, the groundtruth segments {g i } are matched with actionness proposals {b j }. For a groundtruth segment g i , if there exists an actionness proposal b j that has temporal Intersection over Union (tIoU) with g i larger than a threshold θ c , then we label g i as a positive sample (y i = 1); if no such b j exists, then g i is labelled as a negative sample (y i = 0). The unit level features inside g i are mean pooled to a single proposal-level feature x gi ∈ R d f . PATE outputs trustworthiness scores indicating the probabilities that whether the proposals can be correctly detected by actionness scores and TAG:</p><formula xml:id="formula_4">s i = σ (W 4 (ϕ(W 3 x gi + b 3 )) + b 4 )<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">W 3 ∈ R d f ×dm , W 4 ∈ R dm×1 , b 3 ∈ R dm , b 4 ∈ R are training parameters.</formula><p>Other notations are similar to Eq. 1. The network is trained by a standard cross-entropy loss over training samples from each batch (N is the batch size).</p><formula xml:id="formula_6">L pate = − 1 N N i=1 [y i log(s i ) + (1 − y i ) log(1 − s i )]<label>(4)</label></formula><p>Complementary filtering. In test stage, we apply the trustworthiness estimator to every proposal from sliding windows {a k }. For an input proposal, the trustworthiness score p t tells us that "how well the actionness scores are trustworthy on the video content from this proposal". For a sliding window a k , if p t (a k ) is lower than a threshold θ a (means TAG may fail on this segment), this sliding window is collected. The collected proposals from sliding windows and all actionness proposals are denoted as {c m }, and are sent to the next stage, which ranks the proposals and adjusts the temporal boundaries. We call this process as complementary filtering and the name derives from somewhat similar processes used in estimation theory 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proposal Ranking and Boundary Adjustment</head><p>The third stage of CTAP is to rank the proposals and adjust the temporal boundaries. TURN <ref type="bibr" target="#b1">[2]</ref> does this also, however it uses mean-pooling to aggregate temporal features, which losses the temporal ordering information. Instead, we design a Temporal convolutional Adjustment and Ranking (TAR) network which use temporal conv layers to aggregate the unit-level features.</p><p>TAR Architecture. Suppose that the start and end units (i.e. temporal boundary) of an input proposal c m are u s , u e , we uniformly sample n ctl unitlevel features inside the proposal, called proposal units x c ∈ R n ctl ×d f . We sample n ctx unit features at the start and end boundaries respectively, which are [u s − n ctx /2, u s + n ctx /2] and [u e − n ctx /2, u e + n ctx /2], called boundary units (denoted as x s ∈ R nctx×d f , x e ∈ R nctx×d f ). Boundary units and proposal units are illustrated in <ref type="figure">Fig. 2</ref>. These three feature sequences (one sequence for proposal units and two sequences for boundary units) are input to three independent subnetworks. The proposal ranking sub-network outputs probability of action, the boundary adjustment sub-network outputs regression offsets. Each sub-network contains two temporal convolutional layers. which can be represented as:</p><formula xml:id="formula_7">o s = t conv (x s ), p c = σ(t conv (x c )), o e = t conv (x e )<label>(5)</label></formula><p>where o s , o e , p c denote the offsets prediction for start and end boundaries and the action probability for each proposal respectively. Other notations are the same in Eq. 1. Similar to TURN <ref type="bibr" target="#b1">[2]</ref>, we use non-parameterized regression offsets. The <ref type="bibr" target="#b0">1</ref> The original use of complementary filtering is to estimate a signal given two noisy measurements, where one of the noise is mostly high-frequency (maybe precise but not stable) and the other noise is mostly low-frequency (stable but not precise).</p><p>final score for a proposal a k from sliding windows is multiplied by the PATE score (p t (a k ) · p c (a k )). The actionness proposals use p c (a k ) as the final score. TAR Training. To collect training samples, we use dense sliding windows to match with groundtruth action segments. A sliding window is assigned to a groundtruth segments if: (1) it has the highest tIoU overlaps with a certain groundtruth segment among all other windows; or (2) it has tIoU larger than 0.5 with any one of the groundtruth segments. We use the standard Softmax cross-entropy loss to train proposal ranking sub-network and the L1 distance loss for boundary adjustment sub-network. Specifically, the regression loss can be expressed as,</p><formula xml:id="formula_8">L reg = 1 N pos Npos i=1 l * i (|o s,i − o * s,i | + |o e,i − o * e,i |)<label>(6)</label></formula><p>where o s,i is the predicted start offset, o e,i is the predicted end offset, o * s,i is the groundtruth start offset, o * e,i is the groundtruth end offset. l * i is the label, 1 for positive samples and 0 for background samples. N pos is the number of positive samples in a mini-batch, as the regression loss is calculated only for positive samples. Similar to Eq. 4, a cross entropy objective is calculated to guide the learning of prediction score p c for each proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate CTAP on THUMOS-14 <ref type="bibr" target="#b25">[27]</ref> and ActivityNet v1.3 <ref type="bibr" target="#b26">[28]</ref> datasets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>THUMOS-14 contains 1010 and 1574 videos for validation and testing purposes from 20 sport classes. Among them, there are 200 and 212 videos are labeled with temporal information in validation and test set respectively. Following the settings of previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, we train our model on the validation set and conduct evaluation on the test set. ActivityNet v1.3 consists of 19,994 videos collected from YouTube labeled in 200 classes. The whole dataset is divided into three disjoint splits: training, validation and test, with a ration of 50%, 25%, 25%, respectively. Since the annotation of the test split is not publicly available for competition purpose, we compare and report performances of different models on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Setup</head><p>Unit-level feature extraction. We use the twostream model <ref type="bibr" target="#b24">[26]</ref> as the visual encoder E v that is pre-trained on ActivityNet v1.3 training set. In each unit, the central frame is sampled to calculate the appearance CNN feature, it is the output of Flatten 673 layer in ResNet <ref type="bibr" target="#b27">[29]</ref>. For the motion feature, we sample 6 consecutive frames at the center of a unit and calculate optical flows between them; these flows are then fed into the pretrained BN-Inception model <ref type="bibr" target="#b28">[30]</ref> and the output of global pool layer is extracted. The motion features and the appearance features are both 2048-dimensional, and are concatenated into 4096-dimensional vectors (d f = 4096), which are used as unit-level features. On THUMOS-14, we test our model with two settings of unit features Flow-16 and Twostream-6. Flow-16 only uses denseflow CNN features, and the unit size is set to 16, which is the same as <ref type="bibr" target="#b1">[2]</ref>(n u = 16), Twostream-6 use two-stream features and unit size is 6 (n u = 6). On ActivityNet v1.3, two-stream features are used and unit size is 16 (Twostream-16, n u = 16). Sliding window sampling strategy. We follow TURN <ref type="bibr" target="#b1">[2]</ref> and adopt proposals' length set of {16, 32, 64, 128, 256, 512} with tIOU of 0.75, which achieves the optimal results. On ActivityNet v1.3, we adopt proposals' length set of <ref type="bibr">{64, 128, 256, 512, 768, 1024, 1536, 2048, 2560, 3072, 3584, 4096</ref>, 6144} with tIOU = 0.75, which achieves the reported best performance in the submission. Actionness score generation. We set the kernel size for each temporal convolution as 3 (k = 3). The stride for temporal convolution is 1. We choose rectified linear unit (ReLU) as the non-linear activation function ϕ. The first temporal convolution output dimension d m = 1024. t a is set to be 4. Batch size is 128, learning rate is 0.005, and the model is trained for about 10 epochs. TAG algorithm. Following the setting of <ref type="bibr" target="#b3">[4]</ref>, we set the initial value of τ as 0.085. To enumerate all possible combinations of (τ, η), we first iterate τ in the range of [0.085, 1) with a step of 0.085. In each iteration, we further iterate η in the range of [0.025, 1] with a step of 0.025. The threshold of NMS is set as 0.95 to eliminate redundant proposals. PATE setting. We set the first fully-connected layer's output dimension d m = 1024. θ a is set to be 0.1 on THUMOS-14 and ActivityNet v1.3. Batch size is 128 and learning rate is 0.005. PATE is trained for about 10 epochs. TAR setting. On THUMOS-14, we uniformly sample 8 unit features inside each proposal (n ctl = 4), and 4 unit features as context (n ctx = 4). On ActivityNet v1.3, we set n ctl = 8 and n ctx = 4. d m is set to 1024. TAR is optimized using Adam algorithm <ref type="bibr" target="#b29">[31]</ref>. Batch size is 128 and learning rate is 0.005. TAR is trained for 10 epoches on THUMOS-14 and 4 epoches on ActivityNet v1.3. Evaluation Metrics. For temporal action proposal generation task, Average Recall (AR) is usually used as evaluation metrics. Following previous work, we use IoU thresholds set from 0.5 to 1.0 with a step of 0.05 on THUMOS-14 and 0.5 to 0.95 with a step of 0.05 on ActivityNet v1.3. We draw the curve of AR with different Average Number(AN) of retrieved proposals to evaluate the relationship between recall and proposal number, which is called AR-AN curve. On ActivityNet v1.3, we also use area under the AR-AN curve (AUC) as metrics, where AN varies from 0 to 100. For the evaluation of temporal action detection, we follow the traditional mean Average Precision (mAP) metric used in THUMOS-14. A prediction is regarded as positive only when it has correct category prediction and tIoU with ground truth higher than a threshold. We use the official toolkit of THUMOS-14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on THUMOS-14</head><p>In this part, we evaluate our method on THUMOS-14 dataset. First, we compare our proposal ranking and boundary adjustment module TAR with TURN <ref type="bibr" target="#b1">[2]</ref>. Second, we evaluate the effectiveness of PATE and the proposal complementary filtering module. Third, we compare our full model with state-of-the-art methods, and finally we apply our proposals on action detection task to verify the its performance advantage. TAR vs TURN <ref type="bibr" target="#b1">[2]</ref>. As we presented before, TURN <ref type="bibr" target="#b1">[2]</ref> uses temporal mean pooling to aggregate features, it losses temporal ordering information, which could be important for boundary adjustment. TAR uses temporal convolution to extract temporal information from unit features, and adopts independent subnetworks for proposal ranking and boundary adjustment. To fairly compare with TURN, we use flow-16 features, and the same test sliding window settings as TURN. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we can see that, at AN=50, 100 and 200, TAR outperforms TURN at all these points, which shows the effectiveness of TAR. Complementary filtering. Besides using PATE in the proposal complementary filtering, we design three baseline methods to combine the sliding windows and actionness proposals. The first method is a simple "union", in which we simply put all actionness proposals and all sliding windows together, and send them into TAR module for ranking and adjustment. The second method is "union"+NMS, in which we apply NMS to filter the duplicate proposals from the union set; the threshold of NMS is set to 0.7, which achieves the best performance among {0.5,0.7,0.9}. The third method is tIoU-based: all actionness proposals are selected; we calculate the tIoU between the sliding windows and  actionness proposals, if there exists a sliding window whose highest tIoU with all actionness proposals is lower than 0.5, then it is selected. We use flow-16 unit features and the same test sliding windows in "TAR vs TURN" experiments.</p><p>The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. We can see that, complementary filtering achieves the best AR on every AN (50, 100 and 200). The performance of "Union" suffers at low AN, but is higher than "tIoU-selection" at AN=200. We believe the reason is that simple union method adds too many low quality proposals from sliding windows. Union+NMS improves the performance, however due to the lack of priority of TAG and SW proposals, NMS may select an inaccurate SW proposal with a higher score instead of an accurate TAG proposal with a lower score. In contrast, PATE tries to preserve such priority and focuses on picking out the sliding window proposals that TAG may fail on. tIoU-selection also suffers, as it eliminates some high quality windows simply based on the tIoU threshold. Complementary filtering dynamically generates trustworthiness scores on different windows, which make the selection process more effective. We also show the AR performance of two sources, actionness proposals and sliding windows, in <ref type="figure" target="#fig_2">Fig. 3</ref>. Both flow-16 (F16) feature and twostream-6 (TS6) feature are illustrated. It can be seen that the performance of complementary proposals is higher than that of actionness proposals (TAG+TAR) and sliding windows (SW+TAR) at every AN consistently, which shows that our method can effectively select high quality complementary proposals from sliding windows to fill the omitted ones in actionness proposals.</p><p>Comparison with state-of-the-art methods. We compare our full model with state-of-the-art methods on THUMOS-14 dataset by the Average recall on average number of proposals (AR-AN) curve and recall@100-tIoU curve, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. It can be seen that our model outperforms the state-of-the-art model by a large margin on both curves. Specifically, for AR@100, the performance of CTAP is around 43%, while the state-of-the-art method TURN <ref type="bibr" target="#b1">[2]</ref> only achieves about 32%.  CTAP for Temporal action detection. To verify the quality of our proposals, we feed CTAP proposals into SCNN <ref type="bibr" target="#b2">[3]</ref>, and compare with other proposal generation methods on the same action detector (SCNN). The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. We can see that our CTAP-TS6 achieves the best performance, and outperforms the state-of-the-art proposal method TURN <ref type="bibr" target="#b1">[2]</ref> and TAG <ref type="bibr" target="#b3">[4]</ref> by over 4%, which proves the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation on ActivityNet v1.3</head><p>Evaluation of TAR. To show the effectiveness of TAR, we report the AR@100 values and area under AR-AN curve for different models in <ref type="table" target="#tab_3">Table 4</ref>. For sliding window proposals, we observe that TAR's prediction (SW-TAR) achieves 18.29% and 6.86% improvement in AR@100 and AUC compared to those of TURN <ref type="bibr" target="#b1">[2]</ref>   (SW-TURN). The results show that TAR is more effective in temporal boundary adjustment and proposal ranking. For actionness proposals, we observe that TAR achieves 10.70% increase compared to TURN [2] on AUC.</p><p>Evaluation of PATE. Based on TAR, we further explore the function of PATE complementary filtering. We evaluate three different models: (1) sliding window proposals with TAR (SW-TAR) (2) actioness proposals with TAR (TAG-TAR) (3) PATE Complementary proposals with TAR (our full model, CTAP). Different models' performances of AR@100 and AUC are reported in <ref type="table" target="#tab_3">Table 4</ref>. CTAP achieves consistently better performance of AR@100 and AUC compared to SW-TAR and TAG-TAR, which shows its advantage of selecting complementary proposals from sliding windows to fill the omitted ones in actionness proposals. Comparison with state-of-the-art methods. CTAP is compared with stateof-the-art methods on ActivityNet v1.3 validation set by the Average Recall at top 100 ranked proposals (AR@100) and area under AR-AN curve (AUC). In <ref type="table" target="#tab_3">Table 4</ref>, we find CTAP achieves 2.60% and 1.32% increase in AR@100 compared with state-of-the-art methods MSRA <ref type="bibr" target="#b30">[32]</ref> and Prop-SSAD <ref type="bibr" target="#b31">[33]</ref> respectively. Generalization ability of proposals. We evaluate the generalization ability of CTAP on ActivityNet v1.3 validation set. Following the setting of <ref type="bibr" target="#b32">[34]</ref>, we evaluate the AR@100 and AR-AN under curve area (AUC) for 100 seen classes and unseen classes respectively. In <ref type="table" target="#tab_4">Table 5</ref>, we observe that CTAP achieves better performance on 100 seen classes. On unseen 100 classes, there is only a slight drop in AR@100 and AUC, which shows the generalizability of CTAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head><p>We further visualize some temporal action proposals generated by CTAP. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, CTAP is able to select most suitable initial proposals from actionness proposals or sliding windows, and then adjust their temporal boundaries more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Previous methods for temporal action proposal generation can be divided to two groups: sliding window ranking and actionness score grouping, which are complementary to each other: sliding windows uniformly cover all segments in videos, but the temporal boundaries are imprecise; actionness score based method may have more precise boundaries but it may omit some proposals when the quality of actioness scores is low. We propose a novel Complementary Temporal Action Proposal (CTAP) generator, which could collect high quality complementary proposals from sliding windows and actionness proposals. A temporal convolutional network for proposal ranking and boundary adjustment is also designed. CTAP outperforms state-of-the-art methods by a large margin on both THUMOS-14 and ActivityNet 1.3 datasets. Further experiments on action detection show consistent large performance improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>indicates equal contribution. Code is in http://www.github.com/jiyanggao/CTAP. arXiv:1807.04821v2 [cs.CV] 18 Jul 2018</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. The architecture of Complementary Temporal Action Proposal (CTAP) generator. "BA" is short for boundary adjustment, "PR" is short for proposal ranking, "ppl" is short for proposal and "bdy" is short for boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>AR-AN curves of the complementary results with flow-16 feature (F16) and twostream-6 feature (TS6). Complementary filtering proposals outperform sliding windows (SW+TAR) and actionness proposals (TAG+TAR) consistently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>AN-AR curve and recall@AN=100 curve of CTAP and state-of-the-art methods on THUMOS-14 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of temporal action proposals generated by CTAP. First two rows represent 4 temporal action proposals from 2 videos in THUMOS-14. Last two rows represent 4 temporal action proposals from 2 videos in ActivityNet v1.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison between TAR and TURN [2] on THUMOS-14 test set. Same unit feature (flow-16) and test sliding windows are used on TAR and TURN for fair comparison. Average Recall (AR) at different numbers is reported.</figDesc><table><row><cell>Method</cell><cell cols="3">AR@50 AR@100 AR@200</cell></row><row><cell>TURN[2]</cell><cell>21.75</cell><cell>31.84</cell><cell>42.96</cell></row><row><cell>TAR</cell><cell>22.99</cell><cell>32.21</cell><cell>45.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Complementary filtering evaluation on THUMOS-14 test set, compared with "Union" and "tIoU-selection". Average Recall (AR) at different numbers is reported.</figDesc><table><row><cell>Method</cell><cell cols="3">AR@50 AR@100 AR@200</cell></row><row><cell>Union</cell><cell>25.80</cell><cell>34.70</cell><cell>46.19</cell></row><row><cell>Union+NMS</cell><cell>28.07</cell><cell>39.71</cell><cell>49.60</cell></row><row><cell>tIoU-selection</cell><cell>30.35</cell><cell>38.34</cell><cell>42.41</cell></row><row><cell>PATE complementary filtering</cell><cell>31.03</cell><cell>40.23</cell><cell>50.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of CTAP and other proposal generation methods with the same action detector (SCNN) on THUMOS-14 test set, mean Average Precision (mAP % @tIoU=0.5) is reported.</figDesc><table><row><cell>Method</cell><cell cols="7">Sparse [8] DAPs [11] SCNN-prop[3] TURN [2] TAG[4] CTAP-F16 CTAP-TS6</cell></row><row><cell>tIoU=0.5</cell><cell>15.3</cell><cell>16.3</cell><cell>19.0</cell><cell>25.6</cell><cell>25.9</cell><cell>27.9</cell><cell>29.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Evaluation of TURN<ref type="bibr" target="#b1">[2]</ref>, TAR, MSAR<ref type="bibr" target="#b30">[32]</ref>, Prop-SSAD<ref type="bibr" target="#b31">[33]</ref> and CTAP on ActivityNet v1.3 validation set. AR@100 and AUC of AR-AN curve are reported. (The AR@100 of MSRA<ref type="bibr" target="#b30">[32]</ref> is not available.)</figDesc><table><row><cell>Method</cell><cell>SW-TURN [2]</cell><cell>TAG-TURN [4]</cell><cell>SW-TAR</cell><cell>TAG-TAR</cell><cell>MSRA [32]</cell><cell>Prop-SSAD [33]</cell><cell>CTAP</cell></row><row><cell>AR@100</cell><cell>49.73</cell><cell>63.46</cell><cell>68.02</cell><cell>64.01</cell><cell>-</cell><cell>73.01</cell><cell>73.17</cell></row><row><cell>AUC</cell><cell>54.16</cell><cell>53.92</cell><cell>61.02</cell><cell>64.62</cell><cell>63.12</cell><cell>64.40</cell><cell>65.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Generalization evaluation of CTAP on Activity Net v1.3 (validation set) in terms of AR@100 and AR-AN under curve area.</figDesc><table><row><cell></cell><cell cols="2">Seen (100 classes) Unseen (100 classes)</cell></row><row><cell>AR@100</cell><cell>74.06</cell><cell>72.51</cell></row><row><cell>AR-AN</cell><cell>66.01</cell><cell>64.92</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported, in part, by the Office of Naval Research under grant N00014-18-1-2050 and by an Amazon Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TURN TAP: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The watershed transform: Definitions, algorithms and parallelization strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Roerdink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meijster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundamenta informaticae</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Motion-appearance co-memory networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICCV</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Sst: Single-stream temporal action proposals</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Serena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Olga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Greg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal localization of finegrained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ACM MM</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM, ACM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">R-C3D: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">CDC: Convolutionalde-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Red: Reinforced encoder-decoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Online action detection in untrimmed, streaming videos-modeling and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vetro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CUHK &amp; ETHZ &amp; SIAT submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR Workshop</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICLR</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MSR asia msm at activitynet challenge 2017: Trimmed action recognition, temporal action proposals and dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal convolution based action proposal: Submission to activitynet 2017</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV Workshop</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A pursuit of temporal accuracy in general activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
