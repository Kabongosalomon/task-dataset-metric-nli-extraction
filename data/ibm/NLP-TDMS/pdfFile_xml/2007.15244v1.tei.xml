<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HIERARCHICAL ACTION CLASSIFICATION WITH NETWORK PRUNING A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-31">July 31, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Davoodikakhki</surname></persName>
							<email>mahdid@sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangkang</forename><surname>Yin</surname></persName>
							<email>kkyin@sfu.ca</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HIERARCHICAL ACTION CLASSIFICATION WITH NETWORK PRUNING A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-31">July 31, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human Action Recognition · Human Action Classification · Hierarchical Classification · Network Pruning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research on human action classification has made significant progresses in the past few years. Most deep learning methods focus on improving performance by adding more network components. We propose, however, to better utilize auxiliary mechanisms, including hierarchical classification, network pruning, and skeleton-based preprocessing, to boost the model robustness and performance. We test the effectiveness of our method on four commonly used testing datasets: NTU RGB+D 60, NTU RGB+D 120, Northwestern-UCLA Multiview Action 3D, and UTD Multimodal Human Action Dataset. Our experiments show that our method can achieve either comparable or better performance on all four datasets. In particular, our method sets up a new baseline for NTU 120, the largest dataset among the four. We also analyze our method with extensive comparisons and ablation studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action classification and recognition has many important applications, such as autonomous driving, smart surveillance, patient monitoring, and interactive games. Despite extensive research on this topic in recent years, human-level performance is still out of reach. Image classification, however, has achieved human-level performance a few years ago. There are many challenges in human action recognition. First, there are high intra-class variations and inter-class similarities. A powerful deep learning model and a large amount of training data are necessary to achieve good performance. Second, the qualities of input videos vary greatly. There are multiple benchmark datasets, and in this work we focus on captured indoor videos in lab environments. Third, multiple data types and representations can be captured with the video data or extracted from the videos. Skeleton data, for example, should be used whenever possible.</p><p>We propose to extend the Inflated ResNet architecture with hierarchical classification for better feature learning at different scales. Iterative pruning is then incorporated for a further performance boost. We also use skeleton data, captured or extracted, to crop out irrelevant background so the learning can focus on human activities. These mechanisms combined set up a new baseline for the newly-released large-scale dataset NTU RGB+D 120.</p><p>In summary, our main contributions include:</p><p>• We show that Inflated ResNet coupled with hierarchical classification can boost the performance of the baseline model.</p><p>• We show that iterative pruning can help improve the performance even further.</p><p>• We also show that 2D/3D skeleton data, when available, could be used to crop videos in a preprocessing stage to increase the classification accuracy in most cases.</p><p>arXiv:2007.15244v1 [cs.CV] 30 Jul 2020</p><p>• We evaluate our method extensively on four datasets. Our method sets up a new baseline for the NTU RGB+D 120 dataset for future research in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a large body of prior work related to our work. Due to the limited space, we only summarize the most relevant and most recent papers here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Human Action Classification</head><p>Most human action classification methods work on either RGB image sequences and/or skeleton data. Our method uses both 2D skeletons and video inputs for classification, so we will review and compare with state-of-the-art methods from both categories. However, we only use skeletons for preprocessing to crop out irrelevant backgrounds, and not for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Skeleton-based action classification</head><p>For skeleton-based classification, traditional CNN (Convolutional Neural Networks) methods can still be used after converting skeleton data into 2D images. Example works include TSRJI <ref type="bibr" target="#b0">[1]</ref>, Skelemotion <ref type="bibr" target="#b1">[2]</ref>, Enhanced Viz. <ref type="bibr" target="#b2">[3]</ref>, JTM <ref type="bibr" target="#b3">[4]</ref>, JDM <ref type="bibr" target="#b4">[5]</ref>, and Optical Spectra <ref type="bibr" target="#b5">[6]</ref>. RNN (Recurrent Neural Networks) and its two common variations LSTM (Long Short-Term Memories) and GRU (Gated Recurrent Units) can also be used to interpret skeleton sequences. Their ability to learn long and short-term memories help achieve good results. Example works include TS-LSTM <ref type="bibr" target="#b6">[7]</ref> and EleAtt-GRU <ref type="bibr" target="#b7">[8]</ref>. Most advanced methods, however, are based on Graph Convolutional Networks (GCN), which can model sparse joint connections. Example works include MS-G3D Net <ref type="bibr" target="#b8">[9]</ref>, DGNN <ref type="bibr" target="#b9">[10]</ref>, 2s-AGCN <ref type="bibr" target="#b10">[11]</ref>, FGCN <ref type="bibr" target="#b11">[12]</ref>, and GVFE + AS-GCN with DH-TCN <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Video-based action classification</head><p>Most state-of-the-art video-based classification methods are based on CNN. Inflated 3D ConvNet (I3D) and Inflated ResNet proposed by <ref type="bibr" target="#b13">[14]</ref> became the foundation of many advanced algorithms, such as MMTM <ref type="bibr" target="#b14">[15]</ref>, Glimpse Clouds <ref type="bibr" target="#b15">[16]</ref>, Action Machine <ref type="bibr" target="#b16">[17]</ref>, and PGCN <ref type="bibr" target="#b17">[18]</ref>. Such networks inflate 2D kernels of convolutional filters and pooling layers along the temporal domain to process 3D spatio-temporal information. In addition, Glimpse Clouds <ref type="bibr" target="#b15">[16]</ref> extracts attention glimpses from each frame and uses the penultimate feature maps to estimate 2D joint positions and encourage glimpses to focus on the people in the scene. Action Machine <ref type="bibr" target="#b16">[17]</ref> extracts Region of Interests (RoI), which are human bounding boxes, for better pose estimation and classification over these regions. The skeleton classification results are then fused with video-based classification to boost the performance further. PGCN <ref type="bibr" target="#b17">[18]</ref> performs graph convolutions over RGB features extracted around 2D joints rather than over the joint positions to improve the video-based classification performance, which is then also fused with skeleton-based classification scores. Our baseline network is similar to that of Glimpse Clouds <ref type="bibr" target="#b15">[16]</ref>, and our cropping preprocess is inspired by Action Machine <ref type="bibr" target="#b16">[17]</ref>.</p><p>In addition to video and skeleton input, various other data types can be used for input or intermediate feature representations. For instance, PoseMap <ref type="bibr" target="#b18">[19]</ref> extracts pose estimation maps from the RGB frames. 3D optical flow can also be estimated for classification <ref type="bibr" target="#b19">[20]</ref>. RGB and optical flow classifications can be fused to further boost performance <ref type="bibr" target="#b20">[21]</ref>. MMTM <ref type="bibr" target="#b14">[15]</ref> uses a combination of depth, optical flow, and skeleton data with RGB frames as input for different datasets. Different fusion strategies have also been investigated, such as MMTM <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b21">[22]</ref> that fuse features from intermediate layers for the next layers or final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Classification and Loss Functions</head><p>Hierarchical classification and loss functions facilitate learning the most important features in different scales. One straightforward way to apply hierarchical classification is to learn on different resolutions of the same set of images, such as <ref type="bibr" target="#b22">[23]</ref> for skin lesion classification. Semantic graphs can be constructed to form hierarchical relations among classes for text classification <ref type="bibr" target="#b23">[24]</ref>. Our approach is mainly inspired by related works in image classification that use features from intermediate layers for the main classification as well, by accumulating loss functions from all participating layers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network Pruning</head><p>Over-parameterization is a well-known property of deep neural networks. Network pruning is usually used to improve generalization and achieve more compact models for low-resource applications <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. There are multiple choices to implement pruning and the fine-tuning after pruning. One option is to use one-shot pruning <ref type="bibr" target="#b28">[29]</ref>, but usually unimportant filters in convolutional layers are iteratively located and deleted. After each pruning iteration, the network can be re-trained from scratch with reinitialized weights <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Our pruning method is similar to <ref type="bibr" target="#b26">[27]</ref>, as we also iteratively prune filters with the lowest l2 norms. The difference is that we retrain with inherited weights, similar to <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Methods</head><p>We present our video preprocessing procedures including cropping and projecting 3D skeletons to 2D in Section 3.1. We describe our modified ResNet network architecture in Section 3.2. We then detail the hierarchical classification and network pruning in Section 3.3 and 3.4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Preprocessing</head><p>Raw action videos usually contain not only human subjects, but also surrounding objects and background environments, most of which are irrelevant to the performed actions. Neural networks can overfit to such noises instead of focusing on human actions. Inspired by <ref type="bibr" target="#b16">[17]</ref>, we also crop the raw videos to focus on human skeletons inside.</p><p>We use 2D skeletons and joint positions in pixels for cropping. 2D skeleton data can be captured together with the videos, or extracted by pose estimation algorithms such as OpenPose <ref type="bibr" target="#b29">[30]</ref>, or computed from 3D skeletons as we will explain shortly. We first extract the skeleton bounding boxes, and then enlarge them by a factor of 10% on all four sides and cap them at frame borders, in order to leave a margin for errors and retain relevant information of surrounding areas. After cropping, we rescale all the video frames to a resolution of 256 × 256 as input to our neural network.</p><p>For datasets that provide 3D skeleton data, we project the 3D skeletons onto the image plane as 2D skeletons as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> following Equation 1. We denote the 2D and 3D skeletons as S p ∈ R T ×J×2 and S ∈ R T ×J×3 , respectively, where T is the number of frames and J is the number of joints. We denote the individual channels in skeleton data as S px , S py for 2D pixel positions, and S x , S y , S z for 3D world coordinates. b x = 320 and b y = 240 are bias values that correspond to the image centers for both the N-UCLA and UTD-MHAD datasets. c x , c y are coefficients that can be solved for from Equation 2. We randomly sample ten frames from UTD-MHAD video clips and manually estimate the pixel position S px and S py of 5 end-effector joints (head, hands, and feet). A least squares fit returns c x = 558.1 and c y = 579.5. These coefficients work well for both the N-UCLA and UTD-MHAD datasets.</p><formula xml:id="formula_0">S p = S px S py = c x × Sx Sz + b x c y × Sy Sz + b y (1) c x c y = (S px − b x ) × Sz Sx (S py − b y ) × Sz Sy (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modified ResNet Architecture</head><p>Our baseline network is the Inflated ResNet from Glimpse Clouds <ref type="bibr" target="#b15">[16]</ref>, which was created according to the inflation procedure introduced in <ref type="bibr" target="#b13">[14]</ref>. The Inflated ResNet is a variation of ResNet developed by <ref type="bibr" target="#b30">[31]</ref>. It is also similar to the I3D network in <ref type="bibr" target="#b16">[17]</ref>. In Inflated ResNet, 2D convolutional kernels, except the first one, are converted into 3D kernels, to make them suitable for video input. We perform experiments on different variations of Inflated ResNet in Section 4.2, and find the Inflated ResNet50 to be the best architecture for our task.</p><p>Our baseline network consists of four main ResNet stacks of convolutional layers, each consisting of multiple bottleneck building blocks. We label them as stacks 1 to 4 in <ref type="figure" target="#fig_1">Figure 2</ref> for hierarchical classification. More specifically, we modify the baseline network after each ResNet stack by averaging the extracted features over the spatial and temporal domain, and then passing them to a fully-connected linear layer and a softmax layer to obtain multiple levels of classification probabilities.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Classification</head><p>We employ hierarchical classification to encourage our neural network to learn important features at different scales. For each ResNet stack, we enforce a superclass constraint for each action. That is, a fully-connected linear classifier assigns each action to a superclass after each ResNet stack. Each superclass contains the same number of original action classes to keep the learned hierarchy balanced. We use up to four ResNet stacks for hierarchical classification. The final structure of our network is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We train the classifiers in two passes. The first pass trains the network with a cross-entropy loss only after the last stack. We then compute the confusion matrix C from the trained network for assigning the superclasses as described next.</p><p>We define a graph with N nodes, each corresponding to one of the original action classes. Edge e ij denotes a connection between node i and j (i &lt; j), with its cost defined as e ij = c ij + c ji , where c ij is the corresponding element of the confusion matrix C. We denote the assigned superclass for each action class as s l i ∈ {1, ..., N/M l }, where i ∈ {1, ..., N } is the action class index, l ∈ {1, ..., L} is the stack index, and hyperparameter M l is the number of superclasses of stack l. The bigger l is, the larger M l is. That is, front stacks have fewer number of superclasses which only need to find the most distinguishable features to classify the actions. The rear stacks, however, need to concentrate more on finer details to differentiate actions into more categories. We aim to minimize the sum of edge costs among all superclasses at all levels:</p><formula xml:id="formula_1">min i,j e ij , ∀ i, j ∈ {1, ..., N }, s l i = s l j , l ∈ {1, ..., L}<label>(3)</label></formula><p>We use a simple greedy algorithm to minimize the total edge cost. We initialize the superclass assignments randomly but evenly. At each optimization step we swap two superclass assignments that decrease the cost function the most. We continue this procedure until no more deduction can be achieved. We run the greedy optimization 1000 times and then choose the solution with the lowest cost. More advanced optimization algorithms, such as genetic algorithms or deep learning methods, can be used as well. But our early experiments showed similar performance among different algorithms. So we choose the simple greedy algorithm in the end.</p><p>The optimized superclass assignments help classify the original action classes into similar groups. <ref type="figure" target="#fig_2">Figure 3</ref> shows one example for the NTU 60 dataset. All mutual classes with two persons in action are classified into the same level-1 superclass. Most medical actions are in the other superclass. <ref type="figure" target="#fig_3">Figure 4</ref> shows another example for the N-UCLA dataset. All class pairs in the level-3 superclass are similar. For instance the "carry" and "walk around" classes or the "throw" and "drop trash" classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Hierarchical Loss Function</head><p>After the first pass of training, we denote the predicted classification probabilities by each ResNet stack asỹ l . We also denote the ground truth and the superclass assignments from the greedy algorithm as y l . We then train the network for a second time, using a cross-entropy loss after each stack denoted as Loss l (y l ,ỹ l ). The total loss is a weighted sum of the loss for each stack:</p><formula xml:id="formula_2">Loss(y l ,ỹ l ) = l w l × Loss l (y l ,ỹ l ), l ∈ {1, ..., L}<label>(4)</label></formula><p>Generally speaking, we use bigger weights w l for latter stacks. This is because the prior stacks tend to extract low-level features less important for the final classification. In <ref type="figure" target="#fig_4">Figure 5</ref> we show the normalized and averaged absolute gradients backpropagated from each classifier to the last convolutional layer of stack 1, when using equal weights (1, 1, 1, 1) for all ResNet stacks. Note that gradients from latter stacks are richer than those of the previous stacks. We also experiment with different weighting schemes in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Pruning</head><p>We apply network pruning to further improve the accuracy and generalization ability of our model. We also tried to apply dropout but did not find it beneficial in our experiments. We employ two iterative pruning methods while keeping the hierarchical classification approach intact.</p><p>For both pruning methods, we find the p% of the convolutional filters with the lowest l2 norm and zero them out. The first method prunes the bottom p% of filters in all convolutional layers of the ResNet stacks; while the second method prunes the bottom p% of filters in each layer of the ResNet stacks independently. We compare different choices of p values on the NTU 60 dataset using the first pruning method in <ref type="table" target="#tab_1">Table 2</ref>. We will use 10% for all our experiments in Section 4. The pruning can be carried out iteratively for multiple passes. We perform pruning until we observe reductions in testing performance for two consecutive steps. We will show more pruning results for up to seven passes in <ref type="table" target="#tab_0">Tables 9-12</ref> in our ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Our network is based on the Inflated ResNet50 as described in Section 3.2. We train our network for 20 epochs, with an adaptive learning rate initially set to 0.0001. We divide the learning rate by 10 when the validation loss has not improved for two consecutive epochs. We use a pruning ratio of 10% at each pruning pass. We preprocess all input videos with cropping and rescaling as described in Section 3.1. In addition, we flip all the video frames horizontally with a probability of 50%. For both training and testing, we partition each video clip into eight equal segments and uniformly sample eight frames as the input frames, with the location of the first frame randomly chosen within the first segment. For training we also randomly crop a fixed 224 × 224 square from all input frames to feed into our network, but at test time we crop a square of the same size at the center of each frame. We also shuffle the training data at the beginning of each epoch. For testing, five sets of frames are sampled from the eight segments of each clip as input to the network, and the final classification probabilities are averaged similar to <ref type="bibr" target="#b15">[16]</ref>. 10% of the test data is randomly chosen as the validation data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Network Architecture</head><p>We perform experiments to search for the best Inflated ResNet architecture as shown in <ref type="table" target="#tab_2">Table 3</ref>. The original ResNet has five variations: ResNet18, ResNet34, ResNet50, ResNet101 and ResNet152. We do experiments on the inflated version of the first four variations, as ResNet152 is unnecessarily big for our datasets.</p><p>We note that the Inflated ResNet34 has more parameters than the Inflated ResNet50, as they are made of different building blocks. We refer the readers to <ref type="bibr" target="#b13">[14]</ref> for more details of the inflation procedure. From <ref type="table" target="#tab_2">Table 3</ref> we conclude that the Inflated ResNet50 is the best baseline model to use for our further experiments.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyperparameters</head><p>The number of superclasses for each level of the hierarchical classification is one important hyperparameter of our model. However, when the original number of action classes is big, such as 60 for NTU 60, there are too many combinations of these parameters for all the classification levels. We thus chose some representative combinations to test for NTU 60 and the results are shown in <ref type="table" target="#tab_3">Table 4</ref>. We have also performed experiments on different weighting schemes for the hierarchical loss function in <ref type="table" target="#tab_0">Table 1</ref>, and different pruning ratios in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparisons</head><p>We evaluate our method and compare with other state-of-the-art methods on four commonly used datasets: NTU RGB+D 60 Dataset (NTU 60), NTU RGB+D 120 Dataset (NTU 120), Northwestern-UCLA Multiview Action 3D Dataset (N-UCLA), and UTD Multimodal Human Action Dataset (UTD-MHAD). For all these datasets, we crop the videos in a preprocessing stage as described in Section 3.1. In all experiments, we set the number of superclasses heuristically based on testings as described in Section 4.3. More dataset-specific settings are given in the dataset descriptions next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Datasets</head><p>NTU RGB+D 60 Dataset (NTU 60) contains more than 56000 video clips <ref type="bibr" target="#b31">[32]</ref>. 2D and 3D skeleton data, as well as depth, are also available. There are 60 action classes, and two evaluation benchmarks: cross-view and cross-subject. We choose 2, 6, 20 as the number of superclasses for levels 1-3 classification respectively, according to <ref type="table" target="#tab_3">Table 4</ref>. We use weights ( 1 8 , 1 4 , 1 2 , 1) for the hierarchical loss function, according to <ref type="table" target="#tab_0">Table 1</ref>. NTU RGB+D 120 Dataset (NTU 120) adds 60 new action classes to the original NTU 60 dataset <ref type="bibr" target="#b32">[33]</ref>. It contains more than 114000 video clips in total and provides two benchmarks: cross-setup and cross-subject. As the number of action classes is doubled compared with that of NTU 60, we use 4,12,40 as the number of superclasses for levels 1-3 classification. We still use ( 1 8 , 1 4 , 1 2 , 1) for weighting the hierarchical loss function.   Northwestern-UCLA Multiview Action 3D (N-UCLA) contains 1494 video sequences, together with depth and 3D skeleton data <ref type="bibr" target="#b34">[35]</ref>. Each action is recorded simultaneously with three Kinect cameras. We convert the 3D skeleton data into 2D skeletons using the projection method described in Section 3.1. We use three view-based benchmarks where each view is used for testing and the other two for training. There are 10 different actions in this dataset. We thus choose 2 and 5 as the number of superclasses for the level-2 and level-3 classifiers, respectively. We do not need the level-1 classifier for hierarchical classification anymore. We use weights ( 1 16 , 1 4 , 1) for the hierarchical loss function. As the size of this dataset is small, we use the pre-trained network on NTU 60 cross-subject benchmark to initialize the network training on N-UCLA.</p><p>UTD Multimodal Human Action Dataset (UTD-MHAD) contains 861 video sequences, together with depth, 3D skeleton, and wearable inertial sensor data <ref type="bibr" target="#b35">[36]</ref>. It provides one cross-subject evaluation benchmark. Similar as for the N-UCLA dataset, we convert the 3D skeleton data into 2D skeletons by projection and use the pre-trained network on NTU 60 for initialization. There are 27 action classes so we choose 3 and 9 as the number of superclasses for the level-2 and level-3 classifiers, respectively. We also use weights ( 1 16 , 1 4 , 1) for the hierarchical loss function, similar as for the N-UCLA dataset.   <ref type="bibr" target="#b15">[16]</ref> 2018 84.19%* JTM <ref type="bibr" target="#b3">[4]</ref> 2016 85.81% Optical Spectra <ref type="bibr" target="#b5">[6]</ref> 2018 86.97% JDM <ref type="bibr" target="#b4">[5]</ref> 2017 88.10% Action Machine Archived Version <ref type="bibr" target="#b36">[37]</ref> 2019 92.5% PoseMap <ref type="bibr" target="#b18">[19]</ref> 2018 94.51% ours 2020 91.63% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Performance</head><p>Tables 5 <ref type="table" target="#tab_7">-8</ref> show the comparison results. Our method scores the highest or close to the highest for all four datasets. We report the accuracy for prior work by either directly taking numbers reported in the original paper, or running author-released code if relevant performance was not reported in the original papers. For fair comparisons, we check if a method uses RGB input or pose input or both. We also mark if a method uses pre-trained models on ImageNet and/or a bigger human action dataset to initialize the network for training on small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Study</head><p>We detail the performance gains of the video cropping preprocess, the hierarchical classification, and the multipass network pruning components with ablation studies shown in Tables 9-12. The hierarchical classification is proved to be beneficial in all cases after cropping. Without cropping, the hierarchical classification alone is also able to gain performance as shown in the third row of <ref type="table" target="#tab_0">Table 10</ref> for NTU 120.</p><p>We continue the network pruning iterations until we observe lowered performance from the best one achieved so far in two consecutive steps. The network pruning is helpful in most cases. For small datasets, pruning layer by layer outperforms pruning all layers together. For larger datasets such as the NTU datasets, the two pruning methods perform comparably.</p><p>The cropping preprocess yields large performance gains in most cases, but not for the UTD-MHAD dataset, which was captured in one single environment with actors located in the frame centers already.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have augmented the Inflated ResNet50 architecture with hierarchical classification, iterative network pruning, and skeleton-based cropping. These components are simple to implement and effective in improving the classification accuracy for human action datasets captured in lab environments. Our work has set up a new baseline for the NTU 120 dataset, which is the largest dataset of its kind.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Limitations and Future Work</head><p>Our choices of hyperparameters, such as the number of superclasses for each classification level, number of iterations for the pruning, and weights in the hierarchical loss function, are all set by either heuristics and/or manual searching.</p><p>There is no guarantee that these parameters are optimal. Automatic search algorithms that are not computationally prohibitive are desirable.</p><p>We have used the skeleton data only for video preprocessing in this work. GCN-based skeleton classification algorithms, however, can be easily incorporated to further boost the performance of our system. We could simply fuse the classification results of the two algorithms, or implement an integrated two-stream system from both the video stream and the skeleton stream.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Projection of a 3D skeleton onto the image plane as a 2D skeleton.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Structure of our neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>NTU 60: all 60 action classes and the derived superclasses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>N-UCLA: all 10 action classes and the derived superclasses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Averaged and normalized absolute gradients in the last convolutional layer of stack 1, backpropagated from different levels of classifiers. The gradients are computed from the first 500 clips of the NTU 60 dataset. Pure red indicates 1 and pure blue indicates 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Different weighting schemes of the hierarchical loss function on the NTU 60 dataset.</figDesc><table><row><cell cols="3">Superclass weights Cross-Subject Cross-View</cell></row><row><cell>1 64 , 1 16 , 1 4 , 1 1 27 , 1 9 , 1 3 , 1 1 8 , 1 4 , 1 2 , 1 1, 1, 1, 1</cell><cell>95.36% 95.07% 95.45% 95.31%</cell><cell>98.29% 98.41% 98.59% 98.01%</cell></row><row><cell>1, 1 2 , 1 4 , 1 8 1, 1 3 , 1 9 , 1 27 1, 1 4 , 1 16 , 1 64</cell><cell>93.91% 93.22% 92.21%</cell><cell>97.20% 97.24% 95.85%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Different pruning ratios and passes on the NTU 60 cross-subject benchmark.</figDesc><table><row><cell cols="4">Pruning passes 5% pruning 10% pruning 15% pruning</cell></row><row><cell>Pass 1 pruning</cell><cell>94.84%</cell><cell>95.52%</cell><cell>95.08%</cell></row><row><cell>Pass 2 pruning</cell><cell>95.17%</cell><cell>95.61%</cell><cell>95.21%</cell></row><row><cell>Pass 3 pruning</cell><cell>94.92%</cell><cell>95.60%</cell><cell>95.00%</cell></row><row><cell>Pass 4 pruning</cell><cell>94.87%</cell><cell>95.66%</cell><cell>95.10%</cell></row><row><cell>Pass 5 pruning</cell><cell>-</cell><cell>95.41%</cell><cell>-</cell></row><row><cell>Pass 6 pruning</cell><cell>-</cell><cell>95.47%</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Different Inflated ResNet Architectures on NTU 60.</figDesc><table><row><cell cols="4">Architecture #Parameters Cross-Subject Cross-View</cell></row><row><cell>ResNet18</cell><cell>33.7M</cell><cell>93.83%</cell><cell>97.87%</cell></row><row><cell>ResNet34</cell><cell>64.0M</cell><cell>93.91%</cell><cell>97.69%</cell></row><row><cell>ResNet50</cell><cell>46.8M</cell><cell>95.25%</cell><cell>98.34%</cell></row><row><cell>ResNet101</cell><cell>85.8M</cell><cell>95.19%</cell><cell>98.09%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Different superclass sizes for level 1-3 classification on the NTU 60 dataset.</figDesc><table><row><cell cols="3">Superclasses Cross-Subject Cross-View</cell></row><row><cell>2/6/20</cell><cell>95.45%</cell><cell>98.59%</cell></row><row><cell>3/10/30</cell><cell>95.25%</cell><cell>98.34%</cell></row><row><cell>4/12/30</cell><cell>95.25%</cell><cell>98.38%</cell></row><row><cell>5/15/30</cell><cell>95.20%</cell><cell>98.29%</cell></row><row><cell>6/12/30</cell><cell>95.39%</cell><cell>98.28%</cell></row><row><cell>10/20/30</cell><cell>95.04%</cell><cell>98.38%</cell></row><row><cell>60/60/60</cell><cell>95.21%</cell><cell>98.37%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison on NTU 60. -indicates no results available.</figDesc><table><row><cell>Method</cell><cell cols="3">Year Pose Input RGB Input Cross-View Cross-Subject</cell></row><row><cell cols="2">Glimpse Clouds [16] 2018</cell><cell>93.2%</cell><cell>86.6%</cell></row><row><cell>FGCN [12]</cell><cell>2020</cell><cell>96.25%</cell><cell>90.22%</cell></row><row><cell>MS-G3D Net [9]</cell><cell>2020</cell><cell>96.2%</cell><cell>91.5%</cell></row><row><cell>PoseMap [19]</cell><cell>2018</cell><cell>95.26%</cell><cell>91.71%</cell></row><row><cell>MMTM [15]</cell><cell>2019</cell><cell>-</cell><cell>91.99%</cell></row><row><cell cols="2">Action Machine [17] 2019</cell><cell>97.2%</cell><cell>94.3%</cell></row><row><cell>PGCN [18]</cell><cell>2019</cell><cell>-</cell><cell>96.4%</cell></row><row><cell>ours</cell><cell>2020</cell><cell>98.79%</cell><cell>95.66%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison on NTU 120. * indicates results obtained from author-released code. -indicates no results available.</figDesc><table><row><cell>Method</cell><cell cols="3">Year Pose Input RGB Input Cross-Setup Cross-Subject</cell></row><row><cell>Action Machine [17]</cell><cell>2019</cell><cell>-</cell><cell>-</cell></row><row><cell>TSRJI [1]</cell><cell>2019</cell><cell>62.8%</cell><cell>67.9%</cell></row><row><cell cols="2">PoseMap from Papers with Code [34] 2018</cell><cell>66.9%</cell><cell>64.6%</cell></row><row><cell>SkeleMotion [2]</cell><cell>2019</cell><cell>66.9%</cell><cell>67.7%</cell></row><row><cell cols="2">GVFE + AS-GCN with DH-TCN [13] 2019</cell><cell>79.8%</cell><cell>78.3%</cell></row><row><cell>Glimpse Clouds [16]</cell><cell>2018</cell><cell>83.84%*</cell><cell>83.52%*</cell></row><row><cell>FGCN [12]</cell><cell>2020</cell><cell>87.4%</cell><cell>85.4%</cell></row><row><cell>MS-G3D Net [9]</cell><cell>2020</cell><cell>88.4%</cell><cell>86.9%</cell></row><row><cell>ours</cell><cell>2020</cell><cell>94.54%</cell><cell>93.69%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison on N-UCLA. -indicates no results available. The Pre-trained column indicates if the model was pre-trained on ImageNet and/or a bigger human action dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Year Pre-trained Pose Input RGB Input View1</cell><cell>View2</cell><cell cols="2">View3 Average</cell></row><row><cell>PoseMap [19]</cell><cell>2018</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ensemble TS-LSTM [7] 2017</cell><cell>-</cell><cell>-</cell><cell>89.22%</cell><cell>-</cell></row><row><cell>EleAtt-GRU(aug.) [8]</cell><cell>2018</cell><cell>-</cell><cell>-</cell><cell>90.7%</cell><cell>-</cell></row><row><cell>Enhanced Viz. [3]</cell><cell>2017</cell><cell>-</cell><cell>-</cell><cell>92.61%</cell><cell>-</cell></row><row><cell>Glimpse Clouds [16]</cell><cell>2018</cell><cell>83.4%</cell><cell>89.5%</cell><cell>90.1%</cell><cell>87.6%</cell></row><row><cell>FGCN [12]</cell><cell>2020</cell><cell>-</cell><cell>-</cell><cell>95.3%</cell><cell>-</cell></row><row><cell>Action Machine [17]</cell><cell>2019</cell><cell>88.3%</cell><cell>92.2%</cell><cell>96.5%</cell><cell>92.3%</cell></row><row><cell>ours</cell><cell>2020</cell><cell cols="4">91.10% 91.95% 98.92% 93.99%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparison on UTD-MHAD. * indicates results obtained from author-released code. -indicates no results available. The Pre-trained column indicates if the model was pre-trained on ImageNet and/or a bigger human action dataset.MethodYear Pre-trained Pose Input RGB Input Cross-Subject Glimpse Clouds</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Ablation Study on NTU 60 dataset. The baseline model refers to the Inflated ResNet50 network trained without hierarchical loss using the original videos. The full model refers to the baseline network trained with the hierarchical loss using cropped video input.</figDesc><table><row><cell></cell><cell cols="2">Pruning Altogether</cell><cell cols="2">Pruning by Layers</cell></row><row><cell>Method</cell><cell cols="4">Cross-Subject Cross-View Cross-Subject Cross-View</cell></row><row><cell>baseline</cell><cell>89.13%</cell><cell>94.39%</cell><cell>89.13%</cell><cell>94.39%</cell></row><row><cell>baseline + cropping</cell><cell>95.29%</cell><cell>98.34%</cell><cell>95.29%</cell><cell>98.34%</cell></row><row><cell>full model</cell><cell>95.45%</cell><cell>98.59%</cell><cell>95.45%</cell><cell>98.59%</cell></row><row><cell>full model + 1-pass pruning</cell><cell>95.52%</cell><cell>98.61%</cell><cell>95.37%</cell><cell>98.66%</cell></row><row><cell>full model + 2-pass pruning</cell><cell>95.61%</cell><cell>98.60%</cell><cell>95.50%</cell><cell>98.63%</cell></row><row><cell>full model + 3-pass pruning</cell><cell>95.60%</cell><cell>98.65%</cell><cell>95.64%</cell><cell>98.74%</cell></row><row><cell>full model + 4-pass pruning</cell><cell>95.66%</cell><cell>98.40%</cell><cell>95.14%</cell><cell>98.79%</cell></row><row><cell>full model + 5-pass pruning</cell><cell>95.41%</cell><cell>98.43%</cell><cell>95.39%</cell><cell>98.56%</cell></row><row><cell>full model + 6-pass pruning</cell><cell>95.47%</cell><cell>-</cell><cell>-</cell><cell>98.62%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Ablation Study on NTU 120 dataset.</figDesc><table><row><cell></cell><cell cols="2">Pruning Altogether</cell><cell cols="2">Pruning by Layers</cell></row><row><cell>Method</cell><cell cols="4">Cross-Subject Cross-Setup Cross-Subject Cross-Setup</cell></row><row><cell>baseline</cell><cell>84.64%</cell><cell>86.20%</cell><cell>84.64%</cell><cell>86.20%</cell></row><row><cell>baseline + cropping</cell><cell>92.48%</cell><cell>93.99%</cell><cell>92.48%</cell><cell>93.99%</cell></row><row><cell>baseline + hierarchical loss</cell><cell>86.17%</cell><cell>86.98%</cell><cell>86.17%</cell><cell>86.98%</cell></row><row><cell>full model</cell><cell>92.95%</cell><cell>94.25%</cell><cell>92.95%</cell><cell>94.25%</cell></row><row><cell>full model + 1-pass pruning</cell><cell>93.48%</cell><cell>94.12%</cell><cell>93.19%</cell><cell>94.54%</cell></row><row><cell>full model + 2-pass pruning</cell><cell>93.55%</cell><cell>94.31%</cell><cell>93.56%</cell><cell>94.48%</cell></row><row><cell>full model + 3-pass pruning</cell><cell>93.69%</cell><cell>94.42%</cell><cell>93.43%</cell><cell>94.48%</cell></row><row><cell>full model + 4-pass pruning</cell><cell>93.64%</cell><cell>94.39%</cell><cell>93.38%</cell><cell>-</cell></row><row><cell>full model + 5-pass pruning</cell><cell>93.47%</cell><cell>94.19%</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Ablation Study on N-UCLA dataset. The "Viewn" columns indicate the camera view used for testing and the other two views are used for training. 76% 70.73% 97.00% 77.76% 70.73% 97.00% baseline + cropping 88.78% 89.00% 98.50% 88.78% 89.00% 98.50% full model 88.78% 89.98% 98.92% 88.78% 89.98% 98.92% full model + 1-pass pruning 89.75% 86.64% 98.28% 88.39% 88.39% 98.71% full model + 2-pass pruning 90.14% 89.00% 98.28% 91.10% 88.39% 98.07%</figDesc><table><row><cell></cell><cell cols="3">Pruning Altogether</cell><cell cols="3">Pruning by Layers</cell></row><row><cell>Method</cell><cell>View1</cell><cell>View2</cell><cell>View3</cell><cell>View1</cell><cell>View2</cell><cell>View3</cell></row><row><cell cols="2">baseline 77.full model + 3-pass pruning 71.95%</cell><cell>-</cell><cell cols="3">84.36% 86.85% 91.36%</cell><cell>-</cell></row><row><cell cols="2">full model + 4-pass pruning 40.62%</cell><cell>-</cell><cell>-</cell><cell cols="2">85.88% 90.37%</cell><cell>-</cell></row><row><cell>full model + 5-pass pruning</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.95%</cell><cell>-</cell></row><row><cell>full model + 6-pass pruning</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.37%</cell><cell>-</cell></row><row><cell>full model + 7-pass pruning</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.21%</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Ablation Study on UTD-MHAD dataset.</figDesc><table><row><cell></cell><cell cols="2">Pruning Altogether Pruning by Layers</cell></row><row><cell>Method</cell><cell>Cross-Subject</cell><cell>Cross-Subject</cell></row><row><cell>baseline</cell><cell>84.88 %</cell><cell>84.88 %</cell></row><row><cell>baseline + cropping</cell><cell>82.55%</cell><cell>82.55%</cell></row><row><cell>full model</cell><cell>86.05%</cell><cell>86.05%</cell></row><row><cell>full model + 1-pass pruning</cell><cell>84.65%</cell><cell>87.44%</cell></row><row><cell>full model + 2-pass pruning</cell><cell>52.23%</cell><cell>86.51%</cell></row><row><cell>full model + 3-pass pruning</cell><cell>-</cell><cell>90.46%</cell></row><row><cell>full model + 4-pass pruning</cell><cell>-</cell><cell>91.63%</cell></row><row><cell>full model + 5-pass pruning</cell><cell>-</cell><cell>90.23%</cell></row><row><cell>full model + 6-pass pruning</cell><cell>-</cell><cell>90.93%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We have mainly focused on NTU RGB+D type of datasets that were captured in lab settings together with 2D/3D skeletons of reasonable quality. For future work, we would like to incorporate outdoor datasets or datasets obtained from the Internet, such as the Kinetics dataset <ref type="bibr" target="#b37">[38]</ref>. Such datasets are usually more heterogeneous and of much lower quality. 2D skeletons extracted by pose estimation algorithms such as OpenPose <ref type="bibr" target="#b29">[30]</ref> are also less trustworthy. Large-scale pretraining using super large neural network models is likely needed as suggested by <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Robson</forename><surname>Schwartz</surname></persName>
		</author>
		<title level="m">Skeleton image representation for 3d action recognition based on tree structure and reference joints. SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skelemotion: A new representation of skeleton joint sequences based on motion information for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Sena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franeois</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Dos</forename><surname>Jefersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Robson</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint distance maps based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="624" to="628" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Skeleton optical spectra-based action recognition using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="807" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adding attentiveness to the neurons in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7904" to="7913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Feedback graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunda</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Vertex feature encoding and hierarchical temporal modeling in a spatial-temporal graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enjie</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamila</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ottersten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mmtm: Multimodal transfer module for cnn fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Hamid Reza Vaezi Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhito</forename><surname>Iuzzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koishida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<title level="m">Glimpse clouds: Human activity recognition from unstructured feature points. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action machine: Toward person-centric action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Action recognition via pose-based graph convolutional networks with intermediate dense supervision. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Qing</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human action recognition from rgb-d frames based on real-time 3d optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gioia</forename><surname>Ballin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Munaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Menegatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biologically Inspired Cognitive Architectures</title>
		<editor>Antonio Chella, Roberto Pirrone, Rosario Sorbello, and Kamilla Rún Jóhannsdóttir</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems</title>
		<meeting>the International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mfas: Multimodal fusion architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-resolution-tract cnn with hybrid pretrained and skin-lesion trained layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging</title>
		<editor>Li Wang, Ehsan Adeli, Qian Wang, Yinghuan Shi, and Heung-Il Suk</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A hierarchical loss and its problems when classifying non-hierarchically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cinna</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tygert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiscale dense networks for resource efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><forename type="middle">Lisboa</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Kot</forename><surname>Chichung</surname></persName>
		</author>
		<title level="m">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Recognizing human actions as the evolution of pose estimation maps. https:// paperswithcode.com/paper/recognizing-human-actions-as-the-evolution-of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2020" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Action machine: Rethinking action recognition in trimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Omni-sourced webly-supervised learning for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
