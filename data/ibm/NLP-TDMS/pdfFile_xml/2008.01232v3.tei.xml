<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Esat</forename><surname>Kalfaoglu</surname></persName>
							<email>esat.kalfaoglu@metu.edu.tr</email>
							<affiliation key="aff0">
								<address>
									<postCode>0000−0001, 5556−7301</postCode>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Electronics Engineering</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Image Analysis (OGAM)</orgName>
								<orgName type="institution">Middle East Technical University</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Kalkan</surname></persName>
							<email>skalkan@metu.edu.tr</email>
							<affiliation key="aff0">
								<address>
									<postCode>0000−0001, 5556−7301</postCode>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Image Analysis (OGAM)</orgName>
								<orgName type="institution">Middle East Technical University</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Aydin</forename><surname>Alatan</surname></persName>
							<email>alatan@metu.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Electronics Engineering</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action Recognition</term>
					<term>Temporal Attention</term>
					<term>BERT</term>
					<term>Late Tem- poral Modeling</term>
					<term>3D Convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we combine 3D convolution with late temporal modeling for action recognition. For this aim, we replace the conventional Temporal Global Average Pooling (TGAP) layer at the end of 3D convolutional architecture with the Bidirectional Encoder Representations from Transformers (BERT) layer in order to better utilize the temporal information with BERT's attention mechanism. We show that this replacement improves the performances of many popular 3D convolution architectures for action recognition, including ResNeXt, I3D, SlowFast and R(2+1)D. Moreover, we provide the-state-of-the-art results on both HMDB51 and UCF101 datasets with 85.10% and 98.69% top-1 accuracy, respectively. The code is publicly available 4 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Action Recognition (AR) pertains to identifying the label of the action or the activity observed in a video clip. With cameras everywhere, AR has become essential in many domains, such as video retrieval, surveillance, human-computer interaction, and robotics.</p><p>A video clip contains two critical pieces of information for AR: Spatial and temporal information. Spatial information represents the static information in the scene, such as objects, context, entities, etc., which are visible in a single frame of the video, whereas temporal information, obtained by integrating the spatial information over frames, mostly captures the dynamic nature of the action.</p><p>In this work, the joint utilization of two temporal modeling concepts from the literature, which are 3D convolution and late temporal modeling, is proposed and analyzed. Briefly, 3D convolution is a way of generating a temporal relationship hierarchically from the beginning to the end of CNN architectures. On the other hand, late temporal modeling is typically utilized with 2D CNN architectures, where the features extracted by 2D CNN architectures from the selected frames are usually modeled with recurrent architectures, such as LSTM, Conv LSTM.</p><p>Despite its advantages, the temporal global average pooling (TGAP) layer which is used at the end of all 3D CNN architectures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">7,</ref><ref type="bibr" target="#b14">12,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b30">27,</ref><ref type="bibr" target="#b32">28,</ref><ref type="bibr" target="#b39">35]</ref> hinders the richness of final temporal information. The features before TGAP can be considered as features of different temporal regions of a clip or video. Although the receptive field might cover the whole clip, the effective receptive field has a Gaussian distribution <ref type="bibr" target="#b22">[20]</ref>, producing features focusing on different temporal regions of a clip. In order to discriminate actions, one part of the temporal feature might be more important than the others or the order of the temporal features might be more beneficial than simply averaging the temporal information. Therefore, TGAP ignores this ordering and fails to fully exploit the temporal information.</p><p>Therefore, we propose using the attention mechanism of BERT for better temporal modeling than TGAP. BERT determines which temporal features are more important with its multi-head attention mechanism.</p><p>To the best of our knowledge, our work is the first to propose replacing TGAP in 3D CNN architectures with late temporal modeling. We also consider that this study is the first to utilize BERT as a temporal pooling strategy in AR. We show that BERT performs better temporal pooling than average pooling, concatenation pooling, and standard LSTM. Moreover, we demonstrate that late temporal modeling with BERT improves the performances of various popular 3D CNN architectures for AR which are ResNeXt101, I3D, SlowFast, and R(2+1)D by using the split-1 of the HMDB51 dataset. Using BERT R(2+1)D architecture, we obtain the new state of the art results; 85.10% and 98.69% Top-1 performances in HMDB51 and UCF101 datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work on Action Recognition</head><p>In this section, the AR literature is analyzed in two aspects: (i) temporal integration using pooling, fusion or recurrent architectures and (ii) 3D CNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Temporal Integration Using Pooling, Fusion or Recurrent Architectures</head><p>Pooling is a well-known technique to combine various temporal features; concatenation, averaging, maximum, minimum, ROI, feature aggregation techniques and time-domain convolution are some of the possible pooling techniques <ref type="bibr" target="#b13">[11,</ref><ref type="bibr" target="#b23">21]</ref>. Fusion frequently used for AR is very similar to pooling. Fusion is sometimes preferred instead of pooling in order to emphasize pooling location in the architecture or to differentiate information from different modalities. Late fusion, early fusion and slow fusion models on 2D CNN architectures can be performed by combining temporal information along the channel dimension at various points in CNN architectures <ref type="bibr" target="#b17">[15]</ref>. As a method, the two-stream fusion architecture in <ref type="bibr" target="#b10">[8]</ref> creates spatio-temporal relationship with extra 3D convolution layer inserted towards the end of the architecture and fuses information from RGB and optical flow streams.</p><p>Recurrent networks are also commonly used for temporal integration. LSTMs are utilized for temporal (sequential) modeling on 2D CNN features extracted from the frames of a video <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b6">5]</ref>. E.g., VideoLSTM <ref type="bibr" target="#b19">[17]</ref> performs this kind of temporal modeling by using convolutional LSTM with spatial attention. RSTAN <ref type="bibr" target="#b8">[6]</ref> implements both temporal and spatial attention concepts on LSTM and the attention weights of RGB and optical flow streams are fused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">3D CNN Architectures</head><p>3D CNNs are networks formed of 3D convolution throughout the whole architecture. In 3D convolution, filters are designed in 3D, and channels and temporal information are represented as different dimensions. Compared to the temporal fusion techniques, 3D CNNs process the temporal information hierarchically and throughout the whole network. Before 3D CNN architectures, temporal modeling was generally achieved by using an additional stream of optical flow or by using temporal pooling layers. However, these methods were restricted to 2D convolution and temporal information was put into the channel dimension. The downside of the 3D CNN architectures is that they require huge computational costs and memory demand compared to its 2D counterparts.</p><p>The first 3D CNN for AR is the C3D model <ref type="bibr" target="#b28">[26]</ref>. Another successful implementation of 3D convolution is the Inception 3D model (I3D) <ref type="bibr" target="#b0">[1]</ref>, in which 3D convolution is modeled in a much deeper fashion compared to C3D. The ResNet version of 3D convolution is introduced in <ref type="bibr" target="#b14">[12]</ref>. Then, R(2+1)D <ref type="bibr" target="#b32">[28]</ref> and S3D <ref type="bibr" target="#b39">[35]</ref> architectures are introduced in which 3D spatio-temporal convolutions are factorized into spatial and temporal convolutions, and shown to be more effective than traditional 3D convolution architectures. Another important 3D CNN architecture is Channel-Separated Convolutional Networks (CSN) <ref type="bibr" target="#b30">[27]</ref> which separates the channel interactions and spatio-temporal interactions which can be thought as the 3D CNN version of depth-wise separable convolution <ref type="bibr" target="#b16">[14]</ref>.</p><p>Slow-fast networks <ref type="bibr" target="#b9">[7]</ref> can be considered as a joint implementation of both fusion techniques and 3D CNN architectures. There are two streams, namely fast and slow paths. Slow stream operates at a low frame and focuses on spatial information, as the RGB stream in traditional two-stream architectures, while fast stream operates at a high frame and focuses on temporal information as an optical flow stream in traditional two-stream architectures. There is information flow from the fast stream to the slow stream.</p><p>Although 3D CNNs are powerful, they still lack an effective temporal fusion strategy at the end of the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methods</head><p>In this part, the proposed methods of this study are introduced. Firstly, the main proposed method, namely BERT-based temporal modeling with 3D CNN for activity recognition, is presented in Section 3.1. Next, some novel feature reduction blocks are proposed in Section 3.2. These blocks are utilized to reduce the number of parameters of the proposed BERT-based temporal modeling. Thirdly, the proposed BERT-based temporal modeling implementations on SlowFast architecture are examined in Section 3.3. The reason to re-consider the BERT-based late temporal modeling on SlowFast architecture is due to its different two-stream structure from other 3D CNN architectures. Bi-directional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b5">[4]</ref> is a bidirectional self-attention method, which has provided unprecedented success in many downstream Natural Language Processing (NLP) tasks. The bidirectional property enables BERT to fuse the contextual information from both directions, instead of relying upon only a single direction, as in former recurrent neural networks or other self-attention methods, such as Transformer <ref type="bibr" target="#b33">[29]</ref>. Moreover, BERT introduces challenging unsupervised pre-training tasks which leads to useful representations for many tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BERT-based Temporal Modeling with 3D CNNs for Action Recognition</head><p>Our architecture utilizes BERT-based temporal pooling as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. In this architecture, the selected K frames from the input sequence are propagated through a 3D CNN architecture without applying temporal global average pooling at the end of the architecture. Then, in order to preserve the positional information, a learned positional encoding is added to the extracted features. In order to perform classification with BERT, additional classification embedding (x cls ) is appended as in <ref type="bibr" target="#b5">[4]</ref> (represented as red box in <ref type="figure" target="#fig_0">Fig. 1</ref>). The classification of the architecture is implemented with the corresponding classification vector y cls which is given to the fully connected layer, producing the predicted output labelŷ.</p><p>The general single head self-attention model of BERT is formulated as:</p><formula xml:id="formula_0">y i = P F F N   1 N (x) ∀j g(x j )f (x i , x j )   ,<label>(1)</label></formula><p>where x i values are the embedding vectors that consists of extracted temporal visual information and its positional encoding; i indicates the index of the target output temporal position; j denotes all possible combinations; and N (x) is the normalization term. Function g(·) is the linear projection inside the self-attention mechanism of BERT, whereas function f (·, ·) denotes the similarity between x i and x j :</p><formula xml:id="formula_1">f (x i , x j ) = softmax j (θ(x i ) T φ(x j ))</formula><p>, where the functions θ(·) and φ(·) are also linear projections. The learnable functions g(·), θ(·) and φ(·) try to project the feature embedding vectors to a better space where the attention mechanism works more efficiently. The outputs of g(·), θ(·) and φ(·) functions are also defined as value, query and key, respectively <ref type="bibr" target="#b33">[29]</ref>. P F F N (·) is Positionwise Feed-forward Network applied to all positions separately and identically:</p><formula xml:id="formula_2">P F F N (x) = W 2 GELU (W 1 x + b1) + b2, where GELU (·)</formula><p>is the Gaussian Error Linear Unit (GELU) activation function <ref type="bibr" target="#b15">[13]</ref>.</p><p>The final decision of classification is performed with one more linear layer which takes y cls as input. The explicit form of y cls can be written as:</p><formula xml:id="formula_3">y cls = P F F N   1 N (x) ∀j g(x j )f (x cls , x j )   .<label>(2)</label></formula><p>Therefore, our use of the temporal attention mechanism for BERT is not only to learn the convenient subspace where the attention mechanism works efficiently but also to learn the classification embedding which learns how to attend the temporal features of the 3D CNN architecture properly. A similar work for action recognition is implemented with non-local neural networks (NN) <ref type="bibr" target="#b38">[34]</ref>. The main aim of non-local block is to create global spatiotemporal relations, since convolution operation is limited to local regions. For this aim, non-local blocks use a similar attention concept by using 1x1x1 CNN filters, in order to realize g(·), θ(·) and φ(·) functions. The main difference between the non-local and the proposed BERT attention is that non-local concept <ref type="bibr" target="#b38">[34]</ref> is preferred to be utilized not at the end of the architecture, but some preferred locations inside the architecture. However, our BERT-based temporal pooling is implemented on the extracted features of the 3D CNN architecture and utilizes multi-head attention concept to create multiple relations with self-attention mechanism. Moreover, it utilizes positional encoding in order to preserve the order information and utilizes learnable classification token. Another similar study for action recognition is the video action transformer network <ref type="bibr" target="#b12">[10]</ref> where the transformer is utilized in order to aggregate contextual information from other people and objects in the surrounding video. The video action transformer network deals with both action localization and action recognition; therefore, its problem formulation is different from ours and its attention mechanism needs to be reformulated for the late temporal modeling for action recognition. Differently from the video action transformer network, our proposed BERT-based late temporal modeling utilizes the learnable classification token, instead of using the pooled feature of the output of the backbone architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Feature Reduction Blocks: FRAB &amp; FRMB</head><p>The computational complexity of BERT has a quadratic increase with the dimension of the output feature of the CNN backbone. As a result, if the dimension of the output feature is not reduced for specific backbones, the BERT module might have more parameters than the backbone itself. For instance, if the dimension of the output feature is 512, the single-layer BERT module has about 3 Million parameters, while the parameter size would be about 50 Million for the output feature dimension of 2048.</p><p>Therefore, in order to utilize BERT architecture in a more parameter efficient manner, two feature reduction blocks are proposed. These are Feature Reduction with Modified Block (FRMB) and Feature Reduction with Additional Block (FRAB). In FRMB, the final unit block of the CNN backbone is replaced with a novel unit block with the aim of feature dimension reduction. In FRAB, an additional unit block is appended to the backbone to reduce the dimension.</p><p>An example implementation of FRMB and FRAB on ResNeXt101 backbone is presented in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The benefit of FRMB implementation is its better computational complexity and parameter efficiency over the FRAB implementation. Moreover, FRMB has even a better computational complexity and parameter efficiency than the original backbone. One possible downside of FRMB over FRAB is that the final block does not benefit from the pre-trained weights of the larger dataset if the feature reduction block is implemented only in the fine-tuning step but not in the pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proposed BERT Implementations on SlowFast Architecture</head><p>SlowFast architecture <ref type="bibr" target="#b9">[7]</ref> introduces a different perspective for the two-stream architectures. Instead of utilizing two different modalities as two identical streams, the overall architecture includes two different streams (namely fast and slow streams or paths) with different capabilities for a single modality. In SlowFast architecture, the slow stream has a better spatial capability, while the fast stream has a better temporal capability. The fast stream has better temporal resolution and less channel capacity compared to the slow stream.</p><p>Due to its two-stream structure with different temporal resolutions, direct implementation of BERT-based late temporal modeling explained in Section 3.1 is not possible. Therefore, two alternative solutions are proposed in order to carry out BERT-based late temporal modeling on SlowFast architecture: Early-fusion BERT and late-fusion BERT. In early-fusion BERT, the temporal features are concatenated before the BERT layer and only a single BERT module is utilized. To make the concatenation feasible, the temporal resolution of the fast stream is decreased to the temporal resolution of the slow stream. In late-fusion BERT, two different BERT modules are utilized, one for each stream and the outputs of two BERT modules from two streams are concatenated. The figure for earlyfusion and late-fusion is shown in <ref type="figure" target="#fig_3">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this part, dataset, implementation details, ablation study, results on different architectures, and comparison with state-of-the-art sections are presented, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Four datasets are relevant for our study: HMDB51 <ref type="bibr" target="#b18">[16]</ref>, UCF101 <ref type="bibr" target="#b27">[25]</ref>, Kinetics-400 <ref type="bibr" target="#b0">[1]</ref> and IG65M <ref type="bibr" target="#b11">[9]</ref> datasets. HMDB51 consists of ∼7k clips with 51 classes whereas UCF101 includes ∼13k clips with 101 classes. Both HMDB51 and UCF101 define three data splits and performances are calculated by averaging the results on these three splits. Kinetics-400 consists of about 240k clips with 400 classes. IG65M is a weakly supervised dataset which is collected by  using the Kinetics-400 <ref type="bibr" target="#b0">[1]</ref> class names as hashtags on Instagram. There are 65M clips from 400 classes. The dataset is not public for the time being but there are pre-trained models available.</p><p>For analyzing the improvements of BERT on individual architectures (Section 4.4), split 1 of the HMDB51 dataset is used, whereas the comparisons against the-state-of-the-art (See Section 4.5) are performed by using the three splits of the HMDB51 and UCF101 datasets. Additionally, the ablation study (See Section 4.3) is conducted using the three splits of HMDB51. Moreover, Kinetics-400 and IG65M are used for pre-trained weights of the architectures before finetuning on HMDB51 and UCF101. The pre-trained weights are obtained from the authors of architectures, which are ResNeXt, I3D, Slowfast, and R(2+1)D. Among these architectures, R(2+1)D is pre-trained with IG65M but the rest of the architectures are pre-trained with Kinetics-400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For the standard architectures (with TGAP and without any modification to architectures), SGD with learning rate 10 −2 is utilized, except I3D in which the learning rate is set to 10 −1 empirically. For architectures with BERT, the ADAMW optimizer <ref type="bibr" target="#b21">[19]</ref> with learning rate 10 −5 is utilized except I3D for which the learning rate is set to 10 −4 empirically. For all training runs, the "reducing learning rate on the plateau" schedule is followed. The data normalization schemes are selected conforming with the data normalization schemes of the pretraining of the architectures in order to benefit fully from pre-training weights. A multi-scale cropping scheme is applied for fine-tuning and testing of all architectures <ref type="bibr" target="#b36">[32]</ref>. In the test time, the scores of non-overlapping clips are averaged. The optical flow of the frames is extracted with the TV-L1 algorithm (Appendix A).</p><p>In the BERT architecture, there are eight attention heads and one transformer block. The dropout ratio in P F F N (·) is set to 0.9. Mask operation is applied with 0.2 probability. Instead of using a mask token, the attention weight of the masked feature is set to zero. The classification token (x cls ) and the learned positional embeddings are initialized as the zero-mean normal weight with 0.02 standard deviation. Default Torch linear layer initialization is used. Different from the I3D-BERT architecture, the linear layers of BERT are also initialized as the zero-mean normal weight with 0.02 standard deviation since it yields better results for I3D-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we will analyze each step of our proposals and examine how our method compares with alternative pooling strategies (see <ref type="table" target="#tab_0">Table 1</ref>). In this analysis, the ResNeXt101 backbone is utilized with the RGB modality, with a 112x112 input image size, and with 64-frame clips. In <ref type="table" target="#tab_0">Table 1</ref>, temporal pool types, the existence of Feature Reduction with Modified Block (FRMB), the type of the optimizer, top1 performances, the number of parameters, and the number of operations are presented as the columns of the analysis.</p><p>One important issue is the optimizer. For training BERT architectures in NLP tasks, the ADAM optimizer is usually selected <ref type="bibr" target="#b5">[4]</ref>. However, SGD is preferred for 3D CNN architectures <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b9">7,</ref><ref type="bibr" target="#b32">28,</ref><ref type="bibr" target="#b3">3]</ref>. Therefore, for training BERT, we select ADAMW (i.e. not ADAM), since ADAMW improves the generalization capability of ADAM <ref type="bibr" target="#b21">[19]</ref>. In this ablation study, ResNeXt101 architecture (with Average Pooling in <ref type="table" target="#tab_0">Table 1</ref>) is also trained with ADAMW in <ref type="table" target="#tab_0">Table 1</ref> which shows 1.5% increase in performance compared to SGD.</p><p>In this ablation study, FRMB implementation is selected for two reasons over FRAB (see Section 3.2 for FRAB and FRMB). Firstly, FRMB yields about 0.5% better top1 performance than FRAB. Secondly, FRMB has better computational complexity and parameter efficiency than FRAB. From the experiments of the ablation study, it is observed that FRMB has lower computational complexity and better parameter efficiency at the cost of ∼1% decrease in Top-1 performance compared to the standard backbone <ref type="table" target="#tab_0">(Table 1)</ref>.</p><p>For a fair comparison, we set the hyper-parameters of the other pooling strategies (LSTM, concatenation + fully connected layer, and non-local + concatenation + fully connected layer) such that the number of parameters and the number of operations of these temporal pooling strategies is almost the same compared to the proposed BERT pooling. LSTM is implemented in two stacks and with a hidden-layer size 450. The dimension of the inter-channel of a nonlocal attention block (the dimension size of the attention mechanism) is set equal to the input size to the non-local block which is 512. The number of nodes of a fully connected layer is determined according to the need for equal parameter size with the proposed BERT temporal pooling for a fair comparison.</p><p>When <ref type="table" target="#tab_0">Table 1</ref> is analyzed, one can observe that among the six different alternatives (with FRMB), BERT has the best temporal pooling strategy. Additionally, the proposed FRMB-ResNeXt101-BERT provides 3% better Top-1 accuracy than the ResNeXt101-Average Pooling (Baseline) despite the fact that FRMB-ResNeXt101-BERT has a better computational complexity and parameter efficiency than the ResNeXt101-Average Pooling (Baseline) (see <ref type="table" target="#tab_0">Table 1</ref>). The BERT layer itself has about 3M parameters and negligible computational complexity with respect to the ResNeXt101 backbone. For the other temporal pooling strategies, LSTM worsens the performance with respect to the temporal average pooling. Concatenation and concatenation + fully connected layer are also other successful strategies in order to utilize the temporal features better than the average pooling. The addition of a non-local attention block before the concatenation + fully connected layer also decreases the performance compared to only concatenation + fully connected layer pooling implementation. It should be highlighted that the original implementation of the non-local study <ref type="bibr" target="#b38">[34]</ref> also prefers not to utilize the non-local block at the end of the final three bottleneck blocks, which is a consistent fact with the experimental result of this study related with non-local implementation.</p><p>In addition, the ablation study of BERT late temporal modeling is performed and presented in <ref type="table" target="#tab_1">Table 2</ref>. These results examine the effects of the number of layers, the number of heads, and utilization of learnable classification token instead of the average pooled feature. Initially, the experiment of replacing the average of extracted temporal features with learnable classification token results in a 1.42% Top-1 accuracy boost. Next, utilization of multi-head attention with eight attention heads improves the Top-1 performance of single-head attention with 0.52%. Thirdly, increasing the number of layers from one to two worsens the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Different 3D CNN Architectures</head><p>In this section, the improvements obtained by replacing TGAP with BERT pooling on popular 3D convolution architectures for action recognition is presented, including ResNeXt101 <ref type="bibr" target="#b14">[12]</ref>, I3D <ref type="bibr" target="#b0">[1]</ref>, SlowFast <ref type="bibr" target="#b9">[7]</ref> and R(2+1)D <ref type="bibr" target="#b32">[28]</ref>.</p><p>ResNeXt Architecture ResNeXt architecture is essentially ResNet with group convolutions <ref type="bibr" target="#b14">[12]</ref>. For testing this architecture, the input size is selected as 112x112 as in the study of <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b3">3]</ref> and 64 frame length is utilized. The results of the ResNeXt101 architecture is presented in <ref type="table" target="#tab_2">Table 3</ref>. The performance of the architectures is compared over RGB modality, (optical) flow modality, and both (two-stream) in which both RGB and flow-streams are utilized, and the scores are summed from each stream. In this table, the number of parameters and operations of the architectures are also presented. For feature reduction, FRMB is chosen instead of FRAB and its reasoning is explained in Section 4.3). Based on the results in <ref type="table" target="#tab_2">Table 3</ref>, the most important observation is the improvement of the performance by using BERT over the standard architectures (without BERT) in all modalities. I3D Architecture I3D architecture is an Inception-type architecture. During I3D experiments, the input size is selected as 224x224 and 64 frame length is used conforming with the I3D study <ref type="bibr" target="#b0">[1]</ref>. The result of BERT experiments on I3D architecture is given in <ref type="table" target="#tab_3">Table 4</ref>. In this table, there are two BERT implementations that are with and without FRAB. For I3D-BERT architectures with FRAB, the final feature dimension of the I3D backbone is reduced from 1024 to 512 in order to utilize BERT in a more parameter efficient manner. However, contrary to the ResNeXt101-BERT architecture, FRAB is selected instead of FRMB, because FRAB obtains about 3.6% better Top-1 result for RGB-I3D-BERT architecture on split1 of HMDB51. The experimental results in <ref type="table" target="#tab_3">Table 4</ref> indicate that BERT increases the performance of I3D architectures in all modalities. However, the increase in RGB modality is quite limited. For the Flow modality, although there is a performance improvement for BERT without FRAB, the implementation of BERT with FRAB performs worse than the standard I3D architecture, implying that preserving the feature size is more important for flow modality compared to RGB modality in I3D architecture. For the two-stream setting, both of the proposed BERT architectures perform equally with each other and perform better than standard I3D with 0.65% Top-1 performance increase. Comparing with the ResNeXt101 architecture, the performance improvements brought by BERT temporal modeling is lower in I3D architecture.</p><p>SlowFast Architecture The SlowFast architecture in these experiments is derived from a ResNet-50 architecture. The channel capacity of the fast streams is one-eighth of the channel capacity of the slow stream. The temporal resolution of the fast stream is four times the temporal resolution for the slow stream. The input size is selected as 224x224 and 64-frame length is utilized with the SlowFast architecture conforming with the SlowFast study <ref type="bibr" target="#b9">[7]</ref>. Although it might be possible to utilize SlowFast architecture with also optical flow modality, the authors of SlowFast did not consider this strategy in their study. Therefore, in this effort, the analysis of BERT is also implemented by only considering the RGB modality. In order to utilize BERT architecture with fewer parameters, the final feature dimension of SlowFast backbone is reduced similar to the ResNeXt101-BERT and I3D-BERT architectures. Similar to the I3D-BERT architecture, FRAB is chosen instead of FRMB since FRAB obtains about 1.5% better Top-1 result for SlowFast-BERT architecture on the split1 of HMDB51. For early-fusion BERT, the feature dimension of the slow stream is reduced from 2048 to 512 and the feature dimension of the fast stream is reduced from 256 to 128. For late-fusion BERT, only the feature dimension of the slow stream is reduced from 2048 to 512. The details about the size of the dimensions are presented in <ref type="figure" target="#fig_3">Figure 3</ref>. The proposed implementation of BERT-based late temporal modeling on SlowFast architecture is presented in Section 3.3.</p><p>The results for BERT on SlowFast architecture are given in <ref type="table" target="#tab_4">Table 5</ref>. First of all, both BERT solutions perform better than the standard SlowFast architecture but the improvement of early-fusion method is quite limited. Late-fusion BERT improves the top1 performance of standard SlowFast architecture with about 1.3 %. From the number of parameters perspective, the implementation of BERT on SlowFast architecture is not as much as efficient in comparison to ResNeXt101 architecture because of the FRAB implementation instead of FRMB as in the case of I3D-BERT. Moreover, the parameter increase of RGB-SlowFast-BERT is even higher than RGB-I3D-BERT because of the two-stream implementation of SlowFast network for RGB input modality. The increase in the number of operations is also higher in the implementation of SlowFast-BERT than the I3D-BERT and ResNeXt101-BERT because of the higher temporal resolution in SlowFast architecture and two-stream implementation for RGB modality.</p><p>For the two alternatives proposed BERT solution in <ref type="table" target="#tab_4">Table 5</ref>, late-fusion BERT yields better performance with better computational complexity in contrast with early-fusion BERT. Although the attention mechanism is implemented jointly on the concatenated features, the destruction of the temporal richness of fast stream to some degree might be the reason for the inferior performance of the early-fusion BERT.</p><p>R(2+1)D Architecture R(2+1)D <ref type="bibr" target="#b32">[28]</ref> architecture is a ResNet-type architecture consisting of separable 3D convolutions in which temporal and spatial convolutions are implemented separately. For this architecture, 112x112 input dimensions are applied following the paper, and 32-frame length is applied in- stead of 64-frame because of the huge memory demand of this architecture and to be consistent with the paper <ref type="bibr" target="#b32">[28]</ref>. The selected R(2+1)D architecture has 34 layers and implemented with basic block type instead of bottleneck block type (for further details about block types, see <ref type="bibr" target="#b14">[12]</ref>). The most important difference of R(2+1)D experiments from the previous architectures is the utilization of the IG65M pre-trained weights, instead of Kinetics pre-trained weights (see Section 4.1 for details). Therefore, this information should always be considered while comparing this architecture with the aforementioned ones. The analysis of R(2+1)D BERT architecture is limited to RGB modality, since the study <ref type="bibr" target="#b11">[9]</ref> of the IG65M dataset where R(2+1)D architecture is preferred is limited to RGB modality.</p><p>The experiments for BERT on R(2+1)D architecture are presented in <ref type="table" target="#tab_5">Table  6</ref>. The feature dimension of R(2+1)D architecture is already 512 which is the same with the reduced feature dimension of ResNeXt101 and I3D backbones for BERT implementations. Therefore, we do not use FRMB or FRAB for R(2+1)D. There is an increase of about 3M parameters and the increase in the number of operations is still negligible. The performance increase of BERT on R(2+1)D architecture is about 2% which is a significant increase for RGB modality as in the case of ResNeXt101-BERT architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with State-of-the-Art</head><p>In this section, the results of the best BERT architectures from the previous section are compared against the state-of-the-art methods. For this aim, two leading BERT architectures are selected among all the test methods: Two-Stream BERT ResNeXt101 and RGB BERT R(2+1)D (see <ref type="bibr">Section 4.4)</ref>. Note that these two architectures use different pre-training datasets, namely IG65 and Kinetics-400 for ResNext101 and R(2+1)D, respectively.</p><p>The results of the architectures on HMDB51 and UCF101 datasets are presented in <ref type="table" target="#tab_6">Table 7</ref>. The table indicates if an architecture employs explicit optical flow. Moreover, the table lists the pre-training dataset used by the methods.</p><p>As shown in <ref type="table" target="#tab_6">Table 7</ref>, BERT increases the Top-1 performance of the twostream ResNeXt101 with 1.77% and 0.41% in HMDB51 and UCF101, respectively. Additionally, BERT improves the Top-1 performance of RGB R(2+1)D (32f) with 3.5 % and 0.48% in HMDB51 and UCF101, respectively, where 32f corresponds to 32-frame length. The results obtained by the R(2+1)D BERT (64f) architecture pre-trained with the IG65M dataset is the current state-ofthe-art result in AR for HMDB51 and UCF101, to the best of our knowledge. Among the architectures pre-trained in Kinetics-400, the two-stream ResNeXt101 BERT is again the best in HMDB51 but the second-best in the UCF101 dataset. This might be owing to the fact that HMDB51 involves some actions that can be resolved only using temporal reasoning and therefore benefits from BERT's capacity.</p><p>An important point to note from the table is the effect of pre-training with the IG65M dataset. RGB R(2+1)D (32f) (without Flow) pre-trained with IG65M obtains about 6% and 1.4% better Top-1 performance in HMDB51 and UCF101, respectively than the one pre-trained with Kinetics-400, indicating the importance of the number of samples in the pre-training dataset even if the samples are collected in a weakly-supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This study combines the two major components from AR literature, namely late temporal modeling and 3D convolution. Although there are many pooling, fusion, and recurrent modeling strategies that are applied to the features from 2D CNN architectures, we firmly believe that this manuscript is the first study that removes temporal global average pooling (TGAP) and better employs temporal information at the output of 3D CNN architectures. To utilize these temporal features, an attention-based mechanism namely BERT is selected. The effectiveness of this idea is proven on most of the popular 3D CNN architectures which are ResNeXt, I3D, SlowFast, and R(2+1)D. In addition, significant improvements over the-state-of-the-art techniques are obtained in HMDB51 and UCF101 datasets.</p><p>The most important contribution of this study is the introduction of the late temporal pooling concept, paving the way for better late temporal pooling strategies over BERT on 3D CNN architectures as future work, although better performance is obtained with BERT over average pooling, concatenation pooling, and standard LSTM pooling. A possible research direction might be proposals for parameter efficient BERT implementations that do not need feature reduction blocks (FRMB or FRAB) which decreases the capabilities of the final extracted features because of the reduction in the dimension of features. Additionally, as future work, unsupervised concepts can still be proposed on BERT 3D CNN architectures, since the real benefits of BERT architecture rise to the surface with unsupervised techniques. Finally, the proposed method has also the potential to improve similar tasks with AR, such as temporal and spatial action localization and video captioning. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>BERT-based late temporal modeling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The implementations of Feature Reduction with Modified Block (FRMB) and Feature Reduction with Additional Block (FRAB)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Early-fusion and late-fusion implementations of BERT on SlowFast architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>ACKNOWLEDGMENTS</head><label></label><figDesc>This work was supported by an Institutional Links grant under the Newton-Katip Celebi partnership, Grant No. 217M519 by the Scientific and Technological Research Council of Turkey (TUBITAK) and ID [352335596] by British Council, UK. The numerical calculations reported in this paper were partially performed at TUBITAK ULAKBIM, High Performance and Grid Computing Center (TRUBA resources)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation Study of RGB ResNeXt101 architecture for temporal pooling analysis on HMDB51. FRMB: Feature Reduction with Modified Block.</figDesc><table><row><cell>Type of</cell><cell cols="2">FRMB? Optimizer Top1 # of</cell><cell># of</cell></row><row><cell>Temporal Pooling</cell><cell></cell><cell cols="2">(%) Params Operations</cell></row><row><cell>Average Pooling (Baseline)</cell><cell>SGD</cell><cell cols="2">74.46 47.63 M 38.56 GFlops</cell></row><row><cell>Average Pooling</cell><cell cols="3">ADAMW 75.99 47.63 M 38.56 GFlops</cell></row><row><cell>Average Pooling</cell><cell cols="3">ADAMW 74.97 44.22 M 38.36 GFlops</cell></row><row><cell>Concatenation</cell><cell cols="3">ADAMW 76.49 44.30 M 38.36 GFlops</cell></row><row><cell>LSTM</cell><cell cols="3">ADAMW 74.18 47.58 M 38.37 GFlops</cell></row><row><cell>Concatenation + Fully Connected Layer</cell><cell cols="3">ADAMW 76.84 47.45 M 38.36 GFlops</cell></row><row><cell>Non-local +</cell><cell></cell><cell></cell></row><row><cell>Concatenation +</cell><cell cols="3">ADAMW 76.36 47.35 M 38.43 GFlops</cell></row><row><cell>Fully Connected Layer</cell><cell></cell><cell></cell></row><row><cell>BERT pooling (Ours)</cell><cell cols="3">ADAMW 77.49 47.38 M 38.37 GFlops</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation Study of BERT late temporal Modeling on HMDB51.</figDesc><table><row><cell>Number of</cell><cell>Number of</cell><cell cols="2">Learnable Classification Token Top1</cell></row><row><cell cols="2">BERT Layers Attention Heads</cell><cell>against Pooled Features</cell><cell>(%)</cell></row><row><cell>1</cell><cell>8</cell><cell></cell><cell>76.07</cell></row><row><cell>1</cell><cell>1</cell><cell></cell><cell>76.97</cell></row><row><cell>1</cell><cell>8</cell><cell></cell><cell>77.49</cell></row><row><cell>2</cell><cell>8</cell><cell></cell><cell>77.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Analysis of ResNeXt101 architecture with and without BERT for RGB, Flow, and two-stream modalities on HMDB51 split-1 25%. Moreover, the memory trade-off of every layer of BERT is about 3M. The reason behind the deterioration might be the fact that late temporal modeling is not as much complex as capturing rich linguistic information and a single layer might be enough to capture the temporal relationship between the output features of 3D CNN architectures.</figDesc><table><row><cell cols="4">BERT Modality Top-1 # Parameters # Operations</cell></row><row><cell>RGB</cell><cell>74.38</cell><cell>47.63 M</cell><cell>38.56 GFlops</cell></row><row><cell>RGB</cell><cell>77.25</cell><cell>47.38 M</cell><cell>38.37 GFlops</cell></row><row><cell>Flow</cell><cell>79.48</cell><cell>47.60 M</cell><cell>34.16 GFlops</cell></row><row><cell>Flow</cell><cell>82.03</cell><cell>47.36 M</cell><cell>33.97 GFlops</cell></row><row><cell>Both</cell><cell>82.09</cell><cell>95.23 M</cell><cell>72.72 GFlops</cell></row><row><cell>Both</cell><cell>83.99</cell><cell>94.74 M</cell><cell>72.34 GFlops</cell></row><row><cell>top1 performance with 0.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The performance analysis of I3D architecture with and without BERT for RGB, Flow, and two-stream modalities on HMDB51 split-1</figDesc><table><row><cell>BERT Modality</cell><cell cols="4">Feature Top-1 # Parameters # Operations Reduction</cell></row><row><cell>RGB</cell><cell>X</cell><cell>75.42</cell><cell>12.34 M</cell><cell>111.33 GFlops</cell></row><row><cell>RGB</cell><cell>FRAB</cell><cell>75.75</cell><cell>16.40 M</cell><cell>111.72 GFlops</cell></row><row><cell>RGB</cell><cell>X</cell><cell>75.69</cell><cell>24.95 M</cell><cell>111.44 GFlops</cell></row><row><cell>Flow</cell><cell>X</cell><cell>77.97</cell><cell>12.32 M</cell><cell>102.52 GFlops</cell></row><row><cell>Flow</cell><cell>FRAB</cell><cell>77.25</cell><cell>16.37 M</cell><cell>102.91 GFlops</cell></row><row><cell>Flow</cell><cell>X</cell><cell>78.37</cell><cell>24.92 M</cell><cell>102.63 GFlops</cell></row><row><cell>Both</cell><cell>X</cell><cell>82.03</cell><cell>24.66 M</cell><cell>213.85 GFlops</cell></row><row><cell>Both</cell><cell>FRAB</cell><cell>82.68</cell><cell>32.77 M</cell><cell>214.63 GFlops</cell></row><row><cell>Both</cell><cell>X</cell><cell>82.68</cell><cell>49.87 M</cell><cell>214.07 GFlops</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The performance analysis of SlowFast architecture with and without BERT for RGB modality on HMDB51 split-1</figDesc><table><row><cell>BERT</cell><cell cols="3">Top-1 # Parameters # Operations</cell></row><row><cell></cell><cell>79.41</cell><cell>33.76 M</cell><cell>50.72 GFlops</cell></row><row><cell cols="2">(early-fusion) 79.54</cell><cell>43.17 M</cell><cell>52.39 GFlops</cell></row><row><cell cols="2">(late-fusion) 80.78</cell><cell>42.04 M</cell><cell>52.14 GFlops</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The performance analysis of R(2+1)D architecture with and without BERT for RGB modality on HMDB51 split-1</figDesc><table><row><cell cols="3">BERT Top-1 # Parameters # Operations</cell></row><row><cell>82.81</cell><cell>63.67 M</cell><cell>152.95 GFlops</cell></row><row><cell>84.77</cell><cell>66.67 M</cell><cell>152.97 GFlops</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison with the state-of-the-art.</figDesc><table><row><cell></cell><cell>Uses</cell><cell>Extra</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Flow? Training Data HMDB51 UCF101</cell></row><row><cell>IDT [30]</cell><cell></cell><cell></cell><cell>61.70</cell><cell>-</cell></row><row><cell>Two-Stream [24]</cell><cell></cell><cell>ImageNet</cell><cell>59.40</cell><cell>88.00</cell></row><row><cell>Two-stream Fusion + IDT [8]</cell><cell></cell><cell>ImageNet</cell><cell>69.20</cell><cell>93.50</cell></row><row><cell>ActionVlad + IDT [11]</cell><cell></cell><cell>ImageNet</cell><cell>69.80</cell><cell>93.60</cell></row><row><cell>TSN [33]</cell><cell></cell><cell>ImageNet</cell><cell>71.00</cell><cell>94.90</cell></row><row><cell>RSTAN + IDT [6]</cell><cell></cell><cell>ImageNet</cell><cell>79.90</cell><cell>95.10</cell></row><row><cell>TSM [18]</cell><cell></cell><cell>Kinetics-400</cell><cell>73.50</cell><cell>95.90</cell></row><row><cell>R(2+1)D [28]</cell><cell></cell><cell>Kinetics-400</cell><cell>74.50</cell><cell>96.80</cell></row><row><cell>R(2+1)D [28]</cell><cell></cell><cell>Kinetics-400</cell><cell>78.70</cell><cell>97.30</cell></row><row><cell>I3D [1]</cell><cell></cell><cell>Kinetics-400</cell><cell>80.90</cell><cell>97.80</cell></row><row><cell>MARS + RGB + Flow [3]</cell><cell></cell><cell>Kinetics-400</cell><cell>80.90</cell><cell>98.10</cell></row><row><cell>FcF [23]</cell><cell></cell><cell>Kinetics-400</cell><cell>81.10</cell><cell>-</cell></row><row><cell>ResNeXt101</cell><cell></cell><cell>Kinetics-400</cell><cell>81.78</cell><cell>97.46</cell></row><row><cell>EvaNet [22]</cell><cell></cell><cell>Kinetics-400</cell><cell>82.3</cell><cell>-</cell></row><row><cell>HAF+BoW/FV halluc [31]</cell><cell></cell><cell>Kinetics-400</cell><cell>82.48</cell><cell>-</cell></row><row><cell>ResNeXt101 BERT (Ours)</cell><cell></cell><cell>Kinetics-400</cell><cell>83.55</cell><cell>97.87</cell></row><row><cell>R(2+1)D (32f)</cell><cell></cell><cell>IG65M</cell><cell>80.54</cell><cell>98.17</cell></row><row><cell>R(2+1)D BERT (32f) (Ours)</cell><cell></cell><cell>IG65M</cell><cell>83.99</cell><cell>98.65</cell></row><row><cell>R(2+1)D BERT (64f) (Ours)</cell><cell></cell><cell>IG65M</cell><cell>85.10</cell><cell>98.69</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo Vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.502</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.502" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-fiber Networks for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<biblScope unit="page">11205</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/978-3-030-01246-5{_}22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01246-522" />
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="page" from="364" to="380" />
			<date type="published" when="2018" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MARS: Motionaugmented rgb stream for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2019-06-06" />
			<biblScope unit="page" from="7874" to="7883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2019.00807</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00807" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-Term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TPAMI.2016.2599174</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2599174" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent spatial-temporal attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2778563</idno>
		<ptr target="https://doi.org/10.1109/TIP.2017.2778563" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1347" to="1360" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00630</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00630" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision. vol. 2019-October</title>
		<meeting>the IEEE International Conference on Computer Vision. vol. 2019-October</meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional Two-Stream Network Fusion for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.213</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.213" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. vol. 2016-Decem</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. vol. 2016-Decem</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.00561" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video Action Transformer Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1812.02707" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018-12-253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ActionVLAD: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.337</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.337" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00685</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00685" />
		<title level="m">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet? In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>abs/1606.08415</idno>
		<ptr target="http://arxiv.org/abs/1606.08415" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1704.04861" />
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.223</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.223" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2011.6126543</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2011.6126543" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VideoLSTM convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2017.10.011</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2017.10.011" />
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TSM: Temporal Shift Module for Efficient Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1811.08383" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision 2019-October</title>
		<meeting>the IEEE International Conference on Computer Vision 2019-October</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7082" to="7092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.05101" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding the Effective Receptive Field in Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1701.04128" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4905" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299101</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7299101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-10" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1811.10636" />
		<title level="m">Evolving Space-Time Neural Architectures for Videos. Proceedings of the IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Representation Flow for Action Recognition. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1810.01455" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
	<note>Neural information processing systems foundation</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1212.0402" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2015.510</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.510" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision. vol. 2019-Octob</title>
		<meeting>the IEEE International Conference on Computer Vision. vol. 2019-Octob</meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2019.00565</idno>
		<ptr target="http://arxiv.org/abs/1904.02811" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00675</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00675" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5999" to="6009" />
		</imprint>
	</monogr>
	<note>Neural information processing systems foundation</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2013.441</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2013.441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.05910" />
		<title level="m">Hallucinating IDT Descriptors and I3D Optical Flow Features for Action Recognition with CNNs. Proceedings of the IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019-10-8697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Towards Good Practices for Very Deep Two-Stream ConvNets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1507.02159" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Temporal Segment Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2868668</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2868668" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Non-local Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00813</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00813" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01267-0{_}19</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01267-019" />
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). vol. 11219 LNCS</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
