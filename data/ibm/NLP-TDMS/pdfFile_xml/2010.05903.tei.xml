<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Reiss</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection methods require high-quality features. In recent years, the anomaly detection community has attempted to obtain better features using advances in deep self-supervised feature learning. Surprisingly, a very promising direction, using pretrained deep features, has been mostly overlooked. In this paper, we first empirically establish the perhaps expected, but unreported result, that combining pretrained features with simple anomaly detection and segmentation methods convincingly outperforms, much more complex, state-of-the-art methods.</p><p>In order to obtain further performance gains in anomaly detection, we adapt pretrained features to the target distribution. Although transfer learning methods are well established in multi-class classification problems, the one-class classification (OCC) setting is not as well explored. It turns out that naive adaptation methods, which typically work well in supervised learning, often result in catastrophic collapse (feature deterioration) and reduce performance in OCC settings. A popular OCC method, DeepSVDD, advocates using specialized architectures, but this limits the adaptation performance gain. We propose two methods for combating collapse: i) a variant of early stopping that dynamically learns the stopping iteration ii) elastic regularization inspired by continual learning. Our method, PANDA, outperforms the state-of-the-art in the OCC, outlier exposure and anomaly segmentation settings by large margins 1 . arXiv:2010.05903v2 [cs.CV] 6 May 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detecting anomalous patterns in data is of key importance in science and industry. In the computational anomaly detection task, the learner observes a set of training examples. The learner is then tasked to classify novel test samples as normal or anomalous. There are multiple anomaly detection settings investigated in the literature, corresponding to different training conditions. In this work, we deal * Equal contribution <ref type="bibr">1</ref> The code is available at github.com/talreiss/PANDA with three settings: i) anomaly detection -when only normal images are used for training ii) anomaly segmentation -detecting all the pixels that contain anomalies, given normal images as input. iii) Outlier Exposure (OE) -where an external dataset simulating the anomalies is available.</p><p>In recent years, deep learning methods have been introduced for anomaly detection, typically extending classical methods with deep neural networks. Different auxiliary tasks (e.g. autoencoders or rotation classification) are used to learn representations of the data, while a great variety of anomaly criteria are then used to determine if a given sample is normal or anomalous. An important issue for current methods is the reliance on limited normal training data for representation learning, which limits the quality of learned representations. Nearly all state-of-the-art anomaly detection methods rely on self-supervised feature learning -i.e. using the limited normal training data for learning strong features. The motivation for this is twofold: i) the fear that features trained on auxiliary domains will not generalize well to the target domain. ii) the curiosity to investigate the top performance achievable without ever looking at any external dataset (we do not address this question here).</p><p>In other parts of computer vision, features pretrained on external datasets are often used to improve performance on tasks trained on new domains -and our reasonable hypothesis is that this should also be the case for image anomaly detection and segmentation. We present very simple baselines that use pretrained features trained on a large external data and K-nearest neighbor (kNN) retrieval to significantly outperform all previous methods on anomaly detection and segmentation, even on images of distant target domains.</p><p>We then tackle the technical challenge of obtaining stronger performance by further adaptation to the normal training data. Although feature adaptation has been extensively researched in the multi-class classification setting, limited work was done in the OCC setting. Unfortunately, it turns out that feature adaptation for anomaly detection often suffers from catastrophic collapse -a form of deterioration of the pretrained features, where all (including anomalous) samples, are mapped to the same point. DeepSVDD <ref type="bibr" target="#b22">[23]</ref> proposed to overcome collapse by removing biases from the model architecture, but this restricts network expressivity and limits the pretrained models that can be borrowed off-the-shelf. Perera and Patel <ref type="bibr" target="#b20">[21]</ref> proposed to jointly train OCC with the original task which has several limitations and achieves only limited adaptation success.</p><p>Our first finding is that simple training with constantduration early stopping (with no bells-and-whistles) already achieves top performance. To remove the dependence on the number of epochs for early stopping, we propose two techniques to overcome catastrophic collapse: i) an adaptive early stopping method that selects the stopping iteration per-sample, using a novel generalization criterionthis technique is designed to overcome a special problem of OCC, namely that there are no anomalies in the validation set ii) elastic regularization, motivated by continual learning, that postpones the collapse. Thorough experiments demonstrate that we outperform the state-of-the-art by a wide margin (ROCAUC): e.g. CIFAR10 results: 96.2% vs. 90.1% without outlier exposure and 98.9% vs. 95.6% with outlier exposure. We also achieve 96.0% vs. 89.0% on anomaly segmentation on MVTec.</p><p>We present insightful critical analyses: i) We show that pretrained features strictly dominate current self-supervised RotNet-based feature learning methods. We discuss the relative merits of each paradigm and conclude that for most practical purposes, using pretrained features is preferable. ii) We analyse the results of the popular DeepSVDD method and discover that its feature adaptation, which is designed to prevent collapse, does not improve over simple data whitening.</p><p>Contributions: To summarize our main contributions in this paper:</p><p>• Demonstrating that a simple baseline outperforms all current methods in image anomaly detection and segmentation -extensive analysis shows the generality of the result.</p><p>• Identifying that popular SOTA methods do not outperform linear whitening in OCC feature adaptation.</p><p>• Proposing several effective solutions for feature adaptation for OCC.</p><p>• Extensive evaluation, obtaining results that significantly improve over the current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Classical anomaly detection: The main categories of classical anomaly detection methods are: i) reconstructionbased: compressing the training data using a bottleneck, and using a reconstruction loss as an anomaly criterion (e.g. <ref type="bibr">[4,</ref><ref type="bibr" target="#b16">17]</ref>, K nearest neighbors <ref type="bibr">[7]</ref> and K-means <ref type="bibr">[12]</ref>), ii) probabilistic: modeling the probability density function and labeling unlikely sampled as anomalous (e.g. Ensembles of Gaussian Mixture Models <ref type="bibr">[9]</ref>, kernel density estimate <ref type="bibr" target="#b18">[19]</ref>) iii) one-class classification (OCC): finding a separating manifold between normal data and the rest of input space (e.g. One-class SVM <ref type="bibr" target="#b24">[25]</ref>).</p><p>Deep learning methods: The introduction of deep learning has affected image anomaly detection in two ways: extension of classical methods with deep representations and novel self-supervised deep methods. Reconstruction-based methods have been enhanced by learning deep autoencoderbased bottlenecks <ref type="bibr">[6]</ref> which can provide better models of image data. Deep methods extended classical methods by creating a better representations of the data for parametric assumptions about probabilities, a combination of reconstruction and probabilistic methods (such as DAGMM <ref type="bibr" target="#b27">[28]</ref>), or in a combination with OCC methods <ref type="bibr" target="#b22">[23]</ref>. Novel deep methods have also been proposed for anomaly detection including GAN-based methods <ref type="bibr" target="#b27">[28]</ref>. Another set of novel deep methods use auxiliary self-supervised learning for anomaly detection. The seminal work by <ref type="bibr">[10]</ref> was later extended by <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr">[1]</ref>.</p><p>Transferring pretrained representations: Learning deep features requires extensive datasets, preferably with labels. An attractive property of deep neural networks, is that representations learned on very extensive datasets, can be transferred to data-poor tasks. Specifically deep neural representations trained on the ImageNet dataset have been shown by <ref type="bibr" target="#b15">[16]</ref> to significantly boost performance on other datasets that are only vaguely related to some of the ImageNet classes. This can be performed with and without finetuning. Although much recent progress has been performed on self-supervised feature learning <ref type="bibr">[8,</ref><ref type="bibr">5]</ref>, such methods are typically outperformed by transferred pretrained features. Transferring ImageNet pretrained features for out-ofdistribution detection has been proposed by <ref type="bibr">[13]</ref>. Similar pretraining has been proposed for one-class classification has been proposed by <ref type="bibr" target="#b20">[21]</ref>, however they require joint optimization with the original task. Rippel et. al. <ref type="bibr" target="#b21">[22]</ref> follow an early version of this paper and report results with pretrained features on MVTec using the Mahalanobis distance.</p><p>Anomaly segmentation methods: Segmenting the image pixels that contain anomalies has attracted far less research attention than image-level anomaly detection. Several previous anomaly segmentation works used pretrained features, but they have not convincingly outperformed top self-supervised methods. Napoletano et al. <ref type="bibr" target="#b19">[20]</ref> extracted deep features from small overlapping patches, and used a K-means based classifier over dimensionality reduced features. Bergmann et al. <ref type="bibr">[2]</ref> evaluated both a ADGAN and autoencoder approaches on MVTec dataset <ref type="bibr">[2]</ref> finding complementary strengths. More recently, Venkataramanan et al. <ref type="bibr" target="#b26">[27]</ref> used an attention-guided VAE approach combining multiple methods (GAN loss <ref type="bibr">[11]</ref>, GRADCAM <ref type="bibr" target="#b25">[26]</ref>).</p><p>Bergmann et al. <ref type="bibr">[3]</ref> used a student-teacher based autoencoder approach employing pretrained ImageNet deep features. Our simple baseline, SPADE, significantly outperforms the previously mentioned approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A General Framework and Simple Baselines</head><p>for Anomaly Detection and Segmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">A Three-stage Framework</head><p>We present our general framework in which we examine several adaptation-based anomaly detection methods, including our method. Let us assume that we are given a set D train of normal training samples: x 1 , x 2 ..x N . The framework consists of three steps:</p><p>Initial feature extractor: An initial feature extractor ψ 0 can be obtained by pretraining on an auxiliary task with loss function L pretrain . The auxiliary task can be either pretraining on an external dataset (e.g. ImageNet) or by self-supervised learning (auto-encoding, rotation or jigsaw prediction). In the former case, the pretrained extractor can be obtained off-the-shelf. The choice of auxiliary tasks is analyzed in Sec. 4.3.</p><p>Feature adaptation: Features trained on auxiliary tasks or datasets may require adaptation before being used for anomaly scoring on the target data. This is seen as a finetuning stage of the features on the target training data. We denote the feature extractor after adaptation ψ.</p><p>Anomaly scoring:</p><p>Having adapted the features for anomaly detection, we extract the features ψ(x 1 ), ψ(x 2 )..ψ(x N ) of the training set samples. We then proceed to learn a scoring function, which describes how anomalous a sample is. Typically, the scoring function seeks to measure the density of normal data around the test sample ψ(x) (either by direct estimation or via some auxiliary task) and assign a high anomaly score to low density regions.</p><p>We report very simple-to-implement but highly effective baselines for anomaly detection and segmentation, based on our framework. In both baselines we skip the adaptation stage. Implementation details of both methods, including the K-Means-based speedup, can be found in the appendix (App.B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Simple Baseline for Anomaly Detection</head><p>In the anomaly detection baseline "Deep Nearest Neighbors" (DN2), the feature extractor is a large ResNet pretrained the ImageNet dataset. The features are taken from the final pooling layer (before the linear classifier). For each test image we use the average distance from the features of the kNN normal images as the anomaly score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Simple Baseline for Anomaly Segmentation</head><p>In the anomaly segmentation baseline, "Semantic Pyramid Anomaly Detection" (SPADE), we use an ImageNetpretrained ResNet to extract per-pixel features for all images. As both low-level high-resolution features, and semantic low-resolution features are important for determining if a pixel is anomalous, we extract features from multiple layers of the deep neural network. Upstream layers give high resolution but less semantic features maps. Downstream layers are lower in resolution, but are more semantic and include more context. For each pixel, we combine features taken from several few layers (See App.B). We score each pixel by its kNN distance from the feature descriptors of the pixels of all the training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature Adaptation for Anomaly Detection</head><p>Although our two simple-to-implement baselines, DN2 and SPADE, achieve very strong results, we ask if feature adaptation can improve them further. We first review two existing methods for feature adaption for anomaly detection, and proceed to propose our method, PANDA, which significantly improves over them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background: Existing Feature-Adaptation Methods</head><p>DeepSVDD: Ruff et al. <ref type="bibr" target="#b22">[23]</ref> suggest to first train an autoencoder on the normal-only train images. The encoder is then used as the initial feature extractor ψ 0 . As the features of the encoder are not specifically adapted to anomaly detection, DeepSVDD adapts ψ on the training data. The adaptation takes place by minimizing the compactness loss:</p><formula xml:id="formula_0">L compact = x∈Dtrain ψ(x) − c 2<label>(1)</label></formula><p>Where c is a constant vector, typically the average of ψ 0 (x) on the training set. However, the authors were concerned of the trivial solution ψ = c, and suggested architectural restrictions to mitigate it, most importantly removing the biases from all layers. We empirically show that the effect  <ref type="bibr" target="#b20">[21]</ref> proposed to use a deep feature extractor trained for object classification on the ImageNet dataset. Due to fear of "learning a trivial solution in the absence of a penalty for miss-classification", the method does not adapt by finetuning on the compactness loss only. Instead, they relaxed the task setting, by assuming that a number (∼ 50k) of labelled original ImageNet images, D pretrain , are still available at adaptation time. They proposed to train the features ψ under the compactness loss jointly with the original ImageNet classification linear layer W and its classification loss, here the CE loss with the true label pretrain (p, y) = − log(p y ), and SMax indicates Softmax:</p><formula xml:id="formula_1">L Joint = (x,y)∈Dpretrain pretrain (SM ax(W ψ(x)), y) + α · x∈Dtrain ψ(x) − c 2 (2)</formula><p>Where W is the final linear classification layer and α is a hyper-parameter weighting the two losses. We note that the method has two main weaknesses: i) it requires retaining a significant number of the original training images which can be storage intensive ii) jointly training the two tasks may reduce the anomaly detection task accuracy, which is the only task of interest in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">PANDA: pretrained Anomaly Detection Adaptation</head><p>We present PANDA, a new method for anomaly detection in images. Similarly to SVDD and Joint Optimization, we also use the compactness loss (Eq. 1) to adapt the general pretrained features to the task of anomaly detection on the target distribution. Instead of constraining the architecture or introducing external data into the adaptation procedure we tackle catastrophic collapse directly. The main challenge is that the optimal solution of the compactness loss can result in "collapse", where all possible input values are mapped to the same point (ψ(x) = c, ∀x). Learning such features will not be useful for anomaly detection, as both normal and anomalous images will be mapped to the same output, preventing separability. The issue is broader than the trivial "collapsed" solution after full convergence, but rather the more general issue of feature deterioration, where the original good properties of the pretrained features are lost. Even a non-trivial solution might lose some of the discriminative properties of the original features which are none-the-less important for anomaly detection.</p><p>To avoid this collapse, we suggest three options: (i) finetuning the pretrained extractor with compactness loss (Eq.1) and stopping after a constant number of iterations (ii) a novel method for determining early stopping per-sample (iii) when collapse happens prematurely, before any significant adaptation happens, we suggest mitigating it using a Continual Learning-inspired adaptive regularization.</p><p>Simple early stopping (PANDA-Early): An embarrassingly simple but effective solution for controlling the collapse of the original features is to stop training after a constant number of iterations (e.g. 15 epochs on CIFAR10). Inversely scaling the number of epochs by dataset size works for most examined datasets (Sec. 4.3).</p><p>Sample-wise early stopping (PANDA-SES): A weakness of the simple early-stopping approach, is the reliance on a hyper-parameter that may not generalize to new datasets. Although the optimal stopping epoch can be determined with a validation set containing anomalies, it is not available in our setting. We thus propose "samplewise early stopping" (SES) as an unsupervised way of determining the stopping epoch from a single sample. The intuition for the method can be obtained from <ref type="figure" target="#fig_2">Fig. 3</ref>. We can see that anomaly detection accuracy is correlated to having a large ratio between the distance of the anomalous samples to the center, and the distance between the normal samples and the center. We thus propose to save checkpoints of our network at fixed intervals (every 5 epochs) during the training process -corresponding to different early stopping iterations (ψ 1 , ψ 2 ..ψ T ), for each network ψ t we compute the average distance on the training set images s t . During inference, we score a target image x using each model s target t = ψ t (x) − c 2 , and normalize the score by the training average score by s t . We set the maximal ratio, as the anomaly score of this sample, as this roughly estimates the model that achieves the best separation between normal and such anomalous samples.</p><p>Continual Learning (PANDA-EWC): We propose a new solution for overcoming premature feature collapse that draws inspiration from the field of continual learning. The task of continual learning tackles learning new tasks without forgetting the previously learned ones. We note however that our task is not identical to standard continual learning as: i) we deal with the one-class classification setting whereas continual-learning typically deals with multi-class classification ii) we aim to avoid forgetting the expressivity of the features but do not particularly care if the actual classification performance on the old task is degraded. A simple solution for preventing feature collapse is regularization of the change in value of the weights of the feature extractor. However, this solution is lacking as some weights influence the features more than others.</p><p>Following ideas from continual learning, we use elastic weight consolidation (EWC) <ref type="bibr" target="#b17">[18]</ref>. Using a number of minibatches (we use 100) to pretrain on the auxiliary task. We compute the diagonal of the Fisher information matrix F for all weight parameters of the network. Note that this only needs to happen once at the end of the pretraining stage. The value of the Fisher matrix for diagonal element θ is given by:</p><formula xml:id="formula_2">F θ = E (x,y)∈Dpretrain ∂ ∂θ L pretrain (x, y) 2<label>(3)</label></formula><p>We follow <ref type="bibr" target="#b17">[18]</ref> in using the diagonal of the Fisher information matrix F θi , to weight the squared distance of the change of each network pretrained weight θ i ∈ ψ 0 and finetuned weight θ * i ∈ ψ. This can be seen as a measure of the loss landscape curvature as function of the weights -larger values imply high curvature, inelastic weights.</p><p>We use this regularization in combination with the compactness loss weighted by the factor λ, which is a hyperparameter of the method (we use λ = 10 4 ):</p><formula xml:id="formula_3">L θ = L compact (θ * ) + λ 2 · i F θi (θ i − θ * i ) 2<label>(4)</label></formula><p>The network ψ is initialized with the parameters of the pretrained extractor ψ 0 and trained with SGD.</p><p>Anomaly scoring: As in classical anomaly detection, scoring can be done by density estimation. Unless mentioned otherwise, we use kNN for scoring. We also evaluate faster methods and get similar results (see Sec. 4.3.3).</p><p>Outlier Exposure: An extension of the typical image anomaly detection task <ref type="bibr">[14]</ref>, assumes the existence of an auxiliary dataset of images D OE , which are more similar to the anomalies than normal data. In case such information is available, we simply train a linear classification layer w together with the features ψ under a logistic regression loss (Eq. 5). As before, ψ is initialized with the weights from ψ 0 . After training ψ and w, we use w · ψ(x) as the anomaly score. Results and critical analysis of this setting are presented in Sec. 4.3.</p><formula xml:id="formula_4">L OE = x∈Dtrain log(1−σ(w·ψ(x)))+ x∈D OE log(σ(w·ψ(x))) (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Image Anomaly Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments</head><p>In this section, we present high-level results of the our simple baselines, and our full method PANDA-EWC, (PANDA-SES can be found in Sec.4.3) compared to the state-of-the-art: One-class SVM <ref type="bibr" target="#b24">[25]</ref>, DeepSVDD <ref type="bibr" target="#b22">[23]</ref>, Multi-Head RotNet <ref type="bibr" target="#b14">[15]</ref>. All the results of others that were available in the original papers were copied exactly. In cases that the result was not available, we run the experiments ourselves (where possible). As Joint Optimization requires extra data, we did not add it to this table, but compare and outperform it in Tab. 6. We compare our PANDA-OE to the OE baseline in <ref type="bibr" target="#b14">[15]</ref> on CIFAR10, as the code or results for other classes were unavailable. Note that unless specifically mentioned otherwise, PANDA results were run with kNN. PANDA-OE used the original classifier -which performs a little better than kNN. We compare SPADE and relevant state-of-the-art baselines on anomaly segmentation.</p><p>We evaluated our method on a wide range of datasets   <ref type="figure" target="#fig_3">. 4</ref>) demonstrating different challenges in image anomaly detection (see App. C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">High-Level Results</head><p>The main results are i) pretrained features achieve significantly better results than self-supervised features on all datasets, both in anomaly detection and segmentation. ii) Feature adaptation significantly improves the performance on larger datasets iii) Outlier exposure can further improve performance in the case where the given outliers are more similar to the anomalies than the normal data. OE achieves near perfect performance on CIFAR10/100 but hurts performance for Fashion MNIST/CatsVsDogs which are less similar to the 80M Tiny images dataset.</p><p>A detailed analysis of the reason for better performance for each of these methods and an examination of its appropriateness will be presented in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis and Further Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">An analysis of feature representations</head><p>A comparison of self-supervised and pretrained features: In Tab. 2 and Tab. 3, we present a comparison between methods that use self-supervised and pretrained feature representations. We see that the autoencoder used by DeepSVDD is particularly poor. The results of the MHRot-Net as a feature extractor are better, but still underperform PANDA methods (see App. A for more details). The performance of the raw deep ResNet features without adaptation significantly outperforms all methods, including Fashion MNIST and DIOR which have significant differences from the ImageNet dataset. We can therefore conclude that ImageNet-pretrained features typically have significant advantages over self-supervised features. Tab. 3 shows that self-supervised methods do not perform well on small datasets as such methods require large numbers of normal samples in order to learn strong features. On the other hand ImageNet-pretrained features obtain very strong results.</p><p>Does the superiority of pretrained features extend to very different domains? The results in Tab. 3 on FM-NIST, DIOR, WBC, MVTec suggest that out-of-domain pretrained features are better at anomaly detection than indomain self-supervised features. We tested on datasets of various sizes, domains, resolutions and symmetries. On all those datasets pretrained features outperformed the SOTA. These datasets include significantly different objects from those of ImageNet, but also fine-grained intra-object anomalies, and represent a spectrum of data types: aerial images, microscopy, industrial images. This shows that one of the main arguments against using pretrained features, generalizing to distant domains, is not an issue in practice.  Our simple pretrained feature baseline (SPADE) is extremely effective for anomaly segmentation: In Tab. <ref type="bibr">4</ref> we can see that our simple, no-training, baseline (named SPADE) outperforms previous methods for anomaly segmentation, including those that use trained and pretrained features (see App. B,E for metrics, specifications and detailed results). While we suspect feature adaptation can be used for further performance gain even for anomaly segmentation, we find that the MVTec dataset is too small for significant feature adaptation using the compactness loss. We believe that feature adaptation for segmentation calls for new adaptation methods, this is left for future work.</p><p>On the different supervision settings for one-class anomaly detection: Anomaly detection methods employ different levels of supervision. Within the one-class classification task, one may use outlier exposure (OE) -an external dataset (e.g. ImageNet), pretrained features, or no external supervision at all. The most extensive supervision is used by OE, which requires a large external dataset at training time, and performs well only when such a dataset is from a similar domain to the anomalies (see Tab. 2). In cases where the dataset used for OE has significantly different properties, the network may not learn to distinguish between normal and anomalous data, as the normal and anomalous data may have more in common than the OE dataset. E.g. both normal and anomalous classes of Fashion MNIST are grayscale, OE using 80M Tiny Images will not be helpful, as the network may learn to classify only according to color. Pretrained features further improve OE, in cases where is suitable e.g. CIFAR10.</p><p>Pretraining, like Outlier Exposure, is also achieved through an external labelled dataset, but differently from OE, the external dataset is only required once -at the pretraining stage and is not used again. Additionally, the same features are applicable for very different image domains from that of the pretraining dataset (e.g. Fashion MNISTgrayscale images, DIOR -aerial images, WBC-medical images, MVTec -industrial images). Self supervised feature learning requires no external dataset at all, which can potentially be an advantage. While there might be image anomaly detection tasks where ImageNet-pretrained weights are not applicable, we saw no evidence for such cases after examining a broad spectrum of domains and datasets (Tab. 1). This indicates that the extra supervision of the ImageNetpretrained weights comes at virtually no cost. Can pretrained features boost the performance of RotNet-based methods? We did not find evidence that pretrained features improve the performance of RotNet-based AD methods such as <ref type="bibr" target="#b14">[15]</ref> (CIFAR10: 90.1% vs. 86.6% without and with pretraining). As can be seen in Tab. 5, pretrained features improve the auxiliary task performance on the normal data, but also on the anomalous samples. As such methods rely on a generalization gap between normal and anomalous samples, deep features actually reduce this gap, as a solution to the auxiliary task becomes feasible for both types of images. For a more detailed analysis see App. A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Feature adaptation methods</head><p>Benefits of feature adaptation: Feature adaptation aims to make the distribution of the normal samples more compact, w.r.t. the anomalous samples. Our approach of finetuning pretrained features for compactness under EWC regularization, significantly improves the performance over "raw" pretrained features (see Tab.2). While the distance from the normal train samples' center, of both normal and anomalous test samples is reduced (see <ref type="figure" target="#fig_2">Fig.3</ref>), the average distance from the center of anomalous test samples is typically higher than that of normal samples, in relative terms, which makes anomalies easier to detect.</p><p>While PANDA-EWC may train more than 7.8k minibatches without catastrophic collapse on CIFAR10, per-   formance of training without regularization usually peaks higher but collapse earlier. We therefore set our constant early stopping epoch such that the net trains with to 2.3k minibatches on all datasets for comparison. Our PANDA-SES method usually achieves an anomaly score not far from the unregularized early stopping peak performance, but is most important in cases where unregularized training fails. A comparison of feature adaptation methods: In Tab. 6 we compare PANDA against (i) JO [21] -co-training compactness with ImageNet classification which requires ImageNet data at training time. We can see that PANDA -EWC always outperforms JO feature adaptation. (ii) PANDA early stopping, generally has higher performance than PANDA-EWC, but has severe collapse issues on some classes. (iii) PANDA-SES is similar to early stopping, but PANDA-SES does not collapse as badly on CatsVsDogs dataset. We note that replacing the Fisher matrix by equally weighting the changes in all parameters ( i (θ i − θ * i ) 2 ) achieves similar results to early stopping.</p><p>Which are the best layers to finetune? Fine-tuning all layers is prone to feature collapse, even with continual learning. We therefore recommend finetuning only layers 3 &amp; 4 (see ablation in App. D).</p><p>DeepSVDD architectural changes: DeepSVDD <ref type="bibr" target="#b22">[23]</ref> proposes various architectural changes, such as removing the bias parameters from the network, to prevent collapse to trivial features. To understand whether DeepSVDD gains its significant performance from its pretrained features or from its feature adaptation, we tried to replace its feature adaptation by closed-form linear data whitening. For both pretrained features and anomaly scoring, we used the DeepSVDD original code <ref type="bibr" target="#b22">[23]</ref>. We found empirically that the results obtained by the constrained architecture were about the same as those achieved with simple whitening of the data (64.8% vs. 64.6%, see App. D). We ablated DeepSVDD by running it with the original LeNet (including biases) and found this did not deteriorate its anomaly detection performance. As architectural modifications are not the focus of this work, further investigation into architectures less prone to feature collapse is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Anomaly scoring functions</head><p>Does kNN improve over distance to the center? kNN achieves an improvement of around 2% on average w.r.t. to distance to the center (CIFAR10: 94.2% vs 96.2%).</p><p>Can we improve over the linear complexity of kNN? A naive implementation of kNN has linear runtime complexity in the number of training samples. For anomaly segmentation, approximating all the training sample features by 50 means a speeds the method from 2.7 framesper-second to 41 frames-per-second (faster than real-time), with ∼0.5% ROCAUC decrease. For anomaly detection, even for very large datasets, or many thousands of means, both kNN and K-means can run faster than real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Outlook</head><p>We first proposed simple baseline methods for anomaly detection and segmentation, that outperform the state-ofthe-art. We further improved over the strong baselines by proposing a method that adapts pretrained features and mitigates catastrophic collapse. We showed that our results significantly outperform current methods while addressing their limitations. We analysed the reasons for the strong performance of our method. We note that the question of the optimal performance on image anomaly detection without ever having access to auxiliary data is unaddressed here, however we believe it is of mostly pure academic interest.</p><p>The main limitation of this work is the requirement for strong pretrained feature extractors. Much work was done on transferable image and text features and it is likely that current extractors can be effective to obtain features for time series and audio data as well. Generic feature extractors are not currently available for tabular data, their development is an exciting direction for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pretrained Features, RotNet Auxiliary Tasks and Generalization</head><p>Let us take a closer look at the application of RotNetbased methods for image anomaly detection. We will venture to understand why initializing RotNets with pretrained features may actually impair their anomaly detection performance. In such cases, a network for rotation classification is trained on normal samples, and used to classify the rotation (and translations) applied to a test image. Each test image is checked for its rotation prediction accuracy, which is assumed to be worse for an anomalous images than for a a typical normal image.</p><p>To correctly classify a rotation of a new image, the network may use traits within the image that are associated with its correct alignment. Such features may be associated with the normal class, or with the entire dataset (common to both the anomalous classes together). For illustrative purposes, let us consider a normal class with images containing a deer, and the anomalous class with images containing a horse. The horns of the deer may indicate the "upward" direction, but so does the position of the sky in the image, which is often sufficient to classify the rotation correctly. As shown in Tab.5 (in the main text), when initialized with pretrained features, the RotNet network achieves very good performance on the auxiliary tasks, both within and outside the normal class, indicating the use the more general traits that are common to more classes.</p><p>Although at first sight it may appear that the improved auxiliary task performance should improve the performance on anomaly detection, this is in fact not the case! The reason is that features that generalize better, achieve better performance on the auxiliary task for anomalous data. The gap between the performance of normal and anomalous images of the auxiliary tasks, will therefore be smaller than with randomly-initialized networks -leading to degraded anomaly detection performance. For example, consider the illustrative case described above. A RotNet network that "overfits" to work only on the normal class deer, relying on the horns of the deer would classify rotations more accurately on deer images than on horse images (as its main feature is horns). On the other hand, a RotNet that also uses more general traits can use the sky position for rotation angle prediction. In this case, it will achieve higher accuracy for both deer and horse images. The gap in performance is likely to be reduced, leading to lower anomaly detection capabilities.</p><p>The above argument can be formulated using mutual information: In cases where the additional traits which are unique to the class do not add much information regarding the correct rotation, over the general features common to many classes, the class will have limited mutual information with the predicted rotation (conditional on the information already given by traits common to the entire datasets). When the conditional mutual information between the predicted rotation and the class traits decreases, we expect the predicted rotation to be less discriminative for anomaly detection, as we indeed see in Tab.7.</p><p>It is interesting to note that using features learned with RotNet for our transfer learning approach achieves inferior results to both MHRot and our method. Only through an ensemble of all rotations, as MHRot does, it achieves strong performance comparable to the MHRot performance. MHRot achieved 89.7% in our re-implementation. Using the MHRot features as ψ 0 , we compute the kNN distance of the unadapted features between the test set images and the train set image transformed by the same transformation. When ensembling the 36 transformations -and using the average kNN distance, yields 88.7%. Another metric we examined is computing the average kNN distance between test data transformed under a specific transformation and the training set transformed by another transformation. Using the average same-transformation kNN distance minus the average different transformation kNN distance, achieves 89.8% -a little better than the RotNet performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. PANDA</head><p>Optimization: We finetune the two last blocks of an Im-ageNet pretrained ResNet152 using SGD optimizer with weight decay of w = 5 · 10 −5 , and momentum of m = 0.9. We use G = 10 −3 gradient clipping. To have a compara- ble amount of training in the different dataset. We define the duration of each of our train using a constant number of minibatches, 32 samples each. EWC: We use the fisher information matrix as obtained by <ref type="bibr">[6]</ref>, as explained in Sec. 3 in the main text. We weight the EWC loss with λ = 10 4 . After obtaining EWC regularization, we train our net training on 7.8k minibatches.</p><p>Early stopping/Sample-wise early stopping: We save a copy of the net every 5 epochs. For early stopping we used the copy trained on 2.3k minibatches. For samplewise early stopping we try all copies trained on up to 150k image samples (including repetitions).</p><p>Anomaly scoring: Unless specified otherwise, we score the anomalies according to the kNN method with k = 2 nearest neighbours.</p><p>SES distance normalization: When comparing different networks as in PANDA-SES method, we normalize each set of features by the typical kNN distance of its normal train features. To obtain the typical normal distance we would like to compute the average on the normal samples. However, computing the distance between normal training data has an issue: each point is its own nearest neighbour. Instead, we split the train set features (90% vs. 10%), and compute the kNN between the 10% validation images and the gallery 90% images.</p><p>PANDA Outlier Exposure: The method was described in Sec.3 of the main text. For synthetic outlier images, we used the first 48k images of 80 Million Tiny Images <ref type="bibr">[10]</ref> with CIFAR10 and CIFAR100 images removed. We finetune the last block of an ImageNet pretrained ResNet152 with SGD optimizer using 75 epochs and the following parameters: learning rate: 0.1 with gradient clipping: 1e-3, momentum: 0.9, and no weight decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Anomaly Detection Baselines</head><p>We compare to the following methods: OC-SVM: One-class SVM with the RBF kernel. The hyper-parameters (ν ∈ {0.1, ..., 0.9}, γ ∈ {2 −7 , ..., 2 2 }) were optimized to maximize ROCAUC.</p><p>DeepSVDD: We resize all the images to 32 × 32 pixels and use the official pyTorch implementation with the CI-FAR10 configuration.</p><p>MHRot <ref type="bibr">[4]</ref>: An improved version of the original Rot-Net approach. For high-resolution images we used the current GitHub implementation. For low resolution images, we modified the code to the architecture described in the paper, replicating the numbers in the paper on CIFAR10.</p><p>Outlier Exposure (MHRot): We use the outlier exposure performance as reported in <ref type="bibr">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. SPADE</head><p>Architecture: In all experiments, we use a Wide-ResNet50 × 2 feature extractor, which was pretrained on ImageNet.</p><p>Resolution: MVTec images were resized to 256 × 256 and cropped to 224 × 224. All metrics were calculated at 256×256 image resolution, and we used cv2.INTERAREA for resizing when needed.</p><p>Layers: Unless otherwise specified, we used features from the ResNet at the end of the first block (56 × 56), second block (28 × 28) and third block <ref type="bibr">(14 × 14)</ref>, all with equal weights. In Tab. 8 we compare different level of the feature pyramid as feature descriptor. We experienced that using activations of too high resolution (56 × 56) significantly hurts performance due to limited context, while using the higher levels on their own, results in diminished performance (due to lower resolution). Using a combination of all three upstream layers in the pyramid results in the best performance.</p><p>Combining features from different layers: We evaluated two ways of combining per-pixel features extracted from different layers. Concatenation -resampling the activation to the same resolutions and concatenating all per-pixel features to form a combined feature. Ensembling -computing the per-pixel anomaly score using the per-pixel feature of each layer, and adding the per-pixel per-layer scores of all layers to form a combined score. We found the ensemble approach was more robust and yielded a bit better results. Therefore, we report it.</p><p>Postprocessing:</p><p>After computing the pixel-wise anomaly score for each image, we smoothed the results with a Gaussian filter (σ = 5).</p><p>For fast nearest neighbour implementation, we used the FAISS library <ref type="bibr">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Datasets</head><p>Standard datasets: We evaluate our method on a set of commonly used datasets: CIFAR10 [7]: Consists of RGB images of 10 object classes. Fashion MNIST <ref type="bibr">[13]</ref>: Consists of grayscale images of 10 fashion item classes. CIFAR100  <ref type="bibr">[11]</ref>: For each of those datasets we evaluated the methods using only the first 20 classes as normal train set, and using the entire test set for evaluation. MVTec [1]: This datasets contain 15 different industrial products, with normal images of proper products for train and 1 − 9 types of manufacturing errors as anomalies. The anomalies in MVTec are in-class i.e. the anomalous images come from the same class of normal images with subtle variations. We also use the MVTec dataset for the anomaly segmentation results.</p><p>Symmetric datasets: We evaluated our method on datasets that contain symmetries, such as images that have no preferred angle (microscopy, aerial images.): WBC <ref type="bibr">[14]</ref>: We used the 4 big classes in "Dataset 1" of microscopy images of white blood cells, and a 80%/20% train-test split. DIOR <ref type="bibr">[8]</ref>: We preprocessed the DIOR aerial image dataset by taking the segmented object in classes that have more than 50 images with size larger than 120 × 120 pixels. We can see that RotNet-type methods perform particularly poorly on such datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Choosing the Layers to Finetune</head><p>Fine-tuning all layers is prone to feature collapse, even with continual learning (see Tab.9). Finetuning Blocks 3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. SPADE: Detailed Results</head><p>In this section, we report the full results for SPADE and its relevant baselines. We evaluate our method using two established metrics. The first is per-pixel ROCAUC. The ROC curve is calculated by first computing the anomaly score of each pixel and then scanning over the range of thresholds, on pixels from all test images together. The anomalous category is designated as positive. It was noted by several previous works that ROCAUC is biased in favor of large anomalies. In order to reduce this bias, Bergmann et al <ref type="bibr">[2]</ref> propose the PRO (per-region overlap) curve metric. They first separate anomaly masks into their connected components, therefore dividing them into individual anomaly regions. By changing the detection threshold, they scan over false positive rates (FPR), for each FPR they compute PRO i.e. the proportion of the pixels of each region that are detected as anomalous. The PRO score at this FPR is the average coverage across all anomalous regions. The PRO curve metric computes the integral across FPR rates from 0 to 0.3. The PRO score is the normalized value of this integral. We can see from Tab. 12 and Tab. 11 that our method significantly outperforms the baselines in terms of both metrics. Qualitative results of our method are presented in <ref type="figure">Fig. 5</ref>.   <ref type="figure">Figure 5</ref>: An evaluation of SPADE on detecting anomalies between flowers with or without insects (taken from one category of 102 Category Flower Dataset <ref type="bibr">[9]</ref>) and bird varieties (taken from Caltech-UCSD Birds 200) <ref type="bibr">[12]</ref>. (left to right) i) An anomalous image ii) A normal train set image iii) The mask detected by SPADE iv) The predicted anomalous image pixels. SPADE was able to detect the insect on the anomalous flower (top), the white colors of the anomalous albatross (center) and the red spot on the anomalous bird (bottom).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(left) An anomalous image (right) The predicted anomalous image pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of our feature adaptation procedure, the pretrained feature extractor ψ 0 is adapted to make the normal features (blue) more compact resulting in feature extractor ψ. After adaptation, anomalous test features (red) lie in a less dense region of the feature space. of adaptation of the features in DeepSVDD does not outperform simple feature whitening (see Sec. 4.3.2). Joint optimization (JO): Perera et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>CIFAR100 Class 17 (right to left): (1) -During training all samples approach the center of train set features (2) -When normalized by the train average distance s t , the normal samples stay dense, while the anomalous ones initially move further away and then "collapse". The ROC AUC performance behaves similarly to the anomalous samples' normalized distance. (3),(4) -when training with EWC the collapse is mitigated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Representative images of the different datasets, from the left clockwise: CIFAR10, CIFAR100, Fashion MNIST, DogsVsCats, WBC, DIOR, Oxford Flowers and MVTec. Following standard protocol, in all datasets (except MVTec), normal data are one class (e.g. cat in CIFAR10) while anomalies are all other test data from the same dataset (e.g. dog, car in CIFAR10). MVTec contains class-specific anomalies (e.g. for the normal class -Wire, anomalies include bent wires)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>B. 1 .</head><label>1</label><figDesc>PANDA . . . . . . . . . . . . . . . . 11 B.2. Anomaly Detection Baselines . . . . . 12 B.3. SPADE . . . . . . . . . . . . . . . . 12 C. Datasets 12 D. Choosing the Layers to Finetune 13 E. SPADE: Detailed Results 13</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>CIFAR10</cell><cell>10</cell><cell>5,000</cell><cell>10,000</cell></row><row><cell cols="2">Fashion MNIST 10</cell><cell>6,000</cell><cell>10,000</cell></row><row><cell>CIFAR100</cell><cell>20</cell><cell>2,500</cell><cell>10,000</cell></row><row><cell>Flowers</cell><cell>102</cell><cell>10</cell><cell>7,169</cell></row><row><cell>Birds</cell><cell>200</cell><cell>30</cell><cell>5,794</cell></row><row><cell>CatsVsDogs</cell><cell>2</cell><cell cols="2">10,000 5,000</cell></row><row><cell>MVTec</cell><cell>15</cell><cell>242</cell><cell>1,725</cell></row><row><cell>WBC</cell><cell>4</cell><cell>59</cell><cell>62</cell></row><row><cell>DIOR</cell><cell>19</cell><cell>649</cell><cell>9,243</cell></row></table><note>Details of datasets used for evaluation -number of classes, and average number of normal train and (normal and anomalous) test images per-class Dataset N classes N train N test(Tab. 1 Fig</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Anomaly detection performance (Average ROC AUC %)</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Self-Supervised</cell><cell></cell><cell cols="2">Pretrained</cell><cell>OE</cell><cell></cell></row><row><cell></cell><cell cols="7">OC-SVM DeepSVDD MHRot DN2 PANDA MHRot PANDA-OE</cell></row><row><cell>CIFAR10</cell><cell>64.7</cell><cell>64.8</cell><cell>90.1</cell><cell>92.5</cell><cell>96.2</cell><cell>95.6</cell><cell>98.9</cell></row><row><cell>CIFAR100</cell><cell>62.6</cell><cell>67.0</cell><cell>80.1</cell><cell>94.1</cell><cell>94.1</cell><cell>-</cell><cell>97.3</cell></row><row><cell>FMNIST</cell><cell>92.8</cell><cell>84.8</cell><cell>93.2</cell><cell>94.5</cell><cell>95.6</cell><cell>-</cell><cell>91.8</cell></row><row><cell>CatsVsDogs</cell><cell>51.7</cell><cell>50.5</cell><cell>86.0</cell><cell>96.0</cell><cell>97.3</cell><cell>-</cell><cell>94.5</cell></row><row><cell>DIOR</cell><cell>70.7</cell><cell>70.0</cell><cell>73.3</cell><cell>93.0</cell><cell>94.3</cell><cell>-</cell><cell>95.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Pretrained feature performance on various small</cell></row><row><cell cols="3">datasets (Average ROC AUC %)</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell>Self-Supervised</cell><cell></cell><cell>Pretrained</cell></row><row><cell></cell><cell cols="4">OCSVM DeepSVDD MHRot DN2</cell></row><row><cell>Birds</cell><cell>62.0</cell><cell>60.8</cell><cell>64.4</cell><cell>95.3</cell></row><row><cell>Flowers</cell><cell>74.5</cell><cell>78.1</cell><cell>65.9</cell><cell>94.1</cell></row><row><cell>MvTec</cell><cell>70.8</cell><cell>77.9</cell><cell>65.5</cell><cell>86.5</cell></row><row><cell>WBC</cell><cell>75.4</cell><cell>71.2</cell><cell>57.7</cell><cell>87.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of anomaly segmentation methods (pixel-level ROCAUC and PRO %) AE SSIM [2] AE L2 [2] AnoGAN [24] CNN Dict [20] CAVGA-R u [27] Student [3] SPADE</figDesc><table><row><cell cols="2">ROCAUC 87</cell><cell>82</cell><cell>74</cell><cell>78</cell><cell>89</cell><cell>-</cell><cell>96.2</cell></row><row><cell>PRO</cell><cell>69.4</cell><cell>79</cell><cell>-</cell><cell>51.5</cell><cell>-</cell><cell>85.7</cell><cell>92.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of average transformation prediction accuracy (%), horiz. = horizontal, rot. = rotation.</figDesc><table><row><cell>Method</cell><cell cols="2">Normal</cell><cell cols="2">Anomalous</cell></row><row><cell></cell><cell cols="4">Horiz. Rot. Horiz. Rot.</cell></row><row><cell>Self-supervised</cell><cell>94.0</cell><cell>94.0</cell><cell>67.9</cell><cell>51.6</cell></row><row><cell>Pretrained</cell><cell>94.4</cell><cell>92.3</cell><cell>71.4</cell><cell>61.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="5">: A comparison of different feature adaptation meth-</cell></row><row><cell cols="2">ods (Avg. ROC AUC %)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Baseline</cell><cell></cell><cell>PANDA</cell><cell></cell></row><row><cell></cell><cell>JO</cell><cell>Early</cell><cell>SES</cell><cell>EWC</cell></row><row><cell>CIFAR10</cell><cell>93.2</cell><cell>96.2</cell><cell>95.9</cell><cell>96.2</cell></row><row><cell>CIFAR100</cell><cell>91.1</cell><cell>94.8</cell><cell>94.6</cell><cell>94.1</cell></row><row><cell>FMNIST</cell><cell>94.9</cell><cell>95.4</cell><cell>95.5</cell><cell>95.6</cell></row><row><cell>CatsVsDogs</cell><cell>96.1</cell><cell>91.9</cell><cell>95.7</cell><cell>97.3</cell></row><row><cell>DIOR</cell><cell>93.1</cell><cell>95.4</cell><cell>95.6</cell><cell>94.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Pretrained vs. raw initialization anomaly detection performance (ROC AUC %) .1 93.7 84.4 76.1 89.7 87.3 91.1 94.4 86.8 90.8 86.4 MHRot 77.5 96.9 87.3 80.9 92.7 90.2 90.9 96.5 95.2 93.3 90.1</figDesc><table><row><cell>CIFAR10 class</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>Avg</cell></row><row><cell cols="2">Pretrained MHRot 70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Anomaly segmentation accuracy on MVTec with different ResNet layers (PRO %) We use the coarse-grained version that consists of 20 classes. DogsVsCats: High resolution color images of two classes: cats and dogs. The data were extracted from the ASIRRA dataset[3], we split each class to the first 10,000 images as train and the last 2,500 as test.Small datasets: We report results on several small datasets from different domains: 102 Category Flowers &amp; Caltech-UCSD Birds 200[9]  </figDesc><table><row><cell cols="5">Summed layers Layers 0,1,2 Layer 0 Layer 1 Layer 2</cell></row><row><cell>Carpet</cell><cell>98.9</cell><cell>86.7</cell><cell>97.9</cell><cell>98.8</cell></row><row><cell>Grid</cell><cell>97.6</cell><cell>98.9</cell><cell>98.9</cell><cell>96.3</cell></row><row><cell>Leather</cell><cell>99.1</cell><cell>98.3</cell><cell>99.3</cell><cell>99.0</cell></row><row><cell>Tile</cell><cell>94.4</cell><cell>80.5</cell><cell>91.1</cell><cell>94.2</cell></row><row><cell>Wood</cell><cell>93.7</cell><cell>92.3</cell><cell>94.5</cell><cell>92.5</cell></row><row><cell>Bottle</cell><cell>98.1</cell><cell>89.6</cell><cell>98.0</cell><cell>97.9</cell></row><row><cell>Cable</cell><cell>96.4</cell><cell>70.8</cell><cell>93.5</cell><cell>96.9</cell></row><row><cell>Capsule</cell><cell>99.0</cell><cell>97.3</cell><cell>98.8</cell><cell>98.7</cell></row><row><cell>Hazelnut</cell><cell>98.6</cell><cell>96.6</cell><cell>97.6</cell><cell>98.6</cell></row><row><cell>Metal nut</cell><cell>97.4</cell><cell>88.4</cell><cell>97.0</cell><cell>96.7</cell></row><row><cell>Pill</cell><cell>96.4</cell><cell>95.9</cell><cell>95.8</cell><cell>95.9</cell></row><row><cell>Screw</cell><cell>99.2</cell><cell>98.8</cell><cell>99.4</cell><cell>98.6</cell></row><row><cell>Toothbrush</cell><cell>98.8</cell><cell>96.9</cell><cell>98.8</cell><cell>98.6</cell></row><row><cell>Transistor</cell><cell>94.2</cell><cell>71.0</cell><cell>84.0</cell><cell>95.9</cell></row><row><cell>Zipper</cell><cell>98.1</cell><cell>96.4</cell><cell>97.9</cell><cell>97.7</cell></row><row><cell>Average</cell><cell>97.3</cell><cell>90.6</cell><cell>96.2</cell><cell>97.1</cell></row><row><cell>[7]:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Performance of finetuning different ResNet blocks (CIFAR10 w. EWC, ROC AUC %) , results in similar performance. Finetuning only block 4 results in similar performance to linear whitening of the features according to the train samples (94.6 with whitening vs. 94.8 with finetuning only the last block).Similar effect as can be seen in the original DeepSVDD architecture (see Tab.10). We therefore recommend finetuning Blocks 3&amp;4.</figDesc><table><row><cell></cell><cell>with std</cell><cell></cell></row><row><cell cols="3">Trained Blocks 1,2,3,4 2,3,4 3,4</cell><cell>4</cell></row><row><cell>Avg</cell><cell>94.9</cell><cell cols="2">95.9 96.2 94.8</cell></row><row><cell>&amp; 4, or 2, 3 &amp; 4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Deep SVDD vs. PCA Whitening Anomaly Detection Performance (ROC AUC %) .0 63.6 49.7 59.9 59.8 65.8 68.3 68.0 75.5 71.2 64.8 Deep SVDD 59.7 64.3 48.4 61.5 61.3 65.5 70.1 68.9 75.3 72.5 64.6</figDesc><table><row><cell></cell><cell cols="2">with std</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR10 class 0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>Avg</cell></row><row><cell>PCA whitening 62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Sub-Image anomaly detection accuracy on MVTec (ROCAUC %) AE SSIM AE L2 AnoGAN CNN Dict TI VM CAVGA-R u SPADE</figDesc><table><row><cell>Carpet</cell><cell>87</cell><cell>59</cell><cell>54</cell><cell>72</cell><cell>88</cell><cell>-</cell><cell>-</cell><cell>98.6</cell></row><row><cell>Grid</cell><cell>94</cell><cell>90</cell><cell>58</cell><cell>59</cell><cell>72</cell><cell>-</cell><cell>-</cell><cell>99.0</cell></row><row><cell>Leather</cell><cell>78</cell><cell>75</cell><cell>64</cell><cell>87</cell><cell>97</cell><cell>-</cell><cell>-</cell><cell>99.5</cell></row><row><cell>Tile</cell><cell>59</cell><cell>51</cell><cell>50</cell><cell>93</cell><cell>41</cell><cell>-</cell><cell>-</cell><cell>89.8</cell></row><row><cell>Wood</cell><cell>73</cell><cell>73</cell><cell>62</cell><cell>91</cell><cell>78</cell><cell>-</cell><cell>-</cell><cell>95.8</cell></row><row><cell>Bottle</cell><cell>93</cell><cell>86</cell><cell>86</cell><cell>78</cell><cell>-</cell><cell>82</cell><cell>-</cell><cell>98.1</cell></row><row><cell>Cable</cell><cell>82</cell><cell>86</cell><cell>78</cell><cell>79</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>93.2</cell></row><row><cell>Capsule</cell><cell>94</cell><cell>88</cell><cell>84</cell><cell>84</cell><cell>-</cell><cell>76</cell><cell>-</cell><cell>98.6</cell></row><row><cell>Hazelnut</cell><cell>97</cell><cell>95</cell><cell>87</cell><cell>72</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>98.9</cell></row><row><cell>Metal nut</cell><cell>89</cell><cell>86</cell><cell>76</cell><cell>82</cell><cell>-</cell><cell>60</cell><cell>-</cell><cell>96.9</cell></row><row><cell>Pill</cell><cell>91</cell><cell>85</cell><cell>87</cell><cell>68</cell><cell>-</cell><cell>83</cell><cell>-</cell><cell>96.5</cell></row><row><cell>Screw</cell><cell>96</cell><cell>96</cell><cell>80</cell><cell>87</cell><cell>-</cell><cell>94</cell><cell>-</cell><cell>99.5</cell></row><row><cell>Toothbrush</cell><cell>92</cell><cell>93</cell><cell>90</cell><cell>77</cell><cell></cell><cell>68</cell><cell>-</cell><cell>98.9</cell></row><row><cell>Transistor</cell><cell>90</cell><cell>86</cell><cell>80</cell><cell>66</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.0</cell></row><row><cell>Zipper</cell><cell>88</cell><cell>77</cell><cell>78</cell><cell>76</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>98.8</cell></row><row><cell>Average</cell><cell>87</cell><cell>82</cell><cell>74</cell><cell>78</cell><cell>75</cell><cell>77</cell><cell>89</cell><cell>96.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Sub-Image anomaly detection accuracy on MVTec (PRO %)</figDesc><table><row><cell></cell><cell cols="3">Student 1-NN OC-SVM</cell><cell cols="5">2 -AE VAE SSIM-AE CNN-Dict SPADE</cell></row><row><cell>Carpet</cell><cell>69.5</cell><cell>51.2</cell><cell>35.5</cell><cell>45.6</cell><cell>50.1</cell><cell>64.7</cell><cell>46.9</cell><cell>96.1</cell></row><row><cell>Grid</cell><cell>81.9</cell><cell>22.8</cell><cell>12.5</cell><cell>58.2</cell><cell>22.4</cell><cell>84.9</cell><cell>18.3</cell><cell>97.0</cell></row><row><cell>Leather</cell><cell>81.9</cell><cell>44.6</cell><cell>30.6</cell><cell>81.9</cell><cell>63.5</cell><cell>56.1</cell><cell>64.1</cell><cell>98.8</cell></row><row><cell>Tile</cell><cell>91.2</cell><cell>82.2</cell><cell>72.2</cell><cell>89.7</cell><cell>87.0</cell><cell>17.5</cell><cell>79.7</cell><cell>77.1</cell></row><row><cell>Wood</cell><cell>72.5</cell><cell>50.2</cell><cell>33.6</cell><cell>72.7</cell><cell>62.8</cell><cell>60.5</cell><cell>62.1</cell><cell>93.8</cell></row><row><cell>Bottle</cell><cell>91.8</cell><cell>89.8</cell><cell>85.0</cell><cell>91.0</cell><cell>89.7</cell><cell>83.4</cell><cell>74.2</cell><cell>95.6</cell></row><row><cell>Cable</cell><cell>86.5</cell><cell>80.6</cell><cell>43.1</cell><cell>82.5</cell><cell>65.4</cell><cell>47.8</cell><cell>55.8</cell><cell>85.3</cell></row><row><cell>Capsule</cell><cell>91.6</cell><cell>63.1</cell><cell>55.4</cell><cell>86.2</cell><cell>52.6</cell><cell>86.0</cell><cell>30.6</cell><cell>95.5</cell></row><row><cell>Hazelnut</cell><cell>93.7</cell><cell>86.1</cell><cell>61.6</cell><cell>91.7</cell><cell>87.8</cell><cell>91.6</cell><cell>84.4</cell><cell>94.8</cell></row><row><cell>Metal nut</cell><cell>89.5</cell><cell>70.5</cell><cell>31.9</cell><cell>83.0</cell><cell>57.6</cell><cell>60.3</cell><cell>35.8</cell><cell>94.1</cell></row><row><cell>Pill</cell><cell>93.5</cell><cell>72.5</cell><cell>54.4</cell><cell>89.3</cell><cell>76.9</cell><cell>83.0</cell><cell>46.0</cell><cell>96.2</cell></row><row><cell>Screw</cell><cell>92.8</cell><cell>60.4</cell><cell>64.4</cell><cell>75.4</cell><cell>55.9</cell><cell>88.7</cell><cell>27.7</cell><cell>97.4</cell></row><row><cell>Toothbrush</cell><cell>86.3</cell><cell>67.5</cell><cell>53.8</cell><cell>82.2</cell><cell>69.3</cell><cell>78.4</cell><cell>15.1</cell><cell>94.4</cell></row><row><cell>Transistor</cell><cell>70.1</cell><cell>68.0</cell><cell>49.6</cell><cell>72.8</cell><cell>62.6</cell><cell>72.5</cell><cell>62.8</cell><cell>67.8</cell></row><row><cell>Zipper</cell><cell>93.3</cell><cell>51.2</cell><cell>35.5</cell><cell>83.9</cell><cell>54.9</cell><cell>66.5</cell><cell>70.3</cell><cell>96.9</cell></row><row><cell>Average</cell><cell>85.7</cell><cell>64</cell><cell>47.9</cell><cell>79</cell><cell>63.9</cell><cell>69.4</cell><cell>51.5</cell><cell>92.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partly supported by the Federmann Cyber Security Research Center in conjunction with the Israel National Cyber Directorate.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classificationbased anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive realworld dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uninformed students: Studentteacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Robust principal component analysis? JACM</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ennio</forename><surname>Pierluca D&amp;apos;oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Nasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matteucci</surname></persName>
		</author>
		<title level="m">Group anomaly detection via graph autoencoders. NIPS Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A geometric framework for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleazar</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Prerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Portnoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sal</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of data mining in computer security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="77" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ensemble gaussian mixture models for probability density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glodek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedhelm</forename><surname>Schwenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Generative adversarial networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Algorithm as 136: A k-means clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manchek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09960</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04606</idno>
		<title level="m">Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<title level="m">What makes imagenet good for transfer learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Outlier detection with kernel density functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Longin Jan Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragoljub</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pokrajac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning and Data Mining in Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="61" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by cnn-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">209</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep features for one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="5450" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Modeling the distribution of normal data in pre-trained deep features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><surname>Merhof</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14140</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Gornitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention guided anomaly localization in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rajat Vikram Singh, and Abhijit Mahalanobis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="485" to="503" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeki</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive realworld dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Uninformed students: Studentteacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Asirra: a captcha that exploits interest-aligned manual image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>John R Douceur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Computer and Communications Security</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="366" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images: A survey and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IS-PRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="296" to="307" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Caltech-ucsd birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast and robust segmentation of white blood cell images by self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Micron</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="55" to="71" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
