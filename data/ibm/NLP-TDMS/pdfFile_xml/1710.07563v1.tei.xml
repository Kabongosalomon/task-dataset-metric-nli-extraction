<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEGCloud: Semantic Segmentation of 3D Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Stanford</forename><surname>Edu Lyne</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Tchapmi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Choy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Armeni</surname></persName>
							<email>iarmeni@cs.stanford.edujgwak@stanford.edussilvio@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SEGCloud: Semantic Segmentation of 3D Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D FC-CRF Raw 3D Point Cloud Voxelized Point Cloud Voxel Predictions bed wall picture night-stand floor lamp pillow Trilinear Interpolation 3D Point Segmentation Point Cloud Unaries 3D FCNN Pre-processing</p><p>Figure 1: SEGCloud: A 3D point cloud is voxelized and fed through a 3D fully convolutional neural network to produce coarse downsampled voxel labels. A trilinear interpolation layer transfers this coarse output from voxels back to the original 3D Points representation. The obtained 3D point scores are used for inference in the 3D fully connected CRF to produce the final results. Our framework is trained end-to-end.</p><p>Abstract 3D semantic scene labeling is fundamental to agents operating in the real world. In particular, labeling raw 3D point sets from sensors provides fine-grained semantics. Recent works leverage the capabilities of Neural Networks (NNs), but are limited to coarse voxel predictions and do not explicitly enforce global consistency. We present SEG-Cloud, an end-to-end framework to obtain 3D point-level segmentation that combines the advantages of NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields (FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are transferred back to the raw 3D points via trilinear interpolation. Then the FC-CRF enforces global consistency and provides fine-grained semantics on the points. We implement the latter as a differentiable Recurrent NN to allow joint optimization. We evaluate the framework on two indoor and two outdoor 3D datasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance comparable or superior to the state-ofthe-art on all datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene understanding is a core problem in Computer Vision and is fundamental to applications such as robotics, autonomous driving, augmented reality, and the construction industry. Among various scene understanding problems, 3D semantic segmentation allows finding accurate object boundaries along with their labels in 3D space, which is useful for fine-grained tasks such as object manipulation, detailed scene modeling, etc.</p><p>Semantic segmentation of 3D point sets or point clouds has been addressed through a variety of methods leveraging the representational power of graphical models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>. A common paradigm is to combine a classifier stage and a Conditional Random Field (CRF) <ref type="bibr" target="#b38">[39]</ref> to predict spatially consistent labels for each data point <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b68">69]</ref>. Random Forests classifiers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref> have shown great performance on this task, however the Random Forests classifier and CRF stage are often optimized independently and put together as separate modules, which limits the information flow between them.</p><p>3D Fully Convolutional Neural Networks (3D-FCNN) <ref type="bibr" target="#b41">[42]</ref> are a strong candidate for the classifier stage in 3D Point Cloud Segmentation. However, since they require a regular grid as input, their predictions are limited to a coarse output at the voxel (grid unit) level. The final segmentation is coarse since all 3D points within a voxel are assigned the same semantic label, making the voxel size a factor limiting the overall accuracy. To obtain a fine-grained segmentation from 3D-FCNN, an additional processing of the coarse 3D-FCNN output is needed. We tackle this issue in our framework which is able to leverage the coarse output of a 3D-FCNN and still provide a fine-grained labeling of 3D points using trilinear interpolation (TI) and CRF.</p><p>We propose an end-to-end framework that leverages the advantages of 3D-FCNN, trilinear interpolation <ref type="bibr" target="#b46">[47]</ref>, and fully connected Conditional Random Fields(FC-CRF) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37]</ref> to obtain fine-grained 3D Segmentation. In detail, the 3D-FCNN provides class probabilities at the voxel level, which are transferred back to the raw 3D points using trilinear interpolation. We then use a Fully Connected Conditional Random Field (FC-CRF) to infer 3D point labels while ensuring spatial consistency. Transferring class probabilities to points before the CRF step, allows the CRF to use point level modalities (color, intensity, etc.) to learn a fine-grained labeling over the points, which can improve the initial coarse 3D-FCNN predictions. We use an efficient CRF implementation to perform the final inference. Given that each stage of our pipeline is differentiable, we are able to train the framework end-to-end using standard stochastic gradient descent.</p><p>The contributions of this work are:</p><p>• We propose to combine the inference capabilities of Fully Convolutional Neural Networks with the fine-grained representation of 3D Point Clouds using TI and CRF. • We train the voxel-level 3D-FCNN and point-level CRF jointly and end-to-end by connecting them via Trilinear interpolation enabling segmentation in the original 3D points space.</p><p>Our framework can handle 3D point clouds from various sources (laser scanners, RGB-D sensors, etc.), and we demonstrate state-of-the art performance on indoor and outdoor, partial and fully reconstructed 3D scenes, namely on NYU V2 <ref type="bibr" target="#b51">[52]</ref>, Stanford Large-Scale 3D Indoor Spaces Dataset (S3DIS) <ref type="bibr" target="#b4">[5]</ref>, KITTI <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>, and the Seman-tic3D.net benchmark for outdoor scenes <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we present related works with respect to three main aspects of our framework: neural networks for 3D data, graphical models for 3D Segmentation and works that explore the combination of Convolutional Neural Net-works (CNN) and CRF. Other techniques have been employed for 3D Scene Segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b39">40]</ref> but we focus mainly on the ones related to the above topics.</p><p>Neural Networks for 3D Data: 3D Neural Networks have been extensively used for 3D object and parts recognition <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b20">21]</ref>, understanding object shape priors, as well as generating and reconstructing objects <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b11">12]</ref>. Recent works have started exploring the use of Neural Networks for 3D Semantic Segmentation <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>. Qi et al. <ref type="bibr" target="#b52">[53]</ref> propose a Multilayer Perceptron (MLP) architecture that extracts a global feature vector from a 3D point cloud of 1m 3 physical size and processes each point using the extracted feature vector and additional point level transformations. Their method operates at the point level and thus inherently provides a fine-grained segmentation. It works well for indoor semantic scene understanding, although there is no evidence that it scales to larger input dimensions without additional training or adaptation required. Huang et al. <ref type="bibr" target="#b31">[32]</ref> present a 3D-FCNN for 3D semantic segmentation which produces coarse voxellevel segmentation. <ref type="bibr">Dai et al. [16]</ref> also propose a fully convolutional architecture, but they make a single prediction for all voxels in the same voxel grid column. This makes the wrong assumption that a voxel grid column contains 3D points with the same object label. All the aforementioned methods are limited by the fact that they do not explicitly enforce spatial consistency between neighboring points predictions and/or provide a coarse labeling of the 3D data. In contrast, our method makes fine-grained predictions for each point in the 3D input, explicitly enforces spatial consistency and models class interactions through a CRF. Also, in contrast to <ref type="bibr" target="#b52">[53]</ref>, we readily scale to larger and arbitrarily sized inputs, since our classifier stage is fully convolutional.</p><p>Graphical Models for 3D Segmentation: Our framework builds on top of a long line of works combining graphical models( <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38]</ref>) and highly engineered classifiers. Early works on 3D Semantic Segmentation formulate the problem as a graphical model built on top of a set of features. Such models have been used in several works to capture contextual relationships based on various features and cues such as appearance, shape, and geometry. These models are shown to work well for this task <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>A common paradigm in 3D semantic segmentation combines a classifier stage and a Conditional Random Field to impose smoothness and consistency <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b68">69]</ref>. Random Forests <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref> are a popular choice of classifier in this paradigm and in 3D Segmentation in general <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b66">67]</ref>; they use hand-crafted features to robustly provide class scores for voxels, oversegments or 3D Points. In <ref type="bibr" target="#b44">[45]</ref>, the spin image descriptor is used as a feature, while <ref type="bibr" target="#b67">[68]</ref> uses a 14-dimensional feature vector based on geometry and appearance. Hackel et al. <ref type="bibr" target="#b26">[27]</ref> also  define a custom set of features aimed at capturing geometry, appearance and location. In these works, the Random Forests output is used as unary potentials (class scores) for a CRF whose parameters are learned independently. The CRF then leverages the confidence provided by the classifier, as well as similarity between an additional set of features, to perform the final inference. In contrast to these methods, our framework uses a 3D-FCNN which can learn higher dimensional features and provide strong unaries for each data point. Moreover, our CRF is implemented as a fully differentiable Recurrent Neural Network, similar to <ref type="bibr" target="#b76">[76]</ref>. This allows the 3D-FCNN and CRF to be trained endto-end, and enables information flow from the CRF to the CNN classification stage. Joint CNN + CRF: Combining 3D CNN and 3D CRF has been previously proposed for the task of lesion segmentation in 3D medical scans. Kamnitsas et al. <ref type="bibr" target="#b33">[34]</ref> propose a multi-scale 3D CNN with a CRF to classify 4 types of lesions from healthy brain tissues. The method consists of two modules that are not trained end-to-end: a 2-stream architecture operating at 2 different scan resolutions and a CRF. In the CRF training stage, the authors reduce the problem to a 2-class segmentation task in order to find parameters for the CRF that can improve segmentation accuracy.</p><p>Joint end-to-end training of CNN and CRF was first demonstrated by <ref type="bibr" target="#b76">[76]</ref> in the context of image semantic segmentation, where the CRF is implemented as a differentiable Recurrent Neural Network (RNN). The combination of CNN and CRF trained in an end-to-end fashion demonstrated state-of-the-art accuracy for semantic segmentation in images. In <ref type="bibr" target="#b76">[76]</ref> and other related works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b9">10]</ref>, the CNN has a final upsampling layer with learned weights which allows to obtain pixel level unaries before the CRF stage. Our work follows a similar thrust by defining the CRF as an RNN and using a trilinear interpolation layer to transfer the coarse output of the 3D-FCNN to individual 3D points before the CRF stage. In contrast to <ref type="bibr" target="#b33">[34]</ref>, our framework is a single stream architecture which jointly optimizes the 3D CNN and CRF, targets the domain of 3D Scene Point Clouds, and is able to handle a large number of classes both at the CNN and CRF stage. Unlike <ref type="bibr" target="#b76">[76,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b9">10]</ref>, we choose to use deterministic interpolation weights that take into account the metric distance between a 3D point and its neighboring voxel centers (Section 3.2). Our approach reduces the number of parameters to be learned, and we find it to work well in practice. We show that the combination of jointly trained 3D-FCNN and CRF with TI consistently performs better than a stand alone 3D-FCNN.</p><p>In summary, our work differs from previous works in the design of an end-to-end deep learning framework for fine-grained 3D semantic segmentation, the use of deterministic trilinear interpolation to obtain point-level segmentation, and the use of a jointly trained CRF to enforce spatial consistency. The rest of the paper is organized as follows. Sections 3 and 4 present the components of our end-to-end framework and Section 5 provides implementation details. Section 6 presents our experiments including datasets (6.1), benchmark results (6.2), and system analysis (6.3). Section 7 concludes with a summary of the presented results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SEGCloud Framework</head><p>An overview of the SEGCloud pipeline is shown in <ref type="figure">Figure</ref> 1. In the first stage of our pipeline, the 3D data is voxelized and the resulting 3D grid is processed by a 3D fully convolutional neural network (3D-FCNN) <ref type="bibr" target="#b0">1</ref> . The 3D-FCNN down-samples the input volume and produces probability distributions over the set of classes for each downsampled voxel (Section 3.1). The next stage is a trilinear interpolation layer which interpolates class scores from downsampled voxels to 3D points (Section 3.2). Finally, inference is performed using a CRF which combines the original 3D points features with interpolated scores to produce finegrained class distributions over the point set (Section 3.3). Our entire pipeline is jointly optimized and the CRF inference and joint optimization processes are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Fully Convolutional Neural Network</head><p>Our framework uses a 3D-FCNN to learn a representation suitable for semantic segmentation. Moreover, the fully convolutional network reduces the computational overhead needed to generate predictions for each voxel by sharing computations <ref type="bibr" target="#b42">[43]</ref>. In the next section, we describe how we represent 3D point clouds as an input to the 3D-FCNN. 3D-FCNN data representation: Given that the 3D-FCNN input should be in the form of a voxel grid, we convert 3D point clouds as follows. Each data point is a 3D observation o i , that consists of the 3D position p i and other available modalities, such as the color intensity I i and sensor intensity S i . We place the 3D observations O = {o i } in a metric space so that the convolution kernels can learn the scale of objects. This process is usually handled in most 3D sensors. Then we define a regular 3D grid that encompasses the 3D observations. We denote each cell in the 3D grid as a voxel v i and for simplicity, each cell is a cube with length V = 5cm. Most of the space in the 3D input is empty and has no associated features. To characterize this, we use a channel to denote the occupancy as a binary value (zero or one). We use additional channels to represent other modalities. For instance, three channels are used for RGB color, and one channel is used for sensor intensity when available. Architecture: Our 3D-FCNN architecture is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. We use 3 residual modules <ref type="bibr" target="#b27">[28]</ref> sandwiched between 2 convolutional layers, as well as 2 destructive pooling layers in the early stages of the architecture to downsample the grid, and 2 non-destructive ones towards the end. The early down-sampling gives us less memory footprint. The entire framework is fully convolutional and can handle arbitrarily sized inputs. For each voxel v i , the 3D-FCNN outputs scores(logits) L i associated with a probability distribution q i over labels. The resulting scores are transferred to the raw 3D points via trilinear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Trilinear Interpolation</head><p>The process of voxelization and subsequent downsampling in the 3D-FCNN converts our data representation to a coarse 3D grid which limits the resolution of semantic labeling at the CRF stage (to 20 cm in our case). Running the CRF on such coarse voxels results in a coarse segmentation. One option to avoid this information loss is to increase the resolution of the voxel grid (i.e. decrease the voxel size) and/or remove the destructive pooling layers, and run the CRF directly on the fine-grained voxels. However, this quickly runs into computational and memory constraints, since for given 3D data dimensions, the memory requirement of the 3D-FCNN grows cubically with the resolution of the grid. Also, for a given 3D-FCNN architecture, the receptive field decreases as the resolution of the grid increases, which can reduce performance due to having less context available during inference(see <ref type="bibr" target="#b62">[63]</ref>). We therefore dismiss a voxel-based CRF approach and resort to running CRF inference using the raw 3D points as nodes. In this way, the CRF can leverage both the 3D-FCNN output and the fine-grained modalities of the input 3D points to generate accurate predictions that capture scene and object boundaries in detail. We achieve this using trilinear interpolation to transfer the voxel-level predictions from the 3D-FCNN to the raw 3D points as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. Specifically, for each point, o i = {p i , I i , S i }, we define a random variable x i that denotes the semantic class, and the scores(logits) L i associated with the distribution of x i are defined as a weighted sum of scores L i,n (x i,n ) of its 8 spatially closest voxels v i,n , n ∈ {1, ..., 8} whose centers are (p x i,n , p y i,n , p z i,n ) as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Voxel Centers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Cloud &amp;Voxelization Grid</head><formula xml:id="formula_0">V V V Ψ u (x i ) 3D points Ψ u /</formula><formula xml:id="formula_1">ψ u (x i = l) = L i (x i = l) = 8 n=1 w i,n L i,n (x i,n = l) (1) w i,n = s∈{x,y,z} 1 − |p s i − p s i,n |/V</formula><p>where V is the voxel size. During back propagation, we use the same trilinear interpolation weights w i,n to splat the gradients from the CRF to the 3D-FCNN. The obtained point level scores are then used as unaries in the CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Fully Connected Conditional Random Field</head><p>The energy function of a CRF consists of a set of unary and pairwise potential energy terms. The unary potentials are a proxy for the initial probability distribution across semantic classes and the pairwise potentials enforce smoothness and consistency between predictions. The energy of the CRF is defined as,</p><formula xml:id="formula_2">E(x) = i ψ u (x i ) + i&lt;j ψ p (x i , x j )<label>(2)</label></formula><p>where ψ u denotes the unary potential which is defined in Equation 3.2 and ψ p denotes the pairwise potential. Note that all nodes in the CRF are connected with each other through the pairwise potentials. We use the Gaussian kernels from <ref type="bibr" target="#b36">[37]</ref> for the pairwise potentials,</p><formula xml:id="formula_3">ψ p (x i , x j ) = µ(x i , x j ) w s exp − |p i − p j | 2 2θ 2 γ +w b exp − |p i − p j | 2 2θ 2 α − |I i − I j | 2 2θ 2 β<label>(3)</label></formula><p>where w b and w s are the weights of the bilateral and spatial kernel respectively, µ is the label compatibility score, and θ α , θ β , θ γ are the kernels' bandwidth parameters. When RGB information is not available, we only use the spatial kernel. Using Gaussian kernels enables fast variational inference and learning through a series of convolutions on a permutohedral lattice <ref type="bibr" target="#b0">[1]</ref> (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CRF Inference and Joint Optimization</head><p>Exact energy minimization in CRF is intractable, therefore we rely on a variational inference method which allows us to jointly optimize both the CRF and 3D-FCNN <ref type="bibr" target="#b76">[76,</ref><ref type="bibr" target="#b36">37]</ref>. The output after the CRF energy minimization gives us finegrained predictions for each 3D point that takes smoothness and consistency into account. Given the final output of the CRF, we follow the convention and use the distance between the prediction and ground truth semantic labels as a loss function and minimize it. CRF Inference: The CRF with Gaussian potential has a special structure that allows fast and efficient inference. Krähenbühl et al. <ref type="bibr" target="#b36">[37]</ref> presented an approximate inference method which assumes independence between semantic label distributions Q(X) = i Q i (x i ), and derived the update equation:</p><formula xml:id="formula_4">Q + i (x i = l) = 1 Z i exp − ψ u (x i ) − l ∈L µ(l, l ) K m=1 w (m) j =i k (m) (f i , f j )Q j (l )<label>(4)</label></formula><p>The above update equation can be implemented using simple convolutions, sums and softmax as shown by Zheng et al. <ref type="bibr" target="#b76">[76]</ref>, who implemented CRF inference and learning as a Recurrent Neural Network (RNN), named CRF-RNN. CRF-RNN can be trained within a standard CNN framework, so we follow the same procedure to define our 3D CRF as an RNN for inference and learning. This formulation allows us to integrate the CRF within our 3D-FCNN framework for joint training. Loss: Once we minimize the energy of the CRF in Equation 2, we obtain the final prediction distribution of the semantic class x i on each 3D observation o i . Denoting the ground truth discrete label of the observation o i as y i , we follow the convention and define our loss function as the distance between a final prediction distribution and the ground truth distribution using KL divergence:</p><formula xml:id="formula_5">L(x, y) = 1 N N i=1 E yi [− log p(x i )]<label>(5)</label></formula><p>where N is the number of observations. Since the entropy of y i is a constant with respect to all parameters, we do not include it in the loss function equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>We implemented the SEGCloud framework using the Caffe neural network library <ref type="bibr" target="#b32">[33]</ref> 2 . Within theCaf fe framework, we adapted the bilinear interpolation of <ref type="bibr" target="#b10">[11]</ref> and implemented trilinear interpolation as a neural network layer. All computations within the 3D-FCNN, trilinear interpolation layer, and CRF are done on a Graphical Processing Unit (GPU). For CRF inference, we adapt the RNN implementation of Zheng et al. <ref type="bibr" target="#b76">[76]</ref> to 3D point clouds.</p><p>To address the lack of data in some datasets and make the network robust, we applied various data augmentation techniques such as random color augmentation, rotation along the upright direction, and points sub-sampling. The above random transformations and sub-sampling allow to increase the effective size of each dataset by at least an order of magnitude, and can help the network build invariance to rotation/viewpoint changes, as well as reduced and varying context (see <ref type="bibr" target="#b62">[63]</ref>).</p><p>Training is performed in a 2-step process, similar to <ref type="bibr" target="#b76">[76]</ref> (see <ref type="figure" target="#fig_6">Figure 7</ref>). In the first stage, we train the 3D-FCNN in isolation via trilinear interpolation for 200 epochs.</p><p>In the second stage, we jointly train the 3D-FCNN and the CRF end-to-end (both modules connected through the trilinear interpolation layer). The approximate variational inference method we used for the CRF <ref type="bibr" target="#b36">[37]</ref> approximates convolution in a permutohedral grid whose size depends on the bandwidth parameters θ α , θ β , θ γ . We fixed θ γ at 5cm, θ β at 11 and used a grid search with small perturbation on a validation set to find the optimal θ α (see <ref type="bibr" target="#b62">[63]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we evaluate our framework on various 3D datasets and analyze the performance of key components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>Several 3D Scene datasets have been made available to the research community <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b73">74]</ref>. We chose four of them so that they cover indoor and   outdoor, partial and fully reconstructed, as well as small, medium and large scale point clouds. For our evaluation, we favor those for which previous 3D Semantic Segmentation works exist, with replicable experimental setups for comparison. We choose baselines so that they are representative of the main research thrusts and topics related to our method (i.e., Neural Networks, Random Forests, and CRFs). The datasets we chose for evaluation are the Se-mantic3D.net Benchmark <ref type="bibr" target="#b25">[26]</ref>, the Stanford Large-Scale 3D Indoor Spaces Dataset (S3DIS) <ref type="bibr" target="#b4">[5]</ref>, KITTI <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>, and NYU V2 <ref type="bibr" target="#b51">[52]</ref>. The datasets showcase a wide range of sizes from the smallest KITTI dataset with 12 million training points, to the largest Semantic3D.net with 1.9 billion training points 4 (details in <ref type="bibr" target="#b62">[63]</ref>). We evaluate our method on each dataset and provide a comparison against the state-ofthe-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results</head><p>We present quantitative and qualitative results for each of the datasets introduced above. We compare against the state-of-the-art, and perform an ablation study to showcase the benefits of the CRF. The metrics reported are mean <ref type="bibr" target="#b3">4</ref> This excludes the validation set in our data split IOU and mean Accuracy across classes unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic3D.net benchmark:</head><p>We evaluate our architecture on the recent Semantic3D.net benchmark <ref type="bibr" target="#b25">[26]</ref>, which is currently the largest labeled 3D point cloud dataset for outdoor scenes. It contains over 3 billion points and covers a range of urban scenes. We provide results on the reduced-8 challenge of the benchmark in <ref type="table" target="#tab_1">Table 1</ref>. Our method outperforms [6] by 2.2 mIOU points and 2.28% accuracy and sets a new state-of-the-art on that challenge. When compared against the best method that does not leverage extra data through ImageNet <ref type="bibr" target="#b56">[57]</ref> pretrained networks, our method outperforms <ref type="bibr" target="#b26">[27]</ref> by 7.1 mIOU points, 4.1% accuracy. Note that we also do not leverage extra data or ImageNet <ref type="bibr" target="#b56">[57]</ref> pretrained networks. Our base 3D-FCNN trained with Trilinear Interpolation (3D-FCNN-TI) already achieves state-of-the-art performance, and an additional improvement of 3.1 mIOU points and 3.22% can be attributed to the CRF. An example segmentation of our method is shown in <ref type="figure" target="#fig_3">Figure 5</ref>. The 3D-FCNN-TI produces a segmentation which contains some noise on the cars highlighted in the figure. However, the combination with the CRF in the SEGCloud is able to remove the noise and provide a cleaner segmentation of the point cloud.</p><p>Stanford Large-Scale 3D Indoor Spaces Dataset (S3DIS): The S3DIS dataset <ref type="bibr" target="#b4">[5]</ref> provides 3D point clouds for six fully reconstructed large-scale areas, originating from three different buildings. We train our architecture on two of the buildings and test on the third. We compare our method against the MLP architecture of Qi et al., (PointNet) <ref type="bibr" target="#b52">[53]</ref>. Qi et al. <ref type="bibr" target="#b52">[53]</ref> perform a six-fold cross validation across areas rather than buildings. However, with this experimental setup, areas from the same building end up in both the training and test set resulting in increased performance and do not measure generalizability. For a more principled evaluation, we choose our test set to match their fifth fold (ie. we test on Area 5 and train on the rest). We obtain the results from the authors for   <ref type="figure" target="#fig_3">Figure 5)</ref> show an example of how detailed boundaries are captured and refined by our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYU V2:</head><p>The NYU V2 dataset <ref type="bibr" target="#b51">[52]</ref> contains 1149 labeled RGB-D images. Camera parameters are available and are used to obtain a 3D point cloud for each RGB-D frame. In robotics and navigation applications, agents do not have access to fully reconstructed scenes and labeling single frame 3D point clouds becomes invaluable. We compare against 2D and 3D-based methods except for those that leverage additional large scale image datasets (e.g. <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b17">[18]</ref>), or do not use the official split or the 13-class labeling defined in <ref type="bibr" target="#b13">[14]</ref> (e.g. <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b67">[68]</ref>). We obtain a confusion matrix for the highest performing method of <ref type="bibr" target="#b68">[69]</ref> to compute mean IOU in addition to the mean accuracy numbers they report. Wolf et al. <ref type="bibr" target="#b68">[69]</ref> evaluate their method by aggregating results of 10 random forests. Similarly, we use 10 different random initializations of network weights, and use a validation set to select our final trained model for evaluation. Results are shown in <ref type="table" target="#tab_3">Table 3</ref>. We outperform the 3D Entangled Forests method of [69] by 3.94 mIOU points and 0.83% mean accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI:</head><p>The KITTI dataset <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref> provides 6 hours of traffic recording using various sensors including a 3D laser scanner. Zhang et al. <ref type="bibr" target="#b74">[75]</ref> annotated a subset of the KITTI tracking dataset with 3D point cloud and corresponding 2D image annotations for use in sensor fusion for 2D semantic segmentation. As part of their sensor fusion process, they train a unimodal 3D point cloud classifier using Random Forests. We use this classifier as a baseline for evaluating our framework's performance. The comparison on the labeled KITTI subset is reported in <ref type="table" target="#tab_4">Table 4</ref>. We demonstrate performance on par with <ref type="bibr" target="#b74">[75]</ref> where a Random Forests classifier is used for segmentation. Note that for this dataset, we train on the laser point cloud with no RGB information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of results:</head><p>In all datasets presented, our performance is on par with or better than previous methods. As expected, we also observe that the addition of a CRF improves the 3D-FCNN-TI output and the qualitative results showcase its ability to recover clear object boundaries by smoothing out incorrect regions in the bilateral space (e.g. cars in Semantic3D.net or chairs in S3DIS). Quantitatively, it offers a relative improvement of 3.0-5.3% mIOU and 4.4-4.7% mAcc for all datasets. Specifically, we see the largest relative improvement on Semantic3D.net -5.3% mIOU. Since Semantic3D.net is by far the largest dataset (at least 8X times larger), we believe that such characteristic might be representative for large scale datasets as the base networks are less prone to overfitting. We notice however that several classes in the S3DIS dataset, such as board, column and beam are often incorrectly classified as walls. These elements are often found in close proximity to walls and have similar colors, which can present a challenge to both the 3D-FCNN-TI and the CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">System Analysis</head><p>We analyze two additional components of our framework: geometric data augmentation and trilinear interpolation. The experiments presented in this section are performed on the S3DIS dataset. We also analyzed the effect of joint training versus separate CRF initialization (details and results in supplementary material <ref type="bibr" target="#b62">[63]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3DIS</head><p>[7]    Effect of Geometric Data Augmentation: Our framework uses several types of data augmentation. Our geometric data augmentation methods in particular (random 360 • rotation along the z-axis and scaling) are non-standard. Qi et al. <ref type="bibr" target="#b52">[53]</ref> use different augmentation, including random rotation along the z-axis, and jittering of x, y, z coordinates to augment object 3D point clouds, but it is not specified whether the same augmentation is used on 3D scenes. We want to determine the role of our proposed geometric augmentation methods on the performance of our base 3D-FCNN-TI architecture. We therefore train the 3D-FCNN-TI without any geometric augmentation and report the performance in <ref type="table" target="#tab_6">Table 5</ref>. We observe that the geometric augmentation does play a significant role in the final performance and is responsible for an improvement of 3.79 mIOU points. However, even without any geometric augmentation, our base 3D-FCNN-TI outperforms the MLP architecture of [53] by 2.58 mIOU points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trilinear interpolation analysis:</head><p>We now present a study on the effect of trilinear interpolation on our framework. For simplicity, we perform this analysis on the combination of 3D-FCNN and interpolation layer only (no CRF module). We want to study the advantage of our proposed 8neighbours trilinear interpolation scheme (Section 3.2) over simply assigning labels of points according to the voxel they belong to (see <ref type="figure" target="#fig_5">Figure 6</ref> for a schematic explanation of the two methods). The results of the two interpolation schemes are shown in <ref type="table" target="#tab_7">Table 6</ref>. We observe that trilinear interpolation helps improve the 3D-FCNN performance by 2.62 mIOU points over simply transferring the voxel label to the points within the voxel. This shows that considering the metric distance between points and voxels, as well a larger neighborhood of voxels can help improve accuracy in predictions.  Trilinear interpolation (a) versus the conventional approach of the nearest voxel center (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We presented an end-to-end 3D Semantic Segmentation framework that combines 3D-FCNN, trilinear interpolation and CRF to provide class labels for 3D point clouds. Our approach achieves performance on par or better than state-of-the-art methods based on neural networks, randoms forests and graphical models. We show that several of its components such as geometric 3D data augmentation and trilinear interpolation play a key role in the final performance. Although we demonstrate a clear advantage over some Random Forests methods and a point-based MLP method, our implementation uses a standard voxel-based 3D-FCNN and could still adapt to the sparsity of the voxel grid using sparse convolutions (e.g. <ref type="bibr" target="#b54">[55]</ref>) which could add an extra boost in performance, and set a new state-of-the-art in 3D Semantic Segmentation. channels of each observation within the range ±2.5 for each channel.</p><p>Geometric augmentation: We also leverage 2 simple geometric augmentations: random rotation and scaling. We randomly rotate 3D observations around the axis along the gravity direction to mimic a change of viewpoints in a scene. During training, we sample rotation angles in the continuous range of [0 • , 360 • ] and rotate the point cloud on-the-fly. We also scale the data by a small factor that is uniformly sampled in the range [0.9, 1.1] to make the network invariant to small changes in scale.</p><p>Points Subsampling: We also use a random subsampling of points in highly dense datasets, specifically, the Stanford Large-Scale 3D Indoor Spaces Dataset (S3DIS) <ref type="bibr" target="#b4">[5]</ref> and the Semantic3D.net <ref type="bibr" target="#b25">[26]</ref>. During training, we sample points in a scene by a factor empirically chosen based on the number of points in the given point cloud crop (see <ref type="table" target="#tab_8">Table 7</ref>). For point clouds having more than 1e 5 points, the sub-sampling factor for S3DIS is kept at 10 since the density of the point cloud is relatively constant in this dataset. The Semantic3D.net dataset on the other hand has varying density and we use three values of the sub-sampling factor (10, 50 and 100), as shown in <ref type="table" target="#tab_8">Table 7</ref>. This sub-sampling process aims at building invariance to missing points, and increasing the speed of the training process. At test time, the algorithm is evaluated on all input points without sub-sampling.</p><p>The above random transformations and sub-sampling allow us to increase the effective size of each dataset and can help the network build invariance to rotation/viewpoint changes, as well as reduced and varying context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Input Preparation</head><p>The large scale 3D observations are split into areas of at most 5m in the X, Y and Z dimensions, where Z is the gravity axis. One notable exception is the S3DIS dataset, which provides fully reconstructed 3D point clouds of indoor buildings spaces. For this dataset, we limit the X and Y dimensions to 5m like rest of the datasets, but keep the entire Z extent, which allows to include both the ceiling and floor in every crop. During training, such 5m cropped subarea overlap with adjacent sub-areas by 0.5m. There is no overlap at test time in order to obtain a single prediction per point. Sub-areas are then voxelized with a 5cm resolution to obtain a maximum input volume of 100 × 100 × 100. This granularity provides a balance between memory requirements and an adequate representation of the 3D space without information loss. Each voxel has one to five associated channels that correspond to its binary occupancy (1-occupied, 0-empty), RGB value normalized within the range [0, 1], and sensor intensity when available (Seman-tic3D.net dataset). The sensor intensity is mean centered and normalized using the mean and range of the training data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Training</head><p>Training is performed in a 2-step process similar to <ref type="bibr" target="#b76">[76]</ref>. This process is illustrated in <ref type="figure" target="#fig_6">Figure 7</ref>. In the first training stage, we use the Trilinear Interpolation layer to map the voxel-wise predictions to point-wise predictions and minimize the point-wise loss. We train 3D-FCNN with Trilinear Interpolation layer for 200 epochs with a learning rate between 1e − 5 and 1e − 3, and reduce it by a factor of 10 every  50 epochs. In the second training stage, we combine the pre-trained 3D-FCNN, the Trilinear Interpolation layer and the CRF, and train the whole system end-to-end. The base learning rate in this stage is set to a value between 1e − 7 and 1e − 5, and the training is performed for 2 epochs. We use a learning rate multiplier of 1e 4 and 1e 3 for the CRF s bilateral weights and compatibility matrix, however we did not extensively study the effect of these parameters. In most cases, the training of the second stage converges within a few hundred iterations (Convergence is determined using a validation set). In the CRF formulation, although the kernel weights w s , w b and the compatibility matrix µ are learned using gradient descent, the kernel bandwidth parameters θ α , θ β , θ γ are not learned within our efficient variational inference framework. Thus, we used grid search or fixed values for some parameters following <ref type="bibr" target="#b36">[37]</ref>. We fix θ γ at 5cm, θ β at 11, and use a validation set to search for an optimal value of θ α . We limit our search to the range [0.1, 3.2]m. When no RGB information is available, we instead searched for θ γ in the same range and did not use the bilateral filter. The kernel weights and compatibility matrix are learned during training. Similar to <ref type="bibr" target="#b76">[76]</ref> we use 5 CRF iterations during training and 10 CRF iterations at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of end-to-end training vs separate CRF initialization</head><p>We performed an experiment to evaluate the effect of end-to-end training versus separately initializing the CRF module. For the separate initialization, we set the theta parameters to the optimal joint training values we found during end-to-end training, the spatial weight to 3, and the bilateral to 5 for all experiments. Results show that joint training performs better than separate CRF initialization especially in mAcc metric (see <ref type="table" target="#tab_11">Table 9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental and Evaluation Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Datasets</head><p>We now present the characteristics of the datasets we use to evaluate our framework. The datasets we chose for evaluation are Semantic3D.net <ref type="bibr" target="#b25">[26]</ref>, the Stanford Large-Scale 3D Indoor Spaces Dataset (S3DIS) <ref type="bibr" target="#b4">[5]</ref>, KITTI <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>, and NYU V2 <ref type="bibr" target="#b51">[52]</ref>. As shown in <ref type="table" target="#tab_10">Table 8</ref>, our framework is general in that it can handle point clouds from various sources, both indoor and outdoor environments, as well as partial and fully reconstructed point clouds. Specifically, two of the datasets are collected from indoor environments and two from outdoor environments. They also cover a variety of data acquisition methods, including laser scanners (Semantic3D.net, KITTI), Kinect (NYU V2), and Matter-Port (S3DIS). Moreover, the S3DIS is a fully reconstructed point cloud dataset, while NYU V2 provides point clouds extracted from a single frame RGB-D camera. The size of the training sets also vary from 12 million training points for the KITTI dataset to 1.9 billion training points for Se-mantic3D.net (excluding the validation set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Evaluation Metrics</head><p>We use two main metrics for our evaluation: mean class accuracy (mAcc) and mean class IOU (mIOU), where IOU is defined similarly to the Pascal segmentation convention. Accuracy per class is defined as:</p><formula xml:id="formula_6">acc i = tp i gt i = tp i tp i + f n i ,<label>(6)</label></formula><p>where tp i is the number of true positives of class i, f n i is the number of false negatives of class i and gt i is the total number of ground-truth elements of class i. The mean class accuracy is then defined as:</p><formula xml:id="formula_7">mAcc = 1 N N i=1 acc i ,<label>(7)</label></formula><p>where N is the number of classes. We define per class IOU following the Pascal convention as:</p><formula xml:id="formula_8">IOU i = tp i gt i + f p i = tp i tp i + f n i + f p i ,<label>(8)</label></formula><p>where tp i , gt i , f n i are defined as above, and f p i is the number of false positives of class i. Note that IOU is a more difficult metric than accuracy since it doesn't simply reward true positives, but also penalizes false positives. From the definition above, we obtain mean class IOU as:  </p><formula xml:id="formula_9">mIOU = 1 N N i=1 IOU i .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Visualizations</head><p>In this section, we include more qualitative segmentation results for all datasets. The results showcase the initial segmentation of the standalone 3D-FCNN-TI followed by the final result of the SEGCloud framework.   <ref type="figure" target="#fig_2">Figure 14</ref>: Qualitative results on the NYU V2 dataset</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Network architecture: The 3D-FCNN is made of 3 residual layers sandwiched between 2 convolutional layers. Max Pooling in the early stages of the network yields a 4X downsampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Trilinear interpolation of class scores from voxels to points: Each point's score is computed as the weighted sum of the scores from its 8 spatially closest voxel centers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>We follow a 2-stage training by first optimizing over the point-level unary potentials (no CRF) and then over the joint framework for point-level fine-grained labeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of our framework on Semantic3D.net and S3DIS. Additional results provided in suppl.<ref type="bibr" target="#b62">[63]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Assigning voxel labels to 3D points (top view):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>We follow a 2-stage training by first optimizing over the point-level unary potentials (no CRF) and then over the joint framework for point-level fine-grained labeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :Figure 10 :</head><label>8910</label><figDesc>Qualitative results on the Semantic3D.net dataset Qualitative results on the KITTI dataset Qualitative results on the KITTI dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :Figure 12 :Figure 13 :</head><label>111213</label><figDesc>Qualitative results on the S3DIS dataset Qualitative results on the S3DIS dataset Qualitative results on the NYU V2 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on the Semantic3D.net Benchmark (reduced-8 challenge)</figDesc><table><row><cell>Method</cell><cell cols="4">man-made natural terrain terrain vegetation vegetation high low</cell><cell>buildings</cell><cell>hard scape</cell><cell>scanning artefacts</cell><cell>cars</cell><cell>mIOU mAcc 3</cell></row><row><cell>TMLC-MSR [27]</cell><cell>89.80</cell><cell>74.50</cell><cell>53.70</cell><cell>26.80</cell><cell>88.80</cell><cell>18.90</cell><cell>36.40</cell><cell cols="2">44.70 54.20 68.95</cell></row><row><cell>DeePr3SS [41]</cell><cell>85.60</cell><cell>83.20</cell><cell>74.20</cell><cell>32.40</cell><cell>89.70</cell><cell>18.50</cell><cell>25.10</cell><cell cols="2">59.20 58.50 88.90</cell></row><row><cell>SnapNet [6]</cell><cell>82.00</cell><cell>77.30</cell><cell>79.70</cell><cell>22.90</cell><cell>91.10</cell><cell>18.40</cell><cell>37.30</cell><cell cols="2">64.40 59.10 70.80</cell></row><row><cell>3D-FCNN-TI(Ours)</cell><cell>84.00</cell><cell>71.10</cell><cell>77.00</cell><cell>31.80</cell><cell>89.90</cell><cell>27.70</cell><cell>25.20</cell><cell cols="2">59.00 58.20 69.86</cell></row><row><cell>SEGCloud (Ours)</cell><cell>83.90</cell><cell>66.00</cell><cell>86.00</cell><cell>40.50</cell><cell>91.10</cell><cell>30.90</cell><cell>27.50</cell><cell cols="2">64.30 61.30 73.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the Large-Scale 3D Indoor Spaces Dataset (S3DIS) Method ceiling floor wall beam column window door chair table bookcase sofa board clutter mIOU mAcc PointNet [53] 88.80 97.33 69.80 0.05 3.92 46.26 10.76 52.61 58.93 40.28 5.85 26.38 33.22 41.09 48.98 3D-FCNN-TI(Ours) 90.17 96.48 70.16 0.00 11.40 33.36 21.12 76.12 70.07 57.89 37.46 11.16 41.61 47.46 54.91 SEGCloud (Ours) 90.06 96.05 69.86 0.00 18.37 38.35 23.12 75.89 70.40 58.42 40.88 12.96 41.60 48.92 57.35</figDesc><table><row><cell>Stage 1</cell><cell>3D FCNN</cell><cell>Trilinear</cell><cell>Interpolation</cell><cell></cell><cell>Softmax</cell></row><row><cell>Stage 2</cell><cell>3D FCNN</cell><cell>Trilinear</cell><cell>Interpolation</cell><cell>3D FC-CRF</cell><cell>Softmax</cell></row><row><cell>Input</cell><cell></cell><cell cols="2">SEGCloud</cell><cell></cell><cell>Output</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on the NYUV2 dataset Method Bed Objects Chair Furniture Ceiling Floor Deco. Sofa Table Wall Window Booksh. TV mIOU mAcc glob Acc Couprie et al. [14] 38.1 8.7 34.1 42.4 62.6 87.3 40.4 24.6 10.2 86.1 15.Wang et al. [65] 47.6 12.4 23.5 16.7 68.1 84.1 26.4 39.1 35.4 65.9 52.2 45.0 32.4 -Hermans et al. [29] 68.4 8.6 41.9 37.1 83.4 91.5 35.8 28.5 27.7 71.8 46.1 45.4 38.4 -Wolf et al. [69] 74.56 17.62 62.16 47.85 82.42 98.72 26.36 69.38 48.57 83.65 25.56 54.92 31.05 39.51 55.6±0.2 64.9±0.3 3D-FCNN-TI(Ours) 69.3 40.26 64.34 64.41 73.05 95.55 21.15 55.51 45.09 84.96 20.76 42.24 23.95 42.13 53.9 67.38 SEGCloud (Ours) 75.06 39.28 62.92 61.8 69.16 95.21 34.38 62.78 45.78 78.89 26.35 53.46 28.5 43.45 56.43 66.82</figDesc><table><row><cell>9</cell><cell>13.7 6.0</cell><cell>-</cell><cell>36.2</cell><cell>52.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>42.2</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>48.0</cell><cell>54.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on the KITTI dataset.Method building sky road vegetation sidewalk car pedestrian cyclist signage fence mIOU mAcc</figDesc><table><row><cell>Zhang et al. [75] 86.90 -89.20 55.00</cell><cell>26.20 50.0</cell><cell>49.00</cell><cell>19.3 51.7 21.1</cell><cell>-</cell><cell>49.80</cell></row><row><cell>3D-FCNN-TI(Ours) 85.83 -90.57 70.50</cell><cell cols="2">25.56 65.68 46.35</cell><cell cols="3">7.78 28.40 4.51 35.65 47.24</cell></row><row><cell>SEGCloud (Ours) 85.86 -88.84 68.73</cell><cell cols="2">29.74 67.51 53.52</cell><cell cols="3">7.27 39.62 4.05 36.78 49.46</cell></row><row><cell cols="2">comparison shown in Table 2. We outperform the MLP</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">architeture of [53] by 7.83 mIOU points and 8.37% in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">mean accuracy. Our base 3D-FCNN-TI also outper-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">forms their architecture and the effect of our system's</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">design choices on the performance of the 3D-FCNN and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3D-FCNN-TI are analyzed in Section 6.3. Qualitative</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>results on this dataset (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table wall</head><label>wall</label><figDesc></figDesc><table><row><cell></cell><cell>Raw 3D Input</cell><cell>3D FCNN-TI</cell><cell>SEGCloud</cell><cell>Ground Truth</cell></row><row><cell>Semantic3D.net</cell><cell>[1]</cell><cell></cell><cell></cell><cell>scanning artifacts building natural terrain man made terrain cars hard scape low vegetation high vegetation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>bookcase</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>chair</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>column</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>board</cell><cell>window</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>floor</cell><cell>clutter</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Effect of Geometric Augmentation</figDesc><table><row><cell>Method mIOU</cell></row><row><cell>PointNet [53] 41.09</cell></row><row><cell>Ours-no augm. (3D-FCNN-TI) 43.67</cell></row><row><cell>Ours (3D-FCNN-TI) 47.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Effect of trilinear interpolation</figDesc><table><row><cell>Method mIOU</cell></row><row><cell>PointNet [53] 41.09</cell></row><row><cell>Ours-NN (3D-FCNN-NN) 44.84</cell></row><row><cell>Ours (3D-FCNN-TI) 47.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Cloud Sub-sampling Factor (For training-only)</figDesc><table><row><cell>Threshold</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(#points)</cell><cell cols="2">1e 5 1e 6</cell><cell>1e 7</cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>S3DIS</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>Semantic3D.net</cell><cell>10</cell><cell cols="2">50 100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Datasets Characteristics</figDesc><table><row><cell></cell><cell cols="2">KITTI [23, 22] NYU V2 [52]</cell><cell>S3DIS [5]</cell><cell>Semantic3D.net [26]</cell></row><row><cell>Scene</cell><cell>outdoor</cell><cell>indoor</cell><cell>indoor</cell><cell>outdoor</cell></row><row><cell>Point Cloud type</cell><cell>partial</cell><cell>partial</cell><cell>full</cell><cell>partial</cell></row><row><cell>Sensor type</cell><cell>Laser</cell><cell>Kinect</cell><cell>MatterPort</cell><cell>Laser</cell></row><row><cell>Number of training points</cell><cell>12million</cell><cell>125million</cell><cell>228million</cell><cell>1.9billion</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Effect of CRF initialization: End-to-end training vs Semantic3D.net 61.30 73.08 60.72 69.69 S3DIS 48.92 57.35 47.09 53.6 KITTI 36.78 49.46 36.34 46.34 NYUV2 43.45 56.43 41.63 52.28</figDesc><table><row><cell>Manual</cell><cell></cell></row><row><cell>Dataset</cell><cell>End-to-end mIOU mAcc mIOU mAcc manual</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Depending on the type of 3D data a pre-processing step of converting it to a 3D point cloud representation might be necessary.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use<ref type="bibr" target="#b63">[64]</ref> that supports 3D convolution.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge the support of Facebook and MURI (1186514-1-TBCJE) for this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This document presents additional details and qualitative results for the framework presented in our main paper. Section A reports the particulars of our framework's implementation. The following section B offers details and results on the effect of using end-to-end training versus separate CRF intialization. The remaining of the document focuses on additional aspects of the evaluation and experiments. The experimental setup is detailed in Section C. The characteristics of the datasets used in our evaluation are outlined in Section C.1. Section C.2 defines the metrics used in evaluating our framework. Finally, qualitative results of our framework on all four datasets are illustrated in Section C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation</head><p>This section provides additional implementation details, including procedures for 3D data augmentation, data preparation, training, as well as the programming framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Augmentation Procedures for 3D data</head><p>Most of the datasets we used are small to medium in scale. To make up for the lack of data, we perform a series of augmentations for 3D data. We apply the following data augmentations on-the-fly to increase randomness in the data and save storage space.</p><p>Color Augmentation: Color augmentation is a popular data augmentation technique for image datasets. We leverage it in our work by randomly varying the R, G and B</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast High-Dimensional Filtering Using the Permutohedral Lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation based classification of 3d urban point clouds: A supervoxel based approach with evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Aijazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Checchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Trassoudaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1624" to="1650" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contextually Guided Semantic Labeling and Search for 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="34" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">3D Semantic Parsing of Large-Scale Indoor Spaces. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unstructured Point Cloud Semantic Labeling Using Deep Segmentation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval. The Eurographics Association</title>
		<editor>I. Pratikakis, F. Dupont, and M. Ovsjanikov</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Conference on Computer Vision: Part I, ECCV &apos;08</title>
		<meeting>the 10th European Conference on Computer Vision: Part I, ECCV &apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Airborne lidar feature selection for urban classification using random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISPRS Workshop: Laserscanning09</title>
		<meeting>the ISPRS Workshop: Laserscanning09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno>Systems 29. 2016. 5</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient structured parsing of facades using dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3206" to="3213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Decision Forests for Computer Vision and Medical Image Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04405</idno>
		<title level="m">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning hierarchical semantic segmentations of LIDAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matejek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A Point Set Generation Network for 3D Object Reconstruction from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointnet: A 3d convolutional neural network for real-time object class recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez-Donoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Azorin-Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1578" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CVPR &apos;12</title>
		<meeting>the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CVPR &apos;12<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multiview RGB-D dataset for object instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Reza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<idno>abs/1609.07826</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d mesh labeling via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno>3:1- 3:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">NET: A NEW LARGE-SCALE POINT CLOUD CLASSIFICA-TION BENCHMARK. to appear in ISPRS Ann. Photogramm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semantic3d</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fast Semantic Segmentation of 3D Point Clouds With Strongly Varying Density. ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dense 3d semantic mapping of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2631" to="2638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient 3-d scene analysis from streaming data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scenenn: A scene meshes dataset with annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Point Cloud Labeling using 3D Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F J</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D Scene Understanding by Voxel-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic Labeling of 3D Point Clouds for Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24</title>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Associative hierarchical random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1056" to="1077" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Natural terrain classification using three-dimensional ladar data for ground robot mobility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="839" to="861" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep projective 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simplified markov random fields for efficient semantic labeling of 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2690" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3D all the way: Semantic segmentation of urban scenes from start to end in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4456" to="4465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A chronology of interpolation: From ancient astronomy to modern signal and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meijering</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-03" />
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="319" to="342" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Co-inference machines for multi-modal scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Directional associative markov network for 3-d point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth international symposium on 3D data processing, visualization and transmission</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Onboard contextual classification of 3-d point clouds with learned high-order markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<biblScope unit="page" from="2009" to="2016" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A search-classify approach for cluttered indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
		<idno>137:1-137:10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1612.00593</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning Where to Classify in Multi-view Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bódis-Szomorú</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weissenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="516" to="532" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Non-Associative Markov Networks for 3D Point Cloud Classification. Isprs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Velizhev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<idno>XXXVIII-3A:103-108</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep Sliding Shapes for amodal 3D object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning Associative Markov Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Machine Learning</title>
		<meeting>of the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="102" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Max margin Markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Supplementary Material for SEGCloud: Semantic Segmentation of 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<ptr target="http://segcloud.stanford.edu/supplementary.pdf.4" />
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Multimodal Unsupervised Feature Learning for RGB-D Scene Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="453" to="467" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An efficient scene semantic labeling approach for 3d point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2115" to="2120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Semantic 3D scene interpretation: A framework combining optimal neighborhood size selection with relevant features. ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jutzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Fast Semantic Segmentation of 3D Point Clouds using a Dense CRF with Learned Parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prankl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Icra</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Enhancing semantic segmentation for robotics: The power of 3-d entangled forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prankl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="56" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1696" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Category modeling from just a single labeling: Use depth information to guide the learning of 2d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shibasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Sensor fusion for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Candra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional Random Fields as Recurrent Neural Networks. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
