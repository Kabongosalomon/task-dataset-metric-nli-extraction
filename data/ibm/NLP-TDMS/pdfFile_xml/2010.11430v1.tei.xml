<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-training and Pre-training are Complementary for Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paden</forename><surname>Tomasello</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-training and Pre-training are Complementary for Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-training and unsupervised pre-training have emerged as effective approaches to improve speech recognition systems using unlabeled data. However, it is not clear whether they learn similar patterns or if they can be effectively combined. In this paper, we show that pseudo-labeling and pre-training with wav2vec 2.0 are complementary in a variety of labeled data setups. Using just 10 minutes of labeled data from Libri-light as well as 53k hours of unlabeled data from LibriVox achieves WERs of 3.0%/5.2% on the clean and other test sets of Librispeechrivaling the best published systems trained on 960 hours of labeled data only a year ago. Training on all labeled data of Librispeech achieves WERs of 1.5%/3.1%. * Equal contribution.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Speech recognition models trained on labeled speech data has progressed substantially in the recent past <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. A drawback of these models is that they require a lot of labeled data to perform well which is usually only available for English and a few other languages. Therefore, purely supervised training is impractical for the vast majority of the 7,000 languages spoken around the world <ref type="bibr" target="#b4">[5]</ref> which is why there has been a lot of interest in how to better use unlabeled speech data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>This includes classical self-training <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> which demonstrated strong results <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> by pseudo-labeling unannotated audio data and then retraining the final system with the additional labeled data. Another line of work is pre-training representations on unlabeled speech followed by fine-tuning on labeled data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>In this paper we combine self-training and unsupervised pre-training which are different approaches to leveraging unlabeled data. Both achieved excellent results on competitive benchmarks and the central question we explore is whether the two methods are complementary to each other. Specifically, we build on the recently introduced wav2vec 2.0 model <ref type="bibr" target="#b23">[24]</ref> and the self-training approach of <ref type="bibr">[13]</ref>) and <ref type="bibr" target="#b13">Xu et al. (2020;</ref><ref type="bibr">[14]</ref>). We explore training models on the pseudo-labeled data from scratch or by fine-tuning the pre-trained model. To better understand how complementary the two methods are, we use the same unlabeled data for both.</p><p>Experiments on the full Librispeech corpus as well as the low-resource labeled data setups of Librilight show that self-training and unsupervised pre-training are indeed complementary, a finding that is inline with recent work in natural language understanding <ref type="bibr" target="#b24">[25]</ref>. In a very low resource setup with just 10 minutes of labeled data and LibriVox as unlabeled data, the combination of wav2vec 2.0 and self-training achieves a WER of 3.0%/5.2% on the clean and other test sets of Librispeech, a relative WER reduction of 25% and 40% over recent work in pre-training alone <ref type="bibr" target="#b23">[24]</ref>. Using just the acoustic model without a language model achieves WER 3.7%/6.5% -supporting the hypothesis that self-training distills the language model used for pseudo-labeling into the final model. When all 960 hours of labeled training data are used we achieve 1.5%/3.1% WER on Librispeech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unsupervised Pre-training Model</head><p>We experiment with the recently introduced wav2vec 2.0 model of <ref type="bibr">[24]</ref>). This model contains a convolutional feature encoder f : X → Z to map raw audio X to latent speech representations z 1 , . . . , z T which are input to a Transformer g : Z → C to output context representations c 1 , . . . , c T <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>. Each z t represents about 25ms of audio strided by 20ms and the Transformer architecture follows BERT <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b25">26]</ref>. During training, feature encoder representations are discretized to q 1 , . . . , q T with a quantization module Z → Q to represent the targets in the objective. The quantization module uses a Gumbel softmax to choose entries from G = 2 codebooks with V = 320 entries each and the chosen entries are concatenated to obtain q <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>The model is trained by solving a contrastive task over masked feature encoder outputs. At training time, spans of ten time steps with random starting indices are masked. The objective requires identifying the true quantized latent q t for a masked time-step within a set of K = 100 distractors Q t sampled from other masked time steps: − log exp(sim(ct,qt)) q∼Q t exp(sim(ct,q)) where c t is the output of the Transformer, and sim(a, b) denotes cosine similarity. The objective is augmented by a codebook diversity penalty to encourage the model to use all codebook entries <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-training Approach</head><p>We adopt the pseudo-labeling strategy of <ref type="bibr">[13]</ref>) and Synnaeve et al. (2020; <ref type="bibr" target="#b1">[2]</ref>). This first trains an initial acoustic model on the available labeled data and then labels the unlabeled data with the initial model as well as a language model in a step we call pseudo-labeling. Finally, a new acoustic model is trained on the pseudo-labeled data as well as the original labeled data.</p><p>Previous work considered multiple rounds of pseudo-labeling where the labeling step is repeated with each new model to train another model <ref type="bibr" target="#b13">[14]</ref>. While iterative pseudo-labeling is more accurate, we opt for a single iteration which is computationally less demanding while still enabling us to reason about whether unsupervised pre-training and pseudo-labeling are complementary. Another line of work investigated filtering the resulting pseudo-labeled data to match the distribution of the original labeled data <ref type="bibr" target="#b14">[15]</ref>. Both methods may improve results and we leave them to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combining the two Approaches</head><p>To combine the approaches, we replace the initial model for pseudo-labeling with a pre-trained model. The resulting training pipeline is as follows: we first pre-train a wav2vec 2.0 model on the unlabeled data, fine-tune it on the available labeled data, use the model to label the unlabeled data, and finally use the pseudo-labeled data to train the final model. In our experiments, we also consider a variant where we fine-tune the original wav2vec 2.0 model on the pseudo-labeled data.</p><p>3 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>As unlabeled data for pre-training and self-training we consider the speech audio of the Librispeech corpus (LS-960; <ref type="bibr" target="#b31">[32]</ref>) without transcriptions containing 960h of audio as well as the audio data of LibriVox (LV-60k). For the latter we follow the pre-processing of Kahn et al. (2020; <ref type="bibr" target="#b32">[33]</ref>) resulting in 53.2k hours of audio. We consider five labeled data setups: all 960h of transcribed Librispeech, the train-clean-100 subset comprising 100h, as well as the Libri-light limited resource training subsets of train-10h (10h), train-1h (1h), and train-10min (10min). We evaluate on the standard Librispeech dev-other/clean and test-clean/other sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-trained models</head><p>Pre-trained models are implemented in fairseq <ref type="bibr" target="#b33">[34]</ref> and we obtain them from the public fairseq github repository. <ref type="bibr" target="#b1">2</ref> The repository provides fine-tuned models for the five labeled data setups we consider ( § 3.1). We experiment with the LARGE configuration comprising 24 transformer blocks with model dimension 1,024, inner dimension 4,096 and 16 attention heads, comprising a total of about 300M parameters. The feature encoder contains seven blocks and the temporal convolutions in each block have 512 channels with strides (5,2,2,2,2,2,2) and kernel widths (10,3,3,3,3,2,2), resulting in a receptive field of about 25ms and a stride of about 20ms. After pre-training on the unlabeled data, this model is fine-tuned on the labeled data using Connectionist Temporal Classification (CTC; <ref type="bibr" target="#b34">[35]</ref>) and a letter-based output vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-training</head><p>We pseudo-label the audio data of either LS-960 or LV-60k using wav2vec 2.0 LARGE fine-tuned on different labeled data splits. For labeling, we follow the two-pass rescoring procedure of Synnaeve et al. (2020; <ref type="bibr" target="#b1">[2]</ref>): first, we generate a list of candidate transcriptions by combining wav2vec 2.0 and the standard Librispeech 4-gram language model during beam-search with beam 800. Next, the n-best list is pruned to the 50 highest scoring entries and then rescored with a Transformer LM trained on the Librispeech language corpus <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b1">2]</ref>. The Transformer LM has 20 blocks with model dimension 1,280, inner dimension 6,144 and 16 attention heads. The n-gram model obtains perplexity 150.3 on the development set and the Transformer language model 49.2. We found this to be more efficient than directly integrating the Transformer LM into beam search at little loss in accuracy. Decoding and rescoring hyper-parameters are tuned on dev-other of Librispeech for each experiment using a random parameter search. The LM weight and the word insertion penalty <ref type="bibr" target="#b1">[2]</ref> is tuned by randomly sampling values in the range of [0, 5] and [-5, 5] over 128 trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Final Model</head><p>We follow Synnaeve et al. (2020; <ref type="bibr" target="#b1">[2]</ref>) and train a Transformer-based sequence to sequence model with log-Mel filterbank inputs after pseudo-labeling using wav2letter++ <ref type="bibr" target="#b36">[37]</ref>. The encoder uses a convolutional frontend containing 4 layers of temporal convolutions with kernel width 3, followed by 36 Transformer blocks with model dimension 768, 4 attention heads and feed-forward network (FFN) dimension 3072 <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2]</ref>. The model contains about 300M parameters.</p><p>We use a 10k word piece output vocabulary computed from the training transcriptions if the whole Librispeech training set is used as labeled data <ref type="bibr" target="#b37">[38]</ref>. Otherwise, we switch to the 5k WP estimated on the train-clean-100 transcriptions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b7">8]</ref>. Language models are incorporated similar to § 3.3. We use a 4-gram language model and then rescore with a Transformer LM. The beam size used in both decoding and rescoring is 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Low-Resource Labeled Data</head><p>Pre-training has been shown to be very effective in both high-and low-resource labeled training data setups whereas self-training has been most effective when at least a moderate amount of labeled data is available (≥ 100h; <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>). To get a sense of whether the combination of both methods can be even more effective, we start with experiments on the Libri-light setups with 10min, 1h and 10h of labeled data. For pre-training and pseudo-labeling we use either the 960h of Librispeech without transcriptions or the 53.2k hours of LibriVox ( § 3.1). As baseline we consider wav2vec 2.0 pre-trained on Librispeech and fine-tuned on one of the labeled data splits.</p><p>We use the publicly available wav2vec 2.0 models to pseudo-label (ST) the unlabeled data and then evaluate two options to train the final model on the resulting labels: one is to train a new sequence to sequence model from random initialization with a word-piece vocabulary (s2s scratch) following Synnaeve et al. (2019; <ref type="bibr" target="#b1">[2]</ref>; § 3.4). Another option is to fine-tune wav2vec 2.0 on the pseudo-labeled data with CTC and a letter-based vocabulary (ctc ft). <ref type="table">Table 1</ref>: WER on the Librispeech dev and test sets for the Libri-light low-resource labeled data setups of 10 min, 1 hour and 10 hours. As unlabeled data we use the audio of Librispeech (LS-960) or the larger LibriVox (LV-60k). ST (s2s scratch) trains a sequence to sequence model with a word-piece vocabulary on the pseudo-labeled data from random initialization while as ST (ctc ft) fine-tunes wav2vec 2.0 with the pseudo-labels using CTC and a letter-based vocabulary. All results are with language models at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Unlbld  <ref type="table">Table 1</ref> shows that the combination of pre-training and self-training (wav2vec 2.0 + ST) outperforms pre-training alone (wav2vec 2.0) across all low-resource setups. It also achieves a very large improvement over iterative pseudo-labeling <ref type="bibr" target="#b13">[14]</ref> in the 10h labeled setup. This is because the initial model is much stronger due to pre-training and it is very difficult to train a good supervised-only model on just 10h of labeled data.</p><p>With just 10 minutes of labeled data, the combination of pre-training and pseudo-labeling with LibriVox achieves WER 5.2% on test-other. Using Librispeech (LS-960) as unlabeled data and 10 minutes of labeled data, wav2vec 2.0 + ST achieves 4.0%/7.2% WER on test-clean/other compared to 4.2%/8.6% for the best known pseudo-labeling approach <ref type="bibr" target="#b14">[15]</ref> which uses 100 hours of labeled data. More unlabeled data leads to large improvements, reducing WER from 4.0%/7.2% for LS-960 to 3.0%/5.2% for LV-60k, a relative WER reduction of 25-28%. But increasing the amount of labeled data without more unlabeled data leads to diminishing returns -an issue we return to in § 5. Fine-tuning (ctc ft) generally outperforms from scratch training of a sequence to sequence model with a WP vocabulary (s2s scratch). This is likely because the model can leverage the pre-trained representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">High-Resource Labeled Data</head><p>Next, we evaluate performance with more labeled data. We consider the 100h clean subset of Librispeech as well as all 960h of labeled data in Librispeech. <ref type="table" target="#tab_1">Table 2</ref> shows that LS-960 as unlabeled data is not enough to outperform the baseline when 100h of labeled data is available. However, performance improves when using the much larger LV-60k, achieving a 10% relative WER reduction on test-other over wav2vec 2.0.</p><p>When using the full Librispeech benchmark as labeled data, combining wav2vec 2.0 and pseudolabeling achieves WER 1.5%/3.1%. This result was achieved with a strong sequence to sequence model trained from scratch. While less effective than fine-tuning with CTC on smaller setups, a powerful sequence to sequence model excels in this larger setting since the decoder part of the model, which acts in part like a language model, does not overfit. The lower performance of fine tuning is likely due to CTC not being as competitive as more elaborate sequence to sequence models when a lot of pseudo-labeled data is available <ref type="bibr" target="#b1">[2]</ref>. <ref type="table" target="#tab_2">Table 3</ref> shows that combined training models have very good performance even without a language model. This is because the language model used during pseudo-labeling was partly distilled into the pseudo-labeled data <ref type="bibr" target="#b1">[2]</ref>. This effect is particularly striking for the 10 min labeled setup without LM where wav2vec 2.0 + ST (s2s scratch) reduces the WER of the baseline (wav2vec 2.0 -LM) by 83% relative on test-other. As more labeled data becomes available, the performance of the acoustic model without a language model improves but there is still a clear effect of self-trained models having distilled the language model. Generally, the sequence to sequence model in the (s2s scratch) setting is better able to distill the language model used at pseudo-labeling time compared to the CTC model used in fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results without a Language Model at Inference Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We previously saw that improvements decreased with more labeled data ( § 4.1). To better understand this, we perform an experiment on Librispeech where we consider data setups with a fixed ratio between the unlabeled and labeled data. <ref type="table" target="#tab_3">Table 4</ref> shows that relative improvements are a function of the amount of unlabeled data relative to the labeled data, rather than the amount of labeled data alone. <ref type="table">Table 1</ref> showed much larger improvements for the 10 min labeled split but with a fixed ratio  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Unsupervised pre-training and pseudo-labeling are complementary for speech recognition. This enables building speech recognition systems with as little as 10 minutes of transcribed speech with word error rates that only a year ago were reserved to the best systems trained on 960 hours of labeled data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>WER on Librispeech when using the clean 100h subset as labeled data or all 960h of Librispeech (cf.Table 1). Prior work used 860 unlabeled hours (LS-860) but the total with labeled data is 960h and comparable to our setup.</figDesc><table><row><cell>Model</cell><cell>Unlbld data</cell><cell cols="4">dev clean other clean other test</cell></row><row><cell>100h labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Discr. BERT [27]</cell><cell>LS-960</cell><cell>4.0</cell><cell>10.9</cell><cell>4.5</cell><cell>12.1</cell></row><row><cell>ST [13]</cell><cell>LS-860</cell><cell>5.4</cell><cell>19.0</cell><cell>5.8</cell><cell>20.1</cell></row><row><cell>IPL [14]</cell><cell>LS-860</cell><cell>5.0</cell><cell>8.0</cell><cell>5.6</cell><cell>9.0</cell></row><row><cell>Noisy student [15]</cell><cell>LS-860</cell><cell>3.9</cell><cell>8.8</cell><cell>4.2</cell><cell>8.6</cell></row><row><cell>wav2vec 2.0 [24]</cell><cell>LS-960</cell><cell>2.1</cell><cell>4.8</cell><cell>2.3</cell><cell>5.0</cell></row><row><cell>+ ST (s2s scratch)</cell><cell>LS-960</cell><cell>2.3</cell><cell>4.6</cell><cell>2.7</cell><cell>5.4</cell></row><row><cell>+ ST (ctc ft)</cell><cell>LS-960</cell><cell>2.2</cell><cell>4.6</cell><cell>2.4</cell><cell>5.0</cell></row><row><cell>IPL [14]</cell><cell>LV-60k</cell><cell>3.19</cell><cell>6.14</cell><cell>3.72</cell><cell>7.11</cell></row><row><cell>wav2vec 2.0 [24]</cell><cell>LV-60k</cell><cell>1.9</cell><cell>4.0</cell><cell>2.0</cell><cell>4.0</cell></row><row><cell>+ ST (s2s scratch)</cell><cell>LV-60k</cell><cell>1.4</cell><cell>2.8</cell><cell>1.9</cell><cell>3.8</cell></row><row><cell>+ ST (ctc ft)</cell><cell>LV-60k</cell><cell>1.7</cell><cell>3.2</cell><cell>1.9</cell><cell>3.6</cell></row><row><cell>960h labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SpecAugment [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.5</cell><cell>5.8</cell></row><row><cell>ContextNet [3]</cell><cell>-</cell><cell>1.9</cell><cell>3.9</cell><cell>1.9</cell><cell>4.1</cell></row><row><cell>Conformer [4]</cell><cell>-</cell><cell>2.1</cell><cell>4.3</cell><cell>1.9</cell><cell>3.9</cell></row><row><cell>Semi-supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IPL [14]</cell><cell>LV-60k</cell><cell>1.85</cell><cell>3.26</cell><cell>2.10</cell><cell>4.01</cell></row><row><cell>Noisy Student [15]</cell><cell>LV-60k</cell><cell>1.6</cell><cell>3.4</cell><cell>1.7</cell><cell>3.4</cell></row><row><cell>wav2vec 2.0 [24]</cell><cell>LV-60k</cell><cell>1.6</cell><cell>3.0</cell><cell>1.8</cell><cell>3.3</cell></row><row><cell>+ ST (s2s scratch)</cell><cell>LV-60k</cell><cell>1.1</cell><cell>2.7</cell><cell>1.5</cell><cell>3.1</cell></row><row><cell cols="2">+ ST (ctc fine-tune) LV-60k</cell><cell>1.6</cell><cell>2.9</cell><cell>1.8</cell><cell>3.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>WER on Librispeech with and without a language model (LM) for 10 min, and 960h of labeled data and LibriVox as unlabeled data.</figDesc><table><row><cell>Model</cell><cell cols="4">dev clean other clean other test</cell></row><row><cell>10 min labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wav2vec 2.0 [24]</cell><cell>5.0</cell><cell>8.4</cell><cell>5.2</cell><cell>8.6</cell></row><row><cell>-LM</cell><cell>38.3</cell><cell>41.0</cell><cell>40.2</cell><cell>38.7</cell></row><row><cell>wav2vec 2.0 + ST (s2s scratch)</cell><cell>2.6</cell><cell>4.7</cell><cell>3.1</cell><cell>5.4</cell></row><row><cell>-LM</cell><cell>3.3</cell><cell>5.9</cell><cell>3.7</cell><cell>6.5</cell></row><row><cell>wav2vec 2.0 + ST (ctc ft)</cell><cell>2.8</cell><cell>4.6</cell><cell>3.0</cell><cell>5.2</cell></row><row><cell>-LM</cell><cell>4.2</cell><cell>6.9</cell><cell>4.3</cell><cell>7.2</cell></row><row><cell>960h labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wav2vec 2.0 [24]</cell><cell>1.6</cell><cell>3.0</cell><cell>1.8</cell><cell>3.3</cell></row><row><cell>-LM</cell><cell>2.1</cell><cell>4.5</cell><cell>2.2</cell><cell>4.5</cell></row><row><cell>wav2vec 2.0 + ST (s2s scratch)</cell><cell>1.1</cell><cell>2.7</cell><cell>1.5</cell><cell>3.1</cell></row><row><cell>-LM</cell><cell>1.3</cell><cell>3.1</cell><cell>1.7</cell><cell>3.5</cell></row><row><cell>wav2vec 2.0 + ST (ctc ft)</cell><cell>1.6</cell><cell>2.9</cell><cell>1.8</cell><cell>3.3</cell></row><row><cell>-LM</cell><cell>1.7</cell><cell>3.6</cell><cell>1.9</cell><cell>3.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The main driver of performance is the ratio between labeled and unlabeled data. We add 8.6 times as much unlabeled data to each labeled setup. Results are on dev-other with an n-gram model and subsets of LS-960 as unlabeled data.</figDesc><table><row><cell></cell><cell cols="2">labeled unlab</cell><cell cols="2">dev-other % change</cell></row><row><cell cols="2">wav2vec 2.0 10min</cell><cell>86 min</cell><cell>12.9</cell><cell></cell></row><row><cell>+ ST</cell><cell>10min</cell><cell>86 min</cell><cell>12.0</cell><cell>7%</cell></row><row><cell cols="2">wav2vec 2.0 1h</cell><cell>8.6h</cell><cell>8.5</cell><cell></cell></row><row><cell>+ ST</cell><cell>1h</cell><cell>8.6h</cell><cell>7.6</cell><cell>11%</cell></row><row><cell cols="2">wav2vec 2.0 10h</cell><cell>86h</cell><cell>6.9</cell><cell></cell></row><row><cell>+ ST</cell><cell>10h</cell><cell>86h</cell><cell>6.5</cell><cell>6%</cell></row><row><cell cols="2">wav2vec 2.0 100h</cell><cell>860h</cell><cell>5.7</cell><cell></cell></row><row><cell>+ ST</cell><cell>100h</cell><cell>860h</cell><cell>5.3</cell><cell>7%</cell></row><row><cell cols="5">of labeled and unlabeled data, the relative improvement is comparable to the 100h labeled setup</cell></row><row><cell>(Table 2).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/pytorch/fairseq/tree/master/examples/wav2vec</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno>abs/1911.08460</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Contextnet: Improving convolutional neural networks for automatic speech recognition with global context. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Conformer: Convolutionaugmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ethnologue: Languages of the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Fennig</surname></persName>
		</author>
		<ptr target="http://www.ethnologue.com" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>nineteenth edition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial training of end-to-end speech recognition using a criticizing language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence-to-sequence asr using unpaired speech and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černocký</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised speech recognition via local prior matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive pattern-recognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Inform. Theory</title>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatically generating extraction patterns from untagged text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Lessons from building acoustic models with a million hours of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H K</forename><surname>Parthasarathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-training for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Iterative pseudo-labeling for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improved noisy student training for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Representation learning with contrastive predictive coding. arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">vq-wav2vec: Self-supervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An unsupervised autoregressive model for speech representation learning. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving transformer-based speech recognition using unsupervised pre-training. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<idno>abs/1910.09932</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<title level="m">Learning robust and multilingual speech representations. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised pretraining transfers well across languages. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised pre-training of bidirectional speech encoders via masked reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-training improves pre-training for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Celebi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Effectiveness of self-supervised pre-training for speech recognition. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno>abs/1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The challenge of realistic music generation: modelling raw audio at scale. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Libri-light: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL Sys. Demo</title>
		<meeting>of NAACL Sys. Demo</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>In ICML</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A fast open-source speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP Sys</title>
		<meeting>of EMNLP Sys</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
