<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monocular, One-stage, Regression of Multiple 3D People</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<email>yusun@stu.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
							<email>baoqian@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
							<email>liuwu@live.cn</email>
							<affiliation key="aff2">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tuebingen.mpg.de</email>
							<affiliation key="aff4">
								<orgName type="department">MPI for Intelligent Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@live.com</email>
							<affiliation key="aff5">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Monocular, One-stage, Regression of Multiple 3D People</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline, which first detects people with the bounding boxes and then regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP), which is conceptually simple, bounding box-free, and able to learn per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image can be easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with the state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person/occlusion benchmarks, including 3DPW, CMU Panoptic, and 3DOH50K. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. It is also worth noting that our released demo code 1 is the first real-time (over 30 FPS) implementation of monocular multi-person 3D mesh regression to date.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, great progress has been made in monocular 3D human pose and shape estimation, particularly in the single person scene <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>. However, as we progress toward more general scene, it is crucial to deal with the truncation, person-person occlusion, and environmental occlusion in the multi-person cases. Robustness to such occlusions is critical for real-world applications.</p><p>Existing approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> follow a multi-stage design that equips the single-person method with a 2D person detector to handle multi-person scenes. Generally, they 1 https://github.com/Arthur151/ROMP <ref type="figure">Figure 1</ref>. Given the challenging multi-person images in (a), the recent state-of-the-art human pose and shape estimation approaches, e.g., VIBE <ref type="bibr" target="#b19">[20]</ref> (left), fail to deal with truncation, scene occlusion, and person-person occlusion. The reason lies in the multi-stage design (b), where the bounding-box-level features are often implicit, ambiguous, and inseparable in multi-person cases. We propose to regress all meshes in one single stage for multiple 3D people. Specifically, our proposed ROMP develops an explicit pixel-level representation (c) for fine-grained estimation, which increases robustness to the truncation and occlusion and also significantly reduces computational complexity.</p><p>first detect the person areas and then extract the boundingbox-level features from them, which are used to regress each single 3D human mesh <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>. However, as shown in <ref type="figure">Figure 1</ref>, this strategy is prone to fail in cases of multi-person occlusion and truncation. Specifically, as shown in <ref type="figure">Figure 1(b)</ref>, when two people overlap, it is hard for the multi-stage method to estimate diverse body meshes from similar image patches. It is the ambiguity of the implicit bounding-box-level representation that leads to the failure in such inseparable multiperson cases.</p><p>For multi-person 2D pose estimation, this problem is tackled via a subtle and effective bottom-up framework. The paradigm is to first detect all body joints and then assign them to different people by joint grouping. It is the pixellevel body joint representation that guarantees their impressive performance in crowded scenes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref>. However, it is non-trivial to extend the bottom-up one-stage process beyond joints <ref type="bibr" target="#b12">[13]</ref>. Unlike 2D pose estimation that predicts dozens of body joints, we need to regress a human body mesh with thousands of vertices, making it hard to follow the paradigm of body joint detection and grouping.</p><p>In this paper, we introduce ROMP, a one-stage network for regressing multiple 3D people in a per-pixel prediction fashion. It directly estimates multiple differentiable maps from the whole image, from which we can easily parse out the 3D meshes of all people. Specifically, as shown in <ref type="figure">Figure 1</ref>(c), ROMP predicts a Body Center heatmap and a Mesh Parameter map, representing the 2D position of the body center and the parameter vectors of the corresponding 3D body mesh, respectively. Via a simple parameter sampling process, we can easily extract 3D body mesh parameter vectors of all people from the Mesh Parameter map at the body center locations described by the heatmap. Then we put the sampled mesh parameter vectors into the SMPL body model <ref type="bibr" target="#b28">[29]</ref> to derive multi-person 3D meshes. Such a body-center-guided pixel-level representation explicitly points out the target from the background/occlusion, which promotes effective learning from multi-person overlapping cases. Additionally, different from the bounding-box-level features learned in a local view, end-to-end learning from the whole image make the model get used to predicting in a holistic view. This pure holistic view exactly fits the realworld domain, which guarantees the generalization and robustness of our model in unseen/in-the-wild scenes.</p><p>Moreover, considering that the body center of severe overlapping people may collide at the same 2D position, we further develop the body-center-guided representation into an advanced version, collision-aware representation (CAR). The key idea is to construct a repulsion field of body centers, where close body centers are analogous to positive charges that are pushed apart by the mutual repulsion. In this way, the body centers of the overlapping people would be more distinguishable. Especially in the face of severe overlap, most part of the human body is invisible. Mutual repulsion will push the center to the visible body area, making the model tend to sample 3D mesh parameters estimated from the position centered on the visible body parts. It improves the robustness under heavy occlusion between people.</p><p>Compared with previous state-of-the-art (SOTA) methods for multi-person <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> and single-person <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b45">46]</ref> 3D mesh regression, ROMP achieves superior performance on challenging benchmarks, including 3DPW <ref type="bibr" target="#b40">[41]</ref>, CMU Panoptic <ref type="bibr" target="#b15">[16]</ref>, and 3DOH50K <ref type="bibr" target="#b45">[46]</ref>. Experiments on person-person occlusion datasets (Crowdpose <ref type="bibr" target="#b24">[25]</ref> and 3DPW-PC, a person-occluded subset of 3DPW <ref type="bibr" target="#b40">[41]</ref>) demonstrate the effectiveness of the proposed CAR under person-person occlusion. To further evaluate it in general cases, we test ROMP on images from the Internet and web camera videos. With the same backbone as those of the multi-stage counterparts, ROMP achieves real-time performance with over 30 FPS on one 1070Ti GPU.</p><p>In summary, the contributions are: • A simple yet effective one-stage regression network, ROMP, is proposed for monocular multi-person 3D mesh regression. • The proposed explicit body-center-guided representation facilitates the pixel-level human mesh regression in an end-to-end manner. • We develop an collision-aware representation to deal with the severe overlapping cases. • ROMP outperforms the previous SOTA methods in both accuracy and efficiency. It is the first open-source real-time method for multi-person 3D mesh regression from monocular images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single-person 3D mesh regression. Parametric human body models, e.g., SMPL <ref type="bibr" target="#b28">[29]</ref>, have been widely adopted to encode the complex 3D human mesh into a lowdimensional parameter vector. Therefore, many methods tend to estimate the SMPL parameters instead of the 3D mesh vertices, to reduce the complexity. Recently, impressive performance has been achieved for single-person scenes using various weak supervision signals, such as 2D pose <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>, semantic segmentation <ref type="bibr" target="#b42">[43]</ref>, geometric prior <ref type="bibr" target="#b17">[18]</ref>, motion dynamics <ref type="bibr" target="#b18">[19]</ref>, temporal coherence <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref>, texture consistency <ref type="bibr" target="#b33">[34]</ref>, SMPLify <ref type="bibr" target="#b4">[5]</ref> in the loop <ref type="bibr" target="#b20">[21]</ref>, etc. In this way, the available 2D/3D data is well explored to alleviate the lack of 3D data. However, all these methods adopt a bounding-box-level representation, which is implicit and ambiguous for multi-person/occlusion cases. For object occlusion, Zhang et al. <ref type="bibr" target="#b45">[46]</ref> use a 2D UV map to represent a 3D human mesh. Considering the object-occluded body parts are blank areas in the partial UV map, they propose to in-paint the partial UV map to make up the occluded information. However, in the case of person-person occlusion where one person's body parts are occluded by those of another, it is hard to generate the partial UV map.</p><p>Multi-person 3D pose estimation. The mainstream methods can be roughly divided into two categories, the multi-stage paradigm and the one-stage paradigm. Many multi-stage methods follow the top-down design of the well-known Faster R-CNN <ref type="bibr" target="#b36">[37]</ref>, such as LCR-Net++ <ref type="bibr" target="#b37">[38]</ref>  <ref type="figure">Figure 2</ref>. An overview of ROMP. Given an input image, ROMP predicts multiple maps: 1) the Body Center heatmap predicts the probability of each position being a body center, 2) the Camera map and 3) SMPL map contain the camera and SMPL parameters <ref type="bibr" target="#b28">[29]</ref> of the person at each center, respectively. As the concatenation of the Camera map and SMPL map, the Mesh Parameter map contains the information of the predicted 3D body mesh and its location. Via the designed parameter sampling process, we can obtain the final 3D mesh results by parsing the Body Center heatmap and sampling the Mesh Parameter map. and 3DMPPE <ref type="bibr" target="#b31">[32]</ref>. Using the detected bounding boxes or anchor-based feature proposals, they then estimate the target via regression. Other works explore the one-stage solution that reasons about all people in a single forward pass. They estimate all the body joint positions and then group the joints into each person. Mehta et al. <ref type="bibr" target="#b30">[31]</ref> propose the occlusion-robust pose-maps and exploit the body part association to avoid the bounding box prediction. Benzine et al. <ref type="bibr" target="#b3">[4]</ref> propose an anchor-based one-stage model, which directly estimates the 2D/3D pose results for each anchor position. For person-person occlusion, Zhen et al. <ref type="bibr" target="#b46">[47]</ref> adopt the PAFs of OpenPose <ref type="bibr" target="#b5">[6]</ref> to make part association. Benefiting from the explicit bottom-up joint-based representation, such as the volumetric heatmap, 3D pose estimation methods show impressive performance. Our proposed ROMP extends the end-to-end one-stage process beyond the body joints via a body-center-guided pixel-level representation.</p><p>Multi-person 3D mesh regression. There are only a few approaches for multi-person 3D mesh regression. Zanfir et al. <ref type="bibr" target="#b44">[45]</ref> estimate the 3D mesh of each person from its intermediate 3D pose estimate. Zanfir et al. <ref type="bibr" target="#b43">[44]</ref> further employ multiple scene constraints to optimize the multi-person 3D mesh results. Jiang et al. <ref type="bibr" target="#b12">[13]</ref> propose a network for Coherent Reconstruction of Multiple Human (CRMH), which is built on Faster-RCNN <ref type="bibr" target="#b36">[37]</ref>. They use the RoI-aligned feature of each person to predict the SMPL parameters. Additionally, they develop the interpolation and depth ordering loss to supervise the relative position between multiple people. All these existing methods follow the multi-stage design. The complex multi-step process requires repeated feature extraction, which slows down the computational efficiency. Moreover, since relying on the detected bounding boxes, the ambiguity and the limited local view of the bounding-box-level features make them hard to effectively learn from person-person occlusion and truncation. Instead, our proposed simple yet effective one-stage method learns an explicit pixel-level representation with a holistic view, which significantly improves both accuracy and efficiency in multi-person in-the-wild scenes.</p><p>Pixel-level representation has been proven useful in anchor-free detection, such as CornerNet <ref type="bibr" target="#b23">[24]</ref>, Center-Net <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b47">48]</ref>, and ExtremeNet <ref type="bibr" target="#b48">[49]</ref>. They attempt to directly estimate the corner or center point of the bounding box in a heatmap manner. In this way, they avoid the dense anchorbased proposal. Our method draws inspiration from them to develop a pixel-level fine-grained representation for multiperson 3D meshes. Different from the bounding box center used in <ref type="bibr" target="#b47">[48]</ref>, our body center is determined by the body joints, which will be introduced in Sec. 3.3. Furthermore, we develop a collision-aware version of the body center to deal with the inherent center collision problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The overall framework of the one-stage ROMP is illustrated in <ref type="figure">Figure 2</ref>. It adopts a simple multi-head design with a backbone and three head networks. Given a single RGB image as input, it outputs a Body Center heatmap, Camera map, and SMPL map, describing the detailed information of the estimated 3D human mesh. In the Body Center heatmap, we predict the probability of each position being a human body center. At each position of the Camera/SMPL map, we predict the camera/SMPL parameters of the person that takes the position as the center. For simplicity, we combine the Camera map and SMPL map into the Mesh Parameter map. During inference, we sample the 3D body mesh parameter results from the Mesh Parameter map at the 2D body center locations parsed from the Body Center heatmap. Finally, we put the sampled parameters into the SMPL model to generate the 3D body meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Basic Representations</head><p>In this section, we introduce the detailed representation of each map. Each output map is of size n × H × W , where n is the number of channels and H = W = 64.</p><p>Body Center heatmap: </p><formula xml:id="formula_0">C m ∈ R 1×H×W is</formula><formula xml:id="formula_1">k = k l + ( d bb √ 2W ) 2 k r ,<label>(1)</label></formula><p>where k l is the minimum kernel size and k r is the variation range of k. We set k l = 2 and k r = 5 by default. Mesh Parameter map: P m ∈ R 145×H×W consists of two parts, the Camera map and SMPL map. Assuming that each location of these maps is the center of a human body, we estimate its corresponding 3D body mesh parameters. Specifically, we estimate the parameters of SMPL model that encode the 3D body mesh into a set of low-dimensional parameters. Estimating these SMPL parameters instead of the complex 3D mesh greatly reduces the complexity of our task. Additionally, following the previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>, we employ a weak-perspective camera model to project K 3D body joints J = (x k , y k , z k ), k = 1 · · · K of the estimated 3D mesh back to the 2D joints J = ( x k , y k ) on the image plane. This facilitates training the model with in-thewild 2D pose datasets in varied imagery (e.g. COCO <ref type="bibr" target="#b26">[27]</ref>), which helps with robustness and generalization.</p><p>Camera map: A m ∈ R 3×H×W contains the 3-dim camera parameters (s, t x , t y ) that describe the 2D scale s and translation t = (t x , t y ) of the person in the image. The scale s reflects the body size and the depth to some extent. t x and t y , ranging in (−1, 1), reflect the normalized translation of the human body relative to the image center on the x and y axis, respectively. The 2D projection J of 3D body joints J can be derived as x k = sx k + t x , y k = sy k + t y . The translation parameters allow more accurate position estimates than the Body Center heatmap.</p><p>SMPL map: S m ∈ R 142×H×W contains the 142-dim SMPL parameters, which describe the 3D pose and shape of the 3D human body mesh. SMPL establishes an efficient mapping from the pose θ and shape β parameters to the human 3D body mesh M ∈ R 6890×3 . The shape parameter β ∈ R 10 is the top-10 PCA weights of the SMPL statistical shape space. The pose parameters θ ∈ R 6×22 contain the 3D rotation of the 22 body joints in a 6D representation <ref type="bibr" target="#b49">[50]</ref>. Instead of using the full 24 joints of the original SMPL model, we drop the last two hand joints. The 3D rotation of the first joint denotes the body 3D orientation in the camera coordinate, while the remainders are the relative 3D orientations of each body part with respect to its parent in a kinematic chain. Then the 3D joints J are derived via P M , where P ∈ R K×6890 is a sparse weight matrix that describes the linear mapping from 6890 vertices of the body mesh to the K body joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">CAR: Collision-Aware Representation</head><p>The entire framework is based on a concise body-centerguided representation. It is crucial to define an explicit and robust body center so that the model can easily estimate the center location in various cases. Here we introduce the basic definition of the body center for the general case and its advanced version for severe occlusion.</p><p>Basic definition of the body center. Existing centerbased methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b47">48]</ref> define the center of the bounding box as the target center. This works well for general objects (e.g., a ball, bottle) that lack semantically meaningful keypoints. However, bounding box center is not a meaningful point on the human body and may even fall outside the body area. For a stable parameter sampling, we need an explicit body center. Therefore, we calculate each body center from the ground truth 2D pose. Considering that any body joint may be occluded in general cases, we define the body center as the center of visible torso joints (neck, left/right shoulders, pelvis, and left/right hips). When all torso joints are invisible, the center is simply determined by the average of the visible joints. In this way, the model is encouraged to predict the body location from the visible parts.</p><p>However, in cases of severely overlapping people, the body center of the people might be very close or even at the same location on C m . This center collision problem makes the center ambiguous and hard to identify in crowded cases. To tackle this, we develop a more robust representation to deal with person-person occlusion. To alleviate the ambiguity, the center points of overlapping people should be kept at a minimum distance to ensure that they can be well distinguished. Additionally, to avoid sampling multiple parameters for the same person, the network should assign a unique and explicit center for each person.</p><p>Based on these principles, we develop a novel Collision-Aware Representation (CAR). To ensure that the body centers are far enough from each other, we construct a repulsion field. In this field, each body center is treated as a positive charge, whose radius of repulsion is equal to its Gaussian kernel size derived by Eq. (1). In this way, the closer the body centers are, the greater the mutual repulsion and the further they will be pushed apart. <ref type="figure" target="#fig_0">Figure 3</ref> illustrates the principle of CAR. Suppose that c 1 ∈ R 2 , c 2 ∈ R 2 are the body centers of two overlapping people. If their Euclidean distance d cm and Gaussian kernel sizes k 1 , k 2 satisfy d cm &lt; k 1 + k 2 + 1, the repulsion is triggered to push the close centers apart viâ</p><formula xml:id="formula_2">c 1 = c 1 + γd p ,ĉ 2 = c 2 − γd p , d p = k 1 + k 2 + 1 − d cm d cm (c 1 − c 2 ),<label>(2)</label></formula><p>where d p is the repulsion vector from c 2 to c 1 and γ is an intensity coefficient to adjust the strength.</p><p>During training, we use CAR to push apart close body centers to supervise the Body Center heatmap. In this way, the model is encouraged to estimate the centers that maintain a distinguishable distance. For the Body Center heatmap, it helps the model to effectively locate the occluded person. For the Mesh Parameter map, sampling the parameters from these shifted locations enables the model to extract diverse and individual features for each person. The model trained with CAR is more suitable for crowded scenes with significant person-person occlusion such as train stations, canteens, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Parameter Sampling</head><p>To estimate 3D human body meshes from the image, we need to first parse the 2D body center coordinates c ∈ R K×2 from C m and then use them to sample P m for the SMPL parameters. In this section, we introduce the process of the center parsing, matching and sampling.</p><p>C m is a probability map whose local maxima are regarded as the body centers. The local maxima are derived via M p(C m ) ∧ C m where M p is the max pooling operation and ∧ is the logical conjunction operation. Let c be the 2D coordinates of a local maximum with confidence score larger than a threshold t c . We rank the confidence score at each c and take the top N as the final centers. During inference, we directly sample the parameters from P m at c. During training, the estimated c are matched with the nearest ground truth body center according to the L 2 distance.</p><p>Additionally, we approximate the depth order between multiple people by using the center confidence from C m and the 2D body scale s of the camera parameters from A m . For people of different s, we regard the one with a larger s located in the front. For people of similar s, the person with a higher center confidence is considered to be in the front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss functions</head><p>To supervise ROMP, we develop individual loss functions for different maps. In total, ROMP is supervised by the weighted sum of the body center loss L c and mesh parameter loss L p .</p><p>Body Center Loss. L c encourages a high confidence value at the body center c of the Body Center heatmap C m and low confidence elsewhere. To deal with the imbalance between the center location and the non-center locations in C m , we train the Body Center heatmap based on the focal loss <ref type="bibr" target="#b25">[26]</ref>. Given the predicted Body Center heatmap C p m and the ground truth C gt m , L c is defined as</p><formula xml:id="formula_3">L c = − L pos + L neg I pos w c , L neg = log(1 − C p m )(C p m ) 2 (1 − C gt m ) 4 (1 − I pos ), L pos = log(C p m )(1 − C p m ) 2 I pos , I pos = C gt m ≥ 1,<label>(3)</label></formula><p>where I pos is a binary matrix with a positive value at the body center location, and w c is the loss weight. Mesh Parameter Loss. As we introduced in Sec. 3.4, the parameter sampling process matches each ground truth body with a predicted parameter result for supervision. The mesh parameter loss is derived as</p><formula xml:id="formula_4">L p = w pose L pose + w shape L shape + w j3d L j3d +w paj3d L paj3d + w pj2d L pj2d + w prior L prior .<label>(4)</label></formula><p>L pose is the L 2 loss of the pose parameters in the 3 × 3 rotation matrix format. L shape is the L 2 loss of the shape parameters. L j3d is the L 2 loss of the 3D joints J regressed from the body mesh M . L paj3d is the L 2 loss of the 3D joints J after Procrustes alignment. L pj2d is the L 2 loss of the projected 2D joints J . L prior is the Mixture Gaussian prior loss of the SMPL parameters adopted in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref> for supervising the plausibility of 3D joint rotation and body shape. Lastly, w (.) denotes the corresponding loss weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the implementation details and experimental settings. Then compare ROMP with the SOTA approaches on multiple benchmarks. We further conduct ablation study of the proposed CAR and finally discuss the source of our performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Network Architecture. For a fair comparison with other approaches, we use ResNet-50 <ref type="bibr" target="#b9">[10]</ref> as the default backbone. Since our method is not limited to a specific backbone, we also test HRNet-32 <ref type="bibr" target="#b6">[7]</ref> in the experiments. Through the backbone, a feature vector f b ∈ R 32×H b ×W b is extracted from the input single RGB image. Also, we adopt the CoordConv <ref type="bibr" target="#b27">[28]</ref> to enhance the spatial information at each body center. Therefore, the backbone feature f ∈ R 34×H b ×W b is the combination of a coordinate index </p><formula xml:id="formula_5">map ci ∈ R 2×H b ×W b and f b .</formula><p>Next, from f , three head networks are developed to estimate the Body Center, Camera, and SMPL maps. Each head network is composed of two ResNet blocks <ref type="bibr" target="#b9">[10]</ref> with batch normalization. More details of the architecture are in the supplementary material. Setting Details. The input images are resized to 512 × 512, keeping the same aspect ratio and padding with zeros. The size of the backbone feature is H b = W b = 128. The maximum person number N = 64 could be determined by the certain situation. The loss weights are set to w c = 200, w j3d = 360, w paj3d = 400, w pj2d = 420, w pose = 60, w shape = 1, and w prior = 1.6 to ensure that the weighted loss items are in the same magnitude. The threshold of Body Center heatmap is t c = 0.25 and the intensity coefficient of CAR is γ = 0.2.</p><p>Training Datasets. For a fair comparison with previous methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39]</ref>, the basic training datasets we used in experiments includes two 3D pose datasets (Hu-man3.6M <ref type="bibr" target="#b11">[12]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b29">[30]</ref>), one pseudo-label 3D dataset (UP <ref type="bibr" target="#b22">[23]</ref>) and four in-the-wild 2D pose datasets (MS COCO <ref type="bibr" target="#b26">[27]</ref>, MPII <ref type="bibr" target="#b1">[2]</ref>, LSP <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> and AICH <ref type="bibr" target="#b41">[42]</ref>) with their pseudo 3D annotations <ref type="bibr" target="#b20">[21]</ref> of part images.</p><p>Besides, to further explore the upper limit of performance, we also use additional training datasets, including two 3D pose datasets (MuCo-3DHP <ref type="bibr" target="#b29">[30]</ref> and OH <ref type="bibr" target="#b45">[46]</ref>), the pseudo-3D-label <ref type="bibr" target="#b16">[17]</ref> of part 2D pose datasets, and two 2D pose datasets (PoseTrack <ref type="bibr" target="#b0">[1]</ref> and Crowdpose <ref type="bibr" target="#b24">[25]</ref>), to train an advanced model. Evaluation Benchmarks. 3DPW <ref type="bibr" target="#b40">[41]</ref> is employed as the main benchmark for evaluating 3D mesh/joint error since it contains in-the-wild multi-person videos with abundant 2D/3D annotations, such as 2D pose, 3D pose, SMPL parameters, human 3D mesh, etc. Specially, we divide 3DPW into 3 subsets, including 3DPW-PC for personperson occlusion, 3DPW-OC for object occlusion, and 3DPW-NC for the non-occluded/truncated cases, to evaluate the performance in different scenarios. Additionally, to compare with previous multi-person SOTA approaches, we also evaluate on CMU Panoptic <ref type="bibr" target="#b15">[16]</ref>, a standard multiperson 3D pose benchmark captured in a lab environment. Furthermore, we evaluate the stability under object/personperson occlusion on 3DOH50K <ref type="bibr" target="#b45">[46]</ref>, a object-occluded single-person indoor 3D benchmark, and Crowdpose <ref type="bibr" target="#b24">[25]</ref>, a crowded-person in-the-wild 2D pose benchmark.</p><p>Evaluation Metrics. We adopt per-vertex error (PVE) to evaluate the 3D surface error. To evaluate the 3D pose accuracy, we employ mean per joint position error (MPJPE), Procrustes-aligned MPJPE (PMPJPE), percentage of correct keypoints (PCK), area under the PCK-threshold curve (AUC). We also adopt mean per joint angle error (MPJAE), and Procrustes-aligned MPJAE (PA-MPJAE) to evaluate the 3D joint rotation accuracy. Besides, to evaluate the pose accuracy in crowded scenes, we calculate the average precision (AP 0.5 ) between the 2D-projection J and the ground truth 2D poses on Crowdpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons to the State-of-the-Art</head><p>3DPW. We adopt three evaluation protocols to compare from different aspects. To validate the performance in actual scenes, we follow Protocol 1 from the 3DPW Challenge to evaluate the entire 3DPW dataset without using any <ref type="table">Table 4</ref>. Comparisons to the SOTA methods on CMU Panoptic <ref type="bibr" target="#b15">[16]</ref> benchmark. The evaluation metric is MPJPE after centering the root joint. All methods are directly evaluated without any fine-tuning. means using extra datasets for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Haggling Mafia Ultim. Pizza Mean Zanfir et. al. <ref type="bibr" target="#b44">[45]</ref> 141  <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> with a human detector (OpenPose <ref type="bibr" target="#b5">[6]</ref> or YOLO <ref type="bibr" target="#b35">[36]</ref>). For a fair comparison, ROMP uses the same backbone (ResNet-50) and training datasets as the competing SOTA method <ref type="bibr" target="#b20">[21]</ref>. We obtain the results of OpenPose + SPIN from <ref type="bibr" target="#b10">[11]</ref>. The results of YOLO + VIBE are obtained using their officially released code, which already contains the YOLO part for human detection. In Tab. 1, ROMP outperforms previous multi-stage methods in all evaluation metrics by a significant amount. We observe significant improvement in MPJPE, PMPJPE, and MPJAE metrics. These results validate that learning pixel-level representation with a holistic view is vital for improving the robustness and generalization in actual scenes. Results of training with extra datasets show that ROMP could be further improved via adding the training data. As a sanity check, we compare ROMP with the singleperson approaches using their evaluation protocols, which allows the use of the cropped single-person image as input. While as a multi-person method, ROMP still takes the whole image as input. Following VIBE <ref type="bibr" target="#b19">[20]</ref>, Protocol 2 directly uses the 3DPW test set for evaluation without finetuning on the training set, while Protocol 3 fine-tunes the model on the 3DPW training set and utilizes the test set for evaluation. In Tab. 2, ROMP outperforms previous SOTA methods on Protocol 2, which further demonstrates the superior of our one-stage design. In Tab. 3, ROMP achieves comparable results with the SOTA methods. Note that, if we  replace the ResNet-50 with HRNet-32 which extracts multiresolution features, superior performance could be achieved after fine-tuning. While in Tab. 1 and 4, the model using HRNet-32 as backbone only achieves comparable results with the one using ResNet-50. These results show that using a heavier backbone helps to fit a specific domain.</p><p>CMU Panoptic. Following the evaluation protocol of CRMH <ref type="bibr" target="#b12">[13]</ref>, we evaluate ROMP on the multi-person benchmarks, CMU Panoptic, without any fine-tuning. For a fair comparison, we use the same backbone and similar training set as CRMH. As shown in Tab. 4 , ROMP outperforms the existing multi-stage methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> in all activities by a large margin. These results further demonstrate that learning pixel-level representation with a holistic view improves the performance in the multi-person scenes.</p><p>Occlusion benchmarks. To validate the stability under occlusion, we evaluate ROMP on multiple occlusion benchmarks. Firstly, on the person-occluded 3DPW-PC and Crowdpose <ref type="bibr" target="#b24">[25]</ref>, results in Tab. 5 (second column) and 6 show that ROMP significantly outperforms previous SOTA methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>. Additionally, in <ref type="figure" target="#fig_2">Figure 5</ref>, some qualitative comparisons to the multi-person SOTA methods in the person-person occlusion scenes also demonstrate our robustness. These results suggest that the pixel-level representation is important for improving the performance under person-person occlusion. Finally, on the object-occluded 3DOH50K <ref type="bibr" target="#b45">[46]</ref> and 3DPW-OC, ROMP also achieves superior performance in Tab. 7. These results demonstrate that the fine-grained pixel-level representation is beneficial for dealing with various occlusion cases. Runtime. To validate the computational efficiency, we compare ROMP with the SOTA methods in processing videos captured by a web camera. As shown in Tab. 8, ROMP achieves real-time performance (30.9 FPS using ResNet-50 and 20.8 FPS using HRNet-32), significantly faster than the competing methods. Additionally, as shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, compared with the multi-stage methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>, ROMP's processing time is roughly constant regardless of the number of people. Please refer to the supplementary material for more experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study of the CAR</head><p>We perform the ablation study of the proposed CAR on person-occluded 3DPW-PC and Crowdpose. As the results shown in Tab. 5 and 6, the proposed CAR improve the PM-PJPE metric on the 3DPW-PC and the Crowdpose datasets by 4.8% and 10.3%, respectively. Except for the quantitative results of the proposed CAR, we also provide the qualitative ablation study in <ref type="figure" target="#fig_3">Figure 6</ref>. Both results show that adding the proposed CAR further improves the performance, which demonstrates that CAR effectively tackles the center collision problem in the crowded scenes.</p><p>Intensity coefficient γ of the CAR. To figure out the proper setting of the intensity coefficient γ, we conduct a further ablation study on 3DPW-PC. Results in Tab. 5 show that if γ is too large, the 3D pose accuracy on 3DPW-PC decreases. Therefore, we adopt γ = 0.2 to balance between the distinguishable center and consistent representation learning. Further, in Tab. 5, we observe varying degrees of performance degradation by adding the CAR in these cases without strong person-person occlusion. It indicates the reason of performance degradation is probably that pushing apart the body centers affects the consistency of the body-center-guided representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>To determine the source of our performance gains, we conduct an ablation study on 3DPW subsets containing different scenes. Compared with the SOTA methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref> in Tab. 5, our main gains come from the person-occluded and the non-occluded/truncated cases. These results show that the proposed pixel-level representation can promote more effective learning of the person-person occlusion and truncation cases. Additionally, as introduced in Sec. 4.2, ROMP achieves superior results on multiple benchmarks without fine-tuning, which demonstrates our robustness and generalization in various actual scenes. Through controlling the variables (e.g., backbone, training settings, etc.) in the experiments, we narrow the difference between ROMP and the SOTA methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> to the way of representation learning. ROMP learns the pixel-level representation in a holistic view, while the multi-stage SOTA methods learn the bounding-box-level representation in a local view. Except for the guidance of the robust body centers, our fully convolutional design promotes ROMP to learn more discriminative features against rich disturbances outside the bounding box, which is important for better generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce a novel one-stage network, ROMP, for monocular multi-person 3D mesh regression. For pixellevel estimation, we propose an explicit body-center-guided representation and further develop it as a collision-aware version, CAR, which enables robust prediction under the person-person occlusion. ROMP is the first open-source one-stage method that achieves SOTA performance on multiple benchmarks as well as real-time inference speed. For the community, ROMP could be a simple yet effective baseline for the related multi-person 3D tasks, such as depth estimation, tracking, and interaction modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Collision-Aware Repulsion (CAR). The body centers of overlapping people are treated as positive charges, which are pushed apart if they are too close in the repulsion field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The FPS variations of ROMP and YOLO+VIBE<ref type="bibr" target="#b19">[20]</ref> when processing images with different number of people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparisons to the multi-person SOTA method, CRMH<ref type="bibr" target="#b12">[13]</ref>, on the Crowdpose and the internet images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative ablation study of the CAR on the Crowdpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>a heatmap representing the 2D human body center in the image. Each body center is represented as a Gaussian distribution in the Body Center heatmap. For better representation learning, the Body Center heatmap also integrates the scale information of the body in the 2D image. Specifically, we calculate the Gaussian kernel size k of each person center in terms of its 2D body scale in the image. Given the diagonal length d bb of the person bounding box and the width W of the Body Center heatmap, the Gaussian kernel size is derived from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparisons to the SOTA methods on 3DPW following Protocol 1 (without using any ground truth during inference). means using extra datasets for training. Comparisons to the SOTA methods on 3DPW following VIBE<ref type="bibr" target="#b19">[20]</ref>, using Protocol 2 (on the test set only). means using extra datasets (compared with SPIN) for training.</figDesc><table><row><cell cols="2">Method</cell><cell cols="5">MPJPE↓ PMPJPE↓ PCK↑ AUC↑ MPJAE↓ PMPJAE↓</cell></row><row><cell cols="4">OpenPose + SPIN [21] 95.8</cell><cell>66.4</cell><cell>33.3 55.0</cell><cell>23.9</cell><cell>24.4</cell></row><row><cell cols="2">YOLO + VIBE [20]</cell><cell></cell><cell>94.7</cell><cell>66.1</cell><cell>33.9 56.6</cell><cell>25.2</cell><cell>20.46</cell></row><row><cell cols="2">CRMH [13]</cell><cell></cell><cell>105.9</cell><cell>71.8</cell><cell>28.5 51.4</cell><cell>26.4</cell><cell>22.0</cell></row><row><cell cols="2">ROMP (ResNet-50)</cell><cell></cell><cell>87.0</cell><cell>62.0</cell><cell>34.4 57.6</cell><cell>21.9</cell><cell>20.1</cell></row><row><cell cols="2">ROMP (ResNet-50)</cell><cell></cell><cell>80.1</cell><cell>56.8</cell><cell>36.4 60.1</cell><cell>20.8</cell><cell>19.1</cell></row><row><cell cols="2">ROMP (HRNet-32)</cell><cell></cell><cell>82.7</cell><cell>60.5</cell><cell>36.5 59.7</cell><cell>20.5</cell><cell>18.9</cell></row><row><cell>Method</cell><cell cols="3">MPJPE↓ PMPJPE↓ PVE↓</cell><cell></cell><cell></cell></row><row><cell>HMR [18]</cell><cell>130.0</cell><cell>76.7</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell cols="2">Kanazawa et al. [19] 116.5</cell><cell>72.6</cell><cell>139.3</cell><cell></cell><cell></cell></row><row><cell>Arnab et al. [3]</cell><cell>-</cell><cell>72.2</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>GCMR [22]</cell><cell>-</cell><cell>70.2</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>DSD-SATN [39]</cell><cell>-</cell><cell>69.5</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>SPIN [21]</cell><cell>96.9</cell><cell>59.2</cell><cell>116.4</cell><cell></cell><cell></cell></row><row><cell>ROMP (ResNet-50)</cell><cell>91.3</cell><cell>54.9</cell><cell>108.3</cell><cell></cell><cell></cell></row><row><cell>I2L-MeshNet [33]</cell><cell>93.2</cell><cell>58.6</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>EFT [17]</cell><cell>-</cell><cell>54.2</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>VIBE [20]</cell><cell>93.5</cell><cell>56.5</cell><cell>113.4</cell><cell></cell><cell></cell></row><row><cell>ROMP (ResNet-50)</cell><cell>89.3</cell><cell>53.5</cell><cell>105.6</cell><cell></cell><cell></cell></row><row><cell>ROMP (HRNet-32)</cell><cell>85.5</cell><cell>53.3</cell><cell>103.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons to the SOTA methods on 3DPW following Protocol 3 (fine-tuned on the training set). means using extra datasets (compared with EFT) for training.</figDesc><table><row><cell>Method</cell><cell cols="3">MPJPE↓ PMPJPE↓ PVE↓</cell></row><row><cell>EFT [17]</cell><cell>-</cell><cell>52.2</cell><cell>-</cell></row><row><cell>VIBE [20]</cell><cell>82.9</cell><cell>51.9</cell><cell>99.1</cell></row><row><cell>ROMP (ResNet-50)</cell><cell>84.2</cell><cell>51.9</cell><cell>100.4</cell></row><row><cell>ROMP (HRNet-32)</cell><cell>78.8</cell><cell>48.3</cell><cell>94.3</cell></row><row><cell>ROMP (HRNet-32)</cell><cell>76.7</cell><cell>47.3</cell><cell>93.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 7 .Table 8 .</head><label>678</label><figDesc>Comparisons to the SOTA methods on Crowdpose<ref type="bibr" target="#b24">[25]</ref> benchmark. The evaluation metric is AP 0.5 . Comparisons to the SOTA methods on object occlusion benchmark, 3DOH50K. Run-time comparisons on a 1070Ti GPU.</figDesc><table><row><cell>Split</cell><cell cols="5">CRMH [13] ROMP ROMP+CAR</cell></row><row><cell>Test</cell><cell></cell><cell>33.9</cell><cell>54.1</cell><cell>59.7</cell></row><row><cell cols="2">Validation</cell><cell>32.9</cell><cell>55.6</cell><cell>58.6</cell></row><row><cell cols="6">Method SPIN [22] OOH [46] CRMH [13] ROMP</cell></row><row><cell>PMPJPE</cell><cell>67.5</cell><cell>58.5</cell><cell></cell><cell>56.9</cell><cell>43.9</cell></row><row><cell cols="5">Method VIBE [20] CRMH [13] ROMP</cell><cell>ROMP</cell></row><row><cell>FPS</cell><cell>10.9</cell><cell>14.1</cell><cell></cell><cell>20.8</cell><cell>30.9</cell></row><row><cell cols="6">Backbone ResNet-50 ResNet-50 HRNet-32 ResNet-50</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PandaNet: Anchor-based single-shot multi-person 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Benzine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Luvison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Achard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">HigherHRNet: Scaleaware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HoloPose: Holistic 3D human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Beyond weak perspective for monocular 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kissos</forename><surname>Imry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fritz</forename><surname>Lior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goldman</forename><surname>Matan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meir</forename><surname>Omer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oks</forename><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kliger</forename><surname>Mark</surname></persName>
		</author>
		<idno>ECCVW, 2020. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exemplar fine-tuning for 3D human pose fitting towards in-thewild 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning 3D human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CrowdPose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3D pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3D multiperson pose estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-lixel prediction network for accurate 3D human pose and mesh estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TexturePose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2D and 3D pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ai challenger: A large-scale dataset for going deeper in image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DenseRaC: Joint 3D pose and shape estimation by dense render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3D sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Objectoccluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SMAP: Single-shot multiperson absolute 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jingwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Jimei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detailed human shape estimation from a single image by hierarchical mesh deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
