<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Context Aggregation Network for Temporal Action Proposal Refinement Candidate Proposal Segment-Level Regression Output Frame-Level Regression Output Fused Output LGTE TBR Background Skiing Background Segment-Level Regression Start-Boundary Regression End-Boundary Regression Internal Context Starting Context Ending Context Encoded Feature Local Temporal Encoder Global Temporal Encoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
							<email>suhaisheng@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
							<email>ganweihao@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Wang</surname></persName>
							<email>wangdongliang@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
							<email>wuwei@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
							<email>yanjunjie@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
							<email>nsang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Context Aggregation Network for Temporal Action Proposal Refinement Candidate Proposal Segment-Level Regression Output Frame-Level Regression Output Fused Output LGTE TBR Background Skiing Background Segment-Level Regression Start-Boundary Regression End-Boundary Regression Internal Context Starting Context Ending Context Encoded Feature Local Temporal Encoder Global Temporal Encoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* The work was done during an internship at SenseTime. † Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action proposal generation aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet important task in the video understanding field. The proposals generated by current methods still suffer from inaccurate temporal boundaries and inferior confidence used for retrieval owing to the lack of efficient temporal modeling and effective boundary context utilization. In this paper, we propose Temporal Context Aggregation Network (TCANet) to generate high-quality action proposals through "local and global" temporal context aggregation and complementary as well as progressive boundary refinement. Specifically, we first design a Local-Global Temporal Encoder (LGTE), which adopts the channel grouping strategy to efficiently encode both "local and global" temporal inter-dependencies. Furthermore, both the boundary and internal context of proposals are adopted for framelevel and segment-level boundary regressions, respectively. Temporal Boundary Regressor (TBR) is designed to combine these two regression granularities in an end-to-end fashion, which achieves the precise boundaries and reliable confidence of proposals through progressive refinement. Extensive experiments are conducted on three challenging datasets: HACS, ActivityNet-v1.3, and THUMOS-14, where TCANet can generate proposals with high precision and recall. By combining with the existing action classifier, TCANet can obtain remarkable temporal action detection performance compared with other methods. Not surprisingly, the proposed TCANet won the 1 st place in the CVPR 2020 -HACS challenge leaderboard on temporal action localization task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The temporal action detection task requires locating the starting and ending time of action instances from long untrimmed videos and classifying the actions. This task can be applied to many fields, such as video content analysis and video recommendation. Most existing temporal action detection methods follow a two-stage scheme <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b25">26]</ref>, namely temporal action proposal generation and classification. Although action recognition methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b4">5]</ref> have achieved impressive classification accuracy, the temporal action detection performance is still unsatisfactory in several mainstream benchmarks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b44">45]</ref>. Hence, many researchers target improving the quality of temporal action proposals.</p><p>Current proposal generation methods can be mainly divided into two steps. First, the temporal relationship is captured by stacked temporal convolutions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29]</ref> or global temporal pooling operations <ref type="bibr" target="#b7">[8]</ref>. Then proposals are further generated by the boundary-based regression methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref> or the anchor-based regression methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8]</ref>. However, existing methods share the following drawbacks: (1) Neither convolution nor global fusion can effectively model the temporal relationship. The 1D convolution operations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref> lack flexibility in encoding long-term temporal relationships limited by kernel size. The global fusion methods <ref type="bibr" target="#b7">[8]</ref> neglect various global dependencies for each temporal location and the implicit attention to local details, such as local details of boundaries. Besides, simply collecting global features through average pooling may introduce unnecessary noise. (2) Only the internal context or boundary context of proposals used for regression is inferior to generate proposals with precise boundaries. The internal context of proposals adopted in anchor-based methods can obtain reliable confidence scores but fails to generate precise boundaries. On the contrary, the boundary context of proposals considered in boundarybased methods is sensitive to boundary changes but generates proposals with inferior proposal-level confidence.</p><p>To relieve these issues, we propose Temporal Context Aggregation Network (TCANet) for high-quality proposal generation, as shown in <ref type="figure">Figure 1</ref>. First, the Local-Global Temporal Encoder (LGTE) is proposed to simultaneously capture local and global temporal relationships in a channel grouping fashion, which contains two main sub-modules. Specifically, the input features after linear transformation are equally divided into N groups along the channel dimension. Then Local Temporal Encoder (LTE) is designed to handle the first A groups for local temporal modeling. At the same time, the remaining N − A groups are captured by the Global Temporal Encoder (GTE) for global information perception. In this way, LGTE is expected to integrate the long-term context of proposals by global groups while recovering more structure and detailed information by local groups. Second, the Temporal Boundary Regressor (TBR) is proposed to exploit both boundary context and internal context of proposals for frame-level and segmentlevel boundary regressions, respectively. Concretely, the frame-level boundary regression aims to refine the starting and ending locations of candidate proposals with boundary sensitivity, while the segment-level boundary regression aims to refine the center location and duration of proposals under the overall perception of proposals. Finally, highquality proposals are obtained through complementary fusion and progressive boundary refinements.</p><p>In summary, our contributions mainly have three folds:</p><p>• We design a Local-Global Temporal Encoder to simultaneously capture local and global temporal relationships in a channel grouping fashion. It can be easily embedded into any other proposal generation frameworks for efficient temporal relationships modeling.</p><p>• Temporal Boundary Regressor is proposed to perform complementary and progressive boundary refinements, including the local frame-level boundary regression and global segment-level boundary regression.</p><p>• Extensive experiments reveal that TCANet can obtain convincing proposals performance on several benchmarks: HACS, ActivityNet-v1.3, and THUMOS-14.</p><p>By combining with the existing classifier, TCANet can achieve remarkable temporal action detection performance compared with other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action Recognition. Action recognition is an important task in video understanding area, which is in need of spatio-temporal information modeling. Current deep learning-based action recognition methods can be mainly divided into three types. The first type is 2stream networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b5">6]</ref>, which adopt RGB frames and optical flow to capture appearance and motion information. The second type <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b31">32]</ref> directly captures spatio-temporal information from raw videos using 3D convolution. The third type aims to efficiently model spatio-temporal features by decoupled (2 + 1) D convolutions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28]</ref>. In this work, 2stream and SlowFast are adopted to encode the input video features. Temporal Action Proposal Generation and Detection. Current proposal generation methods can be mainly divided into anchor-based and boundary-based methods. The anchor-based methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref> refer to the temporal boundary refinements of sliding windows or pre-defined anchors. Among them, TURN <ref type="bibr" target="#b8">[9]</ref> and CTAP <ref type="bibr" target="#b6">[7]</ref> directly concatenate the boundary context and internal context of proposals for boundary refinements (i.e., starting and ending locations), while other methods aim to refine the duration and center location of proposals. However, refinements on boundary locations only cannot make full use of contextual information of proposals, while mere refinement of duration and center location of candidate proposals would also neglect the local boundary details. Therefore, it is nontrivial to combine these two regression granularities into a unified framework. Boundary-based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref> first generate the boundary probability sequence, then apply the Boundary Matching mechanism to generate candidate proposals. MGG <ref type="bibr" target="#b20">[21]</ref> simply combines a boundary-based stream and anchor-based stream with a shared backbone to extract features, then each stream is optimized independently, and the results are fused during the inference. In our work, we make full use of boundary context and internal context of proposals to predict the frame-level offsets (i.e., starting and ending) and the segment-level offsets (i.e., center and duration), respectively. Meanwhile, we jointly train LGTE is employed to capture local-global temporal inter-dependencies simultaneously. TBR is adopted to perform frame-level and segment-level boundary regressions, respectively. Finally, high-quality proposals are obtained through complementary fusion and progressive boundary refinements.</p><p>these two granularities with supervision performed on the combined results. Finally, complementary and progressive boundary refinements are conducted for better performance.</p><p>Self-Attention Mechanism. The self-attention <ref type="bibr" target="#b37">[38]</ref> mechanism is widely used in the video understanding area since it can effectively capture long-term dependencies compared with other attention methods such as recurrent models <ref type="bibr" target="#b22">[23]</ref> and pooling methods <ref type="bibr" target="#b11">[12]</ref>. The Transformer <ref type="bibr" target="#b32">[33]</ref> is also based on the self-attention mechanism, which is originally applied in the machine translation task. Girdhar et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref> adopt Transformer to capture the interactions between human and objects existing in videos. In this paper, we propose Local-Temporal Global Encoder, which can efficiently capture both "local and global" temporal relationships and then integrate rich contexts into extracted video features for temporal proposals generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TCANet</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we propose Temporal Context Aggregation Network (TCANet) to generate high-quality proposals, which mainly consists of two main modules: Local-Global Temporal Encoder and Temporal Boundary Regressor. Firstly, the Local-Global Temporal Encoder (LGTE) is adopted to simultaneously encode the input video features' local and global temporal relationships. Then the Temporal Boundary Regressor (TBR) is utilized to refine the boundaries of the proposals by exploiting both boundary and internal context for frame-level and segmentlevel boundary regressions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>For an untrimmed video X with l frames, we can denote it as</p><formula xml:id="formula_0">X = {x i } l i=1</formula><p>, where x i is the i-th frame of the video. Temporal proposal generation task is to generate a set of proposals P = {t sj , t ej } Np j=1 that may contain action instances for video X, where t sj and t ej are the starting time and ending time of the j-th proposal, and N p is the number of proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Encoding</head><p>For a given video, features are extracted by SlowFast <ref type="bibr" target="#b4">[5]</ref> and 2stream <ref type="bibr" target="#b24">[25]</ref> since their excellent performance on the  <ref type="figure">Figure 3</ref>. The detailed structure of Local-Global Temporal Encoder (LGTE). The input features are divided into N groups along the channel dimension. Then the first A groups are fed to A Local Temporal Encoders (LTE) separately, where the local dynamic modeling is achieved by calculating the regional attention for each temporal location. The remaining (N − A) groups are adopted by (N − A) Global Temporal Encoders (GTE) separately to calculate the similarity between each location and global feature sequence. Wo is a learnable matrix, and Feed Forward Network (FFN) is a nonlinear projection function. video classification task. The frame rate of videos is set to r fps, and each snippet contains s frames. Each snippet is encoded into a visual feature f i ∈ R C by a feature extractor. Given an untrimmed video, a video feature sequence of</p><formula xml:id="formula_1">T C ( ) × Softmax Scale × C o … ( ) ( ) T Softmax . Scale LTE for location GTE Group 1 Group N × Matrix Multiplication C Concat T Transpose .</formula><formula xml:id="formula_2">F = {f i } T i=1 ∈ R T ×C is obtained by this method, where T = l/δ, l</formula><p>is the total number of video frames, and δ is the number of frames interval between different snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Local-Global Temporal Encoder</head><p>For long videos, long-term temporal dependency modeling is essential, proven by many previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref>. Nonlocal <ref type="bibr" target="#b37">[38]</ref> is often applied to obtain the relationship between different global locations. However, global modeling only is easy to introduce global noise and insensitive to small boundary changes. We propose a local and global joint modeling strategy to alleviate this problem, as shown in <ref type="figure">Figure 3</ref>. Local Temporal Encoder (LTE) is responsible for cap-turing local dependencies based on local details dynamically. Precisely, to measure the relationship between temporal location i and its local areas, the cosine similarity between two temporal locations is adopted to generate similarity vector S l i and weight vector W l i :</p><formula xml:id="formula_3">S l i = γ l (f i )·(ρ l ([(f i− w/2 ) T , ···(f i+ w/2 ) T ] T )) T ∈ R 1×w (1) W l i = Softmax( S l i √ C )<label>(2)</label></formula><p>where C is the number of channels, w is the size of the modeling area for location i, which is defined as W indowSize.</p><p>For example, the value of w in <ref type="figure">Figure 3</ref> LTE is 3. γ l and ρ l are two different linear projection functions that map the input feature vectors to the similarity measure space. With equation 1, the relationship between each location and its corresponding modeling area can be calculated. To achieve local information exchange, the following formula will be utilized to collect local context information from the corresponding local area dynamically:</p><formula xml:id="formula_4">f l i = W l i · (ϕ l ([(f i− w/2 ) T , · · ·(f i+ w/2 ) T ] T )),<label>(3)</label></formula><p>where f l i represents the new expression of location i, and ϕ l is a linear projection function. Global Temporal Encoder (GTE) is designed to model the long-term temporal dependencies of videos. Compared with LTE, GTE needs to aggregate global interactions for each location on the temporal dimension. Therefore, the relationship between each location and the global feature is written as follows:</p><formula xml:id="formula_5">S g i = γ g (f i ) · (ρ g (F )) T ∈ R 1×T ,<label>(4)</label></formula><formula xml:id="formula_6">W g i = Softmax( S g i √ C ),<label>(5)</label></formula><p>where γ g and ρ g are two different linear projection functions. The global interaction feature of location i can be updated by weight vector W g i :</p><formula xml:id="formula_7">f g i = W g i · (ϕ g (F )),<label>(6)</label></formula><p>where f g i represents the new global feature representation of location i, and ϕ g is a linear projection function. Local-Global Temporal Encoder (LGTE). Each location in the video feature sequence can be modeled locally and globally by LTE and GTE, respectively. However, it is inefficient to combine them in the form of "LTE-GTE" simply. To solve this problem, LGTE is implemented in a channel grouping fashion. Specifically, as shown in <ref type="figure">Figure 3</ref>, the input feature is first projected by γ, ρ, and ϕ. These outputs are then divided into N groups along the channel dimension. Hence the channel number of each group is C/N . The first  <ref type="figure">Figure 4</ref>. The detailed structure of Temporal Boundary Regressor (TBR). For each input proposal, TBR collects its starting context Fs, internal context Fc and ending context Fe through Temporal Roi Align (TRA). Fs and Fe are adopted to refine the starting and ending locations, respectively. Fs, Fc, and Fe are concatenated to refine the center location and the duration of proposals. Finally, the accurate proposals are obtained by fusing these two outputs.</p><p>groups are fed to GTEs. For location i, the combined output of local and global features can be written as:</p><formula xml:id="formula_8">f a i = [(f l 1i ) T , · · ·(f l Ai ) T , (f g (A+1)i ) T · · · (f g N i ) T ] T · W o ,<label>(7)</label></formula><formula xml:id="formula_9">f b i = LayerNorm(f a i ) + f a i ,<label>(8)</label></formula><formula xml:id="formula_10">f i = LayerNorm(FFN(f b i ) + f b i ),<label>(9)</label></formula><p>where W o is a learnable parameter matrix. Inspired by Transformer <ref type="bibr" target="#b32">[33]</ref>, FFN is adopted to capture the interaction of features among different groups at i-th temporal location:</p><formula xml:id="formula_11">FFN(x) = ReLu(x · W 1 + b 1 ) · W 2 + b 2 .</formula><p>Discussion. We notice that our LTE is similar to the convolution with fixed kernels. However, the dynamic local interaction modeling for each temporal location is unique for better adaptation to complex temporal changes than conventional convolution. Furthermore, the combination of LTE and GTE enables our LGTE to capture the global dependencies of whole videos and dynamically model local changes with less noise. Besides, the channel grouping fashion ensures high computing efficiency and the diversity of "local and global" relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Temporal Boundary Regressor</head><p>Anchor-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8]</ref> leverage the internal context of proposals to regress center location and duration, which can obtain reliable scores but with lower recall. Boundary-based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref> only utilize local boundary context to locate the boundaries, which are sensitive to boundaries but with inferior confidence. Therefore, we propose to combine the boundary context-based frame-level regression and the internal context-based segment-level regression to refine the boundaries. Complementary Regression Strategy. As shown in <ref type="figure">Figure 4</ref>, the feature of one proposal is divided into three parts: the starting context F s , the internal context F c , and the ending context F e . To achieve frame-level regression, F s and F e are utilized to regress the boundary offset ∆ŝ and ∆ê of the starting time and ending time, respectively:</p><formula xml:id="formula_12">{∆ŝ, ∆ê} = Conv1d(ReLu(Conv1d({F s , F e }))) (10)</formula><p>The boundary offsets ∆ŝ and ∆ê obtained by this method only utilize the starting and ending local features of proposals. It can effectively reduce noise interference and is more sensitive to the boundary position.</p><p>However, only using the local features of the boundary will lose the global context of proposals. Therefore, F s , F c and F e are utilized to achieve segment-level regression, which jointly regress the center location offset ∆x and duration offset ∆ŵ of the proposals:</p><formula xml:id="formula_13">F a = [F s , F c , F e ],<label>(11)</label></formula><formula xml:id="formula_14">{∆x, ∆ŵ, p conf } = Conv1d(ReLu(Conv1d(F a ))),<label>(12)</label></formula><p>By means of ∆ŝ, ∆ê, ∆x and ∆ŵ, two new proposals (ŝ 1 ,ê 1 ) and (ŝ 2 ,ê 2 ) can be obtained:</p><formula xml:id="formula_15">s 1 = s p − ∆ŝw p ,ê 1 = e p − ∆êw p ,<label>(13)</label></formula><formula xml:id="formula_16">x 2 = x p − ∆xw p ,ŵ 2 = w p e ∆ŵ ,<label>(14)</label></formula><formula xml:id="formula_17">s 2 =x 2 −ŵ 2 /2,ê 2 =x 2 +ŵ 2 /2,<label>(15)</label></formula><p>where w p = e p − s p , denotes the length of the proposals. Finally, the two new proposals will be fused as the final proposals prediction of TBR:</p><formula xml:id="formula_18">s = τŝ 1 + (1 − τ )ŝ 2 ,ê = τê 1 + (1 − τ )ê 2 ,<label>(16)</label></formula><p>where τ is a fusion parameter, we set it to 0.5 empirically. Progressive Refinement. To achieve more accurate boundaries of candidate proposals, a progressive refinement strategy is adopted to generate high-quality proposals from coarse to fine. In ablation experiments, we will explore the impact of the number of TBRs on proposal performance. Discussion. The boundary features based frame-level regression is sensitive to the local changes, which are helpful to detect the action boundaries caused by shot switching. And the internal proposal features based segment-level regression has an overall perception of proposals suitable for detecting actions with indistinct boundaries. Therefore, these two features are complementary and essential for generalized action proposal generation. MGG <ref type="bibr" target="#b20">[21]</ref> proposes a multi-granularity generator to integrate boundary-based regression and anchor-based regression into a unified network with a shared backbone used for feature extraction.</p><p>However, these two regression streams are trained independently, and the results are fused directly during inference. On the contrary, the main idea of TBR is to adopt both boundary and internal context of proposals to predict the frame-level and segment-level offsets, respectively, but jointly train these two granularities with supervision performed on the combined results for gradient backpropagating. Meanwhile, complementary and progressive boundary refinements are conducted for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and Inference of TCANet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training</head><p>Proposals Selection. To better demonstrate the effectiveness of TCANet, we adopt the proposals output from the most competitive method(BMN) <ref type="bibr" target="#b17">[18]</ref> as our pool of candidates. In the training phase of TCANet, to make the network learning more efficient, Soft-NMS [1] is adopted to preprocess proposals output by BMN to reduce the redundant samples. Then the top 100 proposals are selected in descending order for training. Label Assignment. During the TBR training process, proposals with ground truth temporal Intersection-over-Union(tIoU) greater than a certain threshold i p are specified as positive samples, and ground truth tIoU less than a certain threshold i n as negative samples, and those between i n and i p Proposals are incomplete samples. The number of positive samples, negative samples and incomplete samples are defined as N pos , N incomp and N neg , respectively. Three kinds of samples are randomly sampled so that N pos : N incomp : N neg = 1 : 1 : 1 in training. Loss Function. The loss functions of the IoU prediction and the position regression of proposals are denoted as L iou and L reg , respectively. We denote L iou and L reg as: <ref type="bibr" target="#b17">(18)</ref> where</p><formula xml:id="formula_19">Liou = 1 Ntrain N train i=1 SmoothL1(p conf,i , giou,i) , (17) Lreg = 1 Npos     Npos i∈P os m∈{x,w, s,e} SmoothL1(r m i − g m i )     ,</formula><formula xml:id="formula_20">N train = N pos + N incomp + N neg , g x i = ∆x i , g w i = ∆w i , r x i = ∆x i , r w i = ∆ŵ i , g s i = ∆s i , g e i = ∆e i , r s i = ∆ŝ i , r e i = ∆ê i<label>(19)</label></formula><p>The final objective function is written as:</p><formula xml:id="formula_21">Loss = L iou + λL reg ,<label>(20)</label></formula><p>where λ is a balance parameter, we set it to 1.0 empirically. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inference</head><p>In inference, proposals utilized by TCANet should have a high recall rate. Therefore, proposals output by BMN <ref type="bibr" target="#b17">[18]</ref> are directly adopted as the input of TCANet. The final confidence of proposals are obtained by fusing the BMN score and TCANet score:</p><formula xml:id="formula_22">S proposal = S BM N * S T CAN et<label>(21)</label></formula><p>Finally, Soft-NMS <ref type="bibr" target="#b0">[1]</ref> is employed to remove redundant proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Setup</head><p>HACS <ref type="bibr" target="#b44">[45]</ref> is a large-scale dataset for temporal action detection. It contains 37.6k training, 6k validation, and 6k testing videos with 200 action categories. ActivityNet-v1.3 <ref type="bibr" target="#b1">[2]</ref> is a popular benchmark for temporal action detection. It contains 10k training, 5k validation, and 5k testing videos with 200 action categories. THUMOS14 <ref type="bibr" target="#b13">[14]</ref> contains 200 validation videos and 213 testing videos, including 20 action categories. In our experiments, we compare TCANet with the state-of-the-art method on all three datasets and performed ablation studies on HACS dataset. Evaluation Metrics. Average Recall (AR) is the average recall rate under specified tIoU thresholds for measuring the quality of proposals. On HACS and ActivityNet-v1.3, these thresholds are set to [0.5:0.05:0.95]. On THU- To reduce information loss, the lengths of the input feature sequence are not down-resized; hence each input sequence is fixed to 1000 and 1500 by zero-padding on HACS and ActivityNet-v1.3 for batch training, respectively. The Number of groups N and A in LGTE are empirically set to 8 and 4. The learning rates on these two datasets are set to 0.0004 and 0.001, and the batch size is 16 for 10 epochs. For THUMOS14 training, a sliding window with a size of 256 is adopted. We set the learning rate, batch size, and epoch number to 0.0004, 16 and 5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with State-of-the-art Results</head><p>This section will compare with the existing state-of-theart methods on HACS, ActivityNet-v1.3, and THUMOS14.  HACS. On HACS, TCANet is compared with the existing methods in <ref type="table" target="#tab_0">Table 1</ref> on the validation set. TCANet using only a single model significantly surpass the existing methods. Compared with the benchmark method BMN, TCANet's mAP is improved by 4%. ActivityNet-v1.3. <ref type="table">Table 2</ref> and <ref type="table" target="#tab_2">Table 5</ref> compare TCANet with other methods, where TCANet significantlys improve both the temporal action proposal and detection performance. For a fair comparison, TCANet is conducted on the 2stream features for experiments. Under the same settings, TCANet can also obtain 1.67% mAP improvement compared with BMN and significantly outperform other existing methods. THUMOS14. We compare TCANet with the state-of-theart methods on THUMOS14 in <ref type="table" target="#tab_1">Table 3</ref> and <ref type="table">Table 4</ref>. Since that our TCANet improves the Average Recall with the first several proposals, the detection performance are more improved than the recall rate. Especially, in <ref type="table" target="#tab_1">Table 3</ref>, when tIou=0.6, TCANet is 4.9% higher than BSN++ <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>In this section, we conduct ablation studies on HACS to verify the effectiveness of each module in TCANet. Is progressive refinement necessary? The progressive refinement strategy is a part of our TCANet. Here, the necessity of progressive refinement is illustrated by the separation experiment of three TBRs in <ref type="table" target="#tab_3">Table 6</ref>. Although each TBR has a positive effect on performance, with more stages, this promotion is gradually weakened. Thus TCANet only contains three stages.</p><p>What WindowSize and groups in LGTE should be set? In <ref type="table">Table 7</ref> and <ref type="table" target="#tab_4">Table 8</ref>, we conduct experiments to explore the effect of WindowSize. If the WindowSize is set extremely small(WindowSize = 5 or smaller), the local groups' features fail to collect enough local details. On the contrary(WindowSize=T), they will introduce excessive global noise. The number of groups N determines whether various temporal relationships can be modeled. Considering the performance, we finally set the WindowSize to 9 and the groups to 8 in our experiments.</p><p>What is the effect of the number of LGTE? As an easyplug-in module, performance can be improved by stacking multiple LGTEs.  <ref type="table" target="#tab_0">Table 10</ref>, which reveals that LGTE can also significantly improve the performance of BMN and demonstrate the importance of temporal relationship modeling for temporal action localization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How does the frame-level regression affect the TBR?</head><p>To verify the effect of frame-level regression in TBR, we  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Visualization</head><p>To further explore the interpretability of LGTE, GTE only and LGTE are both leveraged to embed the input video features. To facilitate observation, the obtained features are utilized to predict the starting and ending probability se-   quences. An example is shown in <ref type="figure" target="#fig_3">Figure 5</ref>. It is observed that the boundary obtained by LGTE is more accurate and smoother than that obtained by GTE. This indicates that LGTE can reduce global noise and enhance the boundary awareness. <ref type="figure" target="#fig_4">Figure 6</ref> shows the output of TBR and TCANet.</p><p>In the top row, both the segment-level and the frame-level output can improve the candidate proposals, but the boundaries are not accurate. The fusion of these two outputs can make the proposal closer to the ground truth. The bottom row shows that our TCANet can generate the proposals from coarse to fine, and provide more reliable confidence scores, especially for short-term action instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel Temporal Context Aggregation Network (TCANet) for temporal action proposal generation. Firstly we introduce the Local-Global Temporal Encoder (LGTE) to capture both local and global temporal relationships simultaneously in a channel grouping fashion. Then the complementary boundary regression mechanism is designed to obtain more precise boundaries and confidence scores. Extensive experiments conducted on several famous benchmarks demonstrate that our TCANet can achieve significant improvement on both action proposal and action detection performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The framework of TCANet. TCANet mainly contains two modules: LGTE and TBR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>A groups are handled by LTEs, while the other N − A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The starting and ending probability sequences generated by LGTE and GTE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative examples of proposals generated by TBR(top) and TCANet(bottom) on HACS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art methods on HACS. The results are measured by mAP(%) at different tIoU thresholds and average mAP(%). * indicates our implementation.</figDesc><table><row><cell>Method</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell cols="2">Average</cell></row><row><cell>2019-Winner [44]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>23.49</cell></row><row><cell>BMN [18]*</cell><cell cols="3">52.49 36.38 10.37</cell><cell></cell><cell>35.76</cell></row><row><cell>TCANet[SW]</cell><cell cols="3">54.14 37.24 11.32</cell><cell></cell><cell>36.79</cell></row><row><cell>TCANet[BMN]</cell><cell cols="3">56.74 41.14 12.15</cell><cell></cell><cell>39.77</cell></row><row><cell cols="6">Table 2. Comparison between our TCANet with other state-of-</cell></row><row><cell cols="6">the-arts methods on ActivityNet-v1.3. The results are measured</cell></row><row><cell cols="6">by mAP(%) at different tIoU thresholds and average mAP(%).</cell></row><row><cell cols="6">For fair comparisons, we combined our proposals with video-level</cell></row><row><cell cols="6">classification results from [41]. * indicates the reproduced results.</cell></row><row><cell>Method</cell><cell></cell><cell>0.5</cell><cell>0.75</cell><cell cols="2">0.95 Average</cell></row><row><cell>BSN [20](2stream)</cell><cell></cell><cell cols="3">46.45 29.96 8.02</cell><cell>30.03</cell></row><row><cell>BMN [18] (2stream)</cell><cell></cell><cell cols="3">50.07 34.78 8.29</cell><cell>33.85</cell></row><row><cell>G-TAD [42] (2stream)</cell><cell></cell><cell cols="3">50.36 34.60 9.02</cell><cell>34.09</cell></row><row><cell>BSN++ [27] (2stream)</cell><cell></cell><cell cols="3">51.27 35.70 8.33</cell><cell>34.88</cell></row><row><cell>BMN [18] (SlowFast)*</cell><cell></cell><cell cols="3">52.24 35.89 8.33</cell><cell>35.28</cell></row><row><cell>PGCN [43][BSN] (I3D)</cell><cell></cell><cell cols="3">48.26 33.16 3.27</cell><cell>31.33</cell></row><row><cell cols="5">TCANet[BSN] (2stream) 51.91 34.92 7.46</cell><cell>34.43</cell></row><row><cell cols="2">TCANet[BMN] (2stream)</cell><cell cols="3">52.27 36.73 6.86</cell><cell>35.52</cell></row><row><cell cols="2">TCANet[BMN] (SlowFast)</cell><cell cols="3">54.33 39.13 8.41</cell><cell>37.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison between our TCANet with other state-of-theart methods on THUMOS14 dataset. The results are measured by mAP(%) at different tIoU thresholds. We combined our proposals with video-level classifier UntrimmedNet<ref type="bibr" target="#b33">[34]</ref>.</figDesc><table><row><cell cols="2">Method</cell><cell cols="4">Classifier 0.7 0.6 0.5 0.4 0.3</cell></row><row><cell cols="2">TURN [9]</cell><cell cols="2">UNet</cell><cell cols="2">6.3 14.1 24.5 35.3 46.3</cell></row><row><cell cols="2">BSN [20]</cell><cell cols="2">UNet</cell><cell cols="2">20.0 28.4 36.9 45.0 53.5</cell></row><row><cell cols="2">MGG [21]</cell><cell cols="2">UNet</cell><cell cols="2">21.3 29.5 37.4 46.8 53.9</cell></row><row><cell cols="2">BMN [18]</cell><cell cols="2">UNet</cell><cell cols="2">20.5 29.7 38.8 47.4 56.0</cell></row><row><cell cols="2">G-TAD [42]</cell><cell cols="2">UNet</cell><cell cols="2">23.4 30.8 40.2 47.6 54.5</cell></row><row><cell cols="2">BSN++ [27]</cell><cell cols="2">UNet</cell><cell cols="2">22.8 31.9 41.3 49.5 59.9</cell></row><row><cell cols="2">TCANet</cell><cell cols="2">UNet</cell><cell cols="2">26.7 36.8 44.6 53.2 60.6</cell></row><row><cell cols="6">Table 4. Comparison of our TCANet with other state-of-the-art</cell></row><row><cell cols="6">methods on THUMOS14 dataset in terms of AR@AN.</cell></row><row><cell>Feature</cell><cell cols="2">Method</cell><cell cols="3">@50 @100 @200 @500 @1000</cell></row><row><cell>2stream</cell><cell cols="2">TAG [46]</cell><cell cols="2">18.55 29.00 39.61</cell><cell>-</cell><cell>-</cell></row><row><cell>2stream</cell><cell cols="2">CTAP [7]</cell><cell cols="2">32.49 42.61 51.97</cell><cell>-</cell><cell>-</cell></row><row><cell>2stream</cell><cell cols="2">BSN [20]</cell><cell cols="3">37.46 46.06 53.23 61.35 65.10</cell></row><row><cell cols="6">2stream MGG [21] 39.93 47.75 54.65 61.36 64.06</cell></row><row><cell cols="6">2stream BMN [18] 39.36 47.72 54.84 62.19 65.49</cell></row><row><cell cols="6">2stream BSN++ [27] 42.44 49.84 57.61 65.17 66.83</cell></row><row><cell>2stream</cell><cell cols="2">TCANet</cell><cell cols="3">42.05 50.48 57.13 63.61 66.88</cell></row><row><cell cols="6">MOS14, they are set to [0.5:0.05:1.0]. By limiting the</cell></row><row><cell cols="6">average number (AN) of proposals for each video , we</cell></row><row><cell cols="6">can calculate the area under the AR vs AN curve to ob-</cell></row><row><cell cols="6">tain AUC. On ActivityNet-v1.3, AN is set from 1 to 100.</cell></row><row><cell cols="6">The quality of temporal action detection requires to eval-</cell></row><row><cell cols="6">uate mean Average Precision(mAP) under multiple tIoU.</cell></row><row><cell cols="6">On HACS and ActivityNet-v1.3, the tIoU thresholds are</cell></row><row><cell cols="6">set to {0.5,0.75,0.95}, and we also test the average mAP</cell></row><row><cell cols="6">of tIoU thresholds between 0.5 and 0.95 with step of</cell></row><row><cell cols="6">0.05. On THUMOS14, these tIoU thresholds are set to</cell></row><row><cell cols="3">{0.3,0.4,0.5,0.6,0.7}.</cell><cell></cell><cell></cell></row><row><cell cols="6">Implementation Details. On HACS and ActivityNet-v1.3,</cell></row><row><cell cols="6">SlowFast [5] is adopted to extract a 2304-dimensional fea-</cell></row><row><cell cols="6">ture vector for each snippet. Each snippet contains s = 32</cell></row><row><cell cols="6">frames and snippet interval δ is 8. For a fair comparison,</cell></row><row><cell cols="6">2stream network [36] is adopted for feature encoding fol-</cell></row><row><cell cols="6">lowing [20, 18] on ActivityNet-v1.3 and THUMOS14.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Comparison between our TCANet with other state-ofthe-art methods CTAP<ref type="bibr" target="#b6">[7]</ref>, BSN<ref type="bibr" target="#b19">[20]</ref>, MGG<ref type="bibr" target="#b20">[21]</ref>, BMN<ref type="bibr" target="#b17">[18]</ref> on ActivityNet-v1.3 in terms of AR@AN and AUC.</figDesc><table><row><cell>Method</cell><cell>CTAP</cell><cell>BSN</cell><cell cols="3">MGG BMN TCANet</cell></row><row><cell>AR@1(val)</cell><cell>-</cell><cell>32.17</cell><cell>-</cell><cell>-</cell><cell>34.55</cell></row><row><cell>AR@100(val)</cell><cell>73.17</cell><cell>74.16</cell><cell>74.54</cell><cell>75.01</cell><cell>76.08</cell></row><row><cell>AUC(val)</cell><cell>65.72</cell><cell>66.17</cell><cell>66.43</cell><cell>67.10</cell><cell>68.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Ablation study of TBR, LGTE and progressive refinement strategy on HACS dataset in terms of average mAP(%).</figDesc><table><row><cell cols="5">TBR1 TBR2 TBR3 LGTE Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>37.16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>37.45</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>37.78</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>38.71</cell></row><row><cell cols="5">Table 7. The effect of different window size settings in the local de-</cell></row><row><cell cols="5">pendency matrix of LGTE on HACS in terms of average mAP(%).</cell></row><row><cell>W indowSize</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Average</cell></row><row><cell>5</cell><cell cols="3">55.27 39.54 11.61</cell><cell>38.41</cell></row><row><cell>9</cell><cell cols="3">55.60 40.01 11.47</cell><cell>38.71</cell></row><row><cell>15</cell><cell cols="3">55.56 39.83 11.89</cell><cell>38.70</cell></row><row><cell>25</cell><cell cols="3">54.99 40.06 11.94</cell><cell>38.67</cell></row><row><cell>T(GTE only)</cell><cell cols="3">54.70 39.64 11.71</cell><cell>38.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 .</head><label>8</label><figDesc>The effect of different groups N LGTE on HACS dataset in terms of average mAP(%).</figDesc><table><row><cell>N</cell><cell>0.5</cell><cell>0.75</cell><cell cols="3">0.95 Average</cell></row><row><cell cols="4">2 55.27 39.86 10.91</cell><cell cols="2">38.49</cell></row><row><cell cols="4">4 55.13 39.65 11.28</cell><cell cols="2">38.38</cell></row><row><cell cols="4">8 55.60 40.01 11.47</cell><cell cols="2">38.71</cell></row><row><cell cols="4">16 54.87 40.00 11.63</cell><cell cols="2">38.56</cell></row><row><cell cols="6">Table 9. The effect of the number of LGTE on HACS dataset in</cell></row><row><cell cols="2">terms of average mAP(%).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Number of LGTE</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Average</cell></row><row><cell>0</cell><cell cols="4">54.73 39.05 10.72</cell><cell>37.78</cell></row><row><cell>1</cell><cell cols="4">55.13 39.67 11.42</cell><cell>38.31</cell></row><row><cell>2</cell><cell cols="4">55.60 40.01 11.47</cell><cell>38.71</cell></row><row><cell>4</cell><cell cols="4">55.72 40.03 11.73</cell><cell>38.85</cell></row><row><cell>6</cell><cell cols="4">55.65 40.02 11.73</cell><cell>38.80</cell></row><row><cell cols="6">Table 10. The generalizability of LGTE under different frame-</cell></row><row><cell cols="6">works on HACS dataset in terms of average mAP(%).</cell></row><row><cell cols="2">Framework LGTE</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Average</cell></row><row><cell>BMN</cell><cell></cell><cell cols="3">52.49 36.38 10.37</cell><cell>35.76</cell></row><row><cell>BMN</cell><cell></cell><cell cols="3">54.75 38.72 11.41</cell><cell>37.76</cell></row><row><cell>TCANet</cell><cell></cell><cell cols="3">54.73 39.05 10.72</cell><cell>37.78</cell></row><row><cell>TCANet</cell><cell></cell><cell cols="3">55.60 40.01 11.47</cell><cell>38.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9</head><label>9</label><figDesc>demonstrates the performance of the TCANet improves significantly with the increase of LGTE. However, excessive LGTE will lead to over-fitting. The performance of TCANet can reach the best with four LGTEs. Nevertheless, two LGTEs are employed in other ablation studies to facilitate the experiments. Is LGTE general? To validate the generalizability of our proposed LGTE, we also add it to the BMN [18] framework. The experimental results are shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 11 .</head><label>11</label><figDesc>shown inTable 11. If only frame-level regression is applied, the detection performance will drop with only boundary local information. The two methods are combined to boost performance in the final average mAP. Why mAP not AUC? In our experiments, we find that the detection metric (mAP) mainly depends on Average Recall (AR) with the first several proposals, while the proposal metric (AUC) depends on the first 100 proposals. Hence AR with a small number of proposals has a higher weight in the evaluation metric of detection performance. Extensive experiments have shown that our TCANet can generate fewer proposals with high recall than other methods. Thus the performance improvement of the detection metric is obvious than the proposal metric. Efficiency Analysis. The input candidate proposals for TCANet need to ensure a high recall rate. Taking the BMNgenerated proposals as an example, when 2000 candidate proposals are selected, the recall rate can reach 91% with tIOU=0.5. Our test results are shown inTable 12. For a video, LGTE only needs to encode video features once, and the TBR can process multiple proposals in parallel. Therefore, TCANet only takes 20.9 ms to handle a 9-minute video with 2000 candidate proposals. Compared with BMN, the time consumed by TCANet is only 10%.</figDesc><table><row><cell cols="5">The effect of Frame-Level Regression on HACS in terms</cell></row><row><cell cols="5">of mAP(%). SLR and FLR indicate Segment-Level Regression</cell></row><row><cell cols="4">and Frame-Level Regression, respectively.</cell><cell></cell></row><row><cell>SLR FLR</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Average</cell></row><row><cell></cell><cell cols="3">54.58 39.24 11.72</cell><cell>38.22</cell></row><row><cell></cell><cell cols="2">55.02 39.61</cell><cell>9.14</cell><cell>37.92</cell></row><row><cell></cell><cell cols="3">55.60 40.01 11.47</cell><cell>38.71</cell></row><row><cell cols="5">conducted experiments using both regression methods sep-</cell></row><row><cell>arately, as</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 12 .</head><label>12</label><figDesc>The inference time of each module in TCANet on HACS dataset. 2000 candidate proposals were utilized as input to TCANet, and a Nvidia 1080Ti graphic card was employed to process a video for about 9 minutes.</figDesc><table><row><cell>Num×Module</cell><cell>1×BMN</cell><cell>2×LGTE</cell><cell>3×TBR</cell><cell>Total</cell></row><row><cell cols="2">Num×Time Cost 1×181ms</cell><cell>2×1.6ms</cell><cell cols="2">3×5.9ms 201.9ms</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgment</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate temporal action proposal generation with relation-aware pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Actor-transformers for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrsan</forename><surname>Javan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="34" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11499" to="11506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Untrimmed video classification for activity detection: submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cuzzolin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01979</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bsn++: Complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07641</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Collaborative distillation in the parameter and spectrum domains for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06902</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascaded pyramid mining network for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="558" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transferable knowledge-based multi-granularity fusion network for weakly supervised temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilan</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04851.2</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cuhk &amp; ethz &amp; siat submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00797</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10156" to="10165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning sparse 2d temporal adjacent networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03612</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hacs: Human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
