<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From source to target and back: Symmetric Bi-Directional Adaptive GAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Russo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department DIAG</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department DIAG</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
							<email>tatiana.tommasi@iit.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Italian Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<email>caputo@dis.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department DIAG</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Italian Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">From source to target and back: Symmetric Bi-Directional Adaptive GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The effectiveness of GANs in producing images according to a specific visual domain has shown potential in unsupervised domain adaptation. Source labeled images have been modified to mimic target samples for training classifiers in the target domain, and inverse mappings from the target to the source domain have also been evaluated, without new image generation.</p><p>In this paper we aim at getting the best of both worlds by introducing a symmetric mapping among domains. We jointly optimize bi-directional image transformations combining them with target self-labeling. We define a new class consistency loss that aligns the generators in the two directions, imposing to preserve the class identity of an image passing through both domain mappings. A detailed analysis of the reconstructed images, a thorough ablation study and extensive experiments on six different settings confirm the power of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to generalize across domains is challenging when there is ample labeled data on which to train a deep network (source domain), but no annotated data for the target domain. To attack this issue, a wide array of methods have been proposed, most of them aiming at reducing the shift between the source and target distributions (see Sec. 2 for a review of previous work). An alternative is mapping the source data into the target domain, either by modifying the image representation <ref type="bibr" target="#b9">[10]</ref> or by directly generating a new version of the source images <ref type="bibr" target="#b3">[4]</ref>. Several authors proposed approaches that follow both these strategies by building over Generative Adversarial Networks (GANs) <ref type="bibr" target="#b12">[13]</ref>. A similar but inverse method maps the target data into the source domain, where there is already an abundance of labeled images <ref type="bibr" target="#b38">[39]</ref>.</p><p>We argue that these two mapping directions should not be alternative, but complementary. Indeed, the main ingredient for adaptation is the ability of transferring successfully the style of one domain to the images of the other. This, given a fixed generative architecture, will depend on the application: there may be cases where mapping from the source to the target is easier, and cases where it is true otherwise. By pursuing both directions in a unified architecture, we can obtain a system more robust and more general than previous adaptation algorithms. With this idea in mind, we designed SBADA-GAN: Symmetric Bi-Directional ADAptive Generative Adversarial Network. Its distinctive features are (see <ref type="figure" target="#fig_0">Figure 1</ref> for a schematic overview):</p><p>• it exploits two generative adversarial losses that encourage the network to produce target-like images from the source samples and source-like images from the target samples. Moreover, it jointly minimizes two classification losses, one on the original source images and the other on the transformed target-like source images; • it uses the source classifier to annotate the source-like transformed target images. Such pseudo-labels help regularizing the same classifier while improving the target-to-source generator model by backpropagation;</p><p>• it introduces a new semantic constraint on the source images: the class consistency loss. It imposes that by mapping source images towards the target domain and then again towards the source domain they should get back to their ground truth class. This last condition is less restrictive than a standard reconstruction loss <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b17">18]</ref>, as it deals only with the image annotation and not with the image appearance. Still, our experiments show that it is highly effective in aligning the domain mappings in the two directions;</p><p>• at test time the two trained classifiers are used respectively on the original target images and on their sourcelike transformed version. The two predictions are integrated to produce the final annotation.</p><p>Our architecture yields realistic image reconstructions while competing against previous state-of-the-art classifiers and exceeding them on four out of six different unsupervised adaptation settings. An ablation study showcasing the importance of each component in the architecture, and investigating the robustness with respect to its hyperparameters, sheds light on the inner workings of the approach, while providing further evidence of its value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>GANs Generative Adversarial Networks are composed of two modules, a generator and a discriminator. The generator's objective is to synthesize samples whose distribution closely matches that of real data, while the discriminator objective is to distinguish real from generated samples. GANs are agnostic to the training samples labels, while conditional GAN variants <ref type="bibr" target="#b24">[25]</ref> exploit the class annotation as additional information to both the generator and the discriminator. Some works used multiple GANs: in CoGAN <ref type="bibr" target="#b21">[22]</ref> two generators and two discriminators are coupled by weight-sharing to learn the joint distribution of images in two different domains without using pair-wise data. Cycle-GAN <ref type="bibr" target="#b40">[41]</ref>, Disco-GAN <ref type="bibr" target="#b17">[18]</ref> and UNIT <ref type="bibr" target="#b20">[21]</ref> encourage the mapping between two domains to be well covered by imposing transitivity: the mapping in one direction followed by the mapping in the opposite direction should arrive where it started. For this image generation process the main performance measure is either a human-based quality control or scores that evaluate the interpretability of the produced images by pre-existing models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Domain Adaptation A widely used strategy consists in minimizing the difference between the source and target distributions <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b6">7]</ref>. Alternative approaches minimize the errors in target samples reconstruction <ref type="bibr" target="#b11">[12]</ref> or impose a consistency condition so that neighboring target samples assigned to different labels are penalized proportionally to their similarity <ref type="bibr" target="#b32">[33]</ref>. Very recently, <ref type="bibr" target="#b14">[15]</ref> proposed to enforce associations between source and target samples of the same ground truth or predicted class, while <ref type="bibr" target="#b29">[30]</ref> assigned pseudo-labels to target samples using an asymmetric tri-training method.</p><p>Domain invariance can be also treated as a binary classification problem through an adversarial loss inspired by GANs, which encourages mistakes in domain prediction <ref type="bibr" target="#b9">[10]</ref>. For all the methods adopting this strategy, the described losses are minimized jointly with the main classification objective function on the source task, guiding the feature learning process towards a domain invariant representation. Only in <ref type="bibr" target="#b38">[39]</ref> the two objectives are kept separated and recombined in a second step. In <ref type="bibr" target="#b4">[5]</ref> the feature components that differentiate two domains are modeled separately from those shared among them.</p><p>Image Generation for Domain Adaptation In the first style transfer methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> new images were synthesized to maintain a specific content while replicating the style of one or a set of reference images. Similar transfer approaches have been used to generate images with different visual domains. In <ref type="bibr" target="#b33">[34]</ref> realistic samples were generated from synthetic images and the produced data could work as training set for a classification model with good results on real images. <ref type="bibr" target="#b3">[4]</ref> proposed a GANbased approach that adapts source images to appear as if drawn from the target domain; the classifier trained on such data outperformed several domain adaptation methods by large margins. <ref type="bibr" target="#b36">[37]</ref> introduced a method to generate source images that resemble the target ones, with the extra consistency constraint that the same transformation should keep the target samples identical. All these methods focus on the source-to-target image generation, not considering adding an inverse procedure, from target to source, which we show instead to be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Model We focus on unsupervised cross domain classification. Let us start from a dataset X s = {x i s , y i s } Ns i=0 drawn from a labeled source domain S, and a dataset X t = {x j t } Nt j=0 from a different unlabeled target domain T , sharing the same set of categories. The task is to maximize the classification accuracy on X t while training on X s . To reduce the domain gap, we propose to adapt the source images such that they appear as sampled from the target domain by training a generator model G st that maps any source samples x i s to its target-like version <ref type="figure" target="#fig_0">Figure 1</ref>, bottom row). The model is also augmented with a discriminator D t and a classifier C t . The former takes as input the target images X t and target-like source transformed images X st , learning to recognize them as two different sets. The latter takes as input each of the transformed images x i st and learns to assign its task-specific label y i s . During the training procedure for this model, information about the domain recognition likelihood produced by D t is used adversarially to guide and optimize the performance of the generator G st . Similarly, the generator also benefits from backpropagation in the classifier training procedure.</p><formula xml:id="formula_0">x i st = G st (x i s ) defining the set X st = {x i st , y i s } Ns i=0 (see</formula><p>Besides the source-to-target transformation, we also consider the inverse target-to-source direction by using a symmetric architecture (see <ref type="figure" target="#fig_0">Figure 1</ref>, top row). Here any target image x j t is given as input to a generator model G ts transforming it to its source-like version x j ts = G ts (x j t ), defining the set X ts = {x j ts } Nt j=0 . As before, the model is augmented with a discriminator D s which takes as input both X ts and X s and learns to recognize them as two different sets, adversarially helping the generator.</p><p>Since the target images are unlabeled, no classifier can be trained in the target-to-source direction as a further support for the generator model. We overcome this issue by self-labeling (see <ref type="figure" target="#fig_0">Figure 1</ref>, blue arrow). The original source data X s is used to train a classifier C s . Once it has reached convergence, we apply the learned model to annotate each of the source-like transformed target images x j ts . These samples, with the assigned pseudolabels y j t self = argmax y (C s (G ts (x j t )), are then used transductively as input to C s while information about the performance of the model on them is backpropagated to guide and improve the generator G ts . Self-labeling has a long track record of success for domain adaptation: it proved to be effective both with shallow models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>, as well as with the most recent deep architectures <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30]</ref>. In our case the classification loss on pseudo-labeled samples is combined with our other losses, which helps making sure we move towards the optimal solution: in case of a moderate domain shift, the correct pseudo-labels help to regularize the learning process, while in case of large domain shift, the possible mislabeled samples do not hinder the performance (see Sec. 4.5 for a detailed discussion on the experimental results).</p><p>Finally, the symmetry in the source-to-target and target-to-source transformations is enhanced by aligning the two generator models such that, when used in sequence, they bring a sample back to its starting point. Since our main focus is classification, we are interested in preserving the class identity of each sample rather than its overall appearance. Thus, instead of a standard imagebased reconstruction condition we introduce a class consistency condition (see <ref type="figure" target="#fig_0">Figure 1</ref>, red arrows). Specifically, we impose that any source image x i s adapted to the target domain through G st (x i s ) and transformed back towards the source domain through G ts (G st (x i s )) is correctly classified by C s . This condition helps by imposing a further joint optimization of the two generators.</p><p>Learning Here we formalize the description above. To begin with, we specify that the generators take as input a noise vector z ∈ N (0, 1) besides the images, this allows some extra degree of freedom to model external variations. We also better define the discriminators as D s (x), D t (x) and the classifiers as C s (x), C t (x). Of course each of these models depends from its parameters but we do not explicitly indicate them to simplify the notation. For the same reason we also drop the superscripts i, j.</p><p>The source-to-target part of the network optimizes the following objective function:</p><formula xml:id="formula_1">min Gst,Ct max Dt αL Dt (D t , G st ) + βL Ct (G st , C t ) , (1)</formula><p>where the classification loss L Ct is a standard softmax cross-entropy</p><formula xml:id="formula_2">L Ct (G st , C t ) = E {xs,ys}∼S zs∼noise [−y s · log(ŷ s )] ,<label>(2)</label></formula><p>evaluated on the source samples transformed by the generator G st , so thatŷ s = C t (G st (x s , z s )) and y s is the one-hot encoding of the class label y s . For the discriminator, instead of the less robust binary cross-entropy, we followed <ref type="bibr" target="#b23">[24]</ref> and chose a least square loss:</p><formula xml:id="formula_3">L Dt (D t , G st ) =E xt∼T [(D t (x t ) − 1) 2 ]+ E xs∼S zs∼noise [(D t (G st (x s , z s ))) 2 ] . (3)</formula><p>The objective function for the target-to-source part of the network is:</p><formula xml:id="formula_4">min Gts,Cs max Ds γL Ds (D s , G ts )+ µL Cs (C s ) + ηL self (G ts , C s ) ,<label>(4)</label></formula><p>where the discriminative loss is analogous to eq. (3), while the classification loss is analogous to eq. (2) but evaluated on the original source samples withŷ s = C s (x s ), thus it neither has any dependence on the generator that transforms the target samples G ts , nor it provides feedback to it. The self loss is again a classification softmax cross-entropy:</p><formula xml:id="formula_5">L self (G ts , C s ) = E {xt,yt self }∼T zt∼noise [−y t self · log(ŷ t self )] .<label>(5)</label></formula><p>whereŷ t self = C s (G ts (x t , z t )) and y tself is the onehot vector encoding of the assigned label y t self . This loss back-propagates to the generator G ts which is encouraged to preserve the annotated category within the transformation. Finally, we developed a novel class consistency loss by minimizing the error of the classifier C s when applied on the concatenated transformation of G ts and G st to</p><formula xml:id="formula_6">produceŷ cons = (C s (G ts (G st (x s , z s ), z t ))): L cons (G ts , G st , C s ) = E {xs,ys}∼S zs,zt∼noise [−y s · log(ŷ cons )] .<label>(6)</label></formula><p>This loss has the important role of aligning the generators in the two directions and strongly connecting the two main parts of our architecture. By collecting all the presented parts, we conclude with the complete SBADA-GAN loss:</p><formula xml:id="formula_7">L SBADA−GAN (G st , G ts , C s , C t , D s , D t ) = αL Dt + βL Ct + γL Ds + µL Cs + ηL self + νL cons .<label>(7)</label></formula><p>Here (α, β, γ, µ, η, ν) ≥ 0 are weights that control the interaction of the loss terms. While the combination of six different losses might appear daunting, it is not unusual <ref type="bibr" target="#b4">[5]</ref>. Here, it stems from the symmetric bi-directional nature of the overall architecture. Indeed each directional branch has three losses as it is custom practice in the GAN-based domain adaptation literature <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b3">4]</ref>. Moreover, the ablation study reported in Sec. 4.5 indicates that the system is remarkably robust to changes in the hyperparameter values.</p><p>Testing The classifier C t is trained on X st generated images that mimic the target domain style, and is then tested on the original target samples X t . The classifier C s is trained on X s source data, and then tested on X ts samples, that are the target images modified to mimic the source domain style. These classifiers make mistakes of different type assigning also a different confidence rank to each of the possible labels. Overall the two classification models complement each other. We take advantage of this with a simple ensemble method σC s (G ts (x t , z t )) + τ C t (x t ) which linearly combines their probability output, providing a further gain in performance. A schematic illustration of the testing procedure is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We set the combination weights σ, τ through cross validation (see Sec. 4.2 for further details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Adaptation Scenarios</head><p>We evaluate SBADA-GAN on several unsupervised adaptation scenarios 1 , considering the following widely used digits datasets and settings:</p><p>MNIST → MNIST-M: MNIST <ref type="bibr" target="#b19">[20]</ref> contains centered, 28 × 28 pixel, grayscale images of single digit numbers on a black background, while MNIST-M <ref type="bibr" target="#b9">[10]</ref> is a variant where the background is substituted by a randomly extracted patch obtained from color photos of BSDS500 <ref type="bibr" target="#b2">[3]</ref>. We follow the evaluation protocol of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>MNIST ↔ USPS: USPS [9] is a digit dataset automatically scanned from envelopes by the U.S. Postal Service containing a total of 9,298 16 × 16 pixel grayscale samples. The images are centered, normalized and show a broad range of font styles. We follow the evaluation protocol of <ref type="bibr" target="#b3">[4]</ref>.</p><p>SVHN ↔ MNIST: SVHN <ref type="bibr" target="#b27">[28]</ref> is the challenging realworld Street View House Number dataset, much larger in scale than the other considered datasets. It contains over 600k 32 × 32 pixel color samples. Besides presenting a great variety of styles (in shape and texture), images from this dataset often contain extraneous numbers in addition to the labeled, centered one. Most previous works simplified the data by considering a grayscale version, instead we apply our method to the original RGB images. Specifically for this experiment we resize the MNIST images to 32 × 32 pixels and use the protocol by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>We also test SBADA-GAN on a traffic sign scenario. Synth Signs → GTSRB: the Synthetic Signs collection <ref type="bibr" target="#b25">[26]</ref> contains 100k samples of common street signs obtained from Wikipedia and artificially transformed to simulate various imaging conditions. The German Traffic Signs Recognition Benchmark (GTSRB, <ref type="bibr" target="#b34">[35]</ref>) consists of 51, 839 cropped images of German traffic signs. Both databases contain samples from 43 classes, thus defining a larger classification task than that on the 10 digits. For the experiment we adopt the protocol proposed in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We composed SBADA-GAN starting from two symmetric GANs, each with an architecture 2 analogous to that used for the PixelDA model <ref type="bibr" target="#b3">[4]</ref>.</p><p>The model is coded in python and we ran all our experiments in the Keras framework <ref type="bibr" target="#b7">[8]</ref> (code will be made <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39]</ref> so we can provide a large benchmark against many methods. The standard Office dataset is not considered here, due to the issues clearly explained in <ref type="bibr" target="#b4">[5]</ref>, specifically in section B of the supplementary material. All the works that show experiments on Office exploit networks pre-trained on Imagenet which means involving an extra source domain in the task. In our case the network is always trained from scratch on the available data of the specific experiment.</p><p>2 See all the model details in the appendix. available upon acceptance). We use the ADAM <ref type="bibr" target="#b18">[19]</ref> optimizer with learning rates for the discriminator and the generator both set to 10 −4 . The batch size is set to 32 and we train the model for 500 epochs not noticing any overfitting, which suggests that further epochs might be beneficial. The α and γ loss weights (discriminator losses) are set to 1, β and µ (classifier losses) are set to 10, to prevent that generator from indirectly switching labels (for instance, transform 7's into 1's). The class consistency loss weight ν is set to 1. All training procedures start with the self-labeling loss weight, η, set to zero, as this loss hinders convergence until the classifier is fully trained. After the model converges (losses stop oscillating, usually after 250 epochs) η is set to 1 to further increase performance. Finally the parameters to combine the classifiers at test time are σ ∈ [0, 0.1, 0.2, . . . , 1] and τ = (1 − σ) chosen on a validation set of 1000 random samples from the target in each different setting. <ref type="table">Table 1</ref> shows results on our six evaluation settings. The top of the table reports results by thirteen competing baselines published over the last two years. The Source-Only and Target-Only rows contain reference results corresponding to the naïve no-adaptation case and to the target fully supervised case. For SBADA-GAN, besides the full method, we also report the accuracy obtained by the separate classifiers (indicated by C s and C t ) before the linear combination. The last three rows show results that appeared recently in pre-prints available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Results</head><p>SBADA-GAN improves over the state of the art in four out of six settings. In these cases the advantage with respect to its competitors is already visible in the separate C s and C t results and it increases when considering the full combination procedure. Moreover, the gain in performance of SBADA-GAN reaches up to +8 percentage points in the MNIST→SVHN experiment. This setting was disregarded in many previous works: differently from its inverse SVHN→MNIST, it requires a difficult adaptation from the grayscale handwritten digits domain to the widely variable and colorful street view house number domain. Thanks to its bi-directionality, SBADA-GAN leverages on the inverse target to source mapping to produce highly accuracy results.</p><p>Conversely, in the SVHN→MNIST case SBADA-GAN ranks eighth out of the thirteen baselines in terms of performance. Our accuracy is on par with ADDA's <ref type="bibr" target="#b38">[39]</ref>: the two approaches share the same classifier architecture, although the number of fully-connected neurons of SBADA-GAN is five time lower. Moreover, com-  <ref type="table">Table 1</ref>: Comparison against previous work. SBADA-GAN C t reports the accuracies produced by the classifier trained in the target domain space. Similarly, SBADA-GAN C s reports the results produced by the classifier trained in the source domain space and tested on the target images mapped to this space. SBADA-GAN reports the results obtained by a weighted combination of the softmax outputs of these two classifiers. Note that all competitors convert SVHN to grayscale, while we deal with the more complex original RGB version. The last three rows report results from online available pre-print papers. pared to DRCN <ref type="bibr" target="#b11">[12]</ref>, the classifiers of SBADA-GAN are shallower with a reduced number of convolutional layers. Overall here SBADA-GAN suffers of some typical drawbacks of GAN-based domain adaptation methods: although the style of a domain can be easily transferred in the raw pixel space, the generative process does not have any explicit constraint on reducing the overall data distribution shift as instead done by the alternative featurebased domain adaptation approaches. Thus, methods like DA ass <ref type="bibr" target="#b14">[15]</ref>, DTN <ref type="bibr" target="#b36">[37]</ref> and DSN <ref type="bibr" target="#b4">[5]</ref> deal better with the large domain gap of the SVHN→MNIST setting.</p><p>Finally, in the Synth Signs → GTSRB experiment, SBADA-GAN is just slightly worse than DA ass , but outperforms all the other competing methods. The comparison remains in favor of SBADA-GAN when considering that its performance is robust to hyperparameter variations (see Sec. 4.5 for more details), while the performance of DA ass drops significantly in case of not tuned, pre-defined fixed parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>To complement the quantitative evaluation, we look at the quality of the images generated by SBADA-GAN. First, we see from <ref type="figure" target="#fig_3">Figure 3</ref> how the generated images actually mimic the style of the chosen domain, even when going from the simple MNIST digits to the SVHN colorful house numbers.</p><p>Visually inspecting the data distribution before and after domain mapping defines a second qualitative evaluation metric. We use t-SNE <ref type="bibr" target="#b22">[23]</ref> to project the data from their raw pixel space to a simplified 2D embedding. <ref type="figure">Figure 6</ref> shows such visualizations and indicates that the transformed dataset tends to replicate faithfully the distribution of the chosen final domain.</p><p>A further measure of the quality of the SBADA-GAN generators comes from the diversity of the produced images. Indeed, a well-known failure mode of GANs is that the generator may collapse and output a single prototype that maximally fools the discriminator. To evaluate  the diversity of samples generated by SBADA-GAN we choose the Structural Similarity (SSIM, <ref type="bibr" target="#b39">[40]</ref>), a measure that correlates well with the human perceptual similarity judgments. Its values range between 0 and 1 with higher values corresponding to more similar images. We follow the same procedure used in <ref type="bibr" target="#b28">[29]</ref> by randomly choosing 1000 pairs of generated images within a given class. We also repeat the evaluation over all the classes and calculate the average results. <ref type="table">Table 2</ref> shows the results of the mean SSIM metric and indicates that the SBADA-GAN generated images not only mimic the same style, but also successfully reproduce the variability of a chosen domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation and Robustness Study</head><p>To clarify the role of each component in SBADA-GAN we go back to its core source-to-target single GAN module and analyze the effect of adding all the other model parts. Specifically we start by adding the symmetric target-to-source GAN model. These two parts are then combined and the domain transformation loop is closed by adding the class consistency condition. Finally  <ref type="table">Table 2</ref>: Dataset mean SSIM: this measure of data variability suggests that our method successfully generates images with not only the same style of a chosen domain, but also similar perceptual variability. the model is completed by introducing the target selflabeling procedure. We empirically test each of these model reconstruction steps on the MNIST→USPS setting and report the results in <ref type="table">Table 3</ref>. We see the gain achieved by progressively adding the different model components, with the largest advantage obtained by the introduction of self-labeling. An analogous boost due to self-labeling is also visible in all the other experimental settings with the exception of MNIST↔SVHN, where the accuracy remains unchanged if η is equal or larger than zero. A further analysis reveals that here the recognition accuracy of the source classifier applied to the source-like transformed target images is quite low (about 65%, while in all the other settings reaches 80 − 90%), thus the pseudo-labels cannot be considered reliable. Still, using them does not hinder the overall performance.</p><p>The crucial effect of the class consistency loss can be better observed by looking at the generated images and  <ref type="table">Table 3</ref>: Analysis of the role of each SBADA-GAN component. We ran experiments by turning on the different losses of the model as indicated by the checkmarks. comparing them with those obtained in two alternative cases: setting ν = 0, i.e. not using any consistency condition between the two generators G st and G ts , or substituting our class consistency loss with the standard cycle consistency loss <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b17">18]</ref> based on image reconstruction. For this evaluation we choose the MNIST→SVHN case which has the strongest domain shift and we show the generated images in <ref type="figure" target="#fig_5">Figure 5</ref>. When the consistency loss is not activated, the G ts output images are realistic, but fail at reproducing the correct input digit and provide misleading information to the classifier. On the other hand, using the cycle-consistency loss preserves the input digit but fails in rendering a realistic sample in the correct domain style. Finally, our class consistency loss allows to preserve the distinct features belonging to a category while still leaving enough freedom to the generation process, thus it succeeds in both preserving the digits and rendering realistic samples. About the class consistency loss, we also note that SBADA-GAN is robust to the specific choice of the weight ν, given that it is different from zero. Changing it in [0.1, 1, 10] induces a maximum variation of 0.6 percentage points in accuracy over the different settings. An analogous evaluation performed on the classification loss weights β and µ reveals that changing them in the same range used for ν causes a maximum overall performance variation of 0.2 percentage points. Furthermore SBADA-GAN is minimally sensitive to the batch size used: halving it from 32 to 16 samples while keeping the same number of learning epochs reduces the performance only of about 0.2 percentage points. Such robustness is particularly relevant when compared to competing methods. For instance the most recent DA ass <ref type="bibr" target="#b14">[15]</ref> needs a perfectly balanced source and target distribution of classes in each batch, a condition difficult to satisfy in real world scenarios, and halving the originally large batch size reduces by 3.5 percentage points the final accuracy. Moreover, changing the weights of the losses that enforce associations across domains with a range analogous to that used for the SBADA-GAN parameters induces a drop in performance up to 16 accuracy percentage points 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presented SBADA-GAN, an adaptive adversarial domain adaptation architecture that maps simultaneously source samples into the target domain and vice versa with the aim to learn and use both classifiers at test time. To achieve this, we proposed to use self-labeling to regularize the classifier trained on the source, and we impose a class consistency loss that improves greatly the stability of the architecture, as well as the quality of the reconstructed images in both domains.</p><p>We explain the success of SBADA-GAN in several ways. To begin with, thanks to the the bi-directional mapping we avoid deciding a priori which is the best strategy for a specific task. Also, the combination of the two network directions improves performance providing empirical evidence that they are learning different, complementary features. Our class consistency loss aligns the image generators, allowing both domain transfers to influence each other. Finally the self-labeling procedure boost the performance in case of moderate domain shift without hindering it in case of large domain gaps.</p><p>• the generators take the form of a convolutional residual network with four residual blocks each composed by two convolutional layers with 64 features;</p><p>• the input noise z is a vector of N z elements each sampled from a normal distribution z i ∼ N (0, 1). It is fed to a fully connected layer which transforms it to a channel of the same resolution as that of the image, and is subsequently concatenated to the input as an extra channel. In all our experiments we used N z = 5;</p><p>• the discriminators are made of two convolutional layers, followed by an average pooling and a convolution that brings the discriminator output to a single scalar value;</p><p>• in both generator and discriminator networks, each convolution (with the exception of the last one of the generator) is followed by a batch norm layer <ref type="bibr" target="#b15">[16]</ref>;</p><p>• the classifiers have exactly the same structure of that in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>;</p><p>• as activation functions we used ReLU in the generator and classifier, while we used leaky ReLU (with a 0.2 slope) in the discriminator.</p><p>• all the input images to the generators are zero-centered and rescaled to [−0.5, 0.5]. The images produced by the generators as well as the other input images to the classifiers and and the discriminators are zero-centered and rescaled to [−127.5, 127.5].</p><p>Thanks to the stability of the SBADA-GAN training protocol, we did not use any injected noise into the discriminators and we did not use any dropout layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head><p>MNIST → MNIST-M: MNIST has 60k images for training. As <ref type="bibr" target="#b3">[4]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Distribution Visualizations</head><p>To visualize the original data distributions and their respective transformations we used t-SNE <ref type="bibr" target="#b22">[23]</ref>. The images were pre-processed by scaling in [−1, 1] and we applied PCA for dimensionality reduction from vectors with Width×Height elements to 64 elements. Finally t-SNE with default parameters was applied to project data to a 2-dimensional space.</p><p>The behavior shown by the t-SNE data visualization presented in the main paper extends also for the other experimental settings. We integrate here the visualization for the MNIST→MNIST-M case in <ref type="figure">Figure 6</ref>. The plots show again a successful mapping with the generated data that cover faithfully the target space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Robustness experiments</head><p>The experiments about SBADA-GAN robustness to hyperparameters values are described at high level in Section 4.5 of the main paper submission. Here we report on (right) for the DA ass method we changed the weight of the walker loss β 1 while keeping that of the visit loss β 2 = 0.1, or alternatively we changed the weight of the visit loss β 2 while fixing that of the walker loss β 1 = 1.</p><p>the detailed results obtained on Synth. Signs → GTSRB when using SBADA-GAN and the DA ass method <ref type="bibr" target="#b14">[15]</ref>. For SBADA-GAN we keep fixed the weights of the discriminative losses α = γ = 1 as well as that of selflabeling η = 1, while we varied alternatively the weights of the classification losses β, µ or the weight of the class consistency loss ν in [0.1, 1, 10]. The results plotted in <ref type="figure" target="#fig_6">Figure 7</ref> (left) show that the classification accuracy changes less than 0.2 percentage point. Furthermore, we used a batch size of 32 for our experiments and when reducing it to 16 the overall accuracy remains almost unchanged (from 96.7 to 96.5).</p><p>DA ass proposes to minimize the difference between the source and target by maximizing the associative similarity across domains. This is based on the two-step round-trip probability of an imaginary random walker starting from a sample (x s i , y i ) of the source domain, passing through an unlabeled sample of the target domain (x t j ) and and returning to another source sample (x s k , y k = y i ) belonging to the same class of the initial one. This is formalized by first assuming that all the categories have equal probability both in source and in target, and then measuring the difference between the uniform distribution and the two-step probability through the so called walker loss. To avoid that only few target samples are visited multiple times, a second visit loss measures the difference between the uniform distribution and the probability of visiting some target samples. We tested the robustness of DA ass by using the code provided by its authors and changing the loss weights β 1 for the walker loss and β 2 for the visit loss in the same range used for the SBADA-GAN: [0.1 <ref type="bibr">1 10]</ref>. <ref type="figure" target="#fig_6">Figure 7</ref> (right) shows that DA ass is particularly sensitive to modifications of the visit loss weights which can cause a drop in performance of more than 16 percentage points. Moreover, the model assumption about the class balance sounds too strict for realistic scenarios: in practice DA ass needs every observed data batch to contain an equal number of samples from each category and reducing the number of samples from 24 to 12 per category causes a drop in performance of more than 4 percentage points from 96.3 to 92.8.</p><p>To conclude, although GAN methods are generally considered unstable and difficult to train, SBADA-GAN results much more robust than a not-GAN approach like DA ass to the loss weights hyperparameters and can be trained with small random batches of data while not losing its high accuracy performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>SBADA-GAN, training: the data flow starts from the source and target images indicated by the Input Data arrows. The bottom and top row show respectively the source-to-target and target-to-source symmetric directions. The generative models G st and G ts transform the source images to the target domain and vice versa. D s and D t discriminate real from generated images of source and target. Finally the classifiers C s and C t are trained to recognize respectively the original source images and their target-like transformed versions. The bi-directional blue arrow indicates that the source-like target images are automatically annotated and the assigned pseudo-labels are re-used by the classifier C s . The red arrows describe the class consistency condition by which source images transformed to the target domain through G st and back to the source domain through G ts should maintain their ground truth label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>SBADA-GAN, test: the two pre-trained classifiers are applied respectively on the target images and on the transformed source-like target images. Their outputs are linearly combined for the final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) MNIST to USPS (b) USPS to MNIST (c) MNIST to MNIST-M (d) MNIST-M to MNIST (e) MNIST to SVHN (f) SVHN to MNIST (g) Synth S. to GTSRB (h) GTSRB to Synth S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Examples of generated digits: we show the image transformation from the original domain to the paired one as indicated under every sub-figure. For each of the (a)-(h) cases, the original/generated images are in the top/bottom row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) MNIST to USPS (b) USPS to MNIST (c) SVHN to MNIST (d) MNIST to SVHN Figure 4: t-SNE visualization of source, target and source mapped to target images. Note how the mapped source covers faithfully the target space both in the (a),(b) case with moderated domain shift and in the more challenging (c),(d) setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>G ts outputs (lower line) and their respective inputs (upper line) obtained with: (a) no consistency loss, (b) image-based cycle consistency loss<ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b17">18]</ref>, (c) our class consistency loss. In (d) we show some real SVHN samples as a reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>(a) MNIST-M to MNIST (b) MNIST to MNIST-M Figure 6: t-SNE visualization of source, target and source mapped to target images. Note how the mapped source covers faithfully the target space in all the settings. Behaviour of the SBADA-GAN and DA ass methods when changing their loss weights. (left) for SBADA-GAN we kept α = γ = 1 and η = 1, while we varied alternatively the weights of the classification losses β, µ with β = µ and keeping ν = 1, or the weight of the class consistency loss ν while fixing β = µ = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>we divided it into 50k samples for actual training and 10k for validation. All the 60k images from the MNIST-M training set were considered as test set. A subset of 1k images and their labels were also used to validate the classifier combination weights at test time.USPS → MNIST: USPS has 6, 562 training, 729 validation, and 2, 007 test images. All of them were resized to 28 × 28 pixels. The 60k training images of MNIST were considered as test set, with 1k samples and their labels also used for validation purposes.MNIST → USPS: even in this case MNIST training images were divided into 50k samples for actual training and 10k for validation. We tested on the whole set of 9, 298 images of USPS. Out of them, 1k USPS images and their labels were also used for validation. SVHN → MNIST: SVHN contains over 600k color images of which 73, 257 samples are used for training and 26, 032 for validation while the remaining data are somewhat less difficult samples. We disregarded this last set and considered only the first two. The 60k MNIST training samples were considered as test set, with 1k MNIST images and their labels also used for validation.MNIST → SVHN: for MNIST we used again the 50k/10k training/validation sets. The whole set of 99, 289 SVHN samples was considered for testing with 1k images and their labels also used for validation.Synth Signs → GTSRB: the Synth Signs dataset contains 100k images, out of which 90k were used for training and 10k for validation. The model was tested on the whole GTSRB dataset containing 51, 839 samples resized with bilinear interpolation to match the Synth Signs images' size of 40 × 40 pixels. Similarly to the previous cases, 1k GTSRB images and their labels were considered for validation purposes.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The chosen experimental settings match the ones used in most of the previous work involving GAN<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32]</ref> and domain adaptation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">More details about these experiments on robustness are provided in the appendix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the ERC grant 637076 -RoboExNovo.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SBADA-GAN network architecture</head><p>We composed SBADA-GAN starting from two symmetric GANs, each with an architecture analogous to that used for the PixelDA model. Specifically</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-Consistent Adversarial Domain Adaptation</title>
	</analytic>
	<monogr>
		<title level="m">Blind Submission, International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
	</analytic>
	<monogr>
		<title level="m">Blind Submission, International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain Separation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation problems: A dasvm classification technique and a circular validation strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marconcini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno>2017. 5</idno>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer series in statistics Springer</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative selflabeling domain adaptation for linear structured image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Peyrache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Artificial Intelligence Tools</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Associative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Frerix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised imageto-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using tsne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04076</idno>
		<title level="m">Multiclass generative adversarial networks with the l2 loss function</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784.2</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluation of traffic sign recognition methods trained on synthetically generated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Concepts for Intelligent Vision Systems (ACIVS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain adaptation of weighted majority votes via perturbed variation-based self-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Morvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="37" to="43" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Asymmetric tritraining for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01705</idno>
		<title level="m">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning transferrable representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The German traffic sign recognition benchmark: a multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised crossdomain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
