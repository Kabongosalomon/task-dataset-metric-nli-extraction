<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mask Scoring R-CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of AI</orgName>
								<orgName type="department" key="dep2">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology ‡ Horizon Robotics Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of AI</orgName>
								<orgName type="department" key="dep2">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology ‡ Horizon Robotics Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of AI</orgName>
								<orgName type="department" key="dep2">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology ‡ Horizon Robotics Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
							<email>chang.huang@horizon.ai</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of AI</orgName>
								<orgName type="department" key="dep2">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology ‡ Horizon Robotics Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<email>xgwang@hust.edu.cnlichao.huang</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of AI</orgName>
								<orgName type="department" key="dep2">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology ‡ Horizon Robotics Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mask Scoring R-CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Letting a deep network be aware of the quality of its own predictions is an interesting yet important problem. In the task of instance segmentation, the confidence of instance classification is used as mask quality score in most instance segmentation frameworks. However, the mask quality, quantified as the IoU between the instance mask and its ground truth, is usually not well correlated with classification score. In this paper, we study this problem and propose Mask Scoring R-CNN which contains a network block to learn the quality of the predicted instance masks. The proposed network block takes the instance feature and the corresponding predicted mask together to regress the mask IoU. The mask scoring strategy calibrates the misalignment between mask quality and mask score, and improves instance segmentation performance by prioritizing more accurate mask predictions during COCO AP evaluation. By extensive evaluations on the COCO dataset, Mask Scoring R-CNN brings consistent and noticeable gain with different models, and outperforms the state-of-the-art Mask R-CNN. We hope our simple and effective approach will provide a new direction for improving instance segmentation. The source code of our method is available at https:// github.com/zjhuang22/maskscoring_rcnn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep networks are dramatically driving the development of computer vision, leading to a series of state-ofthe-art in tasks including classification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref>, object detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, semantic segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b17">18]</ref> etc. From the development of deep learning in computer vision, we can observe that the ability of deep networks is gradually growing from making imagelevel prediction <ref type="bibr" target="#b21">[22]</ref> to region/box-level prediction <ref type="bibr" target="#b11">[12]</ref>, pixel-level prediction <ref type="bibr" target="#b27">[28]</ref> and instance/mask-level prediction <ref type="bibr" target="#b14">[15]</ref>. The ability of making fine-grained predictions re- * The work was done when Zhaojin Huang was an intern in Horizon Robotics Inc. quires not only more detailed labels but also more delicate network designing.</p><p>In this paper, we focus on the problem of instance segmentation, which is a natural next step of object detection to move from coarse box-level instance recognition to precise pixel-level classification. Specifically, this work presents a novel method to score the instance segmentation hypotheses, which is quite important for instance segmentation evaluation. The reason lies in that most evaluation metrics are defined according to the hypothesis scores, and more precise scores help to better characterize the model performance. For example, precision-recall curves and average precision (AP) are often used for the challenging instance segmentation dataset COCO <ref type="bibr" target="#b25">[26]</ref>. If one instance segmentation hypothesis is not properly scored, it might be wrongly regarded as false positive or false negative, resulting in a decrease of AP.</p><p>However, in most instance segmentation pipelines, such as Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> and MaskLab <ref type="bibr" target="#b2">[3]</ref>, the score of the instance mask is shared with box-level classification confidence, which is predicted by a classifier applied on the proposal feature. It is inappropriate to use classification confidence to measure the mask quality since it only serves for distinguishing the semantic categories of proposals, and is not aware of the actual quality and completeness of the instance mask. The misalignment between classification confidence and mask quality is illustrated in <ref type="figure">Fig. 1</ref>, where instance segmentation hypotheses get accurate box-level localization results and high classification score, but the corresponding masks are inaccurate. Obviously, scoring the masks using such classification score tends to degrade the evaluation results.</p><p>Unlike the previous methods that aim to obtain more accurate instance localization or segmentation mask, our method focuses on scoring the masks. To achieve this goal, our model learns a score for each mask instead of using its classification score. For clarity, we call the learned score mask score.</p><p>Inspired by the AP metric of instance segmentation that <ref type="figure">Figure 1</ref>. Demonstrative cases of instance segmentation in which bounding box has a high overlap with ground truth and a high classification score while the mask is not good enough. The scores predicted by both Mask R-CNN and our proposed MS R-CNN are attached above their corresponding bounding boxes. The left four images show good detection results with high classification scores but low mask quality.</p><p>Our method aims at solving this problem. The rightmost image shows the case of a good mask with a high classification score. Our method will retrain the high score. As can be seen, scores predicted by our model can better interpret the actual mask quality.</p><p>uses pixel-level Intersection-over-Union (IoU) between the predicted mask and its ground truth mask to describe instance segmentation quality, we propose a network to learn the IoU directly. In this paper, this IoU is denoted as MaskIoU. Once we obtain the predicted MaskIoU in testing phase, mask score is reevaluated by multiplying the predicted MaskIoU and classification score. Thus, mask score is aware of both semantic categories and the instance mask completeness. Learning MaskIoU is quite different from proposal classification or mask prediction, as it needs to "compare" the predicted mask with object feature. Within the Mask R-CNN framework, we implement a MaskIoU prediction network named MaskIoU head. It takes both the output of the mask head and RoI feature as input, and is trained using a simple regression loss. We name the proposed model, namely Mask R-CNN with MaskIoU head, as Mask Scoring R-CNN (MS R-CNN). Extensive experiments with our MS R-CNN have been conducted, and the results demonstrate that our method provides consistent and noticeable performance improvement attributing to the alignment between mask quality and score.</p><p>In summary, the main contributions of this work are highlighted as follows:</p><p>1. We present Mask Scoring R-CNN, the first framework that addresses the problem of scoring instance segmentation hypothesis. It explores a new direction for improving the performance of instance segmentation models. By considering the completeness of instance mask, the score of instance mask can be penalized if it has high classification score while the mask is not good enough.</p><p>2. Our MaskIoU head is very simple and effective. Experimental results on the challenging COCO benchmark show that when using mask score from our MS R-CNN rather than only classification confidence, the AP improves consistently by about 1.5% with various backbone networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Instance Segmentation</head><p>Current instance segmentation methods can be roughly categorized into two classes. One is detection based methods and the other is segmentation based methods. Detection based methods exploit the state-of-the-art detectors, such as Faster R-CNN <ref type="bibr" target="#b32">[33]</ref>, R-FCN <ref type="bibr" target="#b7">[8]</ref>, to get the region of each instance, and then predict the mask for each region. Pinheiro et al. <ref type="bibr" target="#b30">[31]</ref> proposed DeepMask to segment and classify the center object in a sliding window fashion. Dai et al. <ref type="bibr" target="#b5">[6]</ref> proposed instance-sensitive FCNs to generate the position-sensitive maps and assembled them to obtain the final masks. FCIS <ref type="bibr" target="#b22">[23]</ref> takes position-sensitive maps with inside/outside scores to generate the instance segmentation results. He et al. <ref type="bibr" target="#b14">[15]</ref> proposed Mask R-CNN that is built on the top of Faster R-CNN by adding an instancelevel semantic segmentation branch. Based on Mask R-CNN, Chen et al. <ref type="bibr" target="#b2">[3]</ref> proposed MaskLab that used positionsensitive scores to obtain better results. However, an underlying drawback in these methods is that mask quality is only measured by the classification scores, thus resulting in the issues discussed above.</p><p>Segmentation based methods predict the category labels of each pixel first and then group them together to form instance segmentation results. Liang et al. <ref type="bibr" target="#b23">[24]</ref> used spectral clustering to cluster the pixels. Other works, such as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, add boundary detection information during the clustering procedure. Bai et al. <ref type="bibr" target="#b0">[1]</ref> predicted pixel-level energy values and used watershed algorithms for grouping. Recently, there are some works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10]</ref> using metric learning to learn the embedding. Specifically, these methods learn an embedding for each pixel to ensure that pixels from the same instance have similar embedding. Afterwards, clustering is performed on the learned embed- ding to obtain the final instance labels. As these methods do not have explicit scores to measure the instance mask quality, they have to use the averaged pixel-level classification scores as an alternative. Both classes of the above methods do not take into consideration the alignment between mask score and mask quality. Due to the unreliability of mask score, a mask hypothesis with higher IoU against ground truth is vulnerable to be ranked with low priority if it has a low mask score. In this case, the final AP is consequently degraded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Detection Score Correction</head><p>There are several methods focusing on correcting the classification score for the detection box, which have a similar goal to our method. Tychsen-Smith et al. <ref type="bibr" target="#b35">[36]</ref> proposed Fitness NMS that corrected the detection score using the IoU between the detected bounding boxes and their ground truth. It formulates box IoU prediction as a classification task. Our method differs from this method in that we formulate mask IoU estimation as a regression task. Jiang et al. <ref type="bibr" target="#b18">[19]</ref> proposed IoU-Net that regressed box IoU directly, and the predicted IoU was used for both NMS and bounding box refinement. In <ref type="bibr" target="#b4">[5]</ref>, Cheng et al. discussed the false positive samples and used a separated network for correcting the score of such samples. SoftNMS <ref type="bibr" target="#b1">[2]</ref> uses the overlap between two boxes to correct the low score box. Neumann et al. <ref type="bibr" target="#b28">[29]</ref> proposed Relaxed Softmax to predict temperature scaling factor value in standard softmax for safety-critical pedestrian detection.</p><p>Unlike these methods that focus on bounding box level detection, our method is designed for instance segmentation. The instance mask is further processed in our Mask-IoU head so that the network can be aware of the completeness of instance mask, and the final mask score can reflect the actual quality of the instance segmentation hypothesis.</p><p>It is a new direction for improving the performance of instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>In the current Mask R-CNN framework, the score of a detection (i.e., instance segmentation) hypothesis is determined by the largest element in its classification scores. Due to the problems of background clutter, occlusion etc., it is possible that the classification score is high but the mask quality is low, as the examples shown in <ref type="figure">Fig. 1</ref>. To quantitatively analyze this problem, we compare the vanilla mask score from Mask R-CNN with the actual IoU between the predicted mask and its ground truth mask (MaskIoU). Specifically, we conduct experiments using Mask R-CNN with ResNet-18 FPN on COCO 2017 validation dataset. Then we select the detection hypotheses after Soft-NMS with both MaskIoU and classification scores larger than 0.5. The distribution of MaskIoU over classification score is shown in <ref type="figure" target="#fig_0">Fig. 2</ref> (a) and the average classification score in each MaskIoU interval is shown in blue in <ref type="figure" target="#fig_0">Fig. 2 (c)</ref>. These figures show that classification score and MaskIoU is not well correlated in Mask R-CNN.</p><p>In most instance segmentation evaluation protocols, such as COCO, a detection hypothesis with a low MaskIoU and a high score is harmful. In many practical applications, it is important to determine when the detection results can be trusted and when they cannot <ref type="bibr" target="#b28">[29]</ref>. This motivates us to learn a calibrated mask score according to MaskIoU for every detection hypothesis. Without loss of generality, we work on the Mask R-CNN framework, and propose Mask Scoring R-CNN (MS R-CNN), a Mask R-CNN with an additional MaskIoU head module that learns the Mask-IoU aligned mask score. The predicted mask scores of our framework are shown in <ref type="figure" target="#fig_0">Fig. 2</ref> (b) and the orange histogram in <ref type="figure" target="#fig_0">Fig. 2</ref> (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mask scoring in Mask R-CNN</head><p>Mask Scoring R-CNN is conceptually simple: Mask R-CNN with MaskIoU Head, which takes the instance feature and the predicted mask together as input, and predicts the IoU between input mask and ground truth mask, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We will present the details of our framework in the following sections.</p><p>Mask R-CNN: We begin by briefly reviewing the Mask R-CNN <ref type="bibr" target="#b14">[15]</ref>. Following Faster R-CNN <ref type="bibr" target="#b32">[33]</ref>, Mask R-CNN consists of two stages. The first stage is the Region Proposal Network (RPN). It proposes candidate object bounding boxes regardless of object categories. The second stage is termed as the R-CNN stage, which extracts features using RoIAlign for each proposal and performs proposal classification, bounding box regression and mask predicting.</p><p>Mask scoring: We define s mask as the score of the predicted mask. The ideal s mask is equal to the pixel-level IoU between predicted mask and its matched ground truth mask, which is termed as MaskIoU before. The ideal s mask also should only have positive value for ground truth category, and be zero for other classes, since a mask only belong to one class. This requires the mask score to works well on two task: classifying the mask to right category and regressing the proposal's MaskIoU for foreground object category.</p><p>It is hard to train the two tasks only using a single objective function. For simplify, we can decompose the mask score learning task into mask classification and IoU regression, denoted as s mask = s cls · s iou for all object categories. s cls focuses on classifying the proposal belong to which class and s iou focuses on regressing the MaskIoU.</p><p>As for s cls , the goal of s cls is to classify the proposal belonging to which class, which has been done in the classification task in the R-CNN stage. So we can directly take the corresponding classification score. Regressing s iou is the target of this paper, which is discussed in the following paragraph.</p><p>MaskIoU head: The MaskIoU head aims to regress the IoU between the predicted mask and its ground truth mask. We use the concatenation of feature from RoIAlign layer and the predicted mask as the input of MaskIoU head. When concatenating, we use a max pooing layer with kernel size of 2 and stride of 2 to make the predicted mask have the same spatial size with RoI feature. We only choose to regress the MaskIoU for the ground truth class (for testing, we choose the predicted class) instead of all classes. Our MaskIoU head consists of 4 convolution layers and 3 fully connected layers. For the 4 convolution layers, we follow Mask head and set the kernel size and filter number to 3 and 256 respectively for all the convolution layers. For the 3 fully connected (FC) layers, we follow the RCNN head and set the outputs of the first two FC layers to 1024 and the output of the final FC to the number of classes.</p><p>Training: For training the MaskIoU head, we use the RPN proposals as training samples. The training samples are required to have a IoU between proposal box and the matched ground truth box larger than 0.5, which are the same with the training samples of the Mask head of Mask R-CNN. For generating the regression target for each training sample, we firstly get the predicted mask of the target class and binarize the predicted mask using a threshold of 0.5</p><p>Then we use the MaskIoU between the binary mask and its matched ground truth as the MaskIoU target. We use the Inference: During inference, we just use MaskIoU head to calibrate classification score generated from R-CNN. Specifically, suppose the R-CNN stage of Mask R-CNN outputs N bounding boxes, and among them top-k (i.e. k = 100) scoring boxes after SoftNMS <ref type="bibr" target="#b1">[2]</ref> are selected. Then the top-k boxes are fed into the Mask head to generate multi-class masks. This is the standard Mask R-CNN inference procedure. We follow this procedure as well, and feed the top-k target masks to predict the MaskIoU. The predicted MaskIoU are multiplied with classification score, to get the new calibrated mask score as the final mask confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>All experiments are conducted on the COCO dataset <ref type="bibr" target="#b25">[26]</ref> with 80 object categories. We follow COCO 2017 settings, using the 115k images train split for training, 5k validation split for validation, 20k test-dev split for test. We use COCO evaluation metrics AP (averaged over IoU thresholds) to report the results, including AP@0.5, AP@0.75, and AP S , AP M , AP L (AP at different scales). AP@0.5 (or AP@0.75) means using an IoU threshold 0.5 (or 0.75) to identify whether a predicted bounding box or mask is positive in the evaluation. Unless noted, AP is evaluated using mask IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We use our reproduced Mask R-CNN for all experiments. We use ResNet-18 based FPN network for ablation study and ResNet-18/50/101 based on Faster R-CNN/FPN/DCN+FPN <ref type="bibr" target="#b8">[9]</ref> for comparing our method with other baseline results. For ResNet-18 FPN, input images   <ref type="table">Table 1</ref>. COCO 2017 validation results. We report both detection and instance segmentation results. APm denotes instance segmentation results and AP b denotes detection results. The results without are those of Mask R-CNN, while with are those of our MS R-CNN. The results show that our method is insensitive to different backbone networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>MaskIoU head AP m AP m @0.5 AP m @0.75 AP b AP b @0.5 AP b @0.75   <ref type="bibr" target="#b12">[13]</ref>. We train all the networks for 18 epochs, decreasing the learning rate by a factor of 0.1 after 14 epochs and 17 epochs. Synchronized SGD with momentum 0.9 is used as optimizer. For testing, we use SoftNMS and retain the top-100 score detection for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Results</head><p>We report our results on different backbone networks including ResNet-18/50/101 and different framework including Faster R-CNN/FPN/DCN+FPN <ref type="bibr" target="#b8">[9]</ref> to prove the effectiveness of our method. Results are shown in <ref type="table">Table 1</ref> and <ref type="table" target="#tab_0">Table 2</ref>. We use AP m to report instance segmentation results and AP b to report detection results. We report our reproduced Mask R-CNN results and our MS R-CNN results. As <ref type="table">Table 1</ref> shows, comparing with Mask R-CNN, our MS R-CNN is not sensitive to the backbone network and can achieve stable improvement on all backbone networks: Our MS R-CNN can get a remarkable improvement (about 1.5 AP). Especially for AP@0.75, our method can improve baseline by about 2 points. <ref type="table" target="#tab_0">Table 2</ref> indicates that our MS R-CNN is robust to different framework including Faster R-CNN/FPN/DCN+FPN. Beside, our MS R-CNN does not harm bounding box detection performance; in fact, it improves bounding box detection performance slightly. The results of test-dev are reported in <ref type="table" target="#tab_1">Table 3</ref>, only the instance segmentation results are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We comprehensively evaluate our method on COCO 2017 validation set. We use ResNet-18 FPN for all the ablation study experiments.</p><p>The design choices of MaskIoU head input: We first study the design choices of the MaskIoU head input, which is the fusion of predicted mask score map (28×28×C) from the mask head and the RoI features. There are a few design choices shown in <ref type="figure" target="#fig_4">Fig. 4</ref> and explained as follows:</p><p>(a) Target mask concatenates RoI feature: The score map of the target class is taken, max-pooled and concatenated with RoI feature.</p><p>(b) Target mask multiplies RoI feature: The score map of the target class is taken, max-pooled and multiplied with RoI feature.</p><p>(c) All masks concatenates RoI feature: All the C classes mask score map are max-pooled and concatenated with RoI feature.</p><p>(d) Target mask concatenates High-resolution RoI feature:</p><p>The score map of the target class is taken and concatenated with 28 ×28 RoI features.</p><p>The results are shown in <ref type="table" target="#tab_2">Table 4</ref>. We can see that the performance of MaskIoU head is robust to different ways of fusing mask prediction and RoI feature. Performance gain is observed in all kinds of design. Since concatenating the target score map and RoI feature obtains the best results, we use it as our default choice.   The choices of the training target: As mentioned before, we decompose the mask score learning task as mask classification and MaskIoU regression. Is it possible to learn the mask score directly? In addition, a RoI may contain multiple categories of objects. Should we learn MaskIoU for all categories? How to set the training target for MaskIoU head still need exploration. There are many different choices of training target:</p><p>1. Learning the MaskIoU of the target category, meanwhile the other categories in the proposal are ignored. This is also the default training target in this paper, and the control group for all experiments in this paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Learning the MaskIoU for all categories. If a category does not appear in the RoI, its target MaskIoU is set to 0. This setting denotes using regression only to predict MaskIoU, which requires the regressor to be aware of the absence of unrelated categories.</p><p>3. Learning the MaskIoU of all the positive categories, where a positive category means the category appears in the RoI region. And the rest categories in the proposal are ignored. This setting is used to see whether perform regression for more categories in the RoI region could be better. <ref type="table" target="#tab_3">Table 5</ref> shows the results for above training targets. By Given a threshold τ , we use the samples whose Mask-IoU are larger than τ to train the MaskIoU head. <ref type="table" target="#tab_4">Table 6</ref> shows the results. The results show that training using all the examples obtains the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>In this section, we will first discuss the quality of the predicted MaskIoU, and then investigate the upper bound performance of Mask Scoring R-CNN if the prediction of MaskIoU is perfect, and analyze the computational complexity of MaskIoU head at last. In the discussions, all The quality of the predicted MaskIoU: We use correlation coefficient between ground truth and predicted Mask-IoU to measure the quality of our prediction. Reviewing our testing procedure, we choose the top 100 scoring boxes after SoftNMS according to the classification scores, fed the detected boxes to Mask head and get the predicted mask, then use the predicted mask and RoI feature as the input of MaskIoU head. The output of MaskIoU head and classification score are further integrated into final mask score. We keep 100 predicted MaskIoU for each image in the COCO 2017 validation dataset, collecting 500, 000 predictions from all 5, 000 images. We plot each predictions and their corresponding ground truth in <ref type="figure" target="#fig_5">Fig. 5</ref>. We can see that the MaskIoU predictions have good correlation with their ground truth, especially for those prediction with high MaskIoU. The correlation coefficient between predictions and their ground truth is around 0.74 for both ResNet-18 FPN and ResNet-101 DCN+FPN backbone networks. It indicates that the quality of the prediction is not sensitive to the change of backbone networks. This conclusion is also consistent with <ref type="table">Table 1</ref>. Since there is no method works on predicting MaskIoU before, we refer to a previous work <ref type="bibr" target="#b18">[19]</ref> on predicting bounding box IoU. <ref type="bibr" target="#b18">[19]</ref> obtains a 0.617 correlation coefficient, which is inferior to ours.</p><p>The upper bound performance of MS R-CNN: Here we will discuss the upper bound performance of our method. For each predicted mask, we can find its matched ground truth mask; then we just use the ground truth Mask-IoU to replace the predicted MaskIoU when the ground truth MaskIoU larger than 0. The results are shown in Table 7. The results show that Mask Scoring R-CNN consistently outperforms Mask R-CNN. Compared to the ideal prediction of Mask Scoring R-CNN, there is still a room to improve the practical Mask Scoring R-CNN, which are 2.2% AP for ResNet-18 FPN backbone and 2.6% AP for ResNet-101 DCN+FPN backbone.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we investigate the problem of scoring instance segmentation masks and propose Mask Scoring R-CNN. By adding a MaskIoU head in Mask R-CNN, scores of the masks are aligned with MaskIoU, which is usually ignored in most instance segmentation frameworks. The proposed MaskIoU head is extremely effective and easy to implement. On the COCO benchmark, extensive results show that Mask Scoring R-CNN consistently and obviously outperforms Mask R-CNN. It also can be applied to other instance segmentation networks to obtain more reliable mask scores. We hope our simple and effective approach will serve as a baseline and help the future research in instance segmentation task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Comparisons of Mask R-CNN and our proposed MS R-CNN. (a) shows the results of Mask R-CNN, the mask score has less relationship with MaskIoU. (b)shows the results of MS R-CNN, we penalize the detection with high score and low MaskIoU, and the mask score can correlate with MaskIoU better. (c) shows the quantitative results, where we average the score between each MaskIoU interval, we can see that our method can have a better correspondence between score and MaskIoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Network architecture of Mask Scoring R-CNN. The input image is fed into a backbone network to generate RoIs via RPN and RoI features via RoIAlign. The RCNN head and Mask head are standard components of Mask R-CNN. For predicting MaskIoU, we use the predicted mask and RoI feature as input. The MaskIoU head has 4 convolution layers (all have kernel=3 and the final one uses stride=2 for downsampling) and 3 fully connected layers (the final one outputs C classes MaskIoU.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Different design choices of the MaskIoU head input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visualizations of MaskIoU predictions and their ground truth. (a) Results with ResNet-18 FPN backbone and (b) results with ResNet-101 DCN+FPN backbone. The x-axis presents the ground truth MaskIoU and the y-axis presents the predicted Mask-IoU of the proposed MaskIoU head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>COCO 2017 validation results. We report detection and instance segmentation results. APm denotes instance segmentation results and AP b denotes detection results. In the results area, rows 1&amp;2 use the Faster R-CNN framework; rows 3&amp;4 additionally use FPN framework; rows 5&amp;6 additionally use the DCN+FPN. The results show that consistent improvement of the proposed MaskIoU head.</figDesc><table><row><cell></cell><cell>-18 FPN</cell><cell>27.7 29.3</cell><cell>46.9 46.9</cell><cell>29.0 31.3</cell><cell>31.2 31.5</cell><cell>50.4 50.8</cell><cell>33.2 33.5</cell></row><row><cell cols="2">ResNet-50 FPN</cell><cell>34.5 36.0</cell><cell>55.8 55.8</cell><cell>36.7 38.8</cell><cell>38.6 38.6</cell><cell>59.2 59.2</cell><cell>42.5 42.5</cell></row><row><cell cols="2">ResNet-101 FPN</cell><cell>36.6 38.2</cell><cell>58.6 58.4</cell><cell>39.0 41.5</cell><cell>41.3 41.4</cell><cell>61.7 61.8</cell><cell>45.9 46.3</cell></row><row><cell>Backbone</cell><cell cols="7">MaskIoU head FPN DCN AP m AP m @0.5 AP m @0.75 AP b AP b @0.5 AP b @0.75</cell></row><row><cell></cell><cell></cell><cell>33.9</cell><cell>53.9</cell><cell>36.2</cell><cell>38.6</cell><cell>57.3</cell><cell>42.8</cell></row><row><cell></cell><cell></cell><cell>35.0</cell><cell>54.0</cell><cell>37.7</cell><cell>38.7</cell><cell>57.4</cell><cell>43.0</cell></row><row><cell>ResNet-101</cell><cell></cell><cell>36.6 38.2</cell><cell>58.6 58.4</cell><cell>39.0 41.5</cell><cell>41.3 41.4</cell><cell>61.7 61.8</cell><cell>45.9 46.3</cell></row><row><cell></cell><cell></cell><cell>37.7</cell><cell>60.3</cell><cell>40.0</cell><cell>42.9</cell><cell>63.4</cell><cell>47.8</cell></row><row><cell></cell><cell></cell><cell>39.1</cell><cell>60.0</cell><cell>42.4</cell><cell>43.1</cell><cell>63.5</cell><cell>47.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparing different instance segmentation methods on COCO 2017 test-dev.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="6">AP AP@0.5 AP@0.75 AP S AP M AP L</cell></row><row><cell>MNC [7]</cell><cell>ResNet-101</cell><cell>24.6</cell><cell>44.3</cell><cell>24.8</cell><cell>4.7</cell><cell cols="2">25.9 43.6</cell></row><row><cell>FCIS [23]</cell><cell>ResNet-101</cell><cell>29.2</cell><cell>49.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FCIS+++ [23]</cell><cell>ResNet-101</cell><cell>33.6</cell><cell>54.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mask R-CNN [15]</cell><cell>ResNet-101</cell><cell>33.1</cell><cell>54.9</cell><cell>34.8</cell><cell cols="3">12.1 35.6 51.1</cell></row><row><cell>Mask R-CNN [15]</cell><cell>ResNet-101 FPN</cell><cell>35.7</cell><cell>58.0</cell><cell>37.8</cell><cell cols="3">15.5 38.1 52.4</cell></row><row><cell>Mask R-CNN [15]</cell><cell>ResNeXt-101 FPN</cell><cell>37.1</cell><cell>60.0</cell><cell>39.4</cell><cell cols="3">16.9 39.9 53.5</cell></row><row><cell>MaskLab [3]</cell><cell>ResNet-101</cell><cell>35.4</cell><cell>57.4</cell><cell>37.4</cell><cell cols="3">16.9 38.3 49.2</cell></row><row><cell>MaskLab+ [3]</cell><cell>ResNet-101</cell><cell>37.3</cell><cell>59.8</cell><cell>36.6</cell><cell cols="3">19.1 40.5 50.6</cell></row><row><cell>MaskLab+ [3]</cell><cell>ResNet-101 (JET)</cell><cell>38.1</cell><cell>61.1</cell><cell>40.4</cell><cell cols="3">19.6 41.6 51.4</cell></row><row><cell>Mask R-CNN MS R-CNN</cell><cell>ResNet-101</cell><cell>34.3 35.4</cell><cell>55.0 54.9</cell><cell>36.6 38.1</cell><cell cols="3">13.2 36.4 52.2 13.7 37.6 53.3</cell></row><row><cell>Mask R-CNN MS R-CNN</cell><cell>ResNet-101 FPN</cell><cell>37.0 38.3</cell><cell>59.2 58.8</cell><cell>39.5 41.5</cell><cell cols="3">17.1 39.3 52.9 17.8 40.4 54.4</cell></row><row><cell>Mask R-CNN MS R-CNN</cell><cell>ResNet-101 DCN+FPN</cell><cell>38.4 39.6</cell><cell>61.2 60.7</cell><cell>41.2 43.1</cell><cell cols="3">18.0 40.5 55.2 18.8 41.5 56.2</cell></row></table><note>are resized to have 600px along the short axis and a max- imum of 1000px along the long axis for training and test- ing. Different from the standard FPN [25], we only use C4, C5 for RPN proposal and feature extractor in ResNet-18. For ResNet-50/101, input images are resized to 800 px for the short axis and 1333px for the long axis for training and testing. The rest configurations for ResNet-50/101 follow Detectron</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Results of different design choices of the MaskIoU head input.</figDesc><table><row><cell>Setting</cell><cell cols="3">AP AP@0.5 AP@0.75</cell></row><row><cell>Mask R-CNN baseline</cell><cell>27.7</cell><cell>46.9</cell><cell>29.0</cell></row><row><cell>(a) Target mask + RoI</cell><cell>29.3</cell><cell>46.9</cell><cell>31.3</cell></row><row><cell>(b) Target mask × RoI</cell><cell>29.1</cell><cell>46.6</cell><cell>30.9</cell></row><row><cell>(c) All masks + RoI</cell><cell>29.1</cell><cell>46.6</cell><cell>30.8</cell></row><row><cell cols="2">(d) Target mask + HR RoI 29.1</cell><cell>46.7</cell><cell>31.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Results of using different training targets. How to select training samples: Since the proposed MaskIoU head is built on top of the Mask R-CNN framework, all the training samples for the MaskIoU head have a box-level IoU larger than 0.5 with its ground truth bounding box according to the setting in the Mask R-CNN. However, their MaskIoU may not exceed 0.5.</figDesc><table><row><cell>Setting</cell><cell cols="3">AP AP@0.5 AP@0.75</cell></row><row><cell>Mask R-CNN baseline</cell><cell>27.7</cell><cell>46.9</cell><cell>29.0</cell></row><row><cell>Setting #1: Target ins.</cell><cell>29.3</cell><cell>46.9</cell><cell>31.3</cell></row><row><cell>Setting #2: All cls.</cell><cell>24.5</cell><cell>41.6</cell><cell>25.6</cell></row><row><cell cols="2">Setting #3: Positive ins. 28.2</cell><cell>45.5</cell><cell>30.2</cell></row><row><cell cols="4">comparing setting #1 with setting #2, we can find that train-</cell></row><row><cell cols="4">ing MaskIoU of all categories (regression only based Mask-</cell></row><row><cell cols="4">IoU prediction) will degrade the performance drastically,</cell></row><row><cell cols="4">which verifies our opinion that training classification and</cell></row><row><cell cols="4">regression using a single objective function is difficult.</cell></row><row><cell cols="4">It is reasonable that the performance of setting #3 is infe-</cell></row><row><cell cols="4">rior to setting #1, since regressing MaskIoU for all positive</cell></row><row><cell cols="4">categories increases the burden of MaskIoU head. Thus,</cell></row><row><cell cols="4">learning the MaskIoU of the target category is used as our</cell></row><row><cell>default choice.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Results of selecting different training samples for the MaskIoU head.</figDesc><table><row><cell>Threshold</cell><cell>AP</cell><cell>AP@0.5</cell><cell>AP@0.75</cell></row><row><cell>τ = 0.0</cell><cell>29.3</cell><cell>46.9</cell><cell>31.3</cell></row><row><cell>τ = 0.3</cell><cell>29.2</cell><cell>46.6</cell><cell>31.1</cell></row><row><cell>τ = 0.5</cell><cell>29.0</cell><cell>46.5</cell><cell>30.9</cell></row><row><cell>τ = 0.7</cell><cell>28.8</cell><cell>46.9</cell><cell>30.5</cell></row><row><cell cols="4">the results are obtained on COCO 2017 validation set using</cell></row><row><cell cols="4">both a weak backbone network, i.e., ResNet-18 FPN and a</cell></row><row><cell cols="4">strong backbone network, i.e., ResNet-101 DCN+FPN.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Results of Mask R-CNN, MS R-CNN and the ideal case of MS R-CNN (MS R-CNN ) using ResNet-18 FPN and ResNet-101 DCN+FPN as backbones on COCO 2017 validation set. Model size and running time: Our MaskIoU head has about 0.39G FLOPs while Mask head has about 0.53G FLOPs for each proposal. We use one TITAN V GPU to test the speed (sec./image). As for ResNet-18 FPN, the speed is about 0.132 for both Mask R-CNN and MS R-CNN. As for ResNet-101 DCN+FPN, the speed is about 0.202 for both Mask R-CNN and MS R-CNN. The computation cost of MaskIoU head in Mask Scoring R-CNN is negligible.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>AP</cell></row><row><cell>Mask R-CNN</cell><cell></cell><cell>27.7</cell></row><row><cell>MS R-CNN</cell><cell>ResNet-18 FPN</cell><cell>29.3</cell></row><row><cell>MS R-CNN</cell><cell></cell><cell>31.5</cell></row><row><cell>Mask R-CNN</cell><cell></cell><cell>37.7</cell></row><row><cell>MS R-CNN</cell><cell>ResNet-101 DCN+FPN</cell><cell>39.1</cell></row><row><cell>MS R-CNN</cell><cell></cell><cell>41.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">loss for regressing MaskIoU, and the loss weight is set to 1. The proposed MaskIoU head is integrated into Mask R-CNN, and the whole network is end to end trained.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2858" to="2866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Softnmsimproving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04837</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Revisiting rcnn: On awakening the classification power of faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="473" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation via deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10277</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segmentation-aware convolutional networks using local attention masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5048" to="5057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11721</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="816" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Object detection free instance segmentation with labeling transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08991</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Instancecut: from edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7322" to="7331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Relaxed softmax: Efficient confidence auto-calibration for safe pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Object detection in videos by high quality object linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09823</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep patch learning for weakly supervised object classification and discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="446" to="459" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Improving object localization with fitness nms and bounded iou loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tychsen-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00164</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
