<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JRMOT: A Multi-Modal Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Shenoi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Patel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Goebel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Martín-Martín</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
						</author>
						<title level="a" type="main">JRMOT: A Multi-Modal Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robots navigating autonomously need to perceive and track the motion of objects and other agents in its surroundings. This information enables planning and executing robust and safe trajectories. To facilitate these processes, the motion should be perceived in 3D Cartesian space. However, most recent multi-object tracking (MOT) research has focused on tracking people and moving objects in 2D RGB video sequences. In this work we present JRMOT, a novel 3D MOT system that integrates information from RGB images and 3D point clouds to achieve real-time, state-of-the-art tracking performance. Our system is built with recent neural networks for re-identification, 2D and 3D detection and track description, combined into a joint probabilistic data-association framework within a multi-modal recursive Kalman architecture. As part of our work, we release the JRDB dataset, a novel large scale 2D+3D dataset and benchmark, annotated with over 2 million boxes and 3500 time consistent 2D+3D trajectories across 54 indoor and outdoor scenes. JRDB contains over 60 minutes of data including 360 • cylindrical RGB video and 3D pointclouds in social settings that we use to develop, train and evaluate JRMOT. The presented 3D MOT system demonstrates state-ofthe-art performance against competing methods on the popular 2D tracking KITTI benchmark and serves as first 3D tracking solution for our benchmark. Real-robot tests on our social robot JackRabbot indicate that the system is capable of tracking multiple pedestrians fast and reliably. We provide the ROS code of our tracker at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>An autonomous agent such as a mobile robot needs to move between two locations in a safe and robust manner. To navigate safely, the robot needs to perceive the motion of the multiple dynamic objects and other agents, e.g. people and cars, in its vicinity. This perceived motion allows the agent to predict the possible future trajectories of the other agents and to plan and execute motion strategies that take them into account.</p><p>To facilitate navigation, the motion of the other agents needs to be perceived and represented in the same space the navigation takes place, the 3D Cartesian space. However, most efforts from the robotics and computer vision communities have been dedicated to the development of multiobject tracking (MOT) systems that perceive 2D motion from RGB video streams. The reason for this is two fold. First, detecting and tracking objects in 3D is computationally more expensive than in 2D due to the curse of dimensionality in this search problem. And second, there is a lack of adequate <ref type="figure">Fig. 1</ref>: Robots navigating in human environments need to detect and track humans and other moving targets using their sensor information; JRMOT integrates information from 2D RGB images (top), where appearance is more easily discernible, and 3D point clouds (bottom), where objects are well separated, in a tightly coupled manner to provide real-time 3D multi-object tracking information; JRMOT is developed on the JRDB dataset, a novel annotated dataset captured with our social mobile manipulator JackRabbot <ref type="bibr">(bottom)</ref> large-scale curated datasets of 3D data with annotations of moving agents from the perspective of navigating robots in human environments, impeding the application of successful deep learning techniques to 3D tracking.</p><p>In this paper we present JRMOT, a novel real-time multiobject detection and tracking framework in 3D Cartesian space. JRMOT detects and tracks multiple targets around the agent by constraining the 3D search with 2D cues, effectively combining information from RGB cameras and LiDAR sensors. RGB images and 3D pointclouds carry complementary information. On the one hand, RGB images are dense, which allows us to discern appearances of objects to effectively detect, identify and classify them even at large distances. It is also structured in the form of a pixel grid, well suited to be processed with effective tools such as CNNs. On the other hand, 3D point-cloud data is sparse but the depth information allows us to separate objects that might overlap in the 2D image space. However, the unordered structure of the pointclouds do not allow for the use of efficient algorithmic architectures such as CNNs. JRMOT leverages the information of each modality (appearance in RGB, geometry in point clouds) to address the shortcomings of the other by i) sequentially processing them to guide the 3D search in regions indicated by the RGB image, ii) fusing their information into a multi-modal descriptor to facilitate tracking and data-association, and iii) updating the tracking state with a novel multi-modal measurement model. At its core, JRMOT applies state-of-the-art deep neural network architectures to detect objects of interest in RGB images and 3D point clouds, and to characterize tracks with novel multi-modal descriptors, improving the performance of well-established data-association and filtering techniques. Training such networks requires a large amount of 2D RGB images and 3D pointclouds annotated with ground truth labels of the location of objects of interest. The annotated data should be acquired from the perspective of the agent that will execute JRMOT, i.e. a mobile robot. JRMOT is trained with annotated data from a novel dataset of multimodal data, the JRDB dataset. The dataset is captured from the perspective of our social autonomous agent, JackRabbot, leading to a first-of-its-kind dataset that includes indoor and outdoor scenes, with over 4.2 million annotated bounding boxes in 2D RGB images and 3D pointclouds. This dataset enabled us to leverage the complementarity of 2D RGB and 3D pointcloud data with JRMOT.</p><p>To summarize, our contributions are: 1) We present JRMOT, a novel real-time online 3D MOT system that fuses 2D and 3D information based on latest deep-learning architectures. 2) We release the JRDB dataset and benchmark, a first of its kind 2D+3D dataset for the development and evaluation of 2D-3D MOT frameworks and 2D-3D people detection.</p><p>JRMOT is developed and evaluated on JRDB and serves as first competitive baseline. 3) We show that JRMOT achieves state-of-the-art performance in the competitive KITTI 2D tracking benchmark.</p><p>Our tests also indicate that our method can detect and track effectively in real time, running onboard a mobile robot, with only few ID switches and a single missed track in over 100 s of experiments. We provide JRMOT as ROS code for other researchers to test and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In recent years, there have been impressive advances in MOT, mostly focused on 2D tracking (in images) with some exceptions of new 3D MOT systems (from images and/or 3D data). We now review previous work in the areas of 2D MOT from 2D RGB videos, 3D MOT from 2D RGB and/or 3D sensors, real-time 3D MOT systems and existing datasets for MOT with 3D data.</p><p>2D MOT with 2D Data: Tracking in 2D is the task of perceiving continuously the motion of objects in video sequences. There exists a large body of literature for 2D MOT. Works such as <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> leverage the success of deep learning architectures for re-identification, and utilise appearance cues for track-detection association. Other works such as <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, use motion and continuity cues to do the same. JRMOT builds on top of this body of literature -we use 2D RGB to obtain appearance descriptors using deep learning based feature extractors, and use 3D pointcloud data to circumvent two problems faced by 2D MOT works. First, the problem of occlusion in 2D is largely reduced due to the large separation of objects in 3D space. Second, motion in 3D is often much more simple than the corresponding projected motion in the 2D RGB image and hence can be used as a much better cue for association. Further, the resulting tracks are in 3D space, which is required by most applications involving autonomous agents.</p><p>3D MOT with 2D and/or 3D Data: With the advent of self driving cars, access to large scale datasets containing LiDAR data <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> has reinvigorated interest in the use of 3D sensors. Some methods use exclusively 3D detections and pointcloud data to perform 3D tracking <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, not using any information from RGB images of the scene. JRMOT uses pointcloud data combined with RGB information to improve on tracking of far away objects; this is explored in <ref type="figure" target="#fig_2">Fig 4,</ref> which shows that as the distance of an object from the robot increases, out method largely mitigates the drop in performance seen in the baseline.</p><p>Baser et al. <ref type="bibr" target="#b12">[13]</ref> aggregate both 2D RGB appearance descriptor and the bounding box coordinates to learn a similarity function to perform 3D tracking. It independently detects in the 2D and 3D domains, and also only utilises 3D measurements to perform filtering, distinct from our work which tightly couples both 2D and 3D measurements. Luiten et al. <ref type="bibr" target="#b13">[14]</ref> utilise RGB and depth information to reconstruct a 4D spatio-temporal scene, but unlike our method do so in an offline setting. The effectiveness of these techniques has not been quantitatively tested because of the lack of a large scale 3D tracking benchmark. They are often evaluated via the proxy of 2D tracking, or on custom generated small-scale datasets.</p><p>Real-Time 3D MOT Systems Whereas there is a plethora of real-time 2D MOT systems such as <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b2">[3]</ref> , among many others, the community has developed just a handful of real-time 3D MOT systems. Koide et al. <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> both propose real time 3D MOT systems based exclusively on 3D LiDAR data without incorporating 2D data. This loose (or complete lack of) coupling of 2D and 3D information is sensitive to cases where 3D detection intermittently fails, whereas the 2D detection is robust, a key advantage of our method as shown in Sec V. Linder et al. <ref type="bibr" target="#b18">[19]</ref> and Dondrup et al. <ref type="bibr" target="#b19">[20]</ref> both utilise 2D and 3D data, but do not leverage recent advances in deep learning based detectors and feature descriptors.</p><p>3D Datasets: 3D sensory systems are becoming increasingly commonplace in sensor suites of autonomous agents. Datasets with this multi-modal data such as KITTI <ref type="bibr" target="#b7">[8]</ref>, Apolloscape <ref type="bibr" target="#b8">[9]</ref>, NuScenes <ref type="bibr" target="#b6">[7]</ref> and Oxford's Robotic Car <ref type="bibr" target="#b20">[21]</ref> have widely driven research in the 3D community. Nonetheless, their targeted domain of application is autonomous driving; the data they provide is captured from sensor suites on top of cars and only depicts streets, roads and highways. Frossard et al. <ref type="bibr" target="#b21">[22]</ref> specifically mention a lack of available 3D tracking benchmarks.</p><p>In this paper, we target a unique visual domain tailored to the perceptual tasks related to navigation in human en- We hope that this new domain provides the community an opportunity to develop visual perception frameworks, limited not only to self-driving cars but also various other types of autonomous navigation agents. Furthermore, we hope this dataset and benchmark will support and drive research in a variety of domains related to social robotics, including but not limited to human detection and tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. JRMOT: 3D MULTI-OBJECT TRACKING FROM 2D</head><p>AND 3D DATA Our proposed 3D MOT system fusing 2D and 3D data is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. JRMOT performs tracking by detection. The detector block contains a 2D detector, a 2D appearance feature extractor and a 3D detector (which also generates a 3D feature descriptor). The detector block takes as input a 2D RGB image and the corresponding pointcloud and produces 2D and 3D detections of all objects of interest, along with their 2D and 3D feature descriptors. This is then passed to the tracking block, which performs data association, as well as multi-modal Bayesian filtering. The output of our system is the location in 3D space of all tracked objects, each uniquely identified over time by a track ID. We assume the extrinsic calibration between the RGB camera and the depth sensor to be known. We now explain each component in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 2D Detection</head><p>First, our system needs to detect all moving instances of objects of interest in the environment. Although we are interested on 3D locations, 2D detectors are faster, more robust and accurate than 3D detectors <ref type="bibr" target="#b22">[23]</ref>. Therefore, we exploit state-of-the-art image segmentation (Mask R-CNN <ref type="bibr" target="#b23">[24]</ref>) or object detector (YOLO <ref type="bibr" target="#b24">[25]</ref> modified for real time) architectures as our detector. The input to this module is a 2D RGB image at time t and the output is a set of N detections in 2D,</p><formula xml:id="formula_0">D 2D t = {(u, v, w, h) 0 , . . . , (u, v, w, h) N −1 } t , where (u, v) i</formula><p>is the upper-left corner of the detected bounding box around the instance i and (w, h) i are the width and height of that box. The available pretrained models have been trained with different types of images than the ones our robot encounters during navigation. We make use of our JRDB dataset to finetune the networks and adapt them to the special data distribution of the social navigation setup 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 2D Appearance</head><p>The detections from the previous step need to be associated to existing tracks in JRMOT. To this end, we featurize the 2D appearance (appearance in the RGB image) of both detections and tracks in order to compare features and associate them later (Section III-E). We compute Aligned-ReID <ref type="bibr" target="#b25">[26]</ref> features when the objects of interest are people, and features from Wu et al. <ref type="bibr" target="#b26">[27]</ref> when they are vehicles. The choice of these features is based on their high discriminative capabilities and fast computation time. Both features are trained on JRDB. The input to this module is the 2D RGB image at time t and the N detections from the previous step, and the output are their 2D appearance features,</p><formula xml:id="formula_1">F 2D t = {f 2D 0 , . . . , f 2D N −1 } t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 3D Detection and Appearance</head><p>As mentioned before (Sec. II), it is possible to obtain a noisy estimate of the 3D location of a detected object from its 2D detected box. However, in this work we propose to integrate 2D RGB and 3D data provided by a depth sensor, which is a common part of most autonomous navigating systems. We utilise F-PointNet <ref type="bibr" target="#b27">[28]</ref>, a state-of-the-art algorithm to obtain 3D detections in the form of an oriented cuboid around the object instance for every 2D bounding box. F-PointNet estimates a 3D bounding box for that object within the frustum starting at the RGB camera center and passing through the 2D bounding box as illustrated in <ref type="figure">Fig 1.</ref> We choose F-PointNet because it explicitly gives us an association between every 2D and 3D bounding box, it leverages the robustness of 2D detectors, it has a relatively fast inference time, and it has been shown to be one of the top performing 3D detectors on the KITTI benchmark. The input to the 3D detection module is the set of detected 2D bounding boxes around instances of interest at time t, D 2D t , and the 3D pointcloud at time closest to t. The output is a set of M detections in 3D for the class of interest at time t,</p><formula xml:id="formula_2">D 3D t = {(x, y, z, w, h, l, θ) 0 , . . . , (x, y, z, w, h, l, θ) M −1 } t ,</formula><p>where (x, y, z) j is the center of the bottom face of the detected 3D bounding box around the instance j, (w, h, l) j are the width, height and length of that box and θ j is the rotation of the box around the normal to the floor plane.</p><p>Additionally, we exploit the F-PointNet architecture to generate feature descriptions of the shape of the detected objects,</p><formula xml:id="formula_3">F 3D t = {f 3D 0 , . . . , f 3D M −1 } t .</formula><p>The feature from the penultimate layer of F-PointNet is used to regress the 3D bounding box, and thus, it contains information about the 3D shape of the object. We use this feature as a 3D appearance (shape) descriptor.Note that not every 3D detection has an associated 2D detection. It is possible that F-PointNet does not find a reasonable bounding box within every frustum. Our system accounts for this case, as explained in Sec. III-G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Feature Fusion</head><p>Due to the coupling of 2D and 3D detections, each object of interest now has a 2D feature descriptor, and a 3D feature descriptor. Depending on the conditions (distance, visibility, occlusion) both 2D and 3D appearance can contain valuable information to associate detections and previous tracks. Therefore, we fuse the 2D and 3D features with a 3-layered fully connected network that receives as input</p><formula xml:id="formula_4">F cat t , given by { f 2D 0 , f 3D 0 , . . . , f 2D M −1 , f 3D M −1 } t</formula><p>where [] denotes concatenation. We train this fusion network via metric learning based on a triplet loss and semi-hard negative mining as in Schroff et al. <ref type="bibr" target="#b28">[29]</ref>, resulting in a robust feature for association between new detections and previous tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Data Association</head><p>Given a set of detections at time t, we need to associate them to tracks at t − 1 to update the tracks' locations and appearances. To do so we utilize JPDA <ref type="bibr" target="#b29">[30]</ref> as it has been shown by <ref type="bibr" target="#b5">[6]</ref> to be robust to clutter and reduce the occurrence of ID switches. JPDA requires a cost matrix, C ∈ R K×N in which element c ij represents the cost associated with matching track i to detection j. We utilize both appearance and 3D spatial location to associate objects. We first compute appearance similarity by calculating the pairwise 2 distance between the N features of detections and the K features of tracks and build an appearance cost matrix, C app ∈ R K×N . Then, we compute the location similarity by calculating the pairwise 3D bounding box intersection over union (IoU) assuming that both 3D bounding boxes have the same orientation (same θ), an approximation that generates fairly good results in much shorter computation time. The result is an IoU cost matrix, C IoU ∈ R K×N . To simplify the association, we perform gating with the Mahalanobis distance (M-distance) with a fixed threshold (0.95 quantile from the χ 2 distribution)</p><p>As the size of the cost matrices scales with the square of the number of objects in the scene, association with the entire cost matrix can lead to slow computation. We therefore construct an undirected graph, where every track and detection is a node, and an edge exists between track i and detection j if detection j is within the gate of track i. Every connected component in this graph is a cluster. We perform further processing on a per cluster basis leading to a much lower computation time.</p><p>Since JPDA requires a single cost matrix representing the association costs, we perform a cost matrix selection (IOU vs. appearance) based on an entropy measure. We select the cost matrix that has a lower entropy per track. A lower entropy cost matrix implies that the cost matrix is more 'peaked', and hence more discriminative.</p><p>Given the selected cost matrix, we perform JPDA. To maintain the speed of our tracker, we employ the m-best solution approximation <ref type="bibr" target="#b30">[31]</ref> for large clusters. For smaller clusters, complete enumeration is used to obtain the exact solution of JPDA.</p><p>To deal with the case where an objects has a 2D detection, but not a corresponding 3D detection, we utilise a two step process. In the first step, all measurements with both 2D and 3D detections are associated with tracks using the procedure above. We then do a second round of cost matrix selection, gating, and JPDA with the appearance cost matrix now only based on the 2D feature descriptor, and the IoU cost calculated with 2D IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Filtering</head><p>2D and 3D detections are often noisy. Therefore, we filter them over time with a Kalman filter <ref type="bibr" target="#b31">[32]</ref> to estimate smooth 3D tracks. The Kalman filter is an optimal estimator (assuming Gaussian noise and linearity in motion) and an online, computationally efficient process that allows JRMOT to be accurate and real time.</p><p>The state to estimate per object includes its 3D location, x, y, z, its dimensions approximated as a 3D bounding box, l, w, h and the rotation of the box about the vertical axis, θ. Since objects in most scenes move along the horizontal axes, X and Z, and with very small variation on their orientation, we only track the velocities along X and Y axes, v x and v y respectively. Hence, the state of each object O is</p><formula xml:id="formula_5">x O = {x, y, z, l, h, w, θ, v x , v z } O .</formula><p>We apply an independent Kalman filter for each object with a constant velocity motion model for predictions.</p><p>To leverage the joint nature of the detections and the multi-modal (2D and 3D) sensor source, we use a dual measurement update. Each track has two measurement sources, the 2D bounding boxes, as well as the 3D bounding boxes that we assume to be independent, although this is not strictly the case. We combine a first PDA <ref type="bibr" target="#b32">[33]</ref> Kalman filter update based on 3D measurements, with a second PDA Extended Kalman Filter (EKF) update, with the 2D measurements. The first linear measurement update serves as the primary component and carries most information, with (a) JackRabbot, our data collection platform and its equipped sensors For those 2D detections without a corresponding 3D detection, we perform a PDA update of the tracks with the 2D measurement only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Track Management</head><p>Creating and terminating tracks: When a new object enters the scene, a new track is initiated only if it is outside the gate of all existing tracks. In that case, we create a temporary track (not part of the JRMOT output) and only after n init number of consecutive matches, we promote it to full track. This process reduces noise and avoids false positives. Further, we terminate a track if there has been no matching detection for n term consecutive frames, to account for objects leaving the scene.</p><p>Updating tracks' appearance: At each step, we update the appearance of the tracks with the latest RGB and pointcloud information to facilitate association in the next step. To do so, we need to associate each last detection to only one track. However, JPDA provides a full probability distribution of association between tracks and detections. Therefore, we perform a linear sum assignment on the JPDA output using the Hungarian algorithm <ref type="bibr" target="#b33">[34]</ref> with p assn as the minimum probability for a match to be considered. This process provides one-to-one associations that allow us to update the feature descriptor of each track with the assigned best detection. If for a track no detection is assigned, its features are not updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASET</head><p>As reviewed in Sec. II, datasets with the type of 3D data and annotations necessary to develop and train 3D MOT systems are scarce and focused on autonomous driving scenarios: there is a need for novel datasets with 3D annotations in social environments from the perspective of navigating robots. We present the JRDB dataset, a novel dataset focused on human social environments. Our dataset contains 64 minutes of sensor data acquired from our mobile robot JackRabbot comprising 54 sequences indoors and outdoors in a university campus environment. In this section, we summarize the data collection and labeling process of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The JackRabbot Social Robot</head><p>The JRDB is a multimodal dataset collected with the sensors on-board of our mobile manipulator JackRabbot. JackRabbot is a custom-design robot platform tailored to navigate and interact in human environments. It is equipped with a state-of-the-art sensor suite including stereo RGB 360 • cylindrical video streams (resulting from composing images from to rows of five aligned cameras each), 3D point clouds from two 16 lines LiDAR sensors, front and back single line LiDAR pointclouds, RGB-D and 360 • spherical RGB images from the cameras on the head, audio, IMU and GPS sensing. <ref type="figure">Fig 3a depicts</ref> JackRabbot and its onboard sensors. Our goal is to investigate and develop novel solutions for perception and high-level social interactions between humans and robots through JackRabbot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Collection and Annotation</head><p>To generate JRDB, we collected data in 30 different locations indoors and outdoors, all in a university campus environment, with varying and uncontrolled environmental conditions such as illumination and other natural and dynamical elements. We also ensure the recorded data captures a variation of natural human posture, behaviour and social activities in different crowd densities. Furthermore, to incorporate a diversity in the robot's ego-motion, we use a combination of static and moving sensor (robot) views to capture the data.</p><p>A crucial component in the development of social autonomous navigating agents is to perceive and understand the location and motion of humans surrounding the robot. Therefore, in this first round of annotation we focus on detecting and tracking humans. We include the following ground truth labels in JRDB: a) over 2.4 million 2D bounding boxes for human/pedestrian class in both the ten separate RGB images and the two composed cylindrical 360 • images, b) over 1.8 million 3D oriented bounding boxes for human/pedestrian class in pointclouds from the two 16-lines LiDAR sensors, c) spatial ID association between corresponding 2D and 3D bounding boxes (all 3D boxes have an associated 2D box but not vice versa), and d) temporal ID association with time consistent identities for all annotated pedestrians in both 2D and 3D. <ref type="figure">Fig 3b depicts</ref> examples of JRDB and the annotated ground truth labels on both an RGB 360 • cylindrical image and a 3D LiDAR pointcloud, colored with the information from the RGB image. With this unique dataset, we hope to facilitate and enable novel research in social navigating autonomous agents. We will augment JRDB in the future with additional annotations related to social understanding in human environments such as 2D human skeleton posture and individual, group and social activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL EVALUATION</head><p>We adopt the standard Clear-MOT metrics <ref type="bibr" target="#b34">[35]</ref> in our evaluation, including accuracy (MOTA), precision (MOTP), and number of ID switches (IDS) along with runtime, as we aim at developing an online real-time MOT system. However, Clear-MOT metrics were developed for 2D tracking, e.g. tracks are designated true of false positives based on IoU between estimated and ground truth 2D bounding box in the RGB images. We extend these definitions to 3D based on 3D IoU computed combining the Sutherland-Hodgman algorithm <ref type="bibr" target="#b35">[36]</ref> and Gauss's area formula to determine the volume of the intersection.</p><p>Our goal is to develop a real time online MOT system for navigating robots in human environments. Therefore, we evaluate JRMOT on our novel JRDB dataset (3D) and the well-established KITTI dataset <ref type="bibr" target="#b36">[37]</ref>. The KITTI dataset contains 2D RGB images and 3D pointclouds, but the benchmark only reports 2D tracking results with 2D Clear-MOT metrics. Though JRMOT is a 3D MOT system, evaluating on KITTI allows us to compare to existing tracking methodologies. To be able to evaluate JRMOT on KITTI, we modify the system presented in Sec. 2 by changing the state in our filter architecture to {x, y, w, h, v x , v y }, where x, y, w, h parameterize the 2D bounding box and v x , v y give the velocity in the 2D image. The JRDB dataset and benchmark contains both RGB and pointcloud inputs, groundtruth 3D bounding boxes of pedestrians and an evaluation script for 3D tracking, which we use. We compare the results of JRMOT to a state-of-the-art baseline, AB3DMOT <ref type="bibr" target="#b11">[12]</ref>, on people tracking. We choose AB3DMOT as baseline due to it being a real time, online tracker, and the availability of the open-source code. At the time of submission, no other open-source online 3D MOT systems were available.</p><p>In order to provide comparable results, we aim to use identical detection inputs for all methods. For the KITTI dataset, we only use publicly available detections for the car and pedestrian challenges. For JRDB, we use the same set of Mask-RCNN detections for all methods. The publicly available detections for KITTI we chose were RRC <ref type="bibr" target="#b37">[38]</ref> detections for cars and SubCNN <ref type="bibr" target="#b38">[39]</ref> detections for pedestrians. For our evaluation of AB3DMOT on JRDB, which requires 3D detections as input, we used the 3D detections from F-PointNet which were generated as a by product from our tracking system. <ref type="bibr" target="#b39">[40]</ref> 85.04% 85.53% 301 0.01s mmMOT* <ref type="bibr" target="#b13">[14]</ref> 84.77% 85.21% 284 0.01s MOTBP* <ref type="bibr" target="#b40">[41]</ref> 84.24% 85.73% 468 0.3s IMMDP <ref type="bibr" target="#b41">[42]</ref> 83.04% 82.74% 172 0.19s JCSTD <ref type="bibr" target="#b42">[43]</ref> 80  For experiments on KITTI we use the following parameter settings: p assn = 0.65, n init = 2, n term = 2. For experiments on JRDB, we use: p assn = 0.6, n init = 3, n term = 5.</p><formula xml:id="formula_6">MOTA ↑ MOTP ↑ IDS ↓ Runtime ↓ MASS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>KITTI Dataset: <ref type="table" target="#tab_1">Table I</ref> shows our results in the car tracking challenge. We achieve state-of-the-art performance (highest MOTA) among all online published 2D MOT methods. Our MOTP is within 0.5% of the leader and we are second in terms of ID switches and beat all other top submissions by sizable margins. <ref type="table" target="#tab_1">Table II</ref> shows our results in the pedestrian tracking challenge. Amongst competing real-time methods (computation time less than 0.1s), our tracker ranks second.</p><p>Only one other method uses the same detections as our method. We remain within 1.5% MOTA, while running in only 1 15 th the time. The performance gains in our method are a consequence of fusing and fully leveraging complementary information in 2D RGB and 3D pointcloud information. One point to note is the higher IDS of JRMOT. We found that optimizing MOTA decreases FN's, at the expense of higher IDS. This hyperparameter optimization is only specific to KITTI pedestrian tracking as evidenced by <ref type="table" target="#tab_1">Table I</ref>, where our method achieves the 2 nd lowest IDS. Even though our method was developed for 3D MOT, JRMOT ranks among the state-of-the-art 2D MOT systems in KITTI benchmark, indicating the benefits of our proposed approach, and validating the effectiveness of the system.  JRDB Dataset: JRMOT outperforms the baseline, AB3MOT, on the JRDB benchmark with 20.2% MOTA at 25 fps (compared to 19.3% MOTA of AB3MOT). These MOTA values indicate that the scenes in our dataset are extremely challenging and will guide new research in the field. Based on the 765, 907 false negatives of our method on the test set, we infer that 3D detections are the limiting factor in our tracking system. The benefits of the JRMOT approach to combine 2D and 3D information are clearer for tracks relatively further away from the sensor, where 3D pointcoud data is sparse, but 2D RGB is a rich source of information. To verify this, we analyze the results as a function of the distance between tracks and robot. Although the MOTA remains fairly similar across all distances, with our method outperforming the baseline, we make the following observations. First, we observe that our hypothesis that 2D data is useful to improve orientation of 3D bounding boxes and make fine adjustments to position is validated in <ref type="figure" target="#fig_2">Fig. 4</ref>. It can be seen that as the distance from the robot increases, the MOTP of AB3DMOT degrades considerably, whereas our method is consistent across all distance ranges. Further, our method has 30% fewer ID switches. This shows that our method is able to assign a consistent track ID to individual people, far better than AB3DMOT, across all distances. Additionally, we analyse the contribution of the individual components in the overall performance of JRMOT on a set of ablation studies on the JRDB dataset. First, we conduct an experiment where we update the tracks only with 2D measurements. As expected, we observe that the 3D information is the most crucial for 3D tracking: without 3D data we obtain -20.1% MOTA on the train set. We also analyse the contribution of the 2D RGB appearance feature by using only 3D IoU as association metric. In this case, we see a small degradation in performance of 0.1% MOTA. This indicates that the 3D IoU is the most informative association metric, but it is slightly improved in some cases with 2D appearance. Our last ablation is to verify that 2D inputs without corresponding 3D bounding boxes are indeed helpful measurements in our MOT system. We observed that if we do not use these only-2D updates, the MOTA remains constant at 42.9% but the MOTP drops 0.6%. The overall contribution of 2D is therefore 0.1% MOTA and 0.6% MOTP. However, this is misleading, due to the large number of objects that are close to the sensor, where 2D information is not expected to help much. In the 15-20m range, the increase by using 2D information (both appearance and measurements) is 1.3% MOTA. This confirms our intuition that the 2D measurement can be used to make fine updates on the tracked orientation and location, especially further away from the robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. REAL ROBOT EVALUATION</head><p>Finally, we evaluate the performance of JRMOT when running on-board of a real robot platform. We test on our social robot JackRabbot, which was used to collect the JRDB dataset. We chose not to run JRMOT at the same time as we collected all data for the JRDB dataset as it is not possible due to computing limitations (recording images and pointclouds considerably slows down tracking performance). Therefore, we cannot compute MOTA and MOTP on annotated data while running in real-time on the robot; we instead analyze the number (ID switches), as well as the number of lost tracks.</p><p>We test our solution in three different physical environments, with different lighting conditions (daylight and indoor lighting), with stationary and moving robot, and a different number, distance, and trajectory of moving people. Visualisations of the experimental setup can be seen in <ref type="figure">Fig. 5</ref> We evaluate on a total of 110s of data with 14 unique identities across all scenes. On the on-board computer JRMOT runs between 9-11 fps and we measure only 4 ID switches and 1 lost track. These preliminary results, together with the extensive positive results on KITTI and JRDB, indicate that our tracker provides information to support autonomous navigation in human environments. We make our code publicly available as ROS packages for the community. <ref type="figure">Fig. 5</ref>: We conduct on robot experiments in 3 different scenes, shown above, with a varying number (1 -7) of people, at different distances (1 -10m), with different types of human trajectories (moving and stationary), and with JackRabbot both moving and stationary. We aimed to conduct experiments in diverse, real-world conditions. The above images depict our experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION AND FUTURE WORK</head><p>We presented JRMOT, a novel 3D MOT system that fuses the information contained in 2D RGB images and 3D pointclouds in an efficient manner to provide robust tracking performance even in adversarial and highly crowded environments, all while running in real time. As part of our project we release the JRDB dataset, a novel dataset for 2D and 3D MOT evaluation and development containing multimodal data acquired in human environments, including inside university buildings and pedestrian areas on campus, as well as scenes where the robot navigates among humans. The dataset has been annotated with ground truth 2D bounding boxes and associated 3D cuboids of all persons in the scenes, which will help future research in 2D and 3D MOT. We establish a strong baseline for 3D MOT with JRMOT. JRMOT achieves state of the art performance in the well-known KITTI 2D MOT benchmark and shows better performance than existing 3D MOT systems in our provided JRDB dataset. We also have preliminary on-robot experiments which validate the effectiveness of JRMOT in a real world setting. JRMOT serves as a competitive baseline to encourage further research within the paradigm of leveraging multi-modal sensor measurements to better perform 3D MOT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>JRMOT: Our proposed 3D MOT system is composed of a Detection block, that includes the 2D detector (Sec III-A), 2D appearance model (Sec III-B), 3D detection and feature extractor (Sec III-C), and a Tracking block containing data association (Sec III-D and Sec III-E) and filtering (Sec III-F) and track management (Sec III-G) components; T , D, F refer to tracks, detections, features respectively with the superscript indicating the space; The system integrates information from 2D RGB images and 3D pointclouds into a single 3D multi-object recursive estimation tracker with real-time performance vironments, both indoors and outdoors, in crowded scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) Sample visualization of the dataset with stationary (left) and moving (right) robot; Top: 2D stitched 360 • panorama with human-annotated 2D bounding boxes; Bottom: 3D pointclouds with human-annotated 3D oriented bounding boxes; 2D and 3D annotations have same IDs (indicated by similar box color) the 2D measurement acting as a fine tuning measurement correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Comparison of JRMOT and AB3DMOT as a function of distance. a) JRMOT obtains higher MOTP due to a more accurate estimation of the orientation of boxes and fine grained position information through all distances b) Our method also has less IDS (lower is better) than AB3DMOT, indicating more robust and stable tracking at all distances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Results on online KITTI car tracking benchmark. * indicates that the method used the same public detections as our method</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Results on Online KITTI Pedestrian Tracking. * indicates that the method used the same public detections</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our detections are publicly released as part of the JRDB dataset and benchmark for others to use in their MOT systems.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="941" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-object tracking with multiple cues and switcher-aware classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06129</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep affinity network for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Online multiple pedestrian tracking using deep temporal appearance matching association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00831</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploit the connectivity: Multi-object tracking with trackletnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Joint probabilistic data association revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R A</forename><surname>Dick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ICCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The apolloscape dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on CVPR Workshops</title>
		<meeting>the IEEE Conference on CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="954" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast and furious: Real time endto-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee/Cvf Cvpr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Baseline for 3D Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03961</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fantrack: 3d multi-object tracking with feature association network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Track to reconstruct and reconstruct to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1803" to="1810" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards real-time multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12605</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A portable three-dimensional lidar-based system for long-term and wide-area people behavior measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Menegatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Robotic Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tracking people in 3d using a bottom-up top-down detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in 2011 IEEE ICRA</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">People detection, tracking and visualization using ros on a mobile service robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robot Operating System (ROS)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="187" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Real-time multisensor people tracking for human-robot spatial interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dondrup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanheide</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">End-to-end learning of multi-sensor 3d tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="635" to="642" />
		</imprint>
	</monogr>
	<note>in 2018 IEEEICRA (ICRA</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey on 3d object detection methods for autonomous driving applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Al-Jarrah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dianati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oxtoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mouzakitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3782" to="3795" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE ICCV</title>
		<meeting>the IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vehicle re-identification with the space-time prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-E</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on CVPR Workshops</title>
		<meeting>the IEEE Conference on CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1711.08488</idno>
		<ptr target="http://arxiv.org/abs/1711.08488" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sonar tracking of multiple targets using joint probabilistic data association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fortmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bar-Shalom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scheffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of Oceanic Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="173" to="184" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint probabilistic matching using m-best solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The probabilistic data association filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bar-Shalom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Daum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Magazine</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logist. Quart</title>
		<imprint>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reentrant polygon clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Hodgman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="42" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/1704.05776</idno>
		<ptr target="http://arxiv.org/abs/1704.05776" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Subcategory-aware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE WACV (WACV)</title>
		<imprint>
			<date type="published" when="2017-03" />
			<biblScope unit="page" from="924" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multiple object tracking with attention to appearance, structure, motion and size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karunasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Beyond pixels: Leveraging geometry and shape cues for online multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Krishna</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Madhava Krishna</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in 2018 IEEE ICRA</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Online multi-object tracking using joint domain information in traffic scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Confidence-aware pedestrian tracking using a stereo camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heipke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing and Spatial Information Sciences</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Behavioral pedestrian tracking using a camera and lidar sensors on a moving vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Dimitrievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veelaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bayesian multi-object tracking using motion context from multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE WACV</title>
		<imprint>
			<date type="published" when="2015-01" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
