<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Action Selection Learning in Video Maksims Volkovs Layer6 AI</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Ma</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Satya</surname></persName>
							<email>satya@layer6.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gorti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><forename type="middle">Yu</forename><surname>Layer6</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<postCode>Layer6 AI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<postCode>Layer6 AI</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Action Selection Learning in Video Maksims Volkovs Layer6 AI</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Localizing actions in video is a core task in computer vision. The weakly supervised temporal localization problem investigates whether this task can be adequately solved with only video-level labels, significantly reducing the amount of expensive and error-prone annotation that is required. A common approach is to train a frame-level classifier where frames with the highest class probability are selected to make a video-level prediction. Frame-level activations are then used for localization. However, the absence of frame-level annotations cause the classifier to impart class bias on every frame. To address this, we propose the Action Selection Learning (ASL) approach to capture the general concept of action, a property we refer to as "actionness". Under ASL, the model is trained with a novel class-agnostic task to predict which frames will be selected by the classifier. Empirically, we show that ASL outperforms leading baselines on two popular benchmarks THUMOS-14 and ActivityNet-1.2, with 10.3% and 5.7% relative improvement respectively. We further analyze the properties of ASL and demonstrate the importance of actionness. Full code for this work is available here: https://github.com/layer6ai-labs/ASL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal action localization is a fundamental task in computer vision with important applications in video understanding and modelling. The weakly supervised localization problem investigates whether this task can be adequately solved with only video-level labels instead of frame-level annotations. This significantly reduces the expensive and error-prone labelling required in the fully supervised setting <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b28">29]</ref>, but considerably increases the difficulty of the problem. A standard approach is to apply multiple instance learning to learn a classifier over instances, where an instance is typically a frame or a short segment <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14]</ref>. The classifier is trained using the top-* Authors contributed equally to this work. k aggregation over the instance class activation sequence to generate video probabilities. Localization is then done by leveraging the class activation sequence to generate start and end predictions. However, in many cases, the instances that are selected in the top-k contain useful information for prediction but not the actual action. Furthermore, with topk selection the classification loss encourages the classifier to ignore action instances that are difficult to classify. Both of these problems can significantly hamper localization accuracy and stem from the general inability of the classifier to capture the intrinsic property of action in instances. This property is known as "actionness" in the existing literature <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Ignoring actionness can lead to two major types of error: context error and actionness error. Context error occurs when the classifier activates on instances that do not contain actions but contain context indicative of the overall video class <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14]</ref>. <ref type="figure" target="#fig_0">Figure 1 (a)</ref> shows an example of context error. Here, cricket players are inspecting a cricket pitch. The instance clearly indicates that the video is about cricket and the classifier predicts "Cricket Shot" with high confidence. However, no shot happens in this particular instance and including it in the localization for "Cricket Shot" would lead to an error. Actionness error occurs when the classifier fails to activate on instances that contain actions. This generally occurs in difficult instances that have significant occlusion or uncommon settings. An example of this is shown in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>. The action is "Cricket Bowling", but the classifier fails to activate as the scene is indoors and differs from the usual cricket setting.</p><p>Leading recent work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref> in this area propose an attention model to filter out background and then train a classifier on the filtered instances to predict class probabilities. This has the drawback of making it more challenging for the classifier to learn as important context is potentially removed as background.</p><p>Our motivation is to design a learning framework that can use the context information for class prediction and at the same time learn to identify action instances for localization. We have seen from the supervised setting that leading object detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4]</ref> and temporal localization <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref> methods leverage class-agnostic proposal networks to generate highly accurate predictions. This demonstrates that a general objectness/actionness property can be successfully learned across a diverse set of classes.</p><p>To this end, we propose a new approach called Action Selection Learning (ASL) where the class-agnostic actionness model learns to predict which frames will be selected in the top-k sets by the classifier. During inference, we combine predictions from the actionness model with class activation sequence and show considerable improvement in localization accuracy. Specifically, ASL achieves new stateof-the-art on two popular benchmarks THUMOS-14 and ActivityNet-1.2, where we improve over leading baselines by 10.3% and 5.7% in mAP respectively. We further analyze the performance of our model and demonstrate the advantages of the actionness approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Weakly Supervised Temporal Action Localization A prominent direction in the weakly supervised setting is to leverage the class activation sequence to improve localization. UntrimmedNet <ref type="bibr" target="#b31">[32]</ref> focuses on improving the instance selection step using class activations. Hide-andseek <ref type="bibr" target="#b29">[30]</ref> applies instance dropout to reduce classifier's dependence on specific instances. W-TALC <ref type="bibr" target="#b25">[26]</ref> incorporates a co-activity similarity loss to capture inter-class and intervideo relationships. 3C-Net <ref type="bibr" target="#b22">[23]</ref> adopts a center loss to reduce inter-class variations while applying additional actioncount information for supervision. Focusing on class activations can be susceptible to context error, and a parallel line of research explores how to identify context instances. STPN <ref type="bibr" target="#b23">[24]</ref> extends UntrimmedNet by introducing a classagnostic attention model with sparsity constraints. BM <ref type="bibr" target="#b23">[24]</ref> uses self-attention to separate action and context instances. CMCS <ref type="bibr" target="#b18">[19]</ref> assumes a stationary prior on context and leverages it to model context instances. BaSNet <ref type="bibr" target="#b13">[14]</ref> explicitly models a separate context class that is used to filter instances during inference. DGAM <ref type="bibr" target="#b27">[28]</ref> trains a variational autoencoder to model the class-agnostic instance distribution conditioned on attention to separate context from ac-tions. More recently, TSCN <ref type="bibr" target="#b36">[37]</ref> and EM-MIL <ref type="bibr" target="#b21">[22]</ref> propose two-stream architectures. TSCN separates the RGB and Flow modules and learns from pseudo labels generated by combining the predictions of the two streams. EM-MIL introduces a key instance and a classification module trained alternately to maintain the multi instance learning assumption.</p><p>Actionness Learning Our approach is motivated by related work in the supervised setting where a common design choice is to learn a class-agnostic module to generate proposals that are then labelled by the classifier <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref>. Earlier work defines actionness as a likelihood of a generic but deliberate action that is separate from context <ref type="bibr" target="#b6">[7]</ref>, and applies it to detect human activity in both image <ref type="bibr" target="#b6">[7]</ref> and video <ref type="bibr" target="#b33">[34]</ref> settings. A related concept of "interestingness" has been proposed to identify actions at the pixel level <ref type="bibr" target="#b30">[31]</ref>. Work in action recognition shows that generic attributes exist across action classes and can be leveraged for recognition <ref type="bibr" target="#b20">[21]</ref>. Similar concept has been demonstrated to be successful for tracking applications <ref type="bibr" target="#b14">[15]</ref>. Finally, in object detection, leading approaches heavily leverage class-agnostic proposal networks to first identify regions of high "objectness" <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>In this work, we demonstrate that the analogous "actionness" property in videos can be effectively learned with only video-level labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We treat a video as a set of T instances {x 1 , ..., x T }, dropping video index to reduce notation clutter. An instance can be a frame or a fixed-interval segment, represented by a feature vector x t ∈ R d . In the weakly supervised temporal localization task, each instance x t either contains an action from one of C classes or is the background, however, this is unknown to us. Instead, we are given video-level classes Y ⊆ {1, ..., C} which is the union of all instance classes in the video. The weakly supervised temporal localization task then asks whether video-level class information can be used to localize actions across instances. In this section, we first outline the classification framework in Section 3.1, and then describe our approach in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Base Classifier</head><p>We define a video classifier to predict target video-level classes as:</p><formula xml:id="formula_0">s c,t = F c,t (x 1 , ..., x T )<label>(1)</label></formula><p>where F is a neural network applied to the entire video, and F c,t (·) denotes its output at class c and instance x t . Taken over all T instances we refer to F c,t (·) as the class activation sequence (CAS). Multiple instance learning <ref type="bibr" target="#b4">[5]</ref> is commonly used to train the classifier, where top-k pooling is applied over CAS for each class to aggregate the highest activated instances and make video-level predictions. We denote the set of top-k instances for each class as T c :</p><formula xml:id="formula_1">T c = arg max T ⊆{1,...,T } |T |=k t∈T h c,t<label>(2)</label></formula><p>where k is a hyper-parameter and h c,t is the instance selection probability that is used to select the top instances. In prior work, the selection probability is straightforwardly set to the CAS with h c,t = s c,t . However, we make a deliberate distinction here which allows incorporating actionness as we demonstrate in the following section. Aggregation, such as mean pooling, is applied over the selected instances in T c to make video-level class prediction:</p><formula xml:id="formula_2">p c = softmax 1 |T c | t∈T c s c,t<label>(3)</label></formula><p>Finally, this model is optimized with the multiple instance learning objective:</p><formula xml:id="formula_3">L CLS = − 1 |Y | c∈Y log p c<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Action Selection Learning in Video</head><p>The classifier introduced in the previous section optimizes the classification objective which encourages only instances that strongly support the target video classes to get selected in the top-k set. This can lead to the inclusion of instances that provide strong context support but do not contain the action (actionness error), and also the exclusion of instances that contain the action but are difficult to predict (context error). Both of these problems do not affect video classification accuracy, but can significantly hurt localization. We address this by developing a novel action selection learning (ASL) approach to capture the class-agnostic actionness property of each instance. The main idea behind ASL is that the top-k set T c used for prediction is likely capturing both context and action instances. However, context information is highly class-specific whereas actions share similar characteristics across classes. Consequently, by training a separate class-agnostic model to predict whether an instance will be in the top-k set for any class, we can effectively capture instances that contain actions and filter out context. We begin by defining a neural network actionness model G:</p><formula xml:id="formula_4">a t = σ (G t (x 1 , ..., x T ))<label>(5)</label></formula><p>where σ is the sigmoid activation function, and G t (·) denotes the output of G for instance x t . Here, a t can be interpreted as the probability that x t contains any action.</p><p>For an instance to contain a specific action, it should simultaneously contain evidence of the corresponding class and evidence of actionness. As we discussed, class evidence alone is not sufficient and can lead to context and actionness errors. To account for both properties, we expand the instance selection function:</p><formula xml:id="formula_5">h c,t = h(a t , s c,t )<label>(6)</label></formula><p>This selection function combines beliefs from both models and can be implemented in multiple ways. In this work we fuse the scores with a convex combination h(a t , s c,t ) = βa t + (1 − β)s c,t and leave other possible architectures for future work. After h c,t is computed, we proceed as before and select top-k instances with the highest h c,t values to get T c .</p><p>To train the actionness model G, we design a novel task to predict whether a given instance x t will be in the topk set for any ground truth class. Since context is highly class dependent, we hypothesize that G can only perform well on this task by learning to capture action characteristics that are ubiquitous across classes. This hypothesis is further motivated by the fact that many leading supervised localization methods first generate class-agnostic proposals and then predict classes for them <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref>. High accuracy of these models indicates that the proposal network is able to learn the general actionness property independent of the class, and we aim to do the same here. We first partition instances into positive and negative sets:</p><formula xml:id="formula_6">T pos = c∈Y T c<label>(7)</label></formula><p>T neg = {1, ..., T }\T pos <ref type="bibr" target="#b7">(8)</ref> where positive set T pos contains the union of all instances that appear in the top-k for the ground truth classes Y , and negative set T neg has all other instances. We then train G to predict whether each instance is in the positive or negative set. In our model the classifier and actionness networks are tied by the instance selection function. Empirically, we observe that during training as classification accuracy improves better instances get selected into positive and negative sets. This improves the actionness model which translates to better top-k instance selection for the classifier, leading to further improvement in classification accuracy. The two models are thus complementary to each other, and we show that both classification and localization accuracy improve when the actionness network is added. Since our target positive and negative sets contain both context and action instances, the binary inclusion labels can be noisy. This is particularly the case early in training when classification accuracy is poor and top selected instances are not accurate. Traditional cross entropy classification loss assigns a large penalty when prediction de-   viates significantly from the ground truth. This is a desirable property when labels are clean, enabling the model to converge quickly <ref type="bibr" target="#b37">[38]</ref>. However, recent work shows that cross entropy leads to poor performance under noisy labels, where the high penalty can force the model to overfit to noise <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref>. To address this problem the generalized cross entropy loss has been proposed that softens the penalty in regions of high disagreement <ref type="bibr" target="#b37">[38]</ref>. We adopt this loss here to improve the generalization of the actionness model:</p><formula xml:id="formula_7">T C x t s c,t a t h c,t T c T pos T neg p c L CLS L ASL Y F c,t G t (a)</formula><formula xml:id="formula_8">L ASL = 1 |Tpos| t∈Tpos 1 − (at) q q + 1 |Tneg| t∈Tneg 1 − (1 − at) q q<label>(9)</label></formula><p>where 0 &lt; q ≤ 1 controls the noise tolerance. L ASL is based on the negative Box-Cox transform <ref type="bibr" target="#b1">[2]</ref>, and approaches mean absolute error when q is close to 1 which is more tolerant to deviations from ground truth. On the other hand, as q approaches 0, L ASL behaves similarly to the cross entropy loss with stronger penalties. By appropriately setting q we can control model sensitivity to noise and improve robustness. During training we optimize both classification and ASL losses simultaneously L = L CLS + L ASL and backpropagate the gradients through both classifier and actionness networks.  shown in blue. To successfully predict instances in each list, the actionness model must find commonalities between all instances in T pos and distinguish them from T neg . As we demonstrate in the experimental section this commonality is the presence of actionness which significantly aids the localization task.</p><p>After training, we use the instance selection probabilities h c,t to localize actions in test videos. Given a test video with T instances, we run it through our model to get the corresponding instance selection probability sequence h c,1 , ..., h c,T . We then follow recent work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref> and apply multiple thresholds 0 &lt; α &lt; 1. All instances where selection probability is above the threshold h c,t &gt; α are considered selected, and we take all consecutive sequences as proposal candidates. Repeating this process for each threshold, we obtain a set of proposals for each class. We then apply non-maximal suppression to eliminate overlapping and similar proposals and generate the final localization predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct extensive experiments on two popular weakly supervised temporal localization datasets containing untrimmed videos: THUMOS-14 <ref type="bibr" target="#b10">[11]</ref> and ActivityNet-1.2 <ref type="bibr" target="#b2">[3]</ref>. THUMOS-14 contains 200 training videos with 20 action classes and 212 test videos. ActivityNet-1.2 contains 4,819 training and 2,383 test videos with 100 action classes. Both datasets have videos that vary significantly in length from a few seconds to over 25 minutes. This makes the problem challenging since the model has to perform well on both long and short action sequences. For all experiments, we only use video-level class labels during training.</p><p>To make the comparison fair, we follow the same experimental setup used in literature <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref>, including data splits, evaluation metrics and input features. For all experiments, we report average precision (AP) at the different intersection over union (IoU) thresholds between predicted and ground truth localizations. For brevity, we show selected thresholds in the results table for both datasets. The mAP is computed with IoU thresholds between 0.1 to 0.9 with increments of 0.1 on THUMOS-14 and between 0.5 to 0.95 with increments of 0.05 on ActivityNet-1.2 to stay consistent with previous work. Full results on all thresholds used for computing mAP are found in the supplementary material.</p><p>Implementation Details We generate instance input features x t following the same pipeline as recent leading approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref>. The I3D network <ref type="bibr" target="#b5">[6]</ref> pre-trained on the Kinetics dataset <ref type="bibr" target="#b11">[12]</ref> is applied on each sub-sequence of 16 consecutive frames with RGB and TVL1 flow <ref type="bibr" target="#b32">[33]</ref> inputs to extract 2048-dimensional feature representation by spatiotemporally pooling the Mixed5c layer. Linear interpolation across time is then applied for both datasets. To make the comparison fair we adopt the same base classifier for F as in <ref type="bibr" target="#b13">[14]</ref> with 512 hidden units and ReLU activations. Similarly, the actionness network G is fully connected with 512 hidden units. Both networks are applied across time to every instance and operate similarly to convolutional fil- ters with kernel size 1. We set noise tolerance q = 0.7 for both datasets, and use previously reported instance selection parameters k = T /8 for THUMOS-14 and ActivityNet-1.2 <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref>. We set β to be 0.5 for the instance selection function h(a t , s c,t ). ASL is trained using the ADAM optimizer <ref type="bibr" target="#b12">[13]</ref> with batch size 16, learning rate 1e-4 and weight decay 1e-4 until convergence. During inference, we use ten localization thresholds α from 0 to 1 to generate localization proposals. We then compute final predictions by applying non-maximum suppression to eliminate overlapping and similar proposals. We compare our approach with an extensive set of leading recent baselines: TSM <ref type="bibr" target="#b34">[35]</ref>, CMCS <ref type="bibr" target="#b18">[19]</ref>, MAAN <ref type="bibr" target="#b35">[36]</ref>, 3C-Net <ref type="bibr" target="#b22">[23]</ref>, CleanNet <ref type="bibr" target="#b19">[20]</ref>, BaSNet <ref type="bibr" target="#b13">[14]</ref>, BM <ref type="bibr" target="#b24">[25]</ref>, DGAM <ref type="bibr" target="#b27">[28]</ref>, TSCN <ref type="bibr" target="#b36">[37]</ref> and EM-MIL <ref type="bibr" target="#b21">[22]</ref>. Details for each baseline can be found in the related work section, and we directly use the results reported by the respective authors.</p><p>Results <ref type="table" target="#tab_0">Table 1</ref> summarizes temporal localization results on the THUMOS-14 dataset. Our approach improves over the prior art by a significant margin on all IoU thresholds except 0.7, with a 10.3% relative gain in mAP over the best baseline. A similar pattern can be observed from the ActivityNet-1.2 results summarized in <ref type="table" target="#tab_1">Table 2</ref>. We can see that ASL improves over every baseline on all IoU thresholds except 0.5, with a 5.7% relative gain in mAP. These results indicate that the proposed action selection learning framework is highly effective for the weakly supervised temporal localization task. <ref type="figure">Figure 3</ref> further breaks down THUMOS-14 performance by class. The top four classes with the largest relative improvement are highlighted in blue. The most improved classes have more than 100% relative gain. We believe this is due to the wide range of context settings present in the dataset for these classes, making classagnostic action learning more effective for separating context.</p><p>Ablation To demonstrate the importance of actionness, we conduct ablation study on the THUMOS-14 dataset shown at the bottom of <ref type="table" target="#tab_0">Table 1</ref>. Here, ASL-s c,t uses class probability h c,t = s c,t and ASL-a t uses actionness proba- bility h c,t = a t for localization proposals during inference. The classification for ASL-a t is done at the video level by taking the class with the highest probability and assigning it to every localization. Finally, ASL-BCE trains the actionness network G with the binary cross entropy loss instead of the generalized L ASL loss in Equation <ref type="bibr" target="#b8">9</ref>. We see that incorporating actionness in the full ASL model relatively outperforms the classification-only ASL-s c,t approach by over 36% in mAP. Moreover, ASL-a t has very strong performance and is competitive with prior state-of-the-art results even though a t on its own has no explicit class information. This demonstrates that the actionness network G is able to successfully capture the general class-agnostic concept of action through our top-k instance prediction task. Once captured, this property can be effectively used to identify regions within each video where the action occurs independently of the class. We note here that attention models commonly used in prior work, have not been shown to be capable of localizing actions on their own. The full ASL model further improves performance of ASL-a t by 6% indicating that classifier and actionness networks capture complementary information. Finally, using binary cross entropy instead of the noise tolerant L ASL loss hurts performance. L ASL becomes equivalent to binary cross entropy in the limit as q → 0 <ref type="bibr" target="#b37">[38]</ref>. In all our experiments we found that much higher values of q such as 0.7 produced better performance on both datasets, indicating that cross entropy is indeed not adequate here due to the high degree of noise in the target labels particularly at the beginning of training. Actionness Learning The main idea behind ASL is that actionness can be captured by predicting top-k membership for each instance. In this section, we analyze this learning task in detail. <ref type="figure">Figure 4</ref> shows various properties of the T pos set throughout training. In <ref type="figure">Figure 4</ref>(a) we plot the fraction of instances in T pos that contain ground truth action from any target class over training epochs. For comparison, we also plot this fraction for ASL-s c,t and ASL-a t where topk instances are chosen according to class s c,t and actionness a t probabilities respectively. We observe that without the actionness model, ASL-s c,t hovers around 63% whereas for ASL it steadily increases to over 72%. Furthermore, ASL-a t reaches a much higher fraction of over 70% compared to ASL-s c,t , capturing a significantly larger portion of instances with action. This again indicates that the actionness network is better at identifying action instances than the classifier. <ref type="figure">Figure 4(b)</ref> shows the validation accuracy of the actionness model G in predicting which instances are in the T pos set. Despite the fact that T pos is a moving target that can change with each iteration, the prediction accuracy remains stable and gradually improves throughout learning reaching over 84%. The model is thus able to reach an equilibrium between the two networks and no divergence is observed. Furthermore, <ref type="figure">Figure 4(</ref>  (IoU) between the T pos sets from consecutive epochs during training. A higher IoU indicates a larger overlap between consecutive T pos sets which in turn makes targets for G more stable and easier to learn. We observe that the initial IoU starts around 0.5 and rapidly approaches 1 as training progresses. Furthermore, the variance in IoU across training videos decreases throughout training so T pos sets stabilize after the first few epochs. These results indicate that the top-k selection remains consistent for the majority of training epochs, and G is able to successfully learn these targets; we observe this pattern in all our experiments. <ref type="figure" target="#fig_7">Figure 5a</ref> breaks down the predictions made by ASL by error type. We use all test instances and show the total number of true positives (TP), false positives (FP), false negatives (FN) and true negatives (TN). Note that we treat this as a binary problem and consider an instance as true positive if it contains an action and is selected for localization by ASL. In this setting, FP and FN represent context and actionness errors respectively, TP and TN are correctly predicted instances. We compare ASL with the classifier-only ASL-s c,t model and show the relative increase/decrease for each category in blue. <ref type="figure" target="#fig_7">Figure 5a</ref> shows that ASL improves each category predicting more instances correctly and making fewer mistakes. Specifically, ASL reduces context and actionness errors by 31% and 16% respectively.</p><p>In <ref type="figure">Figure 4</ref> we showed that ASL can capture actionness because a significant proportion of instances in T pos sets contain actions, and these sets remain stable throughout training. Moreover, even though not all instances in T pos contain actions, our noise tolerant loss is robust and can still perform well when a portion of labels is incorrect. Here, we further investigate the degree of noise that can be tolerated in this setting. Specifically, we evaluate the ability of G to capture actionness when it is trained on T pos with different proportions of action instances. Throughout training, we sample instances from T pos sets to lower the fraction of instances that contain actions. This simulates a challenging learning environment where G has to learn from increasingly noisier labels. <ref type="figure" target="#fig_7">Figure 5b</ref> shows these results on the THUMOS-14 dataset where we reduce the fraction of instances containing actions in T pos (Class Rate) from 70% to 30%. For comparison, the actual class rate on this dataset reaches around 72% (see <ref type="figure">Figure 4</ref>(a)). To more directly evaluate the impact of this setting we compute the instance-level Recall@100 using only predictions from G. Recall@100 is computed by measuring the fraction of instances that contain any action amongst the top-100 instances predicted by G. We can see that Recall@100 for the noise-tolerant ASL setting remains relatively stable between 50% and 70% but starts to drop significantly below 40% class rate. These results suggest that the top-k selection strategy can tolerate a high degree of noise with up to 50% of incorrect labels. However, this also implies that the classifier needs to be sufficiently accurate in the top-k selection for ASL to work.</p><p>Throughout the training, we simultaneously update both F and G. We discussed that this results in moving targets for G where instances in T pos change as the classifier is updated. Alternative training strategies are explored in <ref type="figure" target="#fig_7">Figure 5c</ref>. We experiment with first training F to convergence and then G (F then G), and alternating between training F and G. We denote these alternating schedules by F :n G:m to indicate training F for n epochs followed by training G for m epochs and repeating. We observe that training F then G results in the lowest performance, since the model cannot adjust the classifier to work better with the actionness model. Alternating between F and G updates improves performance but still lags behind joint training. This corroborates our intuition that the classifier and actionness models complement each other in ASL and should be trained together.</p><p>We further show DETAD <ref type="bibr" target="#b0">[1]</ref> analysis on THUMOS-14 dataset in <ref type="figure" target="#fig_9">Figure 7</ref>. Here, the top row shows false positive analysis (context error) and false negative analysis (action-   ness error) is shown in the bottom row. The false positive profiles show that ASL-a t significantly reduces the background (41.7 vs 47.7) and localization (28.3 vs 32.5) errors compared to ASL-s c,t , further demonstrating that G is able to learn actionness concepts that the classifier fails to capture. On the other hand, we observe higher double detection (14.8 vs 9.6) and wrong label (2.9 vs 0.9) errors in ASL-a t since it lacks class information. This corroborates our hypothesis that both models are needed to maximize localization accuracy. In the top predictions 1G, nearly all of the reduction in background error translates to more true positives in both ASL and ASL-a t compared to ASL-s c,t . On the false negatives, ASL-a t improves on shorter and more frequent action segments and complements ASL-s c,t which captures longer and more infrequent action segments better.</p><p>Qualitative Results <ref type="figure" target="#fig_8">Figure 6</ref> shows qualitative results for ASL-s c,t and ASL models. Ground truth segments are shown in green, and model predictions are shown in blue with darker colors indicating higher confidence. In figure 6(a) for a video containing the "Hammer throw" action, the ASL-s c,t localization alone is focusing on one very specific region of the video which likely contains the easiest instances (actionness error) to predict the video class. ASL spreads the localization predictions and correctly identifies all regions of actions. The opposite pattern is observed in <ref type="figure" target="#fig_8">Figures 6(b)</ref> and 6(c) which show the "Diving" action class. ASL-s c,t activations are high on many context instances (context error) as they all contain highly informative scenes for the target "Diving" class. ASL, however, focuses on the regions that contain the action. Lastly, <ref type="figure" target="#fig_8">Figure 6(d)</ref> shows "Cricket Bowling" action in an uncommon indoor setting. Here, ASL-s c,t has difficulty recognizing the action and most activations have low confidence. The actionness model is able to identify the action of throwing as shown in the first two sample frames. Predictions of ASL are significantly more confident and identifies all of the action segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose the Action Selection Learning (ASL) approach for weakly supervised video localization. ASL incorporates a class-agnostic actionness network that learns a general concept of action independent of the class. We train the actionness network with a novel prediction task by classifying which instances will be selected in the top-k set by the classifier. Once trained, this network is highly effective on its own and can accurately localize actions with minimal class information from the classifier. Empirically, ASL demonstrates superior accuracy, outperforming leading recent benchmarks by a significant margin. Future work includes further investigation into actionness and its generalization to other related video domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Context error for the "Cricket Shot" action due to the presence of all cricket artifacts but absence of action. (b) Actionness error for the "Cricket Bowling" action due to the atypical scene despite the presence of action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Diagram of the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t , s c,t ) s c,t a t (b) Toy example with C = 4, T = 7, k = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>ASL model architecture and toy example. (a) For every instance x t the classifier F c,t predicts class activation s c,t , and actionness model G t predicts actionness score a t . Class activation and actionness are combined with the instance selection function h to get instance selection probability h c,t = h(a t , s c,t ). Top-k instances with the highest selection probabilities are then added to T c and aggregated together to generate class prediction p c for the video. Finally, the union of top-k instances across ground-truth classes Y is used to generate target sets T pos and T neg for the actionness model. (b) Toy example illustrates how target sets T pos and T neg are computed. The video has T = 7 instances, C = 4 classes and k = 3. For each class we select top-3 instances with the highest action selection probabilities h(a t , s c,t ) indicated by yellow cells. Taking union of selected instances across ground truth classes (c ∈ Y ) we get T pos shown in blue. All other instances form T neg shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The proposed ASL architecture is summarized in Figure 2(a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 2(b) also shows a toy example that illustrates how positive and negative sets T pos and T neg are computed. The video has T = 7 instances and C = 4 classes, two of which are in the ground truth Y = {3, 4}. Moreover, k = 3 so for each class top-3 instances with the highest instances selection probabilities h(a t , s c,t ) are selected, indi-cated by yellow cells. Union of instances selected for the ground truth classes form T pos = {x 1 , x 2 , x 3 , x 4 } shown in red, and all other instances form T neg = {x 5 , x 6 , x 7 }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>B a s e b a l l P i t c h B a s k e t b a l l D u n k B i l l i a r d s C l e a n A n d J e r k C l i f f D i v i n g C r i c k e t B o w l i n g C r i c k e t S h o t D i v i n g F r i s b e e C a t c h G o l f S w i n g H a m m e r T h r o w H i g h J u m p J a v e l i n T h r o w L o n g J u m p P o l e V a u l t S h o t p u t S o c c e r P e n a l t y T e n n i s S w i n g T h r o w D i s c u s V o l l e y b a l l S p i k i n g THUMOS-14 per-class AP results, four largest relative improvements are highlighted in blue. THUMOS-14 T pos analysis. (a) Fraction of instances in T pos that contain ground truth action from any target class. (b) Validation ASL accuracy at predicting which instances will be in T pos . (c) Averaged across training videos IoU of T pos sets between consecutive epochs, error bars show one standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>THUMOS-14 error and ablation analysis. (a) Error analysis across all test instances by type. TP, FP, TN, FN indicate true positives, false positives, true negatives and false negatives respectively. (b) Class-agnostic localization Recall@100 using G when it is trained from scratch on T pos with different proportion of instances that contain action. (c) Effect of training schedules in ASL.F and G is joint ASL training, F then G is sequential training of F followed by G, F :n G:m is an alternating schedule where F is updated for n epochs followed by G for m epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>THUMOS-14 qualitative results comparing ASL-s c,t and ASL on four videos. Ground truth (GT) localizations are indicated with green segments, and we show sample frames from action (green) and context (red) instances. Predictions are shown in blue where darker color indicates higher confidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>THUMOS-14 error analysis following DETAD<ref type="bibr" target="#b0">[1]</ref>. Top row breaks down false positives errors while bottom row shows false negatives by segment lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>THUMOS-14 results. mAP is the mean of AP@IoU scores across thresholds {0.1, 0.2, ..., 0.9}. Ablation results are shown at the bottom where ASL-s c,t and ASL-a t use class probability s c,t and actionness probability a t respectively to localize, and ASL-BCE trains the actionness network G with the binary cross entropy loss.</figDesc><table><row><cell>Approach</cell><cell>0.1</cell><cell>0.3</cell><cell>AP@IoU 0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>mAP</cell></row><row><cell>CMCS [19]</cell><cell cols="3">57.4 41.2 23.1</cell><cell>7.0</cell><cell>-</cell><cell>-</cell></row><row><cell>MAAN [36]</cell><cell cols="3">59.8 41.1 20.3</cell><cell>6.9</cell><cell>0.2</cell><cell>24.9</cell></row><row><cell>3C-Net [23]</cell><cell cols="3">56.8 40.9 24.6</cell><cell>7.7</cell><cell>-</cell><cell>-</cell></row><row><cell>BaSNet [14]</cell><cell cols="5">58.2 44.6 27.0 10.4 0.5</cell><cell>27.9</cell></row><row><cell>BM [25]</cell><cell cols="3">60.4 46.6 26.8</cell><cell>9.0</cell><cell>0.4</cell><cell>28.6</cell></row><row><cell>DGAM [28]</cell><cell cols="5">60.0 46.8 28.8 11.4 0.4</cell><cell>29.2</cell></row><row><cell>ACL [10]</cell><cell>-</cell><cell cols="3">46.9 30.1 10.4</cell><cell>-</cell><cell>-</cell></row><row><cell>TSCN [37]</cell><cell cols="5">63.4 47.8 28.7 10.2 0.7</cell><cell>22.9</cell></row><row><cell cols="5">EM-MIL [22] 59.1 45.5 30.5 16.4</cell><cell>-</cell><cell>-</cell></row><row><cell>ASL (ours)</cell><cell cols="5">67.0 51.8 31.1 11.4 0.7</cell><cell>32.2</cell></row><row><cell></cell><cell></cell><cell cols="2">Ablation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASL-sc,t</cell><cell cols="3">56.9 40.5 19.7</cell><cell>6.0</cell><cell>0.4</cell><cell>24.0</cell></row><row><cell>ASL-at</cell><cell cols="3">55.9 40.3 20.6</cell><cell>6.8</cell><cell>0.4</cell><cell>30.4</cell></row><row><cell>ASL-BCE</cell><cell cols="5">66.4 50.5 30.5 10.9 0.7</cell><cell>31.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ActivityNet-1.2 results. mAP is the mean of AP@IoU scores across thresholds {0.5, 0.55, ..., 0.95}.</figDesc><table><row><cell>Approach</cell><cell>0.5</cell><cell>0.6</cell><cell>AP@IoU 0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>mAP</cell></row><row><cell>TSM [35]</cell><cell cols="4">28.3 23.6 18.9 14.0</cell><cell>7.5</cell><cell>17.1</cell></row><row><cell>3CNet [23]</cell><cell>35.4</cell><cell>-</cell><cell>22.9</cell><cell>-</cell><cell>8.5</cell><cell>21.1</cell></row><row><cell>CMCS [19]</cell><cell>36.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell cols="5">CleanNet [20] 37.1 29.9 23.4 17.2</cell><cell>9.2</cell><cell>21.6</cell></row><row><cell>BaSNet [14]</cell><cell>38.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.3</cell></row><row><cell>DGAM [28]</cell><cell cols="5">41.0 33.5 26.9 19.8 10.8</cell><cell>24.4</cell></row><row><cell>TSCN [37]</cell><cell>37.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>23.6</cell></row><row><cell cols="2">EM-MIL [22] 37.4</cell><cell>-</cell><cell>23.1</cell><cell>-</cell><cell>2.0</cell><cell>20.3</cell></row><row><cell>ASL (ours)</cell><cell cols="5">40.2 34.6 29.4 22.5 12.1</cell><cell>25.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An analysis of transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David R</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="243" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiple instance learning: A survey of problem characteristics and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-André</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghyslain</forename><surname>Gagnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="329" to="353" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the Kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Actionness ranking with lattice conditional ordinal random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning temporal co-attention models for unsupervised video action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Searching action proposals via spatial actionness estimation and temporal path inference and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqiang</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BMN: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BSN: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nanning Zheng, and Gang Hua. Weakly supervised temporal action localization through contrast based evaluation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Actionnessassisted recognition of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loong-Fah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Weakly-supervised action localization with expectation-maximization multiinstance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00163</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3c-net: Category count and center loss for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Phuc Xuan Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wtalc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Weakly-supervised action localization by generative attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12424</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Actionness estimation using hybrid fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An improved algorithm for tv-l 1 optical flow. In Statistical and geometrical approaches to visual motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal structure mining for weakly supervised action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08586</idno>
		<title level="m">Marginalized average attentional network for weakly-supervised learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stream consensus network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="37" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Slac: A sparsely labeled dataset for action classification and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09374</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
