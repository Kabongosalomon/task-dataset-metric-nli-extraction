<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
							<email>seungjun.nah@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge <ref type="bibr" target="#b25">[26]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image super-resolution (SR) problem, particularly single image super-resolution (SISR), has gained increasing research attention for decades. SISR aims to reconstruct a high-resolution image I SR from a single low-resolution image I LR . Generally, the relationship between I LR and the original high-resolution image I HR can vary depending on the situation. Many studies assume that I LR is a bicubic downsampled version of I HR , but other degrading factors such as blur, decimation, or noise can also be considered for practical applications.</p><p>Recently, deep neural networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref> provide significantly improved performance in terms of peak signal-tonoise ratio (PSNR) in the SR problem. However, such networks exhibit limitations in terms of architecture optimality. First, the reconstruction performance of the neural network models is sensitive to minor architectural changes. Also, the same model achieves different levels of performance by dif-0853 from DIV2K <ref type="bibr" target="#b25">[26]</ref> HR (PSNR / SSIM) Bicubic (30.80 dB / 0.9537) VDSR <ref type="bibr" target="#b10">[11]</ref> (32.82 dB / 0.9623) SRResNet <ref type="bibr" target="#b13">[14]</ref> (34.00 dB / 0.9679) EDSR+ (Ours) (34.78 dB / 0.9708) <ref type="figure">Figure 1</ref>: Ã—4 Super-resolution result of our single-scale SR method (EDSR) compared with existing algorithms. ferent initialization and training techniques. Thus, carefully designed model architecture and sophisticated optimization methods are essential in training the neural networks.</p><p>Second, most existing SR algorithms treat superresolution of different scale factors as independent problems without considering and utilizing mutual relationships among different scales in SR. As such, those algorithms require many scale-specific networks that need to to be trained independently to deal with various scales. Exceptionally, VDSR <ref type="bibr" target="#b10">[11]</ref> can handle super-resolution of several scales jointly in the single network. Training the VDSR model with multiple scales boosts the performance substantially and outperforms scale-specific training, implying the redundancy among scale-specific models. Nonetheless, VDSR style architecture requires bicubic interpolated image as the input, that leads to heavier computation time and memory compared to the architectures with scale-specific upsampling method <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>While SRResNet <ref type="bibr" target="#b13">[14]</ref> successfully solved those time and memory issue with good performance, it simply employs the ResNet architecture from He et al. <ref type="bibr" target="#b8">[9]</ref> without much modification. However, original ResNet was proposed to solve higher-level computer vision problems such as image classification and detection. Therefore, applying ResNet architecture directly to low-level vision problems like super-resolution can be suboptimal.</p><p>To solve these problems, based on the SRResNet architecture, we first optimize it by analyzing and removing unnecessary modules to simplify the network architecture. Training a network becomes nontrivial when the model is complex. Thus, we train the network with appropriate loss function and careful model modification upon training. We experimentally show that the modified scheme produces better results.</p><p>Second, we investigate the model training method that transfers knowledge from a model trained at other scales.</p><p>To utilize scale-independent information during training, we train high-scale models from pre-trained low-scale models. Furthermore, we propose a new multi-scale architecture that shares most of the parameters across different scales. The proposed multi-scale model uses significantly fewer parameters compared with multiple single-scale models but shows comparable performance.</p><p>We evaluate our models on the standard benchmark datasets and on a newly provided DIV2K dataset. The proposed single-and multi-scale super-resolution networks show the state-of-the-art performances on all datasets in terms of PSNR and SSIM. Our methods ranked first and second, respectively, in the NTIRE 2017 Super-Resolution Challenge <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>To solve the super-resolution problem, early approaches use interpolation techniques based on sampling theory <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34]</ref>. However, those methods exhibit limitations in predicting detailed, realistic textures. Previous studies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23]</ref> adopted natural image statistics to the problem to reconstruct better high-resolution images.</p><p>Advanced works aim to learn mapping functions between I LR and I HR image pairs. Those learning methods rely on techniques ranging from neighbor embedding <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref> to sparse coding <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref>. Yang et al. <ref type="bibr" target="#b29">[30]</ref> introduced another approach that clusters the patch spaces and learns the corresponding functions. Some approaches utilize image self-similarities to avoid using external databases <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29]</ref>, and increase the size of the limited internal dictionary by geometric transformation of patches <ref type="bibr" target="#b9">[10]</ref>.</p><p>Recently, the powerful capability of deep neural networks has led to dramatic improvements in SR. Since Dong et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> first proposed a deep learning-based SR method, various CNN architectures have been studied for SR. Kim et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> first introduced the residual network for training much deeper network architectures and achieved superior performance. In particular, they showed that skipconnection and recursive convolution alleviate the burden of carrying identity information in the super-resolution network. Similarly to <ref type="bibr" target="#b19">[20]</ref>, Mao et al. <ref type="bibr" target="#b15">[16]</ref> tackled the general image restoration problem with encoder-decoder networks and symmetric skip connections. In <ref type="bibr" target="#b15">[16]</ref>, they argue that those nested skip connections provide fast and improved convergence.</p><p>In many deep learning based super-resolution algorithms, an input image is upsampled via bicubic interpolation before they fed into the network <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Rather than using an interpolated image as an input, training upsampling modules at the very end of the network is also possible as shown in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14]</ref>. By doing so, one can reduce much of computations without losing model capacity because the size of features decreases. However, those kinds of approaches have one disadvantage: They cannot deal with the multi-scale problem in a single framework as in VDSR <ref type="bibr" target="#b10">[11]</ref>. In this work, we resolve the dilemma of multiscale training and computational efficiency. We not only exploit the inter-relation of learned feature for each scale but also propose a new multi-scale model that efficiently reconstructs high-resolution images for various scales. Furthermore, we develop an appropriate training method that uses multiple scales for both single-and multi-scale models.</p><p>Several studies also have focused on the loss functions to better train network models. Mean squared error (MSE) or L2 loss is the most widely used loss function for general image restoration and is also major performance measure (PSNR) for those problems. However, Zhao et al. <ref type="bibr" target="#b34">[35]</ref> reported that training with L2 loss does not guarantee better performance compared to other loss functions in terms of PSNR and SSIM. In their experiments, a network trained with L1 achieved improved performance compared with the network trained with L2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methods</head><p>In this section, we describe proposed model architectures. We first analyze recently published super-resolution network and suggest an enhanced version of the residual network architecture with the simpler structure. We show that our network outperforms the original ones while exhibiting improved computational efficiency. In the following sections, we suggest a single-scale architecture (EDSR) that handles a specific super-resolution scale and a multiscale architecture (MDSR) that reconstructs various scales of high-resolution images in a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Residual blocks</head><p>Recently, residual networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref> exhibit excellent performance in computer vision problems from the lowlevel to high-level tasks. Although Ledig et al. <ref type="bibr" target="#b13">[14]</ref> successfully applied the ResNet architecture to the super-resolution problem with SRResNet, we further improve the performance by employing better ResNet structure. In <ref type="figure" target="#fig_0">Fig. 2</ref>, we compare the building blocks of each network model from original ResNet <ref type="bibr" target="#b8">[9]</ref>, SRResNet <ref type="bibr" target="#b13">[14]</ref>, and our proposed networks. We remove the batch normalization layers from our network as Nah et al. <ref type="bibr" target="#b18">[19]</ref> presented in their image deblurring work. Since batch normalization layers normalize the features, they get rid of range flexibility from networks by normalizing the features, it is better to remove them. We experimentally show that this simple modification increases the performance substantially as detailed in Sec. 4.</p><p>Furthermore, GPU memory usage is also sufficiently reduced since the batch normalization layers consume the same amount of memory as the preceding convolutional layers. Our baseline model without batch normalization layer saves approximately 40% of memory usage during training, compared to SRResNet. Consequently, we can build up a larger model that has better performance than conventional ResNet structure under limited computational resources.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Single-scale model</head><p>The simplest way to enhance the performance of the network model is to increase the number of parameters. In the convolutional neural network, model performance can be enhanced by stacking many layers or by increasing the number of filters. General CNN architecture with depth (the number of layers) B and width (the number of feature channels) F occupies roughly O(BF ) memory with O(BF 2 ) parameters. Therefore, increasing F instead of B can maximize the model capacity when considering limited computational resources.</p><p>However, we found that increasing the number of feature maps above a certain level would make the training procedure numerically unstable. A similar phenomenon was reported by Szegedy et al. <ref type="bibr" target="#b23">[24]</ref>. We resolve this issue by adopting the residual scaling <ref type="bibr" target="#b23">[24]</ref> with factor 0.1. In each residual block, constant scaling layers are placed after the last convolution layers. These modules stabilize the training procedure greatly when using a large number of filters. In the test phase, this layer can be integrated into the previous convolution layer for the computational efficiency.</p><p>We construct our baseline (single-scale) model with our proposed residual blocks in <ref type="figure" target="#fig_0">Fig. 2</ref>. The structure is similar to SRResNet <ref type="bibr" target="#b13">[14]</ref>, but our model does not have ReLU activation layers outside the residual blocks. Also, our baseline model does not have residual scaling layers because we use only 64 feature maps for each convolution layer. In our final single-scale model (EDSR), we expand the baseline model by setting B = 32, F = 256 with a scaling factor 0.1. The model architecture is displayed in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>When training our model for upsampling factor Ã—3 and Ã—4, we initialize the model parameters with pre-trained Ã—2 network. This pre-training strategy accelerates the training and improves the final performance as clearly demonstrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. For upscaling Ã—4, if we use a pre-trained scale Ã—2 model (blue line), the training converges much faster than the one started from random initialization (green line). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-scale model</head><p>From the observation in <ref type="figure" target="#fig_2">Fig. 4</ref>, we conclude that superresolution at multiple scales is inter-related tasks. We further explore this idea by building a multi-scale architecture that takes the advantage of inter-scale correlation as VDSR <ref type="bibr" target="#b10">[11]</ref> does. We design our baseline (multi-scale) models to have a single main branch with B = 16 residual blocks so that most of the parameters are shared across different scales as shown in <ref type="figure">Fig. 5</ref>.</p><p>In our multi-scale architecture, we introduce scalespecific processing modules to handle the super-resolution at multiple scales. First, pre-processing modules are located at the head of networks to reduce the variance from input images of different scales. Each of pre-processing module consists of two residual blocks with 5 Ã— 5 kernels. By adopting larger kernels for pre-processing modules, we can keep the scale-specific part shallow while the larger receptive field is covered in early stages of networks. At the end of the multi-scale model, scale-specific upsampling modules are located in parallel to handle multi-scale reconstruction. The architecture of the upsampling modules is similar to those of single-scale models described in the previous section.</p><p>We  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>DIV2K dataset <ref type="bibr" target="#b25">[26]</ref> is a newly proposed high-quality (2K resolution) image dataset for image restoration tasks. The DIV2K dataset consists of 800 training images, 100 validation images, and 100 test images. As the test dataset ground truth is not released, we report and compare the performances on the validation dataset. We also compare the performance on four standard benchmark datasets: Set5 <ref type="bibr" target="#b1">[2]</ref>, Set14 <ref type="bibr" target="#b32">[33]</ref>, B100 <ref type="bibr" target="#b16">[17]</ref>, and Urban100 <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Details</head><p>For training, we use the RGB input patches of size 48Ã—48 from LR image with the corresponding HR patches. We augment the training data with random horizontal flips and 90 rotations. We pre-process all the images by subtracting the mean RGB value of the DIV2K dataset. We train  our model with ADAM optimizer <ref type="bibr" target="#b12">[13]</ref> by setting Î² 1 = 0.9, Î² 2 = 0.999, and = 10 âˆ’8 . We set minibatch size as 16.</p><p>The learning rate is initialized as 10 âˆ’4 and halved at every 2 Ã— 10 5 minibatch updates.</p><p>For the single-scale models (EDSR), we train the networks as described in Sec. 3.2. The Ã—2 model is trained from scratch. After the model converges, we use it as a pretrained network for other scales.</p><p>At each update of training a multi-scale model (MDSR), we construct the minibatch with a randomly selected scale among Ã—2, Ã—3 and Ã—4. Only the modules that correspond to the selected scale are enabled and updated. Hence, scalespecific residual blocks and upsampling modules that correspond to different scales other than the selected one are not enabled nor updated.</p><p>We train our networks using L1 loss instead of L2. Minimizing L2 is generally preferred since it maximizes the PSNR. However, based on a series of experiments we empirically found that L1 loss provides better convergence than L2. The evaluation of this comparison is provided in Sec. <ref type="bibr">4.4</ref> We implemented the proposed networks with the Torch7 framework and trained them using NVIDIA Titan X GPUs. It takes 8 days and 4 days to train EDSR and MDSR, respectively. The source code is publicly available online. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Geometric Self-ensemble</head><p>In order to maximize the potential performance of our model, we adopt the self-ensemble strategy similarly to <ref type="bibr" target="#b27">[28]</ref>. During the test time, we flip and rotate the input image I LR to generate seven augmented inputs I LR n,i = T i I LR n for each sample, where T i represents the 8 geometric transformations including indentity. With those augmented low-resolution images, we generate corresponding super-resolved images I SR n,1 , Â· Â· Â· , I SR n,8 using the networks. We then apply inverse transform to those output images to get the original geometryÄ¨ SR n,i = T âˆ’1 i I SR n,i . Finally, we average the transformed outputs all together to make the self-ensemble result as follows. I SR n = 1 This self-ensemble method has an advantage over other ensembles as it does not require additional training of separate models. It is beneficial especially when the model size or training time matters. Although self-ensemble strategy keeps the total number of parameters same, we notice that it gives approximately same performance gain compared to conventional model ensemble method that requires individually trained models. We denote the methods using self-ensemble by adding '+' postfix to the method name; i.e. EDSR+/MDSR+. Note that geometric self-ensemble is valid only for symmetric downsampling methods such as bicubic downsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on DIV2K Dataset</head><p>We test our proposed networks on the DIV2K dataset. Starting from the SRResNet, we gradually change various settings to perform ablation tests. We train SRResNet <ref type="bibr" target="#b13">[14]</ref> on our own. <ref type="bibr">2 3</ref> First, we change the loss function from L2 to L1, and then the network architecture is reformed as described in the previous section and summarized in <ref type="table" target="#tab_2">Table  1</ref>.</p><p>We train all those models with 3 Ã— 10 5 updates in this experiment. Evaluation is conducted on the 10 images of DIV2K validation set, with PSNR and SSIM criteria. For the evaluation, we use full RGB channels and ignore the (6 + scale) pixels from the border. <ref type="table" target="#tab_4">Table 2</ref> presents the quantitative results. SRResNet trained with L1 gives slightly better results than the original one trained with L2 for all scale factors. Modifications of the network give an even bigger margin of improvements. The last 2 columns of <ref type="table" target="#tab_4">Table 2</ref> show significant performance gains of our final bigger models, EDSR+ and MDSR+ with the geometric self-ensemble technique. Note that our models require much less GPU memory since they do not have batch normalization layers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Benchmark Results</head><p>We provide the quantitative evaluation results of our final models (EDSR+, MDSR+) on public benchmark datasets in <ref type="table" target="#tab_6">Table 3</ref>. The evaluation of the self-ensemble is also provided in the last two columns. We trained our models using 10 6 updates with batch size 16. We keep the other settings same as the baseline models. We compare our models with the state-of-the-art methods including A+ <ref type="bibr" target="#b26">[27]</ref>, SRCNN <ref type="bibr" target="#b3">[4]</ref>, VDSR <ref type="bibr" target="#b10">[11]</ref>, and SRResNet <ref type="bibr" target="#b13">[14]</ref>. For comparison, we measure PSNR and SSIM on the y channel and ignore the same amount of pixels as scales from the border. We used MAT-LAB <ref type="bibr" target="#b17">[18]</ref> functions for evaluation. Comparative results on DVI2K dataset are also provided. Our models exhibit a significant improvement compared to the other methods. The gaps further increase after performing self-ensemble. We also present the qualitative results in <ref type="figure">Fig. 6</ref>. The proposed models successfully reconstruct the detailed textures and edges in the HR images and exhibit better-looking SR outputs compared with the previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">NTIRE2017 SR Challenge</head><p>This work is initially proposed for the purpose of participating in the NTIRE2017 Super-Resolution Challenge <ref type="bibr" target="#b25">[26]</ref>. The challenge aims to develop a single image superresolution system with the highest PSNR.</p><p>In the challenge, there exist two tracks for different degraders (bicubic, unknown) with three downsample scales (Ã—2, 3, 4) each. Input images for the unknown track are not only downscaled but also suffer from severe blurring. Therefore, more robust mechanisms are required to deal with the second track. We submitted our two SR models (EDSR and MDSR) for each competition and prove that our algorithms are very robust to different downsampling conditions. Some results of our algorithms on the unknown downsampling track are illustrated in <ref type="figure">Fig. 7</ref>. Our methods successfully reconstruct high-resolution images from severely degraded input images. Our proposed EDSR+ and MDSR+ won the first and second places, respectively, with outstanding performances as shown in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed an enhanced super-resolution algorithm. By removing unnecessary modules from conventional ResNet architecture, we achieve improved results while making our model compact. We also employ residual scaling techniques to stably train large models. Our proposed singe-scale model surpasses current models and achieves the state-of-the-art performance.</p><p>Furthermore  <ref type="figure">Figure 7</ref>: Our NTIRE2017 Super-Resolution Challenge results on unknown downscaling Ã—4 category. In the challenge, we excluded images from 0791 to 0800 from training for validation. We did not use geometric self-ensemble for unknown downscaling category.  <ref type="table">Table 4</ref>: Performance of our methods on the test dataset of NTIRE2017 Super-Resolution Challenge <ref type="bibr" target="#b25">[26]</ref>. The results of top 5 methods are displayed for two tracks and six categories. Red indicates the best performance and blue indicates the second best.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of residual blocks in original ResNet, SRResNet, and ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of the proposed single-scale SR network (EDSR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Effect of using pre-trained Ã—2 network for Ã—4 model (EDSR). The red line indicates the best performance of green line. 10 images are used for validation during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>. 1 https://github.com/LimBee/NTIRE2017</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>construct our final multi-scale model (MDSR) with B = 80 and F = 64. While our single-scale baseline models for 3 different scales have about 1.5M parameters each, totaling 4.5M, our baseline multi-scale model has only 3.2 million parameters. Nevertheless, the multi-scale model exhibits comparable performance as the single-scale models. Furthermore, our multi-scale model is scalable in terms of depth. Although our final MDSR has approximately 5 times</figDesc><table><row><cell></cell><cell>Conv</cell><cell>ReLU</cell><cell>Conv</cell><cell>Mult</cell><cell></cell><cell cols="2">Conv</cell><cell>Shuffle X2</cell><cell>Conv</cell><cell>Shuffle X3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv</cell><cell>Shuffle</cell><cell>Conv</cell><cell>Shuffle</cell></row><row><cell>Conv</cell><cell>ResBlock ResBlock ResBlock</cell><cell>(X4) (X3) (X2)</cell><cell></cell><cell>ResBlock</cell><cell>â€¢ â€¢ â€¢</cell><cell>ResBlock</cell><cell></cell><cell>Conv</cell><cell>X4 X3 X2</cell><cell>Conv Conv Conv</cell></row><row><cell cols="10">Figure 5: The architecture of the proposed multi-scale SR</cell></row><row><cell cols="4">network (MDSR).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Options</cell><cell></cell><cell cols="4">SRResNet [14] (reproduced)</cell><cell cols="3">Baseline (Single / Multi)</cell><cell>EDSR MDSR</cell></row><row><cell cols="3"># Residual blocks</cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell>32</cell><cell>80</cell></row><row><cell cols="2"># Filters</cell><cell></cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell>256</cell><cell>64</cell></row><row><cell cols="3"># Parameters</cell><cell></cell><cell cols="2">1.5M</cell><cell></cell><cell></cell><cell cols="2">1.5M / 3.2M</cell><cell>43M 8.0M</cell></row><row><cell cols="3">Residual scaling</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>0.1</cell><cell>-</cell></row><row><cell></cell><cell>Use BN</cell><cell></cell><cell></cell><cell cols="2">Yes</cell><cell></cell><cell></cell><cell></cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell cols="3">Loss function</cell><cell></cell><cell>L2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L1</cell><cell>L1</cell><cell>L1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Model specifications.</figDesc><table /><note>more depth compared to the baseline multi-scale model, only 2.5 times more parameters are required, as the resid- ual blocks are lighter than scale-specific parts. Note that MDSR also shows the comparable performance to the scale- specific EDSRs. The detailed performance comparison of our proposed models is presented in Table 2 and 3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.9673 35.03 / 0.9695 34.96 / 0.9692 35.12 / 0.9699 35.05 / 0.9696 Ã—3 30.82 / 0.9288 30.85 / 0.9292 30.90 / 0.9298 30.91 / 0.9298 31.26 / 0.9340 31.25 / 0.9338 31.39 / 0.9351 31.36 / 0.9346 Ã—4 28.92 / 0.8960 28.92 / 0.8961 28.94 / 0.8963 28.95 / 0.8962 29.25 / 0.9017 29.26 / 0.9016 29.38 / 0.9032 29.36 / 0.9029</figDesc><table><row><cell>Scale</cell><cell>SRResNet (L2 loss)</cell><cell>SRResNet (L1 loss)</cell><cell>Our baseline (Single-scale)</cell><cell>Our baseline (Multi-scale)</cell><cell>EDSR (Ours)</cell><cell>MDSR (Ours)</cell><cell>EDSR+ (Ours)</cell><cell>MDSR+ (Ours)</cell></row><row><cell>Ã—2</cell><cell>34.40 / 0.9662</cell><cell>34.44 / 0.9665</cell><cell>34.55 / 0.9671</cell><cell>34.60 / 0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison between architectures on the DIV2K validation set (PSNR(dB) / SSIM). Red indicates the best performance and blue indicates the second best. EDSR+ and MDSR+ denote self-ensemble versions of EDSR and MDSR.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Qualitative comparison of our models with other works on Ã—4 super-resolution.</figDesc><table><row><cell>img034 from Urban100 [10] img062 from Urban100 [10] 0869 from DIV2K [26] Bicubic A+ [27] 33.66 / 0.9299 36.54 / 0.9544 36.66 / 0.9542 37.53 / 0.9587 HR (PSNR / SSIM) VDSR [11] (22.62 dB / 0.5657) HR (PSNR / SSIM) VDSR [11] (20.75 dB / 0.7504) HR (PSNR / SSIM) VDSR [11] (23.36 dB / 0.8365) SRCNN [4] VDSR [11] 30.39 / 0.8682 32.58 / 0.9088 32.75 / 0.9090 33.66 / 0.9213 28.42 / 0.8104 30.28 / 0.8603 30.48 / 0.8628 31.35 / 0.8838 30.24 / 0.8688 32.28 / 0.9056 32.42 / 0.9063 33.03 / 0.9124 Figure 6: Dataset Scale Ã—2 Set5 Ã—3 Ã—4 Ã—2 Set14 Ã—3 27.55 / 0.7742 29.13 / 0.8188 29.28 / 0.8209 29.77 / 0.8314</cell><cell>Bicubic (21.41 dB / 0.4810) SRResNet [14] (23.14 dB / 0.5891) Bicubic (19.82 dB / 0.6471) SRResNet [14] (21.70 dB / 0.8054) Bicubic (22.66 dB / 0.8025) SRResNet [14] (23.71 dB / 0.8485) SRResNet [14] -/ -38.11 / 0.9601 (22.21 dB / 0.5408) A+ [27] EDSR+ (Ours) (23.48 dB / 0.6048) A+ [27] (20.43 dB 0.7145) EDSR+ (Ours) (22.70 dB / 0.8537) A+ [27] (23.10 dB / 0.8251) EDSR+ (Ours) (23.89 dB / 0.8563) EDSR (Ours) MDSR (Ours) 38.11 / 0.9602 -/ -34.65 / 0.9282 34.66 / 0.9280 32.05 / 0.8910 32.46 / 0.8968 32.50 / 0.8973 -/ -33.92 / 0.9195 33.85 / 0.9198 -/ -30.52 / 0.8462 30.44 / 0.8452</cell><cell>SRCNN [4] (22.33 dB / 0.5461) MDSR+ (Ours) (23.46 dB / 0.6039) SRCNN [4] (20.61 dB / 0.7218) MDSR+ (Ours) (22.66 dB / 0.8508) SRCNN [4] (23.14 dB / 0.8280) MDSR+ (Ours) (23.90 dB / 0.8558) EDSR+ (Ours) 38.20 / 0.9606 34.76 / 0.9290 32.62 / 0.8984 34.02 / 0.9204 30.66 / 0.8481</cell><cell>MDSR+ (Ours) 38.17 / 0.9605 34.77 / 0.9288 32.60 / 0.8982 33.92 / 0.9203 30.53 / 0.8465</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Public benchmark test results and DIV2K validation results (PSNR(dB) / SSIM). Red indicates the best performance and blue indicates the second best. Note that DIV2K validation results are acquired from published demo codes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>, we develop a multi-scale super-resolution network to reduce the model size and training time. With scale-dependent modules and shared main network, our multi-scale model can effectively deal with various scales of super-resolution in a unified framework. While the multi-scale model remains compact compared with a set of single-scale models, it shows comparable performance to the single-scale SR model.Our proposed single-scale and multi-scale models have achieved the top ranks in both the standard benchmark datasets and the DIV2K dataset.</figDesc><table><row><cell></cell><cell>HR</cell><cell>Bicubic</cell><cell></cell><cell>HR</cell><cell>Bicubic</cell></row><row><cell></cell><cell>(PSNR / SSIM)</cell><cell>(22.20 dB / 0.7979)</cell><cell></cell><cell>(PSNR / SSIM)</cell><cell>(21.59 dB / 0.6846)</cell></row><row><cell>0791 from DIV2K [26]</cell><cell>EDSR (Ours)</cell><cell>MDSR (Ours)</cell><cell>0792 from DIV2K [26]</cell><cell>EDSR (Ours)</cell><cell>MDSR (Ours)</cell></row><row><cell></cell><cell>(29.05 dB / 0.9257)</cell><cell>(28.96 dB / 0.9244)</cell><cell></cell><cell>(27.24 dB / 0.8376)</cell><cell>(27.14 dB / 0.8356)</cell></row><row><cell></cell><cell>HR</cell><cell>Bicubic</cell><cell></cell><cell>HR</cell><cell>Bicubic</cell></row><row><cell></cell><cell>(PSNR / SSIM)</cell><cell>(23.81 dB / 0.8053)</cell><cell></cell><cell>(PSNR / SSIM)</cell><cell>(19.77 dB / 0.8937)</cell></row><row><cell>0793 from DIV2K [26]</cell><cell>EDSR (Ours)</cell><cell>MDSR (Ours)</cell><cell>0797 from DIV2K [26]</cell><cell>EDSR (Ours)</cell><cell>MDSR (Ours)</cell></row><row><cell></cell><cell>(30.94 dB / 0.9318)</cell><cell>(30.81 dB / 0.9301)</cell><cell></cell><cell>(25.48 dB / 0.9597)</cell><cell>(25.38 dB / 0.9590)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We confirmed our reproduction is correct by getting comparable results in an individual experiment, using the same settings of the paper<ref type="bibr" target="#b13">[14]</ref>. In our experiments, however, it became slightly different to match the settings of our baseline model training. See our codes at https://github.com/LimBee/NTIRE2017.<ref type="bibr" target="#b2">3</ref> We used the original paper (https://arxiv.org/abs/1609.04802v3) as a reference.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Edge-directed interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allebach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP 1996</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2012</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2004</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image super-resolution with sparse neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3194" to="3205" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2009</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CVPR 2016. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2016</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>ICLR 2014. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>HuszÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">New edge-directed interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1521" to="1527" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>ICCV 2001. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<idno>MATLAB. version 9.1.0</idno>
	</analytic>
	<monogr>
		<title level="j">The MathWorks Inc</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02177</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>MIC- CAI 2015. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>HuszÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2016</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2008</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Super resolution using edge prior and single image detail synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<idno>CVPR 2010. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ntire 2017 challenge on single image superresolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV 2014</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>CVPR 2016. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning super-resolution jointly from external and internal examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4359" to="4371" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast direct super-resolution by simple functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2013</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Curves and Surfaces</title>
		<meeting>the International Conference on Curves and Surfaces</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An edge-guided image interpolation algorithm via directional filtering and data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2226" to="2238" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Loss functions for neural networks for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08861</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
