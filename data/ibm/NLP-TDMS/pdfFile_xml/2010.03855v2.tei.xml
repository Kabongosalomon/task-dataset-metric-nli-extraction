<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Relational Image Captioning via Multi-task Triple-Stream Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
						</author>
						<title level="a" type="main">Dense Relational Image Captioning via Multi-task Triple-Stream Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce dense relational captioning, a novel image captioning task which aims to generate multiple captions with respect to relational information between objects in a visual scene. Relational captioning provides explicit descriptions of each relationship between object combinations. This framework is advantageous in both diversity and amount of information, leading to a comprehensive image understanding based on relationships, e.g., relational proposal generation. For relational understanding between objects, the part-of-speech (POS, i.e., subject-object-predicate categories) can be a valuable prior information to guide the causal sequence of words in a caption. We enforce our framework to not only learn to generate captions but also predict the POS of each word. To this end, we propose the multi-task triple-stream network (MTTSNet) which consists of three recurrent units responsible for each POS which is trained by jointly predicting the correct captions and POS for each word. In addition, we found that the performance of MTTSNet can be improved by modulating the object embeddings with an explicit relational module. We demonstrate that our proposed model can generate more diverse and richer captions, via extensive experimental analysis on large scale datasets and several metrics. We additionally extend analysis to an ablation study, applications on holistic image captioning, scene graph generation, and retrieval tasks.Code has been made available at: https://github.com/Dong-JinKim/DenseRelationalCaptioning. Index Terms-Dense captioning, image captioning, visual relationship, relational analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The human visual system has the capability to effectively and instantly collect the holistic understanding of contextual associations among objects in a scene <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b35">[35]</ref> by densely and adaptively skimming the visual scene through the eyes, i.e., the saccadic eye movement. Such rich information instantly extracted from the scene allows humans to understand the even subtle relationships among objects. Motivated by such human ability, in this work, we present a new concept of scene understanding, called dense relational captioning that provides dense and relational captions.</p><p>Rich representation of an image often leads to performance improvements of computer vision algorithms, i.e., contexts surrounding objects of a scene <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b35">[35]</ref>. To achieve richer objectcentric understanding, Johnson et al. <ref type="bibr" target="#b15">[16]</ref> proposed the DenseCap framework that generates captions for each of the densely sampled local image regions. These regional descriptions facilitate both rich and dense semantic understanding of a scene in the form of interpretable language. In contrast, the information that we want to acquire includes not only that of the objects itself but also the interaction among other objects or the environment.</p><p>As an alternative way of representing an image, we focus on dense relationships between objects. In the context of human cognition, there has been a general consensus that objects and particular environments near the target object affect search and recognition efficiency. Understanding the relationships between objects clearly reveal object interactions and object-attribute combinations <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b31">[31]</ref>. Interestingly, we observe that the human annotations on various computer vision datasets predominantly have relational forms. In the Visual Genome <ref type="bibr" target="#b22">[23]</ref> and MS COCO <ref type="bibr" target="#b29">[29]</ref> caption datasets, most of the labels take the format of subject-predicate-object more so than subject-predicate. Moreover, the UCF101 <ref type="bibr" target="#b46">[46]</ref> action recognition dataset contains 85 actions out of 101 (84.2%) that are described in terms of human interactions with other objects or surroundings. These aspects tell us that understanding interaction and relationships between objects facilitate a major component in visual understanding of object-centric events.</p><p>In this regard, we introduce a novel captioning framework relational captioning that can provide diverse and dense representations from a visual scene, e.g., an image. In this task, we first exploit the relational context between two objects as a representation arXiv:2010.03855v2 [cs.CV] 12 Oct 2020 unit. This allows generating a combinatorial number of localized regional information. Secondly, we make use of captioning and its ability to express significantly richer concepts beyond the limited label space of object classes used in object detection tasks. Due to these aspects, our relational captioning expands the regime further along the label space both in terms of density and complexity, and provides richer representation for an image.</p><p>Our main contributions are summarized as follows. <ref type="bibr" target="#b0">(1)</ref> We introduce relational captioning, a new captioning task that generates captions with respect to relational information between objects in an image. <ref type="bibr" target="#b1">(2)</ref> In order to efficiently train the relational caption information, we propose the multi-task triple-stream network (MTTSNet) that consists of three recurrent units trained via multi-task learning with the part-of-speech prediction. <ref type="bibr" target="#b2">(3)</ref> We show that our proposed method is able to generate denser and more diverse captions by evaluating on our relational captioning dataset augmented from Visual Genome (VG) <ref type="bibr" target="#b22">[23]</ref> dataset. <ref type="bibr" target="#b3">(4)</ref> We demonstrate several use cases of our framework, including "caption graphs" which contain richer and more diverse information than conventional scene graphs.</p><p>This work is the extension of our previous conference paper <ref type="bibr" target="#b17">[18]</ref>. We extend it in several aspects: We extend our architecture by adding a relational embedding module (REM) motivated by the non-local networks <ref type="bibr" target="#b52">[52]</ref> to explicitly augment semantic meanings of surrounding objects. Also, we show that the REM further enhances MTTSNet in all the application scenarios we demonstrate. In addition, we expand our experimental results and analysis to show multiple aspects of our proposed method's algorithmic behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work mainly relates to two topics: image captioning and relationship detection. In this section, we review related work on these categorized topics. Image captioning. By virtue of deep learning and the use of recurrent neural network (e.g., LSTM <ref type="bibr" target="#b12">[13]</ref>) based decoders, image captioning <ref type="bibr" target="#b36">[36]</ref> techniques have been extensively explored <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b59">[59]</ref>, <ref type="bibr" target="#b61">[61]</ref>. One of the research issues in captioning is the generation of diverse and informative captions. Thus, learning to generate diverse captions has been extensively studied recently <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b50">[50]</ref>. As one of the solutions, the dense captioning (DenseCap) task <ref type="bibr" target="#b15">[16]</ref> was proposed which uses diverse region proposals to generate localized descriptions, extending the conventional holistic image captioning to diverse captioning that can describe local contexts. Moreover, our relational captioning is able to generate even more diverse caption proposals than dense captioning by considering relations between objects. Since DenseCap generates each caption per bounding box by only relying on an internal region of the bounding box, Yang et al. <ref type="bibr" target="#b57">[57]</ref> improves the DenseCap model by incorporating a global image feature as context cue as well as a region feature of the desired objects with late fusion. Motivated by this, in order to learn dependencies of subject, object and union representations, we incorporate a triple-stream LSTM for our captioning module and further enhance the relational embedding by a non-local layer <ref type="bibr" target="#b52">[52]</ref>. Visual relationship detection (VRD) and scene graph generation. Understanding visual relationships between objects have been an important concept in various tasks. Conventional VRD usually deals with predicting the subject-predicate-object (in short, subj-pred-obj). A pioneering work by Lu et al. <ref type="bibr" target="#b31">[31]</ref> formalizes the VRD task and provides a dataset, while addressing the subject (or object) and predicate classification models separately. Their VRD dataset has also led to extensive studies on visual relationship understanding <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b60">[60]</ref>, <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b64">[64]</ref>, <ref type="bibr" target="#b65">[65]</ref>, <ref type="bibr" target="#b66">[66]</ref>. On the other hand, similar to the VRD task, scene graph generation (a task to generate a structured graph that express the context relationships of a scene) has also started to be explored <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b54">[54]</ref>, <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b63">[63]</ref>, which provides a compact and interpretable representation of scenes.</p><p>Although the VRD dataset is larger (100 object classes and 70 predicates) than Visual Phrases <ref type="bibr" target="#b43">[43]</ref> , it is still inadequate to handle the real world scale. The Visual Genome (VG) dataset <ref type="bibr" target="#b22">[23]</ref> for relationship detection consists of 31, 000 predicate types and 64, 000 object types giving the combinatorial relationship triplets to the state-of-the-art VRD based models, of which number is too diverse for the VRD models to comply with. This is because, in the VRD task, each object label should be constituted by the various adjective and noun combinations, e.g., "little boy," "small boy." As a result, only the simplified version of VG relationship dataset has been studied. In contrast, our method is able to represent extensive natural language of relations by tokenizing the whole relational expressions into words, and learning from them directly.</p><p>While the recent state-of-the-art VRD <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b60">[60]</ref>, <ref type="bibr" target="#b62">[62]</ref> or scene graph generation <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b54">[54]</ref>, <ref type="bibr" target="#b63">[63]</ref> methods attempted to use language priors to detect relationships, we directly learn the relationship in a descriptive language form. In addition, the expressions of scene graph generation or VRD task are restricted to subj-pred-obj triplets, whereas our proposed relational captioning task is able to provide additional information such as attributes or noun modifiers by adopting free-form natural language expressions.</p><p>In summary, dense captioning facilitates a natural language interpretation of regions in an image, while VRD can predict relational information between objects within a restricted set. Our work combines both axes, resulting in much denser and diverse captions than DenseCap. That is, given B number of region proposals in an image, we can obtain B(B−1) number of relational captions, whereas DenseCap returns only B number of captions. This property can be favorable for subsequent algorithms for other downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-TASK TRIPLE-STREAM NETWORKS</head><p>Our relational captioning generates captions as follows. Given an input image, a bounding box detector generates various object proposals, followed by a captioning module that predicts combinatorial captions describing each pair of objects along with POS labels. This pipeline is illustrated in <ref type="figure">Figure 2</ref>, which is composed of a localization module based on the region proposal network (RPN) <ref type="bibr" target="#b40">[40]</ref>, and a triple-stream RNN (LSTM <ref type="bibr" target="#b12">[13]</ref>) module for captioning. In addition, we introduce the relational embeddding module (REM) as an extension, to encourage explicit encoding of relational information. Our network supports end-to-end training within a single optimization step that allows joint localization, combination, and description with natural language.</p><p>Specifically, given an image, the RPN generates object proposals. Then, the combination layer takes a pair of proposals and assigns them to the subject and object regions at a time. Also, to take the surrounding context information into account, we utilize the union region of the subject and object regions as side information. These triplet features from the subject, object, and union regions are fed to the triple-stream LSTMs, where each stream takes its own purpose, i.e. subject, object, and union. Given these triplet features, the triple-stream LSTMs collaboratively generate a caption and POS classes of each word. We describe details of these processes in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Region Proposal Networks</head><p>Our network uses fully convolutional layers of VGG-16 <ref type="bibr" target="#b45">[45]</ref> 1 up to the final pooling layer (i.e. pool5) for extracting the spatial features via the bilinear ROI pooling <ref type="bibr" target="#b15">[16]</ref>. The object proposals are generated by RPN <ref type="bibr" target="#b40">[40]</ref>. It takes the feature tensor from the pool5 layer, and proposes B number of regions of interest after non-maximum suppression (NMS). Each proposed region comes with its confidence score, region feature of shape 512×7×7, and coordinates b=(x, y, w, h) of the bounding box with center (x, y), width w and height h.</p><p>Relational proposals are generated by building pairwise combinations of B number of region proposals, where in turn we get B(B−1) possible region pair combinations. We call this layer as combination layer. A distinctive point of our model with the previous dense captioning methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b57">[57]</ref> is that, while the methods regard each region proposal as an independent target to describe and produce B number of captions, we consider their pairwise B(B−1) number of combinations, which are much denser and explicitly expressible in terms of relationships. Also, we can asymmetrically use each entry of a pair by assigning the roles of the regions, i.e., (subject, object) or vice versa.</p><p>We vectorize the region features, and then apply two fullyconnected (FC) layers to map them into D-dimensional features, where the intermediate dimensions are D u =512 for the union region and D o =4096 for subject and object regions. Only the first intermediate FC layer for subject and object features share weights. We use rectified linear (ReLU) units <ref type="bibr" target="#b34">[34]</ref> and dropouts <ref type="bibr" target="#b47">[47]</ref> for the FC layers. The subject and object region features are optionally fed to the Relational Embedding Module (REM) which outputs refined features with the same size D=512. The details of the REM is described in the later subsection of 1. One can improve the performance of our relational method by replacing the backbone network with a deeper one, e.g., ResNet <ref type="bibr" target="#b11">[12]</ref>. In this manuscript, we do not explore this direction in that our contribution is on the introduction of relational caption representation and the relational system for it. Sec. 3.2. In short, the aforementioned process encodes region features into D-dimensional features, which is called region codes.</p><p>Furthermore, we leverage an additional region, the union region b u of (subject, object) motivated by Yang et al. Then, the dimension of the union region code is reduced by the following FC layer. This stream of the aforementioned operations is illustrated in <ref type="figure">Fig. 2</ref>. The three features extracted from the subject, object, and union regions are fed to each LSTM described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relational Captioning Networks</head><p>Our relational captioning network consists of multiple LSTM modules to generate captions that describe relational information. In this work, the relational information refers to the context of object pair relationships. To this end, we design a new network that explicitly exploit relational cues.</p><p>In the proposed relational region proposal, a distinctive facet is its capability to provide a triplet of region codes corresponding to the subject, object, and union regions, which can be also viewed as the POS of a sentence (subj-pred-obj). The existence of this correspondence between each region in a triplet and POS information can lead to the following advantages: 1) input region codes can be adaptively merged depending on its POS and be fed to the word prediction module, and 2) the POS prior on predicting a word can effectively affect the quality of caption "horse" <ref type="figure">Fig. 3</ref>. An illustration of the unrolled triple-stream LSTM. Our model consists of two major parts: triple-stream LSTM and a multi-task module. The multi-task module jointly predicts a caption word and its POS class (subj-pred-obj, illustrated as three cells colored according to the POS class), as well as the input vector for the next time step.</p><p>generation by reducing potential spurious words. To leverage these benefits, we propose the multi-task triple-stream network. For the first advantage, to derive POS aware inference, we first propose the triple-stream network which consists of three separate LSTMs respectively corresponding to subj-pred-obj. The outputs of the LSTMs are combined via concatenation. For the second advantage, during word prediction, we jointly infer its POS class. This POS class prediction task allows the network to learn the POS prior knowledge for the word prediction.</p><p>Triple-Stream LSTMs. Intuitively, the region codes of the subject and object would be closely related to the respective subject and object related words in a caption, while the union and geometric features may contribute to the predicate. In our relational captioning framework, the LSTM modules need to adaptively take into account input features to generate a caption according to the POS decoding stage. As shown in <ref type="figure">Fig. 2</ref>, the proposed triple-stream LSTM module consists of three separate LSTMs, each of which is in charge of the subject, object and union region codes respectively. From the region proposal network, a triplet of region codes are fed as input to LSTMs, so that a sequence of words (caption) is generated. At each step (word), the triple-stream LSTMs generate three embedded representations separately, and a single word is predicted by consolidating the three processed representations by the multi-task module (described in the next sub-section). The embedding of the predicted word is distributed into all three LSTMs as inputs and is used to run the next step in a recursive manner. Thus in each step, each entry of the triplet input is used differently, which allows more flexibility than that of a single LSTM as used in traditional captioning models <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b49">[49]</ref>. In other words, the importance of the input features changes at every recursive step according to which POS the word being generated belongs to.</p><p>Multi-task with POS Classification. At each part of the triplestream LSTMs, we obtain three intermediate output features from each LSTM. To predict a word, we aggregate the features from the subject, predicate and object information, via a single FC layer. Also, we add an additional side task, POS prediction, from the same concatenated feature. We call this fusion layer as the multitask module as shown in the right enlarged view of <ref type="figure">Fig. 3</ref>.</p><p>The multi-task module can be viewed as a late fusion approach. An alternative would be an early fusion approach, which consolidates the information in an even earlier step, i.e., the fusion of the three region codes (e.g., concatenation of three codes) followed by a single LSTM model, instead of the triple-stream LSTMs. However, we observe that this early fusion approach has lower performance than that of late fusion, which is also consistent with the observation reported by Yang et al. <ref type="bibr" target="#b57">[57]</ref>. Thus, we take the late fusion approach and compare the performance in Sec. 4.</p><p>The POS classification task is leveraged to more effectively train the relational captioning. We can impose the POS classification loss during training, so that the networks learn which LSTM they should emphasize more at a word prediction. Thereby, relational captioning generates a sequence of words in subjpred-obj order, i.e., the order of POS. The POS task encourages the caption generation to follow the order of POS. The POS tag can be easily obtained by a modern natural language processing toolkit, NLTK POS tagger <ref type="bibr" target="#b30">[30]</ref>, which had been established for a long time; thus, it provides a reliable prediction. In our case, we obtain POS ground truth from automatic label augmentation from relationship triplet labels.</p><p>We empirically observe that this multi-task learning with POS not only helps the shared representation to be richer, but also guides the word predictions, and thus helps to improve the captioning performance overall. Since each POS class prediction relies on respective representations from each LSTM, (e.g., predicate class prediction from the pred-LSTM), the gradients generated from the POS classification would be mainly back-propagated through the feature elements representing a class ambiguously within the concatenated feature. Even for the same word output, the gradients from the multi-task module may differ by this fact, so that representations across LSTMs can be learned to be further distinctive. In addition, the imposed POS prior may make the network suppress spurious word candidates. Relational Embedding Module. Since our triple-stream network only utilizes the triplet features (subject, object and union), it alone may lack global understanding of the constituent objects in an entire image, i.e., global context. In this extension, to strengthen the capability of holistic relational understanding across all the objects, we employ the non-local layer <ref type="bibr" target="#b52">[52]</ref>, we called the relational embedding module (REM), where we apply the nonlocal layer to each object candidate feature different from the approach that Wang et al. <ref type="bibr" target="#b52">[52]</ref> apply it to the feature map densely. The REM enhances the relational information across all objects via the attention mechanism.</p><p>Specifically, let X ∈ R B×Do denote a stack of B number of vectorized region features extracted from the first FC layer after the bilinear ROI pooling. Then, we compute the relational association matrix by: where σ(·) denotes ReLU and W a , W b ∈ R Do×512 are learnable weights that map the region features X to each of its own role, (e.g., subject and object) and the softmax operation is applied rowwise. Then, the relational feature matrix is computed by:</p><formula xml:id="formula_0">R = softmax(σ(XW a )σ(XW b ) ) ∈ R B×B ,<label>(2)</label></formula><formula xml:id="formula_1">A = Rσ(XW x )W z ∈ R B×Do ,<label>(3)</label></formula><p>where W x ∈ R Do×512 and W z ∈ R Do×512 are again learnable weights. The matrix A encodes aggregated features across all the objects according to the degree of relational association by R, which is similar to the message passing that exchanges the information according to the relationship. This relational feature matrix is combined with the original feature X by Z = X + A, so that the holistic relational information is enhanced on top of X. This can be viewed as augmenting richer semantic meanings, e.g., a shirt (X) is augmented to a shirt that someone is in or a shirt on something depending on the surroundings. Also, it is akin to the residual connection, allowing efficient training via the residual learning mechanism <ref type="bibr" target="#b11">[12]</ref>. Different from the nonlocal approaches <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b53">[53]</ref>, we introduce non-linear activations, ReLU, in Eqs. <ref type="formula" target="#formula_0">(2)</ref> and <ref type="formula" target="#formula_1">(3)</ref>, motivated by a low-rank bilinear pooling method <ref type="bibr" target="#b20">[21]</ref>. We empirically found this modification leads to noticeable performance improvement. The REM module is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. Loss functions. The proposed model is trained to minimize the following loss function:</p><formula xml:id="formula_2">L = L cap + αL P OS + βL det + γL box ,<label>(4)</label></formula><p>where L cap , L P OS , L det , and L box denote captioning loss, POS classification loss, detection loss, and bounding box regression loss respectively. α, β, and γ are the balance parameters (we set them to 0.1 for all experiments). The first two terms are for captioning and the next two terms are for the region proposal. L cap and L P OS are cross-entropy losses at every time step for each word and POS prediction respectively. For each time step, L P OS measures a 3-class cross entropy loss. L det is a binary logistic loss for foreground/background regions to distinguish positive and negative object regions <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, while L box is a smoothed L1 loss <ref type="bibr" target="#b40">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we provide the experimental setups, competing methods and performance evaluation of relational captioning with Object both quantitative and qualitative results, so that we empirically show the benefit and potential of the proposed relational captioning task and the proposed method.</p><formula xml:id="formula_3">U + C Single × Subj+Obj Object S + O Single × Subj+Obj+Coord. Object S + O + C Single × Subj+Obj+Union Object S + O + U Single × Union (w/MTL) Object U Single Subj+Obj+Coord.(w/MTL) Object S + O + C Single Subj+Obj+Union (w/MTL) Object S + O + U Single TSNet Object S | O | U + C Triple × MTTSNet Object S | O | U + C Triple</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setups</head><p>Implementation details. We use Torch7 <ref type="bibr" target="#b3">[4]</ref> to implement our model. For the backbone visual feature extraction, we use VGG-16 <ref type="bibr" target="#b45">[45]</ref>, and initialize with weights pre-trained on ImageNet <ref type="bibr" target="#b42">[42]</ref>. We pre-train the RPN on the Visual Genome (VG) dense captioning data <ref type="bibr" target="#b22">[23]</ref>. For sequence modeling, we set the dimension of all the LSTM hidden layers to be 512. A training batch contains a single image that is resized to have the longer side of 720 pixels. We use Adam <ref type="bibr" target="#b1">[2]</ref> optimizer for training (learning rate lr=10 −6 , b1=0.9, b2=0.999). For the RPN, we use 12 anchor boxes for generating the anchor positions in each cell of the feature map, and 128 boxes are sampled in each forward pass of training. We use Titan X GPU and it takes about three to four days for a model to convergence when training on our relational captioning dataset. We use the similar setting for the region proposals to that of <ref type="bibr" target="#b15">[16]</ref> for fairness. For training, a region is positive if it has at least 0.7 IoU ratio with a corresponding ground truth region, and a region is negative if its IoUs are less than 0.3 with all ground truth regions. For evaluation, after non-maximum suppression (NMS) based on the predicted proposal confidences, 50 confident bounding boxes are selected. We can additionally reduce box pair predictions by discarding pairs that produce captions with low confidence scores. Caption confidence scores can be computed by sequentially multiplying all of the generated word probabilities. Relational captioning dataset. Since there is no existing dataset for the relational captioning task, we construct a dataset by utilizing VG relationship dataset version 1.2 <ref type="bibr" target="#b22">[23]</ref> which consists of 85,200 images with 75,456/4,871/4,873 splits for train/validation/test sets respectively. We tokenize the relational expressions into word level tokens, and for each word, we assign the POS class from the triplet association which will be explained in detail.</p><p>However, the VG relationship dataset has a limited diversity of the words used. Therefore, naïvely converting such VRD dataset to a captioning dataset is not desirable, in that the captions generated from a trained model on the dataset tends to be too simple (e.g., "building-has-window"). This limited data restricts the expressiveness of the model. To examine the diverse expressions of our relational captioner, we make our relational captioning dataset to have more natural sentences with richer expressions.</p><p>Through observation, we noticed that the relationship dataset labels lack attributes describing the subject and object, which are perhaps what enriches the sentences the most. We enrich the dataset by leveraging the VG attribute dataset <ref type="bibr" target="#b22">[23]</ref>. The specific procedure of this attribute enrichment is described in Appendix.  After this enrichment, we obtain 15,595 different vocabularies for our relational captioning dataset, which was 11,447 different vocabularies before this process. We train our model with this dataset, and report its result in this section. In the following subsections, we evaluate in multiple views including a holistic image captioning performance and various analysis such as comparison with scene graph generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relational Dense Captioning: Ablation Study</head><p>Baselines. Since no direct related work for relational captioning exists, we implement several baselines by modifying the most relevant methods, which facilitate our ablation study. All the configurations are summarized in <ref type="table" target="#tab_1">Table 1</ref> and described as follows.</p><p>• Direct Union has the same architecture with DenseCap <ref type="bibr" target="#b15">[16]</ref>, but of which RPN is trained to directly predict union regions. A union region is converted to a 512dimensional region code, and followed by a single LSTM to generate a relational caption. • Union also resembles DenseCap <ref type="bibr" target="#b15">[16]</ref> and Direct union, but its RPN predicts individual object regions. The object regions are paired as (subject, object), and then only a union region from each pair is fed to a single LSTM for captioning. Also, we implement two additional variants: Union (w/MTL) additionally predicts the POS classification task, and Union+Coord. appends the geometric feature to the region code of the union. • Subj+Obj and Subj+Obj+Union models use the concatenated region code of (subject, object) and (subject, object, union) respectively and pass them through a single LSTM (early fusion approach   <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b49">[49]</ref> and two dense captioners <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b57">[57]</ref>. To compare with stronger baselines, we modify the image captioners by deploying a stochastic sampling. We annotate the modified versions with stochastic sampling with †. We annotate (GT ) for the methods that replace RPN with ground truth bounding boxes; thus, those represent proxy upper bounds of performance.</p><p>suggest a modified evaluation metric for the relational dense captioning. Firstly, to assess the caption quality, we measure the average METEOR score <ref type="bibr" target="#b7">[8]</ref> for predicted captions (noted as METEOR In particular, we only consider the samples with IOUs of both the subject and object bounding boxes greater than the localization threshold, which yields a more challenging metric. For all cases, we use percentage as the unit of metric. In addition, we suggest another metric, called "image-level (Img-Lv.) recall." This measures the caption quality at the holistic image level by considering the bag of all captions generated from an image as a single prediction. This metric evaluates the diversity of the produced representations by the model for a given image. Specifically, with the aforementioned language thresholds of METEOR, we measure the recall of the predicted captions over about 20 ground truth captions. Results. <ref type="table" target="#tab_3">Table 2</ref> compares the performance of various methods for the relational dense captioning task on the relational captioning dataset. To compare with a different representation of relationship, we additionally compare with the state-of-the-art scene graph generator, Neural Motifs <ref type="bibr" target="#b63">[63]</ref>. Due to the different output structure, we compare with Neural Motifs trained with the supervision for relationship detection. Similar to the setup in <ref type="bibr" target="#b15">[16]</ref>, we fix the number of region proposals after NMS to 50 for all methods for a fair comparison.</p><p>Within the second row section (2-7th rows) of <ref type="table" target="#tab_3">Table 2</ref>, our TSNet shows the best result suggesting that the triple-stream component alone is a sufficiently strong baseline over the others. On top of TSNet, applying the MTL loss (i.e., MTTSNet) improves overall performance, and especially improves mAP, where the detection accuracy is dominantly improved compared to the Image Captioner: ■ A man riding a motorcycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>■ Person wearing red and black jacket. ■ A cloud in blue sky. ■ The helmet is black. ■ Front wheel of motorcycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>■→■ The man on a black motorcycle. ■→■ The man in blue sky. ■→■ Red motorcycle has a black wheel. ■→■ The man wearing black helmet. ■→■ Black wheel on a motorcycle. ■→■ The head of man. ■→■ Blue sky has white clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋮</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Captioner:</head><p>■ A person is playing frisbee in the snow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>■ A blue sky with clouds. ■ A dog. ■ Man in the air. ■ A sandy beach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>■→■ The man on white beach. ■→■ Green tree has leaves. ■→■ Black dog on sandy beach. ■→■ The tree in background. ■→■ The man on a ground. ■→■ Green grass on ground. ■→■ The tree in a large field. ⋮ Image Captioner: ■ A group of people riding bikes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>■ A man riding a bike. ■ Man wearing blue jeans. ■ A train on the tracks. ■ A building in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>■→■ The bicycle on street. ■→■ Black wheel on bicycle. ■→■ The bicycle has a black wheel. ■→■ Red bus has a black wheel. ■→■ The car on street. ■→■ The man on black motorcycle. ■→■ Red bus on street.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>■ Snowboarder in the air. ■ Red jacket on man. ■ A snowboard is white. ■ Red and white snow board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>■→■ Green trees in background. ■→■ The snow on ground.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>■ Baseball player swinging a bat. ■ A black shirt on a man. ■ A red and white baseball field. ■ A black and white tennis racket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>■→■ The man in green grass.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>■ Clock on the wall. ■ A brick building with a clock. ■ White lines on the road ■ Woman wearing a red shirt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>■→■ Brick building has a window. ■→■ The woman in front of building. ■→■ The shadow on ground. ■→■ The woman on road. ■→■ White clock on building. ■→■ The woman on black sidewalk. ■→■ The window on red building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋮</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Captioner:</head><p>■ A man riding a skateboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Captioner:</head><p>■ A man doing a trick on a skateboard. ■ A man on a skateboard. ■ Skateboard in the air. ■ Doorway in front of building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioner (Ours):</head><p>■→■ The man wearing black shirt. ■→■ The people on sidewalk. ■→■ The people on a black skateboard. ■→■ White building has a window. ■→■ The man holding black skateboard. ■→■ Black window on building. ■→■ The man on sidewalk. ⋮ <ref type="figure">Fig. 5</ref>. Example captions and region generated by the proposed model on Visual Genome test images. The region detection and caption results are obtained by the proposed model from Visual Genome test images. We compare our result with the image captioner <ref type="bibr" target="#b49">[49]</ref> and the dense captioner <ref type="bibr" target="#b15">[16]</ref> in order to contrast the amount of information and diversity. other metrics. This shows that triple-stream LSTM is the key module that most leverages the MTL loss across other early fusion approaches (see the third row section of the table). Moreover, by adding REM to our late fusion method, MTTSNet, we have achieved further improvements in both mAP and Img-Lv. Recall scores (more strongly on Img-Lv. Recall). As another factor, we can see from <ref type="table" target="#tab_3">Table 2</ref> that the relative spatial information (Coord.) and union feature information (Union) improves the results. This is because the union feature itself preserves the spatial information to some extent from the 7 × 7 grid form of its activation. Also, the relational captioner baselines including our TSNet and MTTSNet perform favorably against Neural Motifs in all metrics. Note that handling free-form language generation which we aim to achieve is more challenging than the simple triplet prediction of scene graph generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Holistic Image Captioning</head><p>We also compare our approach with other image captioning frameworks, Image Captioner (Show&amp;Tell <ref type="bibr" target="#b49">[49]</ref>, SCST <ref type="bibr" target="#b41">[41]</ref>, and RFNet <ref type="bibr" target="#b13">[14]</ref>), and Dense Captioner (DenseCap <ref type="bibr" target="#b15">[16]</ref> and TLSTM <ref type="bibr" target="#b57">[57]</ref>) in a holistic image description perspective. To measure the performance of holistic image-level captioning for dense captioning methods, we use Img-Lv. Recall metric defined in the previous section. We compare them with two relational dense captioning methods, Union and MTTSNet (as well as +REM), denoted as Relational Captioner. For a fair comparison,  for Dense and Relational Captioner, we adjust the number of region proposals after NMS to be similar, which is different from the setting in the previous section which fixes the number of proposals before NMS. For fair comparison with the Image Captioner, in addition to the typical selection of words according to maximum probabilities in caption generation, we introduce another baselines using a stochastic sampling (probabilistically selecting a word proportional to the probabilities of words from a model) to allow diverse caption generation from the LSTM. We generate 10 captions from the stochastic variant image captioners in order to match the number of captions between Image Captioner and Dense Captioner. Finally, in order to isolate the performance of the caption generation and the box localization modules, we measure the captioning performance by setting the bounding boxes as the ground truth boxes. We annotate such variant of relational captioners with (GT ). <ref type="table" target="#tab_5">Table 3</ref> compares the image-level recall, METEOR, and additional quantities. #Caption denotes the average number of captions generated from an input image and Caption/Box denotes the average ratio of the number of captions generated and the number of boxes remaining after NMS. Therefore, Caption/Box demonstrates how many captions can be generated given the same number of boxes generated after NMS. By virtue of multiple captions per image from multiple boxes, the Dense Captioner is able to achieve higher performance than all the Image Captioners. While the stochastic sampling methods slightly improve image captioning performance in terms of recall, the performance is still ■→■ Standing man wearing blue shirt. ■→■ The man holding black racket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><p>■→■ Young man on green court. ■→■ The fence in background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>■→■ The elephant has a brown head. ■→■ Gray elephant in background. far lower than Dense Captioners or Relational Captioners by a large margin, as the diversity of an image captioner's output is still very limited by its inherent design. Compared with the Dense Captioners, MTTSNet as a Relational Captioner can generate an even larger number of captions, given the same number of boxes. Hence, as a result of learning to generate diverse captions, the MTTSNet achieves higher recall and METEOR. TLSTM <ref type="bibr" target="#b57">[57]</ref> improves the performance of DenseCap <ref type="bibr" target="#b15">[16]</ref> due to a better representational power, but the performance is still lower than that of MTTSNet. Comparing to Union, we can see that it is difficult to obtain better captions than Dense Captioner by only learning to use the union of subject and object boxes, despite having a larger number of captions. Adding REM to our MTTSNet, further improves the performance in both the Recall and the METEOR score. In addition, even when setting the bounding boxes as the ground truth bounding boxes, by virtue of the more powerful language module, MTTSNet (especially MTTSNet+REM) shows favorable performance compared to Union.</p><p>We show prediction examples of our relational captioning model in <ref type="figure">Fig. 5</ref> along with the comparisons against the traditional frameworks, image captioner <ref type="bibr" target="#b49">[49]</ref> and dense captioner <ref type="bibr" target="#b15">[16]</ref>. Our model is able to generate rich and diverse captions for an image, compared to other paradigms. While the dense captioner is able to generate diverse descriptions than an image captioner by virtue of  <ref type="table">TABLE 4</ref> Sentence based image retrieval performance comparison across different representations. We evaluate ranking using recall at k (R@K, higher is better) and the median rank of the target image (Med, lower is better). The random chance performance is provided for reference. We compare with TLSTM in addition to the baselines (Full Image RNN, Region RNN, DenseCap) suggested in Johnson et al. <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Sentence + Reference Image Retrieved Images &amp; Region-pair</head><p>White cars parked on side of road.</p><p>White plane has a red wing.</p><p>Green trees across blue water.</p><p>White sign near paved road. localized regions, our model can generate an even more number of captions from the combination of the bounding boxes. <ref type="figure" target="#fig_11">Figure 7</ref> shows caption prediction examples for multiple box pair combinations. Based on the output of the POS predictor, we color the words of the caption as (red, green, blue) for (subjpred-obj) respectively. We note that, while the traditional dense captioning simply takes a single region as input and predicts one dominant description, in our framework, different captions can be obtained from different subject and object pairs. In addition, one can see that the predicted POS is correctly aligned with the words in the generated captions. Although the POS classification is not our target task, for completeness, we measure the accuracy of the MTTSNet POS estimation by comparing it with the ground truth POS, which is 89.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Scene Graph</head><p>Motivated by scene graph, which is derived from the VRD task, we extend to a new type of a scene graph, which we call "caption graph." <ref type="figure">Figure 6</ref> shows the caption graphs generated from our MTTSNet as well as the scene graphs from Neural Motifs <ref type="bibr" target="#b63">[63]</ref>. For caption graph, we follow the same procedure with Neural Motifs, but replace the relationship detection network with our MTTSNet. In both methods, we use ground truth bounding boxes to generate scene (and caption) graphs for fair comparison.</p><p>By virtue of being free form, our caption graph can have richer expression and information including attributes, whereas the traditional scene graph is limited to a closed set of the subj-pred-obj triplet. For example, in <ref type="figure">Fig. 6-(b,d)</ref>, given the same object "person," our model is able to distinguish the fine-grained category (i.e., man vs boy and man vs woman). In addition, our model can provide more status information about the object (e.g., standing, black), by virtue of the attribute contained in our relational captioning data. Most importantly, the scene graph can contain unnatural relationships (e.g., tree-on-tree in <ref type="figure">Fig. 6-(c)</ref>), because the back-end relationship detection methods, e.g., <ref type="bibr" target="#b63">[63]</ref>, predict object classes independently. In contrast, by predicting the full sentence for every object pair, the relational captioner can assign a more appropriate word with attributes for an object by considering the relations, e.g., "Green leaf on a tree." Lastly, our model is able to assign different words for the same object by considering the context (the man vs baseball player in <ref type="figure">Fig. 6-(d)</ref>), whereas the scene graph generator can only assign one most likely class (man). Thus, our relational captioning framework enables more diverse interpretation of the objects compared to the traditional scene graph generation models, which would be more favorable representation to scene context understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Sentence-based Image and Region-pair Retrieval</head><p>Since our relational captioning framework produces richer image representations than other frameworks, it may have benefits on image and region-pair retrieval by sentence. Our method can directly deal with free-form natural language queries, whereas scene graph or VRD models require additional processing to handle </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;person&gt;-&lt;hold&gt;-&lt;phone&gt;</head><p>Sitting man holding small phone. <ref type="figure">Fig. 9</ref>. Qualitative comparison with visual relationship detection model <ref type="bibr" target="#b31">[31]</ref>. The proposed relational captioning model is able to provide more detailed information than the traditional relationship detection model.  the free-form queries. In this section, we evaluate our method on the retrieval task. Following the same procedure suggested by Johnson et al. <ref type="bibr" target="#b15">[16]</ref> but with our relational captioning dataset, we randomly choose 1,000 images from the test set, and from these chosen images, we collect 100 query sentences by sampling four random captions from 25 randomly chosen images. The task is to retrieve the correct image for each query by matching it with the generated captions. Our relational captioning based retrieval is done as follows. For every test image, we generate 100 region proposals from the RPN followed by NMS. To measure the degree of association, i.e., matching score, between a query and a region pair in the image, we compute the probability that the query text may occur from the region pair by multiplying the probability of words over recursive steps. Among all the scores of the region pairs from the image, we take the maximum matching score value as a representative score of matching between the query text and the image. The retrieved images are sorted according to these computed matching scores.</p><p>We compare the retrieval performance with several baselines in <ref type="table">Table 4</ref>. We measure recall at top K, R@K, which is the success ratio across all the queries that, by each given query, its ground-truth image is retrieved within top K ranks. We report K ∈ {1, 5, 10} cases. We also report the median rank of the correctly retrieved images across all 1000 test images. We follow the same procedure by Johnson et al. of running through random test sets 3 times to report the average results. We add an additional retrieval result with a more competitive dense captioning model, TLSTM <ref type="bibr">[</ref>  performance against the baselines. This is meaningful because a region pair based method deals with a more difficult input form than that of the single region based approaches. Moreover, MTTSNet+REM consistently shows better retrieval performance compared to MTTSNet. <ref type="figure" target="#fig_12">Figure 8</ref> shows the qualitative results on the sentence based image and region-pair retrieval. Given a sentence query, we show the retrieved images and their region pairs with the maximum matching score. Image retrieval based on our approach has a distinct advantage in that it retrieves images containing similar contextual relationships despite significant visual differences. More specifically, in the 3rd row of <ref type="figure" target="#fig_12">Fig. 8</ref>, our method can retrieve images with an abstract contextual relationship of "White sign near paved road." The retrieved images are visually diverse but share the same contextual information. Also, the natural language based retrieval from our framework is distinctive compared to traditional relationship detection methods (classification) which cannot handle natural language queries with variable length due to their fixed form input (i.e. subj-pred-obj). For example, in the 1st row, given a query that specifies the color "red," our model is able to retrieve images of a plane with red wings which VRD models are not capable of.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison with VRD Model</head><p>In order to demonstrate the flexibility of our model's output, i.e., natural language based sentences, we qualitatively compare our model with one of the benchmark models of visual relationship detection (VRD) task. We test the VRD benchmark model <ref type="bibr" target="#b31">[31]</ref> and our MTTSNet (and with +REM). The comparison is shown in <ref type="figure">Fig. 9</ref>. While the output of the VRD model is limited to the subj-pred-obj triplet with a smaller number of classes in a closed set, the output of our model has more flexibility and can contain more contextual information by virtue of being free form. For example, given the same object "person," our model is able to distinguish the fine-grained category, i.e., man and woman. In addition, our model can provide rich information about the object (e.g., smiling, gray) by virtue of leveraging attribute information of our relational captioning data. Thus, our relational captioning framework enables higher level interpretation of the objects compared to the relationship detection framework.</p><p>Since the output of the VRD task has a relatively simple form (i.e., subj-pred-obj triplet) compared to that of our captioning framework (caption with free-form and variable length), a VRD model is easier to train given the same relationship detection dataset. Thus, a direct comparison with a VRD model on the VRD dataset <ref type="bibr" target="#b31">[31]</ref> is unfair for our method. Despite this, we perform quantitative comparisons with VRD models by restricting the output vocabulary of our model such that the words appeared in the VRD dataset without attributes are only used. We use the VRD dataset that contains in total 5000 images with 4000/1000 splits for train/test sets respectively. Similar to the construction process of our relational captioning dataset, we tokenize the form of triplet expression, i.e., subj-pred-obj, to form natural language expressions, and for each word, we assign the POS class from the triplet association. By tokenizing, we obtain 160 vocabularies for the VRD dataset.</p><p>We evaluate on this regime in <ref type="table" target="#tab_10">Tables 5 and 6</ref> with the relational captioning metrics and VRD metrics, respectively. Firstly, <ref type="table" target="#tab_10">Table 5</ref> shows the comparisons with VRD models <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b58">[58]</ref> on the VRD dataset along with the ablation study. Overall, the ablation study shows similar trends to that of using our relational captioning dataset (c.f ., <ref type="table" target="#tab_3">Table 2</ref>). Our TSNet and MTTSNet (both with and without +REM) show top performance among the relational captioning models, of which difference is with and without POS prediction (w/MTL), respectively. This suggests that, even on the VRD dataset, the triplet-stream component is still a strong baseline  over others. Moreover, interestingly, while the POS classification appears to be an easy and basic task, adding the POS classification in the form of multi-task learning consistently helps the caption generation performance by a noticeable margin in our context, as shown in <ref type="table" target="#tab_3">Tables 2 and 5</ref>.</p><p>In the last row, we show the performance of the VRD models by Lu et al. <ref type="bibr" target="#b31">[31]</ref> and Yang et al. <ref type="bibr" target="#b58">[58]</ref> with the relational captioning metrics. Note that these VRD models are designed specifically for triplet classification on the VRD dataset. Thus, in terms of mAP, it has an advantage compared to the results of the other relational captioning baselines. Nonetheless, compared to the VRD model, our relational captioners (especially our MTTSNet+REM) show favorable performance on Img-Lv Recall and METEOR with a notable margin. This suggests that the proposed relational captioning framework is advantageous in generating diverse and semantically natural expressions. On the other hand, VRD models are disadvantageous in these aspects because they use a closed vocabulary set and predict object classes individually without considering the context. <ref type="table" target="#tab_12">Table 6</ref> shows the comparison between our MTTSNet (both with and without +REM) and other VRD models measured on the VRD metrics. Due to the difference of our output type to that of VRD, we use METEOR score thresholds proposed by <ref type="bibr" target="#b15">[16]</ref> as the matching criteria between model outputs and ground truth labels. Among the three VRD tasks (predicate classification, phrase detection and relationship detection) defined in <ref type="bibr" target="#b31">[31]</ref>, we do not</p><p>The man has a white Frisbee.</p><p>The bus has a black wheel. The man wearing blue shirt.</p><p>The man holding white racket. The man wearing black helmet. The light on blue water. <ref type="figure" target="#fig_0">Fig. 11</ref>. Failure cases of our model. The reasons for failure cases are often due to visual ambiguity and illumination. subj-pred-obj are color-coded by red, green, and blue colors according to the output of the POS predictor, respectively.</p><p>measure predicate classification because a simple classification is out of scope for our model, but context understanding. As shown in the table, our model shows favorable or comparable performance to the VRD models despite the fact that they are specifically designed for the VRD task. This is worth noting in that, as opposed to VRD, our output label space is more complex than that of VRD due to variable caption length and a much larger number of vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Additional Analysis</head><p>Vocabulary statistics. In addition, we measure the vocabulary statistics and compare those of the frameworks in <ref type="table" target="#tab_14">Table 7</ref>. The types of statistics measured are: 1) an average number of unique words that have been used to describe an image, and 2) an average number of words to describe each box. Specifically, we count the number of unique words in all the predicted sentences and present the average number per image or box. Thus, the metric is proportional to the amount of information we can obtain given an image or a fixed number of boxes. These statistics increase in the order of Image Cap., Scene Graph, Dense Cap., and Relational Cap (both with and without +REM). In conclusion, the proposed relational captioning is favorable in diversity and amount of information (especially when the REM module is added), compared to both of the traditional object-centric scene understanding frameworks, i.e., Dense Cap. and Scene Graph. Importance transition along the triple-LSTMs. Since we have the three state LSTMs to predict a single word, it might be questionable whether each LSTM learns their own semantic roles properly. To see the behavior of each LSTMs, we visualize the weight transition from each LSTM for each time step. For this, given a set of features fed to the triple-stream LSTMs, we compute the L2 norm of the LSTM hidden state vector for each time step as a measure of importance value. These values from the three LSTMs are normalized across time through mean value subtraction. These values can be regarded as information or importance quantities. <ref type="figure" target="#fig_0">Figure 10</ref> shows the transitions of the representative values across time. As the POS phase changes through subjectpredicate-object, the weight of the subject LSTM consistently decreases while that of the object LSTM increases. The predicate LSTM has a relatively consistent intensity between subject and object LSTMs as the POS changes. Thus, it appears that LSTMs plausibly disentangle their own roles according to POS. Discussion of the failure cases. <ref type="figure" target="#fig_0">Figure 11</ref> shows failure cases of our relational captioning. The captions generated from our method can be inaccurate for several reasons. One of the important factors is visual ambiguity. Ambiguity may come from visually similar but different objects (first column) or by geometric ambiguity (second column). Lastly, due to illumination, the model may describe the object with a different color (e.g., "blue") (third column). Each of cases requires challenging capabilities, such as geometric reasoning, high resolution spatial representation learning, illumination invariance, etc., which are all fundamental computer vision challenges. we postulate that these problems may be resolved by improving visual feature representation; we leave these failure cases as a future direction. Despite this, note that the predicted POS is still correctly aligned with the words in the generated captions. the relational region pairs with natural language. To this end, we propose the MTTSNet, which facilitates POS aware relational captioning. In several sibling-tasks, we empirically demonstrate the effectiveness of our framework over scene graph generation and the traditional captioning frameworks. As a way to represent imagery, the relational captioning can provide dense, diverse, abundant, high-level and interpretable representations in a caption form. In this regard, our new framework would open new interesting applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX -ATTRIBUTE ENRICHMENT</head><p>As described in Sec. 4.1, we construct the relational captioning dataset based on the VG relationship dataset, but the dataset lacks attribute information in the captions. To compensate the lack of attributes, we leverage VG attribute dataset <ref type="bibr" target="#b22">[23]</ref>. The configuration of the VG attribute dataset is depicted in <ref type="figure" target="#fig_0">Fig 12.</ref> In the dataset, each object bounding box in an image is associated with "object name" and "attributes" of the object. Note that each object can have multiple attributes at the same time. Since the VG relationship dataset and the attribute dataset share the same image set, while the ground-truth bounding boxes are not shared, to associate the attribute with our captions, we conduct the process to find corresponding bounding boxes between datasets.</p><p>We simply find the attribute that matches the subject/object of the relationship label and assign it to the subject/object caption label. In particular, if an attribute label describes the same subject/object for a relationship label while an associated bounding box overlaps enough, the label is considered to be matched to the subject/object in the relationship label.</p><p>The specific procedure to decide association are as follows:</p><p>1) The category words of the subject / object in the relationship label and the object names of the attribute label must match, and the boxes should sufficiently overlap (higher IOU than 0.7), 2) Among the several boxes satisfying this condition, the box with the highest IOU is selected. 3) In the case that a single box is associated with multiple attribute labels, we check the part-of-speech (POS) of candidate attribute labels using the NLTK POS tagger <ref type="bibr" target="#b30">[30]</ref>. The words classified as (NN, VBN, VBG, VBD, JJ) are regarded as appropriate candidates for natural attributes. We filter out the other POS categories. 4) Among the attribute candidates, the words in the original relationship triplet (i.e. subj-pred-obj) are excluded from the candidates to prevent redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>If there are still more than one candidate attributes satisfying all these conditions, we select a random one among the final candidates. 6) If a subject does not have any matched attribute, the article "the" is added.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Difference of our proposed relational captioning from existing image understanding frameworks. Compared to traditional frameworks, our work is advantageous in both interaction understanding and highlevel interpretation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b57">[57]</ref>.Yang et  al.  demonstrate that the global context of an image as a sideinformation can improve the captioning performance. Compared to the global context of Yang et al., our union region has more localized information that incorporate both subject and object. In addition, to provide relative spatial information, we append geometric features for the subject and object box pair, i.e.(b s , b o ), to the union feature. Given two bounding boxes b s =(x s , y s , w s , h s ) and b o =(x o , y o , w o , h o ), we use the following geometric feature r similar to that of Peyre et al. [37] as r = xo−xs √ wshs , yo−ys √ wshs , woho wshs , ws hs , wo ho , bs bo bs bo ∈ R 6 , (1) where b s b o and b s b o denotes the intersection and union of the two boxes respectively. The geometric feature r is encoded into a 64-dimensional geometric vector by passing through an additional FC layer. By concatenating the 64-dimensional geometric vector with the union feature, the shape of this feature is D + 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Architecture of the relational embedding module (REM). denotes the matrix multiplication, and the element-wise sum. The softmax operation is applied row-wise. The blue boxes with the FC label denote FC layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>■</head><label></label><figDesc>Boy in playing baseball. ■ A baseball player wearing a helmet. ■ Green grass on the field ■ Grass on the ground Relational Captioner (Ours): ■→■ The boy wearing hat. ■→■ Green grass on a ground. ■→■ Standing man on a ground. ■→■ The Head of a man. ■→■ The man wearing gray shirt. ■→■ The boy on a large field. ■→■ The grass on ground.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>⋮</head><label></label><figDesc>Image Captioner:■ A man flying through the air.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>■→■ White clouds in blue sky. ■→■ The man on white snow. ■→■ The man wearing black pants. ■→■ White snow on top of pole. ■→■ The man on white surfboard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>⋮</head><label></label><figDesc>Image Captioner:■ A baseball player swinging a bat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>■→■ The head on man. ■→■ Standing man on ground. ■→■ The bat of a baseball player. ■→■ The man wearing black hat. ■→■ Large shadow on ground. ■→■ The people on a baseball court.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>⋮</head><label></label><figDesc>Image Captioner:■ A clock on the side of a road.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>8 .Fig. 6 .</head><label>86</label><figDesc>The man wearing black helmet. 5-6. Sitting woman behind the stand. 1-2. Baseball player wearing helmet 1-4. The man wearing white pants. Scene Graph 7-8. man-wearing-helmet 5-6. man-behind-stand 1-2. man-wearing-helmet 1-4. man-wearing-pant (d) Results of generating "caption graph" from our relational captioniner. In order to compare the diversity of the outputs, we also show the result of the scene graph generator, Neural Motifs<ref type="bibr" target="#b63">[63]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Examples of different captions predicted from relational captioning by (a) changing objects, (b) changing subjects, and (c) switching the subject and object. Our model shows different predictions from different subject and object pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Sentence based image and region-pair retrieval results on Visual Genome test images. The retrieved results are shown in the ranked order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Smiling woman riding white bike.&lt;laptop&gt;-&lt;on&gt;-&lt;table&gt;Grey laptop on brown desk.&lt;wheel&gt;-&lt;on&gt;-&lt;motorcycle&gt;Black wheel on blue motorcycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 .</head><label>10</label><figDesc>Visualization of POS importance transition. Y-axis represents respective representative hidden values of Subject-Predicate-Object LSTMs, and X-axis represents words of each caption in order. subj-pred-obj are color-coded by red, green, and blue colors according to the output of the POS predictor, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 12 .</head><label>12</label><figDesc>A sample of the VG attributes dataset. Each bounding box is labeled with an object name and attributes (Attribute labels for a bounding box can be multiple).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Comparison of model configurations. '|' and '+' indicate separation and concatenation of input respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Ablation study for the relational dense captioning task on relational captioning dataset. The second and third row sections (2-7 and 8-12th rows) show the comparison of the baselines with and without POS classification (w/MTL). In the last row, we show the performance of the state-of-the-art scene graph generator, Neural Motifs<ref type="bibr" target="#b63">[63]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>). Also, Subj+Obj+Coord. uses the geometric feature instead of the region code of the union. Moreover, we evaluate the baselines, Subj+Obj+{Union,Coord} again by adding POS classification (MTL loss).</figDesc><table><row><cell></cell><cell cols="4">Recall METEOR #Caption Caption/Box</cell></row><row><cell>Image Cap. (Show&amp;Tell) [49]</cell><cell>23.55</cell><cell>8.66</cell><cell>1</cell><cell>N/A</cell></row><row><cell>Image Cap. (Show&amp;Tell) [49]  †</cell><cell>23.81</cell><cell>9.46</cell><cell>10</cell><cell>N/A</cell></row><row><cell>Image Cap. (SCST) [41]</cell><cell>24.04</cell><cell>14.00</cell><cell>1</cell><cell>N/A</cell></row><row><cell>Image Cap. (SCST) [41]  †</cell><cell>24.17</cell><cell>13.87</cell><cell>10</cell><cell>N/A</cell></row><row><cell>Image Cap. (RFNet) [14]</cell><cell>24.91</cell><cell>17.78</cell><cell>1</cell><cell>N/A</cell></row><row><cell>Image Cap. (RFNet) [14]  †</cell><cell>25.26</cell><cell>17.83</cell><cell>10</cell><cell>N/A</cell></row><row><cell>Dense Cap. (DenseCap) [16]</cell><cell>42.63</cell><cell>19.57</cell><cell>9.16</cell><cell>1</cell></row><row><cell>Dense Cap. (TLSTM) [57]</cell><cell>43.15</cell><cell>20.48</cell><cell>9.24</cell><cell>1</cell></row><row><cell>Relational Cap. (Union)</cell><cell>38.88</cell><cell>18.22</cell><cell>85.84</cell><cell>9.18</cell></row><row><cell>Relational Cap. (MTTSNet)</cell><cell>46.78</cell><cell>21.87</cell><cell>89.32</cell><cell>9.36</cell></row><row><cell>Relational Cap. (MTTSNet+REM)</cell><cell>56.52</cell><cell>22.03</cell><cell>80.95</cell><cell>9.24</cell></row><row><cell>Relational Cap. (Union) (GT )</cell><cell>41.64</cell><cell>18.90</cell><cell></cell><cell></cell></row><row><cell>Relational Cap. (MTTSNet) (GT )</cell><cell>48.50</cell><cell>21.63</cell><cell>83.44</cell><cell>9.30</cell></row><row><cell cols="2">Relational Cap. (MTTSNet+REM) (GT ) 56.62</cell><cell>22.50</cell><cell></cell><cell></cell></row></table><note>• TSNet denotes the proposed triple-stream LSTM model with- out a branch for the POS classifier. Each stream takes the region codes of (subject, object, union+coord.) separately. MTTSNet (i.e., TSNet+POS) denotes the multi-task triple-stream network with POS classifier, and MTTSNet+REM denotes the model combined with the REM. Evaluation metrics. Motivated by the evaluation metric sug- gested for the dense captioning task by Johnson et al. [16], we</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Comparisons of the holistic level image captioning. We compare the results of the relational captioners with those of three image captioners</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The roof on yellow train. 5-2. Black wheel on a yellow train. 7-2. The window on a train. 9-2. Off light on yellow train. 2-4. Yellow train on old track. Old man wearing blue hat. 7-3. Red pants on young man. 3-4. Standing boy wearing red hat. 1-2. The man wearing purple hat.</figDesc><table><row><cell>Relational Captioning 1-2. Scene Graph 1-2. building-on-train 5-2. wheel-on-train 7-2. window-on-train 9-2. light-on-train 2-4. train-on-track</cell><cell>Relational Captioning 5-6. Scene Graph 5-6. man-wearing-hat 7-3. pant-on-man 3-4. man-wearing-hat 1-2. man-wearing-hat</cell></row><row><cell>(a)</cell><cell>(b)</cell></row><row><cell>Relational Captioning</cell><cell></cell></row><row><cell>3-4 Green leaf on a tree.</cell><cell></cell></row><row><cell>1-2 White cap on standing man.</cell><cell></cell></row><row><cell>2-6 The man wearing blue pants.</cell><cell></cell></row><row><cell>2-8 Standing man wearing black shirt.</cell><cell></cell></row><row><cell>2-10 The man wearing white hat.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5</head><label>5</label><figDesc>Ablation study on the relational dense captioning task with VRD dataset. Our TSNet and MTTSNet (both with and without +REM) show top performance among the relational captioning models. In addition, MTTSNet (both with and without +REM) shows favorable performance against the VRD models<ref type="bibr" target="#b31">[31]</ref>,<ref type="bibr" target="#b58">[58]</ref> with a noticeable margin.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 6</head><label>6</label><figDesc>Comparison of our MTTSNet with VRD models on the VRD metrics on VRD dataset. Despite the disadvantages for predicting complex captions compared to simple triplets, our MTTSNet and MTTSNet+REM show favorable or comparable performance against the VRD models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 7</head><label>7</label><figDesc>Diversity comparison between image captioning, scene graph generation, dense captioning, and relational captioning frameworks. We measure the number of different words per image (words/img) and the number of words per bounding box (words/box).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We introduce relational captioning, a new notion which requires a model to localize regions of an image and describe each of </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Groupcap: Group-based image captioning with structured relevance and diversity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards diverse and natural image descriptions via a conditional gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contrastive learning for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The workshop on statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scene graph generation with external knowledge and image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent fusion network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tae-Hyun Oh, and In So Kweon. Dense relational captioning: Triple-stream networks for relationship-based captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tae-Hyun Oh, and In So Kweon. Image captioning with very scarce supervised data: Adversarial semi-supervised learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Youngjin Yoon, and In So Kweon. Disjoint multi-task learning between heterogeneous humancentric tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hierarchical approach for generating descriptive image paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The organization of visually mediated actions in a subject without eye movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><forename type="middle">M</forename><surname>Michael F Land</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><forename type="middle">D</forename><surname>Furneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gilchrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocase</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="87" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">VIP-CNN: A visual phrase reasoning convolutional neural network for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Factorizable net: An efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep variation-structured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nltk: The natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics</title>
		<meeting>the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weaklysupervised learning of visual relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive linguistic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attentive relational networks for mapping images to scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengshi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Amin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Speaking the same language: Matching machine to human captions by adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Captioning images with diverse objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Exploring context and visual pattern of relationship for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Donghyeon Cho, and In So Kweon. Linknet: Relational embedding for scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dense captioning with joint inference and visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Shuffle-then-assemble: learning object-agnostic visual relationship features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Zoom-net: Mining deep feature interactions for visual relationship recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Visual relationship detection with internal and external linguistic knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Relationship proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Towards context-aware interaction recognition for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
