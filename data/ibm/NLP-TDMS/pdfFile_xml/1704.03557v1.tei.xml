<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cutting the Error by Half: Investigation of Very Deep CNN and Advanced Training Strategies for Document Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Zeshan</forename><surname>Afzal</surname></persName>
							<email>afzal@iupr.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MindGarage</orgName>
								<orgName type="institution" key="instit2">University of Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Insiders Technologies GmbH</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">§</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kölsch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MindGarage</orgName>
								<orgName type="institution" key="instit2">University of Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheraz</forename><surname>Ahmed</surname></persName>
							<email>sheraz.ahmed@dfki.de</email>
							<affiliation key="aff1">
								<orgName type="department">DFKI</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
							<email>marcus.liwicki@unifr.ch</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MindGarage</orgName>
								<orgName type="institution" key="instit2">University of Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Fribourg</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cutting the Error by Half: Investigation of Very Deep CNN and Advanced Training Strategies for Document Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Document Image Classification</term>
					<term>Deep CNN</term>
					<term>Convolutional Neural Network</term>
					<term>Transfer Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an exhaustive investigation of recent Deep Learning architectures, algorithms, and strategies for the task of document image classification to finally reduce the error by more than half. Existing approaches, such as the DeepDoc-Classifier, apply standard Convolutional Network architectures with transfer learning from the object recognition domain. The contribution of the paper is threefold: First, it investigates recently introduced very deep neural network architectures (GoogLeNet, VGG, ResNet) using transfer learning (from real images). Second, it proposes transfer learning from a huge set of document images, i.e. 400, 000 documents. Third, it analyzes the impact of the amount of training data (document images) and other parameters to the classification abilities. We use two datasets, the Tobacco-3482 and the large-scale RVL-CDIP dataset. We achieve an accuracy of 91.13 % for the Tobacco-3482 dataset while earlier approaches reach only 77.6 %. Thus, a relative error reduction of more than 60 % is achieved. For the large dataset RVL-CDIP, an accuracy of 90.97 % is achieved, corresponding to a relative error reduction of 11.5 %.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>An important step of the Document Image Processing Pipeline (DIPP) is the classification of the documents. An early classification of documents helps to process the subsequent processes in DIPP such as information extraction, text recognition etc <ref type="bibr" target="#b0">[1]</ref>. Due to its fundamental importance, this area has been explored extensively. Earlier methods that have been dealing with document classification focused mainly on either exploiting the structural similarity constraints <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> or extracting features from the documents that may be able to help for document classification <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Some of the methods combine both of the features <ref type="bibr" target="#b6">[7]</ref>.</p><p>Deep Learning has been used for many document analysis tasks such as binarization <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, layout analysis <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, Optical Character Recognition (OCR) <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b16">[17]</ref> etc. Recently, deep learning methods have also been exploited for document image classification <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. Deep learning methods do not * These authors contributed equally to this work require any manual feature extraction. However, the existing state-of-the-art methods do transfer learning. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the sample images both real and document images from the ImageNet <ref type="bibr" target="#b20">[21]</ref> and Tobacco-3482 datasets respectively. While the images are visually very different, the visual queues are generic and thus, transfer learning helps to boost the performance of the document image classification <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>. The networks that are not using transfer learning (i.e., they are randomly initialized) are under-performing <ref type="bibr" target="#b18">[19]</ref>. The performance evaluation for the deep neural networks was only performed using Tobacco-3482 images. Another dataset introduced by Harley et al. <ref type="bibr" target="#b19">[20]</ref> consist of 400, 0000 images that are divided into 16 classes. Representative images from each of the classes are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Although now we have a large dataset available for training document images, there is no study that shows the performance of very deep networks for large datasets of document images. Furthermore, the potential of pretraining using document images only is not explored either. Therefore, in this work, we evaluate deep neural networks both on the small and big dataset. An exhaustive evaluation of the deep neural networks is performed to show the impact of the amount of data for training in combination with very deep Convolutional Neural Network (CNN). Furthermore, an evaluation is performed for transfer learning when the weights are adapted both from natural images and document images. The proposed approach shows a significant improvement over the current state-of-the-art by reducing the error by more than half.</p><p>II. RELATED WORK Over the years, different methods have been proposed for document image classification. The overall classification methods can be divided into three distinct categories. The first category exploits structure and layout similarities, while the second focuses on developing local and global features that could be used for document classification. The third category is based on deep CNNs that extract the features automatically for document classification. This section provides a summary of the important related work regarding the above mentioned three categories.</p><p>Dengel and Dubiel <ref type="bibr" target="#b0">[1]</ref> used layout structure printed documents. They used top-down induction in decision trees to convert printed documents into a complementary logical structure. Bagdanov and Worring <ref type="bibr" target="#b21">[22]</ref> classify machine-printed documents by using the Attributed Relational Graphs (ARGs). Byun and Lee <ref type="bibr" target="#b1">[2]</ref> used parts of the documents for the recognition. They reasoned that processing complete documents is time-consuming. The document classification was performed on parts of the documents using DP algorithm. Their approach was fast but the applicability is limited to forms. Shin and Doermann <ref type="bibr" target="#b2">[3]</ref> proposed an approach that used layout structural similarity for full or partial image matching for retrieval. Kevyn and Nickolov <ref type="bibr" target="#b6">[7]</ref> used both the layout and the text features for matching the documents for retrieval.</p><p>Jayant et al. <ref type="bibr" target="#b3">[4]</ref> propose a method that relies on the patch code words derived from the document images. The code book is learned independently of the class labels of the documents. In the first step, the images are recursively partitioned both in horizontal and vertical direction for modeling spatial relationships. Subsequently, a histogram for each partition is calculated that is used for the classification. Following the same idea of developing the code book, another work presented by Jayant et al. <ref type="bibr" target="#b5">[6]</ref> build a codebook of SURF descriptors extracted from training images. Then, histograms of codewords are created similar to <ref type="bibr" target="#b3">[4]</ref>. A Random Forest classifier is used for classification. The applicability of the approach is shown in the presence of limited data. Chen et al. <ref type="bibr" target="#b4">[5]</ref> propose a method based on low-level image features to classify documents. The approach is limited to structured documents. An important point is that one could obtain the registration of two images by matching the feature points. Joutel et al. <ref type="bibr" target="#b22">[23]</ref> proposed a method that used curvelet transformation for indexing and querying the documents at different image scales. Their method is designed particularly for large databases of handwritten manuscripts. Kochi and Saitoh <ref type="bibr" target="#b23">[24]</ref> used textual descriptions of document images for information extraction from documents. The method is limited to semi-structured documents and assumes a pre-defined knowledge is available for the document classes. Reddy and Govindaraju <ref type="bibr" target="#b24">[25]</ref> used binary images for the classification of the documents. They use pixel information and calculate pixel densities. They used kmeans clustering supported by adaptive boosting. The method is evaluated on the benchmark NIST scanned special tax form databases 2 and 6.</p><p>The pioneering work that performed document classification using CNNs used a rather shallow network for classification <ref type="bibr" target="#b18">[19]</ref>. Nevertheless, the proposed approach outperformed structural similarity based methods and shows the potential of automatic feature learning for document classification using CNNs. The reason may be that deep networks require a lot of data for training and at that time the standard challenging dataset consisted of only 3, 482 images. Afzal et. al. <ref type="bibr">[</ref> that it is possible to use transfer learning and the features that are learned from general (daily life) images can be used for the classification of document images <ref type="bibr" target="#b17">[18]</ref>. They achieved a significant improvement over CNN based methods that were the state-of-the-art at that time. Another notable contribution by Harley et. al. <ref type="bibr" target="#b19">[20]</ref> was that they introduced a dataset consisting of 400, 000 documents divided into 16 classes. This allowed for the evaluation of deep neural networks using a significant amount of data.</p><p>The state-of-the-art in deep CNNs has advanced significantly in recent years and there has been no comprehensive study regarding the impact of deep architectures for document classification. Moreover, there is no study that explores transfer learning from document images and also there is no report of the impact of the amount of training images. The presented work takes into account these issues and performs a comprehensive set of experiments to fill the gaps that exist. Eventually, this study leads to an approach that can reduce the error by more than half and therefore provides another leap forward in the domain of document image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP CONVOLUTIONAL NEURAL NETWORKS</head><p>This section briefly presents the deep CNN architectures used in this work. Furthermore, the image preprocessing and training details are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architectures</head><p>The deep CNN architectures used in this paper are well known in the domain of object recognition but are not used frequently for document image classification. The networks are of very different nature (cf. <ref type="figure">Fig. 3</ref>).</p><p>1) AlexNet: AlexNet <ref type="bibr" target="#b25">[26]</ref> is the eight-layer CNN that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 <ref type="bibr" target="#b20">[21]</ref> by a large margin. It employs five convolutional layers with optional pooling and local response normalization. These are then followed by three fully-connected layers and a softmax classifier (cf. <ref type="figure">Fig. 3a)</ref>.</p><p>2) VGG-16: VGG-16, as the name suggests is a 16-layer CNN <ref type="bibr" target="#b26">[27]</ref>. Unlike AlexNet, it uses only convolutional filters of size 3 × 3. Just like AlexNet, it has a straightforward architecture, but with 13 convolutional layers and 3 fully connected layers (cf. <ref type="figure">Fig. 3b</ref>) it is quite a bit deeper and has a repetitive pattern of layers. This architecture has won the localization category of the ILSVRC 2014.</p><p>3) GoogLeNet: GoogLeNet, just like VGG-16, won a category of the ILSVRC 2014, namely the classification category <ref type="bibr" target="#b27">[28]</ref>. The architecture of this network, however, is a bit more sophisticated (cf. <ref type="figure">Fig. 3c</ref>). Unlike AlexNet and VGG-16, it is not just a stack of Convolution layers and Pooling layers, but rather a stack of building blocks, which themselves consist of Convolution and Pooling layers. It is therefore a Networkin-Network approach <ref type="bibr" target="#b28">[29]</ref>. Due to its high depth, the network employs three softmax classifiers during training, to enable efficient backpropagation of the error. At test time, the two auxiliary classifiers are discarded. 4) Resnet-50: ResNets are a family of very deep CNN architectures which make use of residual connections <ref type="bibr" target="#b29">[30]</ref> to overcome the challenge of efficient error backpropagation. ResNet-50 is a variant of the network with 50 layers, which, as in GoogLeNet, are grouped in building blocks (cf. <ref type="figure">Fig. 3d</ref>). An even deeper variant with 152 layers won the ILSVRC classification task in 2015. Interestingly, despite its increased depth, the network has fewer parameters to fit than VGG-16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preprocessing</head><p>As the networks used in this paper require images of a fixed size as input, we first downscale all images to the expected input size of the networks. For AlexNet, the images are resized to 227 × 227 pixels, for the other networks the images are resized to 224×224 pixels. Typically, when training CNNs, the training data is augmented by resizing the images to a larger size, e.g. 256 × 256 pixels and then cropping random patches of these images in the size of the network input. This approach has shown to be effective for real-world image classification <ref type="bibr" target="#b25">[26]</ref>. In real-world images, the objects are typically close to the center of the image and therefore always contained in the random crops. However, the most discriminative parts of document images are not always close to the center of the image but reside in the outer regions, e.g. the head of a letter. Therefore, we do not enlarge our training dataset in this way but train solely with images containing the entire document.</p><p>After resizing the images, we compute the mean pixel values of the training images and subtract them from all images to center the training data.</p><p>As a last preprocessing step, we convert the grayscale images to RGB images by simply copying the pixel values of the single-channel images to three channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Details</head><p>We train all networks using stochastic gradient descent with a momentum of 0.9 and a learning rate that is updated every iteration to</p><formula xml:id="formula_0">lr = initial_lr * 1 − iter max_iter 0.5 (1)</formula><p>The initial learning rate is set to a value between 0.01 and 0.0001 depending on the network architecture, the training dataset and the weight initialization.</p><p>The number of training epochs depends on the task and ranges between 40 and 80 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To evaluate the performance of the deep neural networks presented in section III, two datasets are used. First, we train a variety of networks on the Ryerson Vision Lab Complex Document Information Processing (RVL-CDIP) dataset <ref type="bibr" target="#b19">[20]</ref>. This dataset consists of 400, 000 labeled document images from 16 classes. The dataset is already split into a training dataset which contains 320, 000 images and a validation and a test dataset which each contain 40, 000 images.</p><p>Secondly, we use the Tobacco-3482 dataset <ref type="bibr" target="#b5">[6]</ref> to evaluate the performance of the deep CNNs and to investigate to which extent transfer learning from the first dataset is applicable. The Tobacco-3482 dataset contains 3, 482 images from ten document classes.</p><p>Both datasets are quite similar and there even exists some overlap. Therefore, at the transfer learning experiments, we pretrain the networks not on the full RVL-CDIP dataset, but only on the images that are not contained in the Tobacco-3482 dataset. Thus, the networks are pretrained on only 319, 784 training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>For the RVL-CDIP dataset, we just train the networks and report the top-1 accuracy achieved on the test set. Since the Tobacco-3482 dataset is so small, we use a slightly more sophisticated evaluation technique to get an expected accuracy and to avoid unrepresentative results due to random initialization or a specific dataset split. To come up with a robust estimate of how well the networks perform, we split the dataset such, that 10 to 100 images per class are used for training while the rest are for testing. The training dataset is again split with an 80/20 ratio, so that 20 % of the training data are used for validation. For each split size, we randomly create ten dataset partitions and report the median accuracy achieved by the networks. This is similar to the evaluation scheme that was also used by Kang et al. <ref type="bibr" target="#b18">[19]</ref> and Kumar et al. <ref type="bibr" target="#b5">[6]</ref> and allows for a fair comparison with their approaches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on Tobacco-3482</head><p>We have trained the four deep CNNs described in Section III on the two datasets with different weight initializations to investigate the benefits of transfer learning. As shown in <ref type="table" target="#tab_1">Table I</ref> and <ref type="figure" target="#fig_2">Fig. 4</ref> which correspond to the achieved performance on the Tobacco-3482 dataset, transfer learning does improve the classification performance significantly. When the networks are pretrained on a similar dataset, the accuracy achieved on the final dataset is higher than 90 % and already with as little training data as 10 samples per class, we could outperform the current state-of-the-art which achieves only 77.6 % <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis</head><p>We also compare the networks with ImageNet initialization against randomly initialized networks and find that even though the images are substantially different (cf. <ref type="figure" target="#fig_0">Fig. 1</ref>), it helps to use pretrained models. Fortunately, there are models which are pretrained on ImageNet available online for many architectures, including the four networks used in this work. So, in case there is no large document dataset available for pretraining, one can and should always resort to using an ImageNet pretrained model for finetuning. Depending on the amount of available training data, AlexNet and VGG-16 are the best choices when finetuning the networks from models that were pretrained on ImageNet (cf. <ref type="figure" target="#fig_2">Fig. 4</ref>). When pretrained on the RVL-CDIP dataset, GoogLeNet is significantly worse than the other networks, especially for a small amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on RVL-CDIP</head><p>On the large-scale RVL-CDIP dataset, all networks achieve very good results (cf <ref type="table" target="#tab_1">. Table II)</ref> with the VGG-16 performing best at an accuracy of 90.97 %. The current state-of-the-art on this dataset only achieves an accuracy of 89.8 %, thus we could decrease the relative error by more than 11 % by simply using a different network architecture. Note, that even though the training dataset is quite large, all of the networks still benefit from Imagenet pretraining.</p><p>On average, VGG-16 performs very well on all experiments performed in this work. As can be seen in <ref type="figure" target="#fig_3">Fig. 5</ref> which shows the confusion matrix of a trained VGG-16 network, even the classes that were pointed out to be hard by Afzal et al. <ref type="bibr" target="#b17">[18]</ref>, get significant performance boosts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>The outcome of the study brings insights both for the deep neural network architectures and the amount of required training data. The proposed approach of training on document images and then finetuning for a document based dataset improved the error by 60 %. We show that the random initialization performs worst and initialization based on document images performs best. Furthermore, on the large-scale RVL-CDIP dataset, VGG-16 outperforms the other networks. Finally, a relative error reduction of 11.5 % compared to the state-of-the-art is achieved. Future work may evaluate recurrent neural networks or a combination of convolutional and recurrent neural networks to improve the performance further. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Sample images from the ImageNet dataset (b) Sample images from the RVL-CDIP dataset Sample Images from Imagenet and RVL-CDIP datasets are shown in (a) and (b) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Sample images from the RVL-CDIP dataset. One image from each class is depicted. From left to right: Letter, Form, Email, Handwritten, Advertisement, Scientific report, Scientific publication, Specification, File folder, News article, Budget, Invoice, Presentation, Questionnaire, Resume, Memo</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Mean accuracy achieved on the Tobacco-3482 dataset. The dashed graphs represent the networks that are pretrained on the RVL-CDIP dataset, the solid lines represent the network with ImageNet pretraining and the dotted lines show the network accuracy when trained from scratch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Confusion Matrix by a VGG-16 network trained on the Tobacco-3482 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>18] and Harley et. al.<ref type="bibr" target="#b19">[20]</ref> provided a breakthrough when they showed</figDesc><table><row><cell>softmax</cell><cell></cell><cell>softmax softmax</cell><cell></cell><cell cols="2">Softmax Softmax</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dropout Dropout</cell><cell>FC FC</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>FC FC</cell><cell cols="2">Dropout Dropout</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FC</cell><cell>Dropout</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AveragePooling 7x7 AveragePooling 7x7</cell><cell cols="2">Softmax Softmax</cell></row><row><cell></cell><cell></cell><cell>Dropout Dropout</cell><cell>FC FC</cell><cell cols="2">Building Block Building Block</cell><cell>Dropout Dropout</cell><cell>FC FC</cell></row><row><cell>Dropout</cell><cell>FC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Building Block Building Block</cell><cell>FC FC</cell><cell></cell></row><row><cell></cell><cell></cell><cell>FC FC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MaxPooling 3x3 MaxPooling 3x3</cell><cell cols="2">Convolution 1x1 Convolution 1x1</cell><cell>Softmax Softmax</cell></row><row><cell>FC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Building Block Building Block</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Building Block Building Block</cell><cell cols="2">AveragePooling 5x5 AveragePooling 5x5</cell><cell>Dropout Dropout</cell><cell>FC FC</cell></row><row><cell cols="2">MaxPooling 3x3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell><cell></cell><cell>FC FC</cell></row><row><cell></cell><cell></cell><cell cols="2">Building Block Building Block</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell><cell></cell><cell>Convolution 1x1 Convolution 1x1</cell></row><row><cell cols="2">Convolution 3x3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MaxPooling 2x2 MaxPooling 2x2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Building Block Building Block</cell><cell>AveragePooling 5x5 AveragePooling 5x5</cell></row><row><cell cols="2">Convolution 3x3 Convolution 3x3</cell><cell cols="2">Convolution 3x3 Convolution 3x3 Convolution 3x3 Convolution 3x3</cell><cell>Building Block</cell><cell></cell><cell cols="2">Building Block Building Block MaxPooling 3x3 MaxPooling 3x3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell cols="2">Convolution 3x3 Convolution 3x3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MaxPooling 3x3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Concatenation Concatenation</cell></row><row><cell cols="2">Convolution 11x11 Normalization Convolution 5x5 Normalization MaxPooling 3x3 data (a) AlexNet</cell><cell cols="2">Convolution 3x3 MaxPooling 2x2 MaxPooling 2x2 data Convolution 3x3 Convolution 3x3 Convolution 3x3 Convolution 3x3 MaxPooling 2x2 MaxPooling 2x2 data Convolution 3x3 Convolution 3x3 Convolution 3x3 (b) VGG-16</cell><cell>Convolution 1x1 Convolution 1x1</cell><cell></cell><cell cols="3">Convolution 7x7 MaxPooling 3x3 Normalization MaxPooling 3x3 Convolution 1x1 Convolution 1x1 Convolution 3x3 Normalization Convolution 3x3 Convolution 1x1 data Convolution 5x5 Convolution 7x7 MaxPooling 3x3 Convolution 3x3 Convolution 5x5 Normalization MaxPooling 3x3 Convolution 1x1 Convolution 1x1 Convolution 3x3 Normalization Convolution 1x1 data (c) GoogLeNet</cell><cell>MaxPooling 3x3 Convolution 1x1 MaxPooling 3x3 Convolution 1x1</cell><cell>Building Block</cell><cell>Convolution 1x1 (optional) Building Block + Convolution 1x1 Convolution 3x3 Convolution 7x7 MaxPooling 3x3 data Convolution 1x1 Convolution 1x1 (optional) + Building Block Building Block Convolution 1x1 Convolution 3x3 Convolution 7x7 MaxPooling 3x3 data Convolution 1x1 Building Block (d) ResNet-50</cell><cell>B u il d in g B lo c k</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building Block Building Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MaxPooling 7x7 MaxPooling 7x7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC FC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>softmax softmax</cell></row></table><note>Fig. 3: Deep CNN architectures used in this work</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Performance of the networks on the Tobacco-3482 dataset with 100 training samples per class and different weight initializations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Document Pretraining</cell><cell>ImageNet Pretraining</cell><cell>No Pretraining</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>AlexNet</cell><cell></cell><cell>90.04 %</cell><cell>75.73 %</cell><cell>62.49 %</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>GoogLeNet</cell><cell></cell><cell>88.40 %</cell><cell>72.98 %</cell><cell>70.28 %</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>VGG-16</cell><cell></cell><cell>91.01 %</cell><cell>77.52 %</cell><cell>69.50 %</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Resnet-50</cell><cell></cell><cell>91.13 %</cell><cell>67.93 %</cell><cell>59.55 %</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.6 0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10 0.3 0.4</cell><cell>20</cell><cell>30</cell><cell>40 # Training Samples per Class 50 60 70 Document Pretraining ImageNet Pretraining No Pretraining</cell><cell>80</cell><cell>90 VGG-16 AlexNet ResNet-50 GoogLeNet</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Performance of the networks on the RVL-CDIP dataset with different weight initializations.</figDesc><table><row><cell></cell><cell>ImageNet Pretraining</cell><cell>No Pretraining</cell></row><row><cell>AlexNet</cell><cell>88.60 %</cell><cell>88.19 %</cell></row><row><cell>GoogLeNet</cell><cell>89.02 %</cell><cell>88.60 %</cell></row><row><cell>VGG-16</cell><cell>90.97 %</cell><cell>89.41 %</cell></row><row><cell>Resnet-50</cell><cell>90.40 %</cell><cell>89.24 %</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clustering and classification of document structure-a machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dubiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="587" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Form classification using dp matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Applied Computing</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Document image retrieval based on layout structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="606" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning document structure for retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1558" to="1561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structured document classification by matching local salient features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Naoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012-11" />
			<biblScope unit="page" from="653" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structural similarity for document image classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="119" to="126" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A clustering-based algorithm for automatic document separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nickolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Document image binarization using lstm: A sequence learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pastor-Pellicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Historical Document Imaging and Processing</title>
		<meeting>the 3rd International Workshop on Historical Document Imaging and Processing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Insights on the use of convolutional neural networks for document image binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pastor-Pellicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>España-Boquera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zamora-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Castro-Bleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Work-Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="115" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Complete system for text line extraction using convolutional neural networks and watershed transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pastor-Pellicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Castro-Bleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis Systems (DAS), 2016 12th IAPR Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pca-initialized deep neural networks applied to document image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00177</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="367" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Highperformance OCR for printed English and Fraktur using LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ul-Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Azawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2013 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="683" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scale and rotation invariant ocr for pashto cursive script using mdlstm network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2015 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1101" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluation of cursive and non-cursive scripts using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Naz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Razzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="603" to="613" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A generic method for automatic ground truth generation of camera-captured documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01189</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sequence learning approach for multiple script identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ul-Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2015 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1046" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepdocclassifier: Document classification with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Capobianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marinai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2015 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1111" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Document Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluation of deep convolutional nets for document image classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ufkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="991" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-grained document genre classification using first order random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="79" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Curvelets based queries for cbir application in handwriting collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Joutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Emptoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="649" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">User-defined template for identifying document type and extracting information from documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kochi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saitoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="1999-09" />
			<biblScope unit="page" from="127" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Form classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V U</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DRR&apos;08</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
