<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Guided Low-light Image Enhancement with a Large Scale Low-light Simulation Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Lv</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Feng</surname></persName>
							<email>lufeng@buaa.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Corresponding author)</roleName><forename type="first">F</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Applied Research Center (ARC)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent PCG</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Peng Cheng Laboratory</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention Guided Low-light Image Enhancement with a Large Scale Low-light Simulation Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Low-light image enhancement · Low-light simulation · Synthetic dataset · Attention guidance · Deep neural network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low-light image enhancement is challenging in that it needs to consider not only brightness recovery but also complex issues like color distortion and noise, which usually hide in the dark. Simply adjusting the brightness of a low-light image will inevitably amplify those artifacts. To address this difficult problem, this paper proposes a novel end-to-end attention-guided method based on multi-branch convolutional neural network. To this end, we first construct a synthetic dataset with carefully designed low-light simulation strategies. The dataset is much larger and more diverse than existing ones. With the new dataset for training, our method learns two attention maps to guide the brightness enhancement and denoising tasks respectively. The first attention map distinguishes underexposed regions from well lit regions, and the second attention map distinguishes noises from real textures. With their guidance, the proposed multi-branch decomposition-and-fusion enhancement network works in an input adaptive way. Moreover, a reinforcement-net further enhances color and contrast of the output image. Extensive experiments on multiple datasets demonstrate that our method can produce high fidelity enhancement results for lowlight images and outperforms the current state-of-theart methods by a large margin both quantitatively and visually.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Images captured in insufficiently illuminated environment usually contain undesired degradations, such as poor visibility, low contrast, unexpected noise, etc. Resolving these degradations and converting low-quality low-light images to normally exposed high-quality images require well developed low-light enhancement techniques. Such a technique has a wide range of applications. For example, it can be used in consumer photography to help the users capture appealing images in the low-light environment. It is also useful for a variety of intelligent systems, e.g., automated driving and video surveillance, to capture high-quality inputs under low-light conditions. Low-light image enhancement is still a challenging task, since it needs to manipulate color, contrast, brightness and noise simultaneously given the low quality input only. Although numbers of methods have been proposed for this task in recent years, there is still large room for improvement. <ref type="figure">Figure 1</ref> shows some limitations of existing methods, which follow typical assumptions of histogram equalization (HE) and Retinex theory <ref type="bibr" target="#b34">[44]</ref>. HE-based methods aim to increase the contrast by simply stretching the dynamic range of images, while Retinex-based methods recover the contrast by using the estimated illumination map. Mostly, they focus on restoring brightness and contrast and ignore the influences of noise. However, in reality, the noise is inevitable and non-negligible in the low-light images, especially after increasing brightness and contrast. arXiv:1908.00682v3 [eess.IV] 15 Mar 2020 RetinexNet LLNet Ours LIME Input <ref type="bibr" target="#b18">[28]</ref> [50] <ref type="bibr" target="#b67">[77]</ref> Fig. 1: Low-light enhancement example. Comparing with existing methods, our method can generate results with satisfactory visibility, natural color, and higher contrast.</p><p>To suppress the low-light image noise, some methods directly include a denoising process as a separate component in their enhancement pipeline. However, it is dilemma to make a simple cascade of the denoising and enhancement procedures. In particular, applying denoising before enhancement will result in blurring, while applying enhancement before denoising will cause noise amplification. Therefore, in this paper, we propose to model and solve the denoising and low-light enhancement problems simultaneously.</p><p>Specifically, this paper proposes an attention-guided enhancement solution that achieves denoising and enhancing simultaneously and effectively. We find that the severity of low brightness/contrast and high image noise show certain spatial distributions related to the underexposed areas. Therefore, the key is to handle the problem in a region-aware adaptive manner. To this end, we propose the under-exposed (ue) attention map to evaluate the degree of underexposure. It guides the method to pay more attention to the underexposed areas in low light enhancement. In addition, based on the ue-attention map, we derive the noise map to guide the denoising according to the joint distribution of exposure and noise intensity. Subsequently, we design a multi-branch CNN to simultaneously achieve low-light enhancement and denoising under the guidance of both maps. In the final step, we add a fully-convolutional network for improving the image contrast, exposure and color as the second enhancement.</p><p>The remaining difficulty lies in the lack of largescale paired low-light image dataset, making it challenging to train an effective network. To address this issue, we propose a low-light image simulation pipeline to synthesize realistic low-light images with well exposed ground truth images. Image contrast and color are also improved to provide good references for our image reenhancement step. Following the above ideas, we propose a large-scale low-light image dataset as an efficient benchmark for low-light enhancement researches.</p><p>Overall, our contributions are in three folds: 1) We propose a full pipeline for low-light image simulation with high fidelity, based on which we build a new largescale paired low-light image dataset to support low-light enhancement researches. 2) We propose an attentionguided enhancement method and the corresponding multibranch network architecture. Guided by the ue-attention map and noise map, the proposed method achieves lowlight enhancement and denoising simultaneously and effectively. 3) Comprehensive experiments have been conducted and the experiment results demonstrate that our method outperforms state-of-the-art methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image enhancement and denoising have been studied for a long time. In this section, we will briefly overview the most related methods.</p><p>Traditional enhancement methods. Traditional methods can be mainly divided into two categories. The first category is built upon the histogram equalization (HE) technique. The differences of different HE-based methods are using different additional priors and constraints. In particular, BPDHE <ref type="bibr" target="#b25">[35]</ref> tries to preserve image brightness dynamically; Arici et al. <ref type="bibr" target="#b3">[4]</ref> propose to analyze and penalize the unnatural visual effects for better visual quality; DHECI <ref type="bibr" target="#b44">[54]</ref> introduces and uses the differential gray-level histogram; CVC [9] uses the interpixel contextual information; LDR <ref type="bibr" target="#b36">[46]</ref> focuses on the layered difference representation of 2D histogram to try to enlarge the gray-level differences between adjacent pixels. These methods expand the dynamic range and focus on improving the contrast of the entire image instead of considering the illumination. They may cause the problem of over-and under-enhancement.</p><p>The other category is based on the Retinex theory <ref type="bibr" target="#b34">[44]</ref>, which assumes that an image is composed of reflection and illumination. Typical methods, e.g., MSR <ref type="bibr" target="#b30">[40]</ref> and SSR <ref type="bibr" target="#b31">[41]</ref>, try to recover and use the illumination map for low-light image enhancement. Recently, AMSR <ref type="bibr" target="#b37">[47]</ref> proposes a weighting strategy based on SSR. NPE <ref type="bibr" target="#b64">[74]</ref> balances the enhancement level and image naturalness to avoid over-enhancement. MF <ref type="bibr" target="#b12">[22]</ref> processes the illumination map in a multi-scale fashion to improve the local contrast and maintain naturalness. SRIE <ref type="bibr" target="#b13">[23]</ref> develops a weighted vibrational model for illumination map estimation. LIME <ref type="bibr" target="#b18">[28]</ref> develops a structure-aware smoothing model to estimate the illumination map. BIMEF <ref type="bibr" target="#b72">[82]</ref> proposes a dual-exposure fusion algorithm and Ying et al. <ref type="bibr" target="#b73">[83]</ref> use the camera response model for further enhancement. Mading et al. <ref type="bibr" target="#b38">[48]</ref> propose a robust Retinex model by considering the noise map for enhancing low-light images accompanied by intensive noise. However, the key to these Retinex-based methods is the estimation of the illumination map, which is hand-crafted and relied on careful parameters tuning. Besides, most of these Retinexbased methods do not consider noise removal and often amplify the noise.</p><p>Learning-based enhancement methods. Recently, deep learning has achieved great success in the field of low-level image processing <ref type="bibr" target="#b56">[66]</ref> and nighttime scenes modeling <ref type="bibr" target="#b47">[57]</ref> and understanding <ref type="bibr" target="#b8">[18,</ref><ref type="bibr" target="#b52">62]</ref>. Powerful tools such as end-to-end networks and GANs <ref type="bibr" target="#b15">[25]</ref> have been used in image enhancement. LLNet <ref type="bibr" target="#b40">[50]</ref> uses the multilayer perception auto-encoder for low-light image enhancement and denoising. HDRNet <ref type="bibr" target="#b14">[24]</ref> learns to make local, global, and content-dependent decisions to approximate the desired image transformation. LLCNN <ref type="bibr" target="#b62">[72]</ref> and <ref type="bibr" target="#b61">[71]</ref> rely on some traditional methods and are not end-to-end solutions to handle brightness/contrast enhancement and denoising simultaneously. MSRNet <ref type="bibr" target="#b58">[68]</ref> learns an end-to-end mapping between dark/bright images by using different Gaussian convolution kernels. MBLLEN <ref type="bibr" target="#b41">[51]</ref> uses a novel multi-branch low-light enhancement network architecture to learn the mapping from low-light images to normal light ones. Retinex-Net <ref type="bibr" target="#b67">[77]</ref> combines the Retinex theory with CNN to estimate the illumination map and enhance the lowlight images by adjusting the illumination map. Similarly, KinD <ref type="bibr" target="#b78">[88]</ref> designs a similar network by adding a Restoration-Net for noise removal. Ren et al. <ref type="bibr" target="#b49">[59]</ref> propose a novel hybrid network contains a content stream and a salient edge stream for low-light image enhancement. DeepUPE <ref type="bibr" target="#b63">[73]</ref> proposes a network for enhancing underexposed images by estimating an image-toillumination mapping. However, it does not consider the low-light noise.</p><p>Besides, DPED <ref type="bibr" target="#b26">[36,</ref><ref type="bibr" target="#b60">70]</ref> proposes an end-to-end approach using a composite perceptual error function for translating low-quality mobile phone photos into DSLR-quality photos. PPCN <ref type="bibr" target="#b24">[34]</ref> designs a compact network and combines teacher-student information transfer to reduce computational cost. WESPE <ref type="bibr" target="#b27">[37]</ref> proposes a weaklysupervised method to overcome the restrictions on requiring paired images. Also, <ref type="bibr">Chen et al. [13]</ref> propose an unpaired learning method for image enhancement by improving two-way GANs. As for extremely lowlight scenes, SID [10] develops a CNN-based pipeline to directly process raw sensor images. Lv et al. <ref type="bibr" target="#b42">[52]</ref> propose an enhancement solution by separating the visible and near-infrared signal from a single image and fusing them for high-quality images. Most of these learningbased methods do not explicitly contain the denoising process, and some even rely on traditional denoising methods. However, our approach considers the effects of noise and uses two attention maps to guide the enhancing and denoising process. So, our method is complementary to existing learning-based methods.</p><p>Image denoising methods. Existing works for image denoising are massive. For Gaussian denoising, BM3D <ref type="bibr" target="#b6">[16]</ref> and DnCNN <ref type="bibr" target="#b74">[84]</ref> are representatives of the filter-based and deep-learning-based methods. For Poisson denoising, NLPCA <ref type="bibr" target="#b55">[65]</ref> combines elements of dictionary learning with sparse patch-based representations of images and employs an adaptation of Principal Component Analysis. Azzari et al.</p><p>[5] propose an iterative algorithm combined with variance-stablizing transformation (VST) and BM3D filter <ref type="bibr" target="#b6">[16]</ref>. DenoiseNet <ref type="bibr" target="#b48">[58]</ref> uses a deep convolutional network to calculate the negative noise components, which adds directly to the original noisy image to remove Poisson noise. For Gaussian-Poisson mixed denoising, CBDNet <ref type="bibr" target="#b17">[27]</ref> presents a convolutional blind denoising network by incorporating asymmetric learning. It is applicable to real noise images by training on both synthetic and real images. For realworld image denoising, TWSC <ref type="bibr" target="#b68">[78]</ref> develops a trilateral weighted sparse coding scheme. Chen et al.</p><p>[11] propose a two-step framework which contains noise distribution estimation using GANs and denoising using CNNs. Directly combining these methods with enhancement methods will result in blurring. To avoid this, our solution performs enhancing and denoising simultaneously.</p><p>Low-light Image Enhancement Datasets. Several previous datasets are constructed by manually capturing paired low-light and normal-light images. Multiple shootings with different camera configurations or retouching captured images are the two main solutions. The LOL <ref type="bibr" target="#b67">[77]</ref> and SID [10] datasets are constructed using the former solution. Images in the LOL dataset are captured in the daytime by controlling the exposure and ISO. Meanwhile, the underexposed images are generated by linear degradation approximately, which may differ from real cases. This will result in performance Our method optimally selects normal exposed images from public datasets, performs low-light simulation, and adds noise to synthesize realistic low-light images. Meanwhile, the original normal exposed images are enhanced by exposure correction and contrast/details amplification, so as to generate high-quality reference images. Details can be found in Section 3.</p><p>variation in low-light image enhancement (see the result of RetinexNet in <ref type="figure">Figure 1</ref>). The SID dataset is composed of raw sensor data under extremely low-light scenes, which is different from those used in general low-light image enhancement researches. As for the latter solution, the DeepUPE <ref type="bibr" target="#b63">[73]</ref> dataset collects 3,000 underexposed images, each with an expert-retouched reference. However, the under-exposed levels of the images are relatively low, which may not cover the heavily low-light scenes. Besides, the SICE [8] dataset collects multi-exposed image sequences and uses the Exposure Fusion methods to construct the reference image under the supervision of human. However, imperfect alignment of image sequences will result in blur and ghosting. Although these datasets have made great contributions to the field of low-light image enhancement, they still show limitations. On one hand, their data amounts are relatively small with respect to the number of images. Since the variation of scenes and light conditions are limited, the trained models may not be generalized well in many cases. On the other hand, due to the lack of annotations, these datasets are difficult to be used for other relevant vision tasks, such as detection and segmentation in the dark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Large Scale Low-Light Simulation Dataset</head><p>In this paper, we propose an effective low-light simulation method to synthesize low-light images from normallight images. The purpose is to offer a large diversity in scenes and light conditions which is required by our method and other further researches. Many previous works <ref type="bibr" target="#b19">[29,</ref><ref type="bibr" target="#b54">64]</ref> have proven that the synthetic data is an effective alternative to real data in different vision tasks. Using synthetic data allows easy model adaptation for target conditions without requiring additional manual annotations <ref type="bibr" target="#b7">[17,</ref><ref type="bibr" target="#b53">63]</ref>. Similarly, we believe that generating synthetic low-light image datasets from public datasets [6, <ref type="bibr" target="#b11">21,</ref><ref type="bibr" target="#b16">26,</ref><ref type="bibr" target="#b39">49]</ref> with rich annotations also has the potential to achieve model adaptation in low-light conditions. The proposed dataset construction pipeline is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Candidate Image Selection</head><p>Our proposed low-light simulation requires high-quality normally exposed images as input, and these images also serve as the reference for low-light enhancement. Therefore, we need to distinguish such high-quality images from low-quality ones given large-scale public image datasets, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. To this end, we propose a candidate image selection method which takes the proper exposure, rich color, blur-free and rich details into account. The selection method contains three steps: darkness estimation, blur estimation and color estimation.</p><p>Darkness estimation. To select images with sufficient exposure, we first apply over-segmentation <ref type="bibr" target="#b2">[3]</ref> and restore the segmentation results. Subsequently, we calculate the mean/variance of the V component in HSV color space based on the segmentation results. If the calculated mean/variance is larger than thresholds, we set this segmentation block to be sufficiently exposed. Finally, images with more than 85% sufficiently bright blocks are selected as candidates.</p><p>Blur estimation. This stage aims to select unblurred images with rich details. Following the same pipeline in <ref type="bibr" target="#b46">[56]</ref>, we apply the Laplacian edge extraction, calculate the variance among all the output pixels and use a threshold 500 to determine whether this image can be selected.</p><p>Color estimation. We directly estimate the color according to <ref type="bibr" target="#b20">[30]</ref> to select images with rich color. A threshold is set to 500 to eliminate those low-quality, gray-scale or unnatural images.</p><p>To ensure diversity, we select 97, 030 images from a total of 344, 272 images (collected from [6, 21, 26, 49]) based on the above rules to build the dataset. We randomly select 1% of them as the test set which contains 965 images. In this paper, we use the data-balanced subset including 22, 656 images as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Target Image Synthesis</head><p>We propose a low-light image simulation method to synthesize realistic low-light images from normal-light images, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. This produces an adequate number of paired low/normal light images which are needed for training of learning-based methods.</p><p>Low-light image synthesis. Low-light images differ from normal images due to two dominant features: low brightness/contrast and the presence of noise. In our low-light image synthesis, we try to fit a transformation to covert the normal image to underexposed low-light image. By analyzing images with different degree of exposure, we find that the combination of linear and gamma transformation can approximate this job well. To verify this, we test on multi-exposure images and use the histogram of Y channel in YCbCr color space as the metric. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, the synthetic low-light images are approximately the same to real low-light images. The low-light image simulation pipeline (without additional noise) can be formulated as:</p><formula xml:id="formula_0">I (i) out = β × (α × I (i) in ) γ , i ∈ {R, G, B}.<label>(1)</label></formula><p>where α and β are linear transformations, the X γ means the gamma transformation. The three parameters is sampled from uniform distribution: α ∼ U (0.9, 1), β ∼ U (0.5, 1), γ ∼ U (1.5, 5).</p><p>As for the noise, many previous methods fail to consider, while our method takes it into account. In particular, we follow <ref type="bibr" target="#b17">[27,</ref><ref type="bibr" target="#b70">80]</ref> to use the Gaussian-Poisson mixed noise model and take the in-camera image processing pipeline into account to simulate real low-light noise. The noise model can be formulated as:</p><formula xml:id="formula_1">I out = f (M −1 (P(M (f −1 (I in ))) + N G )),<label>(2)</label></formula><p>where P(x) represents adding Poisson noise with variance σ 2 p , N G is modeled as AWGN with noise variance σ 2 g , f (x) stands for the camera response function, M (x) is the function that convert RGB images to Bayer images and M −1 (x) is the demosaicing function. We do not consider compression in this paper and the configuration is the same as <ref type="bibr" target="#b17">[27]</ref>.</p><p>Image contrast amplification. The high-quality images in our dataset serve as the reference for low-light enhancement. However, directly using them to train an image-to-image regression method may result in lowcontrast results (see MBLLEN <ref type="bibr" target="#b41">[51]</ref> results in <ref type="figure" target="#fig_6">Figure 10</ref>). The possible reason caused low-contrast might that some selected images are slightly over-exposed, which will guide enhancement algorithms tend to generate slightly over-exposed results. Besides, the smoothness caused by noise removal also result in low-contrast. 7× <ref type="bibr" target="#b67">[77]</ref> [73]</p><p>[8] To overcome this limitation, we propose a contrast amplification method by synthesizing a new set of highquality images as the ground truth of our second enhancement step. In particular, we apply exposure fusion to improve the contrast/color and correct the exposure. First, we use gamma transforms to synthesize 10 images with different exposure settings and saturation levels from each original image. Subsequently, we fuse these differently exposed images following the same routine in <ref type="bibr" target="#b43">[53]</ref> (the results called colorful images). Finally, we apply image smoothing <ref type="bibr" target="#b69">[79]</ref> to further enhance the image details. The final output images called highcontrast images that can be used as ground truth to train a visually better low-light enhancement network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison of Low-light Enhancement Datasets</head><p>There are some existing datasets for low-light image enhancement. However, these datasets still have their own limitations. In this section, we highlight the differences between our synthetic dataset and other low-light image enhancement datasets, to show that our synthetic dataset is a good complement to existing datasets. The characteristics of different datasets are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Scale and Diversity. Having a large dataset with diverse scenes and lighting conditions is significant for training a model that can generalize well. Manually col- lecting images or editing images as in other datasets is a costly process, which make them hard to acquire data at scale. Therefore, existing datasets are all relatively small in size. In contrast, as our data generation is based on simulation, our method can synthesize paired lowlight and normal light images as much as needed for different scenes.</p><p>Low-light Level. Covering a large range of lowlight conditions is another important factor for the generalization capabilities of the trained models. To illustrate the range of different under-exposed levels, we calculate the exposure adjustment curve, which is the transform to the luminance channel of the low-light image (the V component in HSV color space) to make the luminance histogram match that of reference ground truth image. The estimated curve can serve as an estimation of the under-exposure levels, that is, steeper change of the curve indicates higher under-exposure levels.</p><p>The exposure adjustment curves for all data pair in each dataset are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. It shows that the LOL <ref type="bibr" target="#b67">[77]</ref> dataset contains many heavy low-light images. The DeepUPE <ref type="bibr" target="#b63">[73]</ref> dataset mainly covers medium under-exposure levels. As for the SICE [8] dataset, the under-exposure degree is sparse, which is caused by its specific exposure pre-settings. Notice that, in this research, we use the medium exposed images as the ground truth to estimate the curves for SICE [8] dataset. In contrast, our synthetic dataset contains a large variety of under-exposure levels, which is useful for improving the generalization capabilities of our trained models.</p><p>Quality. For learning-based methods, the quality of images is crucial as it directly decides the performance of training models. SCIE [8] uses multi-exposure image fusion result as the ground truth, which inevitably contains ghosting and blur. SID [10] prolongs exposure time to obtain high-quality night images, which may cause local overexposure and blur. LOL <ref type="bibr" target="#b67">[77]</ref> captures paired images by adjusting the ISO, which results in the exposure adjustment being approximately linear. In many cases, simply increasing low-light images linearly can result in good results. As for DeepUPE <ref type="bibr" target="#b63">[73]</ref>, using retouched images as the ground truth does not have the ability to deal with noise and artifacts. In contrast, our synthetic dataset does not have these problems. Besides, it provides the noise distribution map and exposure map that can be used as supervision to improve the performance of the trained model.</p><p>Compatibility. Beside making the visual quality more appealing, improving the performance of other vision systems under low-light conditions is another important application for low-light enhancement. However, existing datasets do not contains manual annotations as they are only designed for visual quality enhancement. In contrast, our synthetic dataset can directly use existing public datasets (e.g., COCO <ref type="bibr" target="#b39">[49]</ref>) to render low-light images and keep their corresponding annotations, such as bounding boxes for object detection and semantic segmentation masks. Previous works <ref type="bibr" target="#b7">[17,</ref><ref type="bibr" target="#b53">63]</ref> have proven that synthetic data is useful for model adaptation under adverse conditions. Thus, our synthetic dataset also has potential ability to improve the performance of fundamental vision methods to handle low-light conditions, such as object detection and semantic segmentation, etc.</p><p>In summary, our synthetic dataset has many advantages over existing datasets. Our synthetic dataset contains high quality paired pixel-aligned images with various scenes, diverse lighting conditions, and different underexposed levels. Moreover, this simulation can be applied to datasets with annotations, which is useful for model adaptation under low-light conditions. Our synthetic dataset is an important complement to existing low-light enhancement datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Attention-guided Low-light Enhancement</head><p>In this section, we introduce the proposed attentionguided enhancement solution, including the network architecture, the loss function and the implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network Architecture</head><p>We propose a fully convolutional network containing four subnets: an Attention-Net, a Noise-Net, an Enhancement Net and a Reinforce-Net. <ref type="figure" target="#fig_4">Figure 6</ref> shows the overall network architecture. The Attention-Net is designed for estimating the illumination to guide the method to pay more attention to the underexposed areas in enhancement. Similarly, the Noise-Net is designed to guide the denoising process. Under their guidance, the multi-branch Enhancement-Net can perform enhancing and denoising simultaneously. The Reinforce-Net is designed for contrast re-enhancement to solve the low-contrast limitation caused by regression. The detailed description is provided below.</p><p>Attention-Net. We directly adopt U-Net in our implementation. The motivation is to provide a guidance to let Enhancement-Net correctly enhance the underexposed areas and avoid over-enhance the normally exposed areas. The output is an ue-attention map indicating the regional under-exposure level, as shown in <ref type="figure">Figure 7</ref>. The higher the illumination is, the lower ueattention map values are. The ue-attention map's value range is [0, 1] and is determined by:</p><formula xml:id="formula_2">A = |max c (I) − max c (F(I))| max c (I) ,<label>(3)</label></formula><p>where max c (x) returns the maximum value among three color channels, I is the original bright image and F(I) is the synthetic under exposed image. As shown in <ref type="figure">Figure 7</ref>, the inverted ue-attention map looks somewhat similar to the illumination map of the Retinex model. This infers that our ue-attention map carries important information used by the popular Retinex model. On the other hand, using our inverted ue-attention  <ref type="figure">Figure 1</ref>) and will result in noise amplification (see LIME results in <ref type="figure" target="#fig_7">Figure 11</ref>). Therefore, we propose to use the ue-attention map as a guidance for our Enhancement Net introduced later.</p><p>Noise-Net. The image noise can be easily confused with image textures, causing unwanted blurring effect after applying simple denoising methods. Estimating the noise distribution beforehand and making the denoising adaptive may help reduce such an effect. The noise map's value range is [0, 1] and is determined by:</p><formula xml:id="formula_3">N = max c ( |F n (I) − F(I)| F(I) ),<label>(4)</label></formula><p>where max c (x) returns the maximum value among three color channels, F n (I) is the synthetic low-light image and F(I) is the synthetic under exposed image. Note that the noise distribution is highly related to the distribution of exposure, and thus we propose to use the ue-attention map to help derive a noise map. Under their guidance, the enhancement-net can perform denoising effectively. The Noise-Net is composed of dilated convolutional layers to increase the receptive field, which is conducive to noise estimation.</p><p>Enhancement-Net. The motivation is to decompose the enhancement problem into several sub-problems of different aspects (such as noise removal, texture preserving, color correction and so on) and solve them respectively to produce the final output via multi-branch</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Illum. map Inverted ue-attn. Our ue-attn. map Illum. map <ref type="bibr" target="#b18">[28]</ref> [77] <ref type="figure">Fig. 7</ref>: Comparison between our ue-attention map and the illumination maps used for retinex-based methods. Our ue-attention map can generate similar illumination information with more details.</p><p>fusion. It is the core component of the proposed network and it consists of three types of modules: the feature extraction module (FEM), the enhancement module (EM) and the fusion module (FM). FEM is a single stream network with several convolutional layers, each of which uses 3 × 3 kernels, stride of 1 and ReLU nonlinearity. The output of each layer is both the input to the next layer and also the input to the corresponding subnet of EM. EMs are modules following each convolutional layer of the FEM. The input to EM is the output of a certain layer in FEM, and the output size is the same as the input. FM accepts the outputs of all EMs to produce the final enhanced image. We concatenate all the outputs from EMs in the color channel dimension and use the 1 × 1 convolution kernel to merge them, which equals to the weighted summation with learnable weights. We propose five different EM structures. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, the design of EM follows U-Net <ref type="bibr" target="#b51">[61]</ref> and Res-Net <ref type="bibr" target="#b23">[33]</ref> which have been proven effective exten-sively. In brief, EM-1 is a stack of convolutional and deconvolutional layers with large kernel size. EM-2 and EM-3 has U-Net like structures, and the difference is the skip connection realization and the feature map size. EM-4 has a Res-Net like structure. We remove the Batch-Normalization <ref type="bibr" target="#b29">[39]</ref> and use just a few res-blocks to reduce the model parameter. EM-5 is composed of dilated convolutional layers whose output size is the same as the input.</p><p>Reinforce-Net. The motivation is to overcome the low-contrast drawback and improve the details (see the difference between MBLLEN <ref type="bibr" target="#b41">[51]</ref> and ours in <ref type="figure" target="#fig_6">Figure 10</ref>). Previous research [12] demonstrates the effectiveness of dilated convolution in image processing. Therefore, we use a similar network to improve contrast and details simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss Function</head><p>In order to improve the image quality both qualitatively and quantitatively, we propose a new loss function by further considering the structural information, perceptual information and regional difference of the image. It is expressed as:</p><formula xml:id="formula_4">L = ω a L a + ω n L n + ω e L e + ω r L r ,<label>(5)</label></formula><p>where the L a , L n , L e and L r represent the loss function of Attention-Net, Noise-Net, Enhancement-Net and Reinforce-Net, and ω a , ω n , ω e , ω r are the corresponding coefficients. The details of the four loss functions are given below.</p><p>Attention-Net loss. To obtain the correct ue-attention map for guiding the Enhancement-Net, we use the L2 error metric to measure the prediction error as:</p><formula xml:id="formula_5">L a = F a (I) − A 2 ,<label>(6)</label></formula><p>where I is the input image, F a (I) and A are the predicted and expected ue-attention maps.</p><p>Noise-Net loss. Similarly, we use the L1 error metric to measure the prediction error of the Noise-Net as:</p><formula xml:id="formula_6">L n = F n (I, A ) − N 1 ,<label>(7)</label></formula><p>where A = F a (I), F n (I, A ) and N are the predicted and expected noise maps. Enhancement-Net loss. Due to the low brightness of the image, only using common error metrics such as mse or mae may cause structure distortion such as blur effect and artifacts. We design a new loss that consists of four components to improve the visual quality. It is defined as:</p><formula xml:id="formula_7">L e = ω eb L eb + ω es L es + ω ep L ep + ω er L er ,<label>(8)</label></formula><p>where the L eb , L es , L ep and L er represent bright loss, structural loss, perceptual loss and regional loss. And ω eb , ω es , ω ep and ω er are the corresponding coefficients. The bright loss is designed to ensure that the enhanced results have sufficient brightness. It is defined as:</p><formula xml:id="formula_8">L eb = S(F e (I, A , N ) − I) 1 ,<label>(9)</label></formula><p>where F e (I, A , N ) and I are the predicted and expected enhancement images. S is defined as:</p><formula xml:id="formula_9">S(x &lt; 0) = −λx, S(x ≥ 0) = x, s.t. λ &gt; 1.</formula><p>The structural loss is introduced to preserve the image structure and avoid blurring. We use the wellknown image quality assessment algorithm SSIM <ref type="bibr" target="#b66">[76]</ref> to build our structure loss. The structural loss is defined as:</p><formula xml:id="formula_10">L es = 1 − 1 N p∈img 2µ x µ y + C 1 µ 2 x + µ 2 y + C 1 · 2σ xy + C 2 σ 2 x + σ 2 y + C 2 ,<label>(10)</label></formula><p>where µ x and µ y are pixel value averages, σ 2 x and σ 2 y are variances, σ xy is the covariance, and C 1 and C 2 are constants to prevent the denominator to zero. The perceptual loss is introduced to use higher-level information to improve the visual quality. We use the well-behaved VGG network <ref type="bibr" target="#b59">[69]</ref> as the content extractor <ref type="bibr" target="#b35">[45]</ref>. In particular, we define the perceptual loss based on the output of the ReLU activation layers of the pre-trained VGG-19 network. The perceptual loss is defined as follows:</p><formula xml:id="formula_11">L ep = 1 w ij h ij c ij wij x=1 hij y=1 cij z=1 φ ij (I ) xyz − φ ij ( I) xyz ,<label>(11)</label></formula><p>where I = F e (I, A , N ) and I are the predicted and expected enhancement images, and w ij , h ij and c ij describe the dimensions of the respective feature maps within the VGG-19 network. Besides, φ ij indicates the feature map obtained by j-th convolution layer in i-th block of the VGG-19 Network.</p><p>For low-light image enhancement, except taking the image as a whole, we should pay more attention to the underexposed regions. We propose the regional loss to balances the degree of enhancement for different regions. It is defined as:</p><formula xml:id="formula_12">L er = I · A − I · A 1 + 1 − ssim(I · A , I · A ) (12)</formula><p>where ssim(·) represents the image quality assessment algorithm SSIM <ref type="bibr" target="#b66">[76]</ref> and A is the predicted ue-attention map which is used as the guidance.</p><p>Reinforce-Net loss. Similar to the Enhancement-Net loss, the Reinforce-Net loss is defined as:</p><formula xml:id="formula_13">L r = ω rb L rb + ω rs L rs + ω rp L rp ,<label>(13)</label></formula><p>where L rb , L rs and L rp represent bright loss, structural loss and perceptual loss, and are the same as L rb , L rs and L rp . In the experiments, we empirically set λ = 10, ω a , ω n , ω e , ω r = {100, 10, 10, 1}, ω eb , ω es , ω ep , ω er = {1, 1, 0.35, 5}, ω rb , ω rs , ω rp = {1, 1, 0.35}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Our implementation is done with Keras <ref type="bibr" target="#b5">[15]</ref> and Tensorflow <ref type="bibr" target="#b0">[1]</ref>. The proposed network can be quickly converged after being trained for 20 epochs on a Titan-X GPU using the proposed dataset. In order to prevent overfitting, we use random clipping, flipping and rotating for data augmentation. We set the batch-size to 8 and the size of random clipping patches to 256×256×3. The input image values is scaled to [0, 1]. We use the output of the fourth convolutional layer in the third block of VGG-19 network as the perceptual loss extraction layer.</p><p>In the experiment, training is done using the Adam optimizer <ref type="bibr" target="#b32">[42]</ref> with parameters of α = 0.0002, β 1 = 0.9, β 2 = 0.999 and = 10 −8 . We also use the learning rate decay strategy, which reduces the learning rate to 98% before the next epoch. At the same time, we reduce the learning rate to 50% when the loss metric has stopped improving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>We compare our method with existing methods through extensive experiments. We use the publicly-available codes with recommended parameter settings. In quantitative comparison, we used PSNR and SSIM <ref type="bibr" target="#b66">[76]</ref>, along with some recently proposed metrics Average Brightness (AB) <ref type="bibr" target="#b4">[14]</ref>, Visual Information Fidelity (VIF) <ref type="bibr" target="#b57">[67]</ref>, Lightness Order Error (LOE) <ref type="bibr" target="#b72">[82]</ref>, Tone Mapped Image Quality Index (TMQI) <ref type="bibr" target="#b71">[81]</ref> and Learned Perceptual Image Patch Similarity Metric (LPIPS) <ref type="bibr" target="#b76">[86]</ref>. For all metrics higher number means better, except LPIPS, LOE  and AB. Note that in the tables below, red, green and blue colors indicate the best, second, and third place results, respectively.</p><p>Our experiment is organized as following. First, we make qualitative and quantitative comparisons based on our synthetic dataset and two public-available real low-light datasets. Second, we make visual comparisons with state-of-the-art methods on natural low-light images and provide a user study. We also show the robustness of our method and the benefit to some high-level tasks. Finally, we provide an ablation study to evaluate the effect of different elements and discuss unsatisfying cases.  <ref type="figure">Fig. 8</ref>: Visual comparison on synthetic low-light images. We fine tune the GLADNet <ref type="bibr" target="#b65">[75]</ref> using our synthetic datasets. Please zoom in for a better view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments on Synthetic Datasets</head><p>Direct comparison. We compare our method with state-of-the-art methods on our synthetic dataset. Since most methods do not have the ability to remove noise, we combine them with the state-of-the-art denoising method CBDNet <ref type="bibr" target="#b17">[27]</ref> to produce the final comparison results. We fine tune the GLADNet <ref type="bibr" target="#b65">[75]</ref> and LL-Net <ref type="bibr" target="#b40">[50]</ref> for fair comparison. Quantitative comparison results are shown in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref>. Our result significantly outperforms other methods in all quality metrics, which fully demonstrates the superiority of our approach.</p><p>Representative results are visually shown in <ref type="figure">Figure 8</ref>. By checking the details, it is clear that our method achieves better visual effects, including good brightness/contrast and less artifacts. Please zoom in to compare the details.</p><p>Efficiency comparison. In addition to the result quality, efficiency is also an important metric to algorithms. In order to demonstrate the superiority of our method, we use 10 HD images with size 1920 × 1080 as the benchmark to test running time. In order to more intuitively demonstrate the relationship between performance and efficiency, we show <ref type="figure" target="#fig_5">Figure 9</ref>. Our method performs well in terms of both quality and efficiency. Notice that, JED <ref type="bibr" target="#b50">[60]</ref> and Robust <ref type="bibr" target="#b38">[48]</ref> need large computational resources, which will cause out-of-memory problem when processing large images. Due to the MLP architecture, LLNet <ref type="bibr" target="#b40">[50]</ref> needs to enhance large images one patch by one patch, which will limits its efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments on Real Datasets</head><p>Besides synthetic datasets, our method also performs well on real low-light image datasets. We evaluate the performance based on two public-available real low-  LOL dataset. This dataset is captured by control the exposure and ISO in the daytime. We finetune our model using this dataset to compare with RetinexNet [10], which is trained on the LOL dataset. In addition, we replace the Enhancement-Net by a standard U-Net to build a lightweight version. Following PPCN <ref type="bibr" target="#b24">[34]</ref>, we also adopt knowledge transfer to further promote its performance. Quantitative comparison is shown in <ref type="table" target="#tab_5">Table 4</ref>. For both quality and efficiency comparisons, our method performs better, manifesting that our method effectively learns the adjustment and restoration. Visual comparison is shown in <ref type="figure" target="#fig_6">Figure 10</ref>. Compared with RetinexNet <ref type="bibr" target="#b67">[77]</ref> and MBLLEN <ref type="bibr" target="#b41">[51]</ref>, our results with clear details, better contrast, normal brightness and natural white balance.</p><p>SID dataset. This dataset contains raw short-exposure images with corresponding long-exposure reference images and is benchmarking single-image processing of extremely low-light raw images. Due to the larger bit depth, raw images are more suitable for extremely lowlight scenes compared with rgb images. Different from traditional pipelines, SID [10] develop a pipeline based on an end-to-end network and achieve excellent results. Need to notice that, processing low-light raw images is a related but not identical problem. However, to prove the ability of our multi-branch network, we use the same configuration except that the network is replaced by our Enhancement-Net. Quantitative comparison is shown in <ref type="table" target="#tab_5">Table 4</ref>. Our model is lightweight and more efficient, but achieves comparable enhancement quality. In addi- tion, our results have better visual effects as shown in <ref type="figure" target="#fig_6">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments on Real Images</head><p>In this section, we evaluate our method on real lowlight images, including natural, monochrome and game scenes. We also show the benefit to object detection and semantic segmentation under low-light environment by directly using our method as the pre-processing. Natural low-light images. We first compare our method with state-of-the-art methods on natural lowlight images and the representative visual comparison results are shown in <ref type="figure" target="#fig_7">Figure 11</ref>. Our method surpasses other methods in two key aspects. On the one hand, our method can restore vivid and natural color to make the enhancement results more realistic. In contrast, Retinexbased methods (such as RetinexNet <ref type="bibr" target="#b67">[77]</ref> and LIME <ref type="bibr" target="#b18">[28]</ref>)  will cause different degrees of color distortion. On the other hand, our method is able to recover better contrast and more details. This improvement is especially evident when compared with LLNet <ref type="bibr" target="#b40">[50]</ref>, BIMEF <ref type="bibr" target="#b72">[82]</ref> and MBLLEN <ref type="bibr" target="#b41">[51]</ref>.</p><p>User study. We invite 100 participants to attend a user study to test the subjective preference of low-light image enhancement methods. We randomly select 20 natural low-light image cases and enhance them using five representative methods. For each case, the input data and the five enhanced results will be shown to the participants at the same time. We then ask the participants to rank the quality of the five enhancements from 1 (best) to 5 (worst) in terms of recovery of brightness, contrast, and color. We also provide zoom-in function to let participants to check details like texture and noises controls. The other four methods used besides ours in this study are DHECI <ref type="bibr" target="#b44">[54]</ref>, DeepUPE <ref type="bibr" target="#b63">[73]</ref>, LIME <ref type="bibr" target="#b18">[28]</ref> and Robust <ref type="bibr" target="#b38">[48]</ref>. <ref type="figure" target="#fig_0">Figure 12</ref> shows the rating distribution of the user study. Our method receives more "best" ratings, which shows that our results are more preferred by human subjects.</p><p>Generalization study. To prove the robustness of our method, we directly apply our trained model to enhance some specific types of low-light scenes (such as monochrome surveillance and game night scenes) that  are unseen in the training dataset. <ref type="figure" target="#fig_1">Figure 13</ref> shows the enhancement results. The results demonstrate that our method is robust and effective for general low-light image enhancement tasks. Besides, we also show that our approach is beneficial to some high-level tasks in low-light scenes, such as object detection and instance segmentation, as shown in <ref type="figure" target="#fig_2">Figure 14</ref>. The performance of Mask-RCNN <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">31]</ref> has been improved a lot by using our method in a pre-processing stage without any fine-tuning. Besides, we have tested end-to-end training our multi-branch network for many other low-level computer vision tasks and demonstrated the effectiveness. Visual examples on denoising, dehazing, deblurring, etc., are shown in <ref type="figure" target="#fig_4">Figure 16</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>In this section, we quantitatively evaluate the effectiveness of different components in our method based on our synthetic low-light dataset. <ref type="table" target="#tab_8">Table 5</ref> reports the accuracy of the presented change in terms of PSNR and SSIM <ref type="bibr" target="#b66">[76]</ref>. Note that the Reinforce-Net is not considered in this study. Loss functions. We mainly evaluate the loss function of the Enhancement-Net, as shown in <ref type="table" target="#tab_8">Table 5</ref> (row 2-5). We use mse as the naive loss function under condition 2. The results show that the quality of enhancement is improving by containing more loss components.</p><p>Network structures. As shown in <ref type="table" target="#tab_8">Table 5</ref> (row 6-7), we evaluate the effectiveness of different network components. Similar to the loss function, the results demonstrate that more components of our network will result in better performance.</p><p>Number of branches. We analyze the effect of different branch numbers (model size) on the network performance, as shown in <ref type="table" target="#tab_8">Table 5</ref> (row 8-9). Obviously, the increase of model size will not always improve performance, so we set 10 branches as the default configuration. <ref type="figure" target="#fig_3">Fig. 15</ref>: This image has totally dark region where textures are lost and heavy compression. These cause issues in the enhancement result. <ref type="figure" target="#fig_3">Figure 15</ref> presents a case where our method performs not perfectively. Our method fails to recover the face details on the top image, as some parts of the face are totally dark. Another issue is the blocking artifacts due to heavy image compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Unsatisfying Cases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposes an attention-guided enhancement solution for low-light image enhancement. We design a multi-branch network to handle enhance the brightness and handle the noise simultaneously. The key is to use the proposed ue-attention map and noise map to guide the enhancement in a region adaptive manner. We also propose a low-light image simulation pipeline and build a large-scale low-light enhancement benchmark dataset for model training and evaluation. Extensive experiments demonstrate that our solution outperforms state-of-the-art methods by a large margin. As for future direction, extending the proposed method to low-light video enhancement is of our interest.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Pipeline of constructing the proposed low-light simulation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Samples of large-scale public datasets: (left) lowquality examples, (right) high-quality examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Verification of the low-light simulation method: visual comparison and the histogram of Y channel in YCbCr between synthetic images and real different exposure images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison with existing paired low-light datasets. Top: Example images of different datasets. Bottom: The distribution of exposure adjustment curves of different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>The proposed network with four subnets. The Attention-Net and Noise-Net are used to estimate the attention of exposure and noise. The Enhancement-Net and Reinforce-Net are corresponding to the two enhancement processes. The core network is the multi-branch Enhancement-Net, which is composed of feature extraction module (FEM), enhancement module (EM) and fusion module (FM). The dashed lines represent skip connections and the circles represent discontinuous connections. map in Retinex model still cannot ensure satisfactory results. This is because the Retinex-based solution faces difficulties in handling black regions (see black regions in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Runtime and performance comparison of different enhancement methods. Test machine is a PC with Intel i5-8400 CPU, 16 GB memory and NVIDIA Titan-Xp GPU. "*" represents using GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Visual comparison on the LOL dataset (row 1 and 2) and the SID dataset (row 3). Please zoom in for a better view. light datasets and show the visual comparison on challenging images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Visual comparison of real low-light images, which are taken at night. Please zoom in for a better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 :</head><label>12</label><figDesc>Rating distribution of the user study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 :</head><label>13</label><figDesc>Generalizing our method to enhance (upper) monochrome surveillance scenes and (bottom) nighttime game scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 :</head><label>14</label><figDesc>After processing the low-light scene (upper row) with our method, the performance of both object detection and instance segmentation are greatly improved (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 16 :</head><label>16</label><figDesc>Visual comparison of several low-level vision tasks. From top to bottom: dehazing, denoising, colorization, defocus deblurring, super-resolution (×2) and motion deblurring. 5. Azzari, L., Foi, A.: Variance stabilization for noisy+ estimate combination in iterative poisson denoising. IEEE signal processing letters 23(8), 1086-1090 (2016) 6. Bileschi, S.M.: Streetscenes: Towards scene understanding in still images. Tech. rep., MASSACHUSETTS INST OF TECH CAMBRIDGE (2006) 7. Cai, B., Xu, X., Jia, K., Qing, C., Tao, D.: Dehazenet: An end-to-end system for single image haze removal. IEEE Transactions on Image Processing (TIP) 25(11), 5187-5198 (2016) 8. Cai, J., Gu, S., Zhang, L.: Learning a deep single image contrast enhancer from multi-exposure images. IEEE Transactions on Image Processing (TIP) 27(4), 2049-2062 (2018) 9. Celik, T., Tjahjadi, T.: Contextual and variational contrast enhancement. IEEE Transactions on Image Processing (TIP) 20(12), 3431-3441 (2011) 10. Chen, C., Chen, Q.C., Xu, J., Koltun, V.: Learning to see in the dark. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 11. Chen, J., Chen, J., Chao, H., Yang, M.: Image blind denoising with generative adversarial network based noise modeling. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 12. Chen, Q., xu, J., Koltun, V.: Fast image processing with fully-convolutional networks. IEEE International Conference on Computer Vision (ICCV) (2017) 13. Chen, Y.S., Wang, Y.C., Kao, M.H., Chuang, Y.Y.: Deep photo enhancer: Unpaired learning for image enhance-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with existing low-light enhancement datasets. H(eavy), M(edium) and S(light) means the underexposed level. "MEF" means Multi Expose Fusion methods. "Comp." means Compatibility, which indicates whether the data acquire method can directly extend to existing public dataset for computer vision problems that have other annotations.</figDesc><table><row><cell>Dataset</cell><cell>Level</cell><cell>Source</cell><cell cols="2">Noise Comp.</cell><cell>Scenes</cell></row><row><cell>SID [10]</cell><cell>H</cell><cell>Camera</cell><cell></cell><cell>×</cell><cell>424</cell></row><row><cell>LOL [77]</cell><cell>H-M</cell><cell>Camera</cell><cell></cell><cell>×</cell><cell>500</cell></row><row><cell>SICE [8]</cell><cell>H-M-S</cell><cell>MEF</cell><cell>×</cell><cell>×</cell><cell>589</cell></row><row><cell>DeepUPE [73]</cell><cell>M-S</cell><cell>Retouch</cell><cell>×</cell><cell>×</cell><cell>3,000</cell></row><row><cell>Ours</cell><cell cols="2">H-M-S Synthesis</cell><cell></cell><cell></cell><cell>22,656</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison of synthetic low-light image (without additional noise) enhancement. "↑" indicates the higher the better, "↓" indicates the lower the better, '' ⇓" indicates the lower absolute value the better.</figDesc><table><row><cell></cell><cell cols="4">PSNR↑ SSIM↑ LPIPS↓ VIF↑</cell><cell cols="2">LOE↓ TMQI↑</cell><cell>AB⇓</cell></row><row><cell>Input</cell><cell>11.99</cell><cell>0.45</cell><cell>0.26</cell><cell>0.33</cell><cell>677.85</cell><cell cols="2">0.80 -59.22</cell></row><row><cell>BIMEF [82]</cell><cell>18.28</cell><cell>0.76</cell><cell>0.11</cell><cell>0.49</cell><cell>550.20</cell><cell cols="2">0.85 -28.06</cell></row><row><cell>LIME [28]</cell><cell>15.80</cell><cell>0.68</cell><cell>0.20</cell><cell cols="2">0.48 1121.17</cell><cell>0.80</cell><cell>-2.46</cell></row><row><cell>MSRCR [40]</cell><cell>14.87</cell><cell>0.72</cell><cell>0.15</cell><cell cols="2">0.52 1249.24</cell><cell>0.82</cell><cell>35.07</cell></row><row><cell>MF [22]</cell><cell>15.89</cell><cell>0.68</cell><cell>0.18</cell><cell>0.44</cell><cell>766.00</cell><cell cols="2">0.83 -36.88</cell></row><row><cell>SRIE [23]</cell><cell>13.83</cell><cell>0.56</cell><cell>0.21</cell><cell>0.37</cell><cell>787.42</cell><cell cols="2">0.82 -47.86</cell></row><row><cell>Dong [20]</cell><cell>15.37</cell><cell>0.65</cell><cell>0.22</cell><cell cols="2">0.35 1228.49</cell><cell cols="2">0.81 -33.80</cell></row><row><cell>NPE [74]</cell><cell>14.93</cell><cell>0.66</cell><cell>0.18</cell><cell>0.42</cell><cell>875.15</cell><cell cols="2">0.83 -41.35</cell></row><row><cell>DHECI [54]</cell><cell>18.13</cell><cell>0.76</cell><cell>0.17</cell><cell>0.39</cell><cell>547.12</cell><cell cols="2">0.87 -17.37</cell></row><row><cell>BPDHE [35]</cell><cell>13.62</cell><cell>0.60</cell><cell>0.24</cell><cell>0.34</cell><cell>609.89</cell><cell cols="2">0.82 -47.82</cell></row><row><cell>HE</cell><cell>17.88</cell><cell>0.76</cell><cell>0.18</cell><cell>0.47</cell><cell>596.67</cell><cell>0.88</cell><cell>19.24</cell></row><row><cell>Ying [83]</cell><cell>19.21</cell><cell>0.80</cell><cell>0.11</cell><cell>0.56</cell><cell>778.67</cell><cell>0.83</cell><cell>-9.28</cell></row><row><cell>WAHE [4]</cell><cell>15.46</cell><cell>0.65</cell><cell>0.18</cell><cell>0.44</cell><cell>564.83</cell><cell cols="2">0.84 -39.38</cell></row><row><cell>JED [60]</cell><cell>16.11</cell><cell>0.65</cell><cell>0.21</cell><cell cols="2">0.41 1212.66</cell><cell cols="2">0.82 -25.95</cell></row><row><cell>Robust [48]</cell><cell>16.83</cell><cell>0.69</cell><cell>0.20</cell><cell cols="2">0.47 1052.22</cell><cell cols="2">0.82 -22.09</cell></row><row><cell>LLNet [50]</cell><cell>20.11</cell><cell>0.80</cell><cell>0.39</cell><cell cols="2">0.40 1088.43</cell><cell>0.87</cell><cell>4.30</cell></row><row><cell>DeepUPE [73]</cell><cell>16.55</cell><cell>0.64</cell><cell>0.17</cell><cell>0.55</cell><cell>516.47</cell><cell cols="2">0.84 -30.48</cell></row><row><cell>GLADNet [75]</cell><cell>24.57</cell><cell>0.90</cell><cell>0.09</cell><cell cols="2">0.62 513.18</cell><cell>0.91</cell><cell>5.52</cell></row><row><cell>MBLLEN [51]</cell><cell>24.21</cell><cell>0.90</cell><cell>0.08</cell><cell cols="2">0.63 536.75</cell><cell>0.91</cell><cell>-3.66</cell></row><row><cell>Ours</cell><cell>25.24</cell><cell>0.94</cell><cell>0.08</cell><cell cols="2">0.67 495.48</cell><cell>0.93</cell><cell>2.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison of synthetic low-light images (with additional noise) enhancement. "↑" indicates the higher the better, "↓" indicates the lower the better, '' ⇓" indicates the lower absolute value the better.</figDesc><table><row><cell></cell><cell cols="4">PSNR↑ SSIM↑ LPIPS↓ VIF↑</cell><cell cols="2">LOE↓ TMQI↑</cell><cell>AB⇓</cell></row><row><cell>Input</cell><cell>11.23</cell><cell>0.37</cell><cell>0.41</cell><cell>0.23</cell><cell>925.06</cell><cell cols="2">0.77 -65.32</cell></row><row><cell>BIMEF [82]</cell><cell>16.57</cell><cell>0.64</cell><cell>0.32</cell><cell>0.28</cell><cell>978.96</cell><cell cols="2">0.83 -32.65</cell></row><row><cell>LIME [28]</cell><cell>14.79</cell><cell>0.59</cell><cell>0.34</cell><cell cols="2">0.26 1462.64</cell><cell>0.79</cell><cell>-7.39</cell></row><row><cell>MSRCR [40]</cell><cell>14.83</cell><cell>0.62</cell><cell>0.34</cell><cell cols="2">0.27 1559.05</cell><cell>0.84</cell><cell>30.98</cell></row><row><cell>MF [22]</cell><cell>15.29</cell><cell>0.59</cell><cell>0.33</cell><cell cols="2">0.26 1095.33</cell><cell cols="2">0.82 -37.46</cell></row><row><cell>SRIE [23]</cell><cell>13.10</cell><cell>0.48</cell><cell>0.37</cell><cell cols="2">0.25 1095.30</cell><cell cols="2">0.80 -52.53</cell></row><row><cell>Dong [20]</cell><cell>14.69</cell><cell>0.56</cell><cell>0.35</cell><cell cols="2">0.21 1592.27</cell><cell cols="2">0.79 -33.99</cell></row><row><cell>NPE [74]</cell><cell>14.56</cell><cell>0.58</cell><cell>0.33</cell><cell cols="2">0.25 1302.10</cell><cell cols="2">0.82 -41.17</cell></row><row><cell>DHECI [54]</cell><cell>16.57</cell><cell>0.61</cell><cell>0.37</cell><cell>0.23</cell><cell>924.78</cell><cell cols="2">0.86 -15.20</cell></row><row><cell>BPDHE [35]</cell><cell>12.60</cell><cell>0.48</cell><cell>0.38</cell><cell>0.23</cell><cell>925.56</cell><cell cols="2">0.79 -54.66</cell></row><row><cell>HE</cell><cell>16.65</cell><cell>0.64</cell><cell>0.36</cell><cell cols="2">0.26 1036.22</cell><cell>0.87</cell><cell>20.21</cell></row><row><cell>Ying [83]</cell><cell>17.18</cell><cell>0.67</cell><cell>0.31</cell><cell cols="2">0.28 1152.94</cell><cell cols="2">0.83 -13.97</cell></row><row><cell>WAHE [4]</cell><cell>13.97</cell><cell>0.52</cell><cell>0.36</cell><cell>0.27</cell><cell>935.21</cell><cell cols="2">0.81 -46.87</cell></row><row><cell>JED [60]</cell><cell>13.70</cell><cell>0.48</cell><cell>0.46</cell><cell cols="2">0.22 1531.84</cell><cell cols="2">0.77 -33.11</cell></row><row><cell>Robust [48]</cell><cell>14.03</cell><cell>0.50</cell><cell>0.46</cell><cell cols="2">0.23 1448.03</cell><cell cols="2">0.77 -29.09</cell></row><row><cell>LLNet [50]</cell><cell>18.40</cell><cell>0.69</cell><cell>0.56</cell><cell cols="2">0.26 1168.75</cell><cell>0.85</cell><cell>-5.25</cell></row><row><cell>DeepUPE [73]</cell><cell>14.94</cell><cell>0.53</cell><cell>0.35</cell><cell cols="2">0.25 1084.08</cell><cell cols="2">0.81 -36.53</cell></row><row><cell>GLADNet [75]</cell><cell>19.86</cell><cell>0.76</cell><cell>0.19</cell><cell cols="2">0.30 796.87</cell><cell>0.88</cell><cell>5.09</cell></row><row><cell>MBLLEN [51]</cell><cell>19.27</cell><cell>0.73</cell><cell>0.23</cell><cell cols="2">0.30 864.57</cell><cell>0.89</cell><cell>-4.87</cell></row><row><cell>Ours</cell><cell>20.84</cell><cell>0.82</cell><cell>0.17</cell><cell cols="2">0.33 785.64</cell><cell>0.91</cell><cell>4.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison between our method and state-of-the-arts on the LOL dataset and the SID dataset. "ours-1" means the result of the Enhancement-Net, "ours-2" means the result of the Reinforce-Net.</figDesc><table><row><cell>Method</cell><cell cols="5">PSNR SSIM LPIPS Time Params</cell></row><row><cell>RetinexNet [77]</cell><cell>16.77</cell><cell>0.56</cell><cell>0.47</cell><cell>0.06</cell><cell>0.44M</cell></row><row><cell>RetinexNet [77] + BM3D</cell><cell>17.91</cell><cell>0.73</cell><cell>0.22</cell><cell>2.75</cell><cell>0.44M</cell></row><row><cell>MBLLEN [51]</cell><cell>18.56</cell><cell>0.75</cell><cell>0.19</cell><cell>0.05</cell><cell>0.31M</cell></row><row><cell>Ours-lightweight-1</cell><cell>19.08</cell><cell>0.74</cell><cell>0.17</cell><cell>0.05</cell><cell>0.21M</cell></row><row><cell>Ours-lightweight-2</cell><cell>18.79</cell><cell>0.77</cell><cell>0.21</cell><cell>0.05</cell><cell>0.25M</cell></row><row><cell>Ours-1</cell><cell>20.24</cell><cell>0.79</cell><cell>0.14</cell><cell>0.06</cell><cell>0.88M</cell></row><row><cell>Ours-2</cell><cell>19.48</cell><cell>0.81</cell><cell>0.16</cell><cell>0.06</cell><cell>0.92M</cell></row><row><cell>SID [10]</cell><cell>28.88</cell><cell>0.79</cell><cell>0.36</cell><cell>0.51</cell><cell>7.76M</cell></row><row><cell>Ours</cell><cell>27.96</cell><cell>0.77</cell><cell>0.36</cell><cell>0.48</cell><cell>0.88M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation study. This table reports the performance under each condition based on the synthetic low-light dataset. In this table, "w/o" means without. L eb , w/o L es , w/o L ep , w/o L er 19.36 0.73 3. with L eb , w/o L es , w/o L ep , w/o L er 20.01 0.76 4. with L eb , with L es , w/o L ep , w/o L er 19.92 0.78 5. with L eb , with L es , with L ep , w/o L er</figDesc><table><row><cell>Condition</cell><cell cols="2">PSNR SSIM</cell></row><row><cell>1. default configuration</cell><cell>20.84</cell><cell>0.82</cell></row><row><cell cols="2">2. w/o 20.58</cell><cell>0.81</cell></row><row><cell>6. w/o Attention-Net, w/o Noise-Net</cell><cell>19.12</cell><cell>0.71</cell></row><row><cell>7. with Attention-Net, w/o Noise-Net</cell><cell>20.66</cell><cell>0.80</cell></row><row><cell>8. branch number ×1 (5)</cell><cell>20.66</cell><cell>0.79</cell></row><row><cell>9. branch number ×3 (15)</cell><cell>20.83</cell><cell>0.82</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mask r-cnn for object detection and instance segmentation on keras and tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdulla</surname></persName>
		</author>
		<ptr target="https://github.com/matterport/Mask_RCNN" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-theart superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A histogram modification framework and its application for image contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dikbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altunbasak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1921" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note>) ment from photographs with gans</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graylevel grouping (glg): an automatic method for optimized image contrast enhancement-part i: the basic method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2290" to="2302" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/keras-team/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image denoising with block-matching and 3d filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing: Algorithms and Systems, Neural Networks, and Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6064</biblScope>
			<biblScope unit="page">606414</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European conference on computer</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast efficient algorithm for enhancement of low lighting video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fusion-based enhancing method for weakly illuminated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="82" to="96" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A weighted variational model for simultaneous reflectance and illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep bilateral learning for real-time image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The iapr tc-12 benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Int. Workshop OntoImage</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Toward convolutional blind denoising of real photographs. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lime: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="982" to="993" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>TIP)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic understanding of foggy scenes with purely synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Zaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring colorfulness in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Suesstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human vision and electronic imaging VIII</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5007</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2341" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptionpreserving convolutional networks for image enhancement on smartphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop (ECCVW)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Brightness preserving dynamic histogram equalization for image contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S P</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1752" to="1758" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dslr-quality photos on mobile devices with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wespe: weakly supervised photo enhancer for digital cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A multiscale retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">U</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Properties and performance of a center/surround retinex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">U</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="462" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The retinex theory of color vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="108" to="129" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation of 2d histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5372" to="5384" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive multiscale retinex for image contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal-Image Technology &amp; Internet-Based Systems (SITIS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structurerevealing low-light image enhancement via robust retinex model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2828" to="2841" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Llnet: A deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Lore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akintayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="650" to="662" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mbllen: Low-light image/video enhancement using cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An integrated enhancement solution for 24-hour colorful imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exposure fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Reeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="page" from="382" to="390" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Color image contrast enhacement method based on differential intensity/saturation gray-levels histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Signal Processing and Communications Systems (ISPACS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Blind image deblurring using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Diatom autofocusing in brightfield microscopy: a comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Pech-Pacheco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cristóbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chamorro-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fernández-Valdivia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="314" to="317" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From dusk till dawn: Modeling in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5488" to="5496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01687</idno>
		<title level="m">Deep convolutional denoising of low-light images</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Low-light image enhancement via a deep hybrid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">TIP</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Joint enhancement and denoising method via sequential decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Guided curriculum model adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Poisson noise reduction with non-local pca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harmany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Deledalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical imaging and vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="279" to="294" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Classification-driven dynamic image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4033" to="4041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>TIP)</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02488</idno>
		<title level="m">Msr-net: Low-light image enhancement using deep convolutional network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fast perceptual image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>De Stoutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop (ECCVW)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="260" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Lowlight image enhancement using cnn and bright channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Llcnn: A convolutional neural network for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Communications and Image Processing</title>
		<imprint>
			<publisher>VCIP</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Underexposed photo enhancement using deep illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Naturalness preserved enhancement algorithm for non-uniform illumination images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3538" to="3548" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Gladnet: Low-light enhancement network with global awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep retinex decomposition for low-light enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A trilateral weighted sparse coding scheme for real-world image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Image smoothing via l 0 gradient minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="174" to="185" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Low-light color image enhancement via iterative noise reduction using rgb/nir sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sugimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43017</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Objective quality assessment of tone-mapped images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yeganeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="657" to="667" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>TIP)</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">A bio-inspired multi-exposure fusion framework for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00591</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A new low-light image enhancement algorithm using camera response model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Kindling the darkness: A practical low-light image enhancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04161</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
