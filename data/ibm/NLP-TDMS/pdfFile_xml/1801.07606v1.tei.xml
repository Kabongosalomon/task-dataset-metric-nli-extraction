<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
							<email>zhhan@student.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
							<email>xiao-ming.wu@polyu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semisupervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection.</p><p>In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of oversmoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The breakthroughs in deep learning have led to a paradigm shift in artificial intelligence and machine learning. On the one hand, numerous old problems have been revisited with deep neural networks and huge progress has been made in many tasks previously seemed out of reach, such as machine translation and computer vision. On the other hand, new techniques such as geometric deep learning <ref type="bibr" target="#b0">(Bronstein et al. 2017)</ref> are being developed to generalize deep neural models to new or non-traditional domains.</p><p>It is well known that training a deep neural model typically requires a large amount of labeled data, which cannot be satisfied in many scenarios due to the high cost of labeling training data. To reduce the amount of data needed for training, a recent surge of research interest has focused on fewshot learning <ref type="bibr" target="#b5">(Lake, Salakhutdinov, and Tenenbaum 2015;</ref><ref type="bibr" target="#b6">Rezende et al. 2016</ref>) -to learn a classification model with very few examples from each class. Closely related to fewshot learning is semi-supervised learning, where a large amount of unlabeled data can be utilized to train with typically a small amount of labeled data.</p><p>Many researches have shown that leveraging unlabeled data in training can improve learning accuracy significantly if used properly <ref type="bibr" target="#b13">(Zhu and Goldberg 2009)</ref>. The key issue is to maximize the effective utilization of structural and feature information of unlabeled data. Due to the powerful feature extraction capability and recent success of deep neural networks, there have been some successful attempts to revisit semi-supervised learning with neural-network-based models, including ladder network <ref type="bibr" target="#b6">(Rasmus et al. 2015)</ref>, semi-supervised embedding <ref type="bibr" target="#b9">(Weston et al. 2008)</ref>, planetoid <ref type="bibr" target="#b10">(Yang, Cohen, and Salakhutdinov 2016)</ref>, and graph convolutional networks <ref type="bibr" target="#b5">(Kipf and Welling 2017)</ref>.</p><p>The recently developed graph convolutional neural networks (GCNNs) <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016)</ref> is a successful attempt of generalizing the powerful convolutional neural networks (CNNs) in dealing with Euclidean data to modeling graph-structured data. In their pilot work (Kipf and Welling 2017), Kipf and Welling proposed a simplified type of GCNNs, called graph convolutional networks (GCNs), and applied it to semi-supervised classification. The GCN model naturally integrates the connectivity patterns and feature attributes of graph-structured data, and outperforms many state-of-the-art methods significantly on some benchmarks. Nevertheless, it suffers from similar problems faced by other neural-network-based models. The working mechanisms of the GCN model for semisupervised learning are not clear, and the training of GCNs still requires considerable amount of labeled data for parameter tuning and model selection, which defeats the purpose for semi-supervised learning.</p><p>In this paper, we demystify the GCN model for semisupervised learning. In particular, we show that the graph convolution of the GCN model is simply a special form of Laplacian smoothing, which mixes the features of a vertex and its nearby neighbors. The smoothing operation makes the features of vertices in the same cluster similar, thus greatly easing the classification task, which is the key reason why GCNs work so well. However, it also brings potential concerns of over-smoothing. If a GCN is deep with many convolutional layers, the output features may be oversmoothed and vertices from different clusters may become indistinguishable. The mixing happens quickly on small datasets with only a few convolutional layers, as illustrated by <ref type="figure" target="#fig_1">Fig. 2</ref>. Also, adding more layers to a GCN will make it much more difficult to train.</p><p>However, a shallow GCN model such as the two-layer GCN used in (Kipf and Welling 2017) has its own limits. Besides that it requires many additional labels for validation, it also suffers from the localized nature of the convolutional filter. When only few labels are given, a shallow GCN cannot effectively propagate the labels to the entire data graph. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the performance of GCNs drops quickly as the training size shrinks, even for the one with 500 additional labels for validation.</p><p>To overcome the limits and realize the full potentials of the GCN model, we propose a co-training approach and a self-training approach to train GCNs. By co-training a GCN with a random walk model, the latter could complement the former in exploring global graph topology. By self-training a GCN, we can exploit its feature extraction capability to overcome its localized nature. Combining both the co-training and self-training approaches can substantially improve the GCN model for semi-supervised learning with very few labels, and exempt it from requiring additional labeled data for validation. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, our method outperforms GCNs by a large margin.</p><p>In a nutshell, the key innovations of this paper are: 1) providing new insights and analysis of the GCN model for semi-supervised learning; 2) proposing solutions to improve the GCN model for semi-supervised learning. The rest of the paper is organized as follows. Section 2 introduces the preliminaries and related works. In Section 3, we analyze the mechanisms and fundamental limits of the GCN model for semi-supervised learning. In Section 4, we propose our methods to improve the GCN model. In Section 5, we conduct experiments to verify our analysis and proposals. Finally, Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Related Works</head><p>First, let us define some notations used throughout this paper. A graph is represented by G = (V, E), where V is the vertex set with |V| = n and E is the edge set. In this paper, we consider undirected graphs. Denote by A = [a ij ] ∈ R n×n the adjacency matrix which is nonnegative. Denote by D = diag(d 1 , d 2 , . . . , d n ) the degree matrix of A where d i = j a ij is the degree of vertex i. The graph Laplacian <ref type="bibr" target="#b3">(Chung 1997</ref>) is defined as L := D − A, and the two versions of normalized graph Laplacians are defined as L sym := D − 1 2 LD − 1 2 and L rw := D −1 L respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-Based Semi-Supervised Learning</head><p>The problem we consider in this paper is semi-supervised classification on graphs. Given a graph G = (V, E, X), where X = [x 1 , x 2 , · · · , x n ] ∈ R n×c is the feature matrix, and x i ∈ R c is the c-dimensional feature vector of vertex i. Suppose that the labels of a set of vertices V l are given, the goal is to predict the labels of the remaining vertices V u . Graph-based semi-supervised learning has been a popular research area in the past two decades. By exploiting the graph or manifold structure of data, it is possible to learn with very few labels. Many graph-based semi-supervised learning methods make the cluster assumption <ref type="bibr" target="#b2">(Chapelle and Zien 2005)</ref>, which assumes that nearby vertices on a graph tend to share the same label. Researches along this line include min-cuts (Blum and Chawla 2001) and randomized min-cuts <ref type="bibr" target="#b0">(Blum et al. 2004)</ref>, spectral graph transducer <ref type="bibr" target="#b5">(Joachims 2003)</ref>, label propagation <ref type="bibr" target="#b13">(Zhu, Ghahramani, and Lafferty 2003)</ref> and its variants <ref type="bibr" target="#b12">(Zhou et al. 2004;</ref><ref type="bibr" target="#b0">Bengio, Delalleau, and Le Roux 2006)</ref>, modified adsorption (Talukdar and Crammer 2009), and iterative classification algorithm <ref type="bibr" target="#b7">(Sen et al. 2008)</ref>.</p><p>But the graph only represents the structural information of data. In many applications, data instances come with feature vectors containing information not present in the graph. For example, in a citation network, the citation links between documents describe their citation relations, while the documents are represented as bag-of-words vectors which describe their contents. Many semi-supervised learning methods seek to jointly model the graph structure and feature attributes of data. A common idea is to regularize a supervised learner with some regularizer. For example, manifold regularization (LapSVM) <ref type="bibr" target="#b0">(Belkin, Niyogi, and Sindhwani 2006)</ref> regularizes a support vector machine with a Laplacian regularizer. Deep semi-supervised embedding <ref type="bibr" target="#b9">(Weston et al. 2008</ref>) regularizes a deep neural network with an embedding-based regularizer. Planetoid <ref type="bibr" target="#b10">(Yang, Cohen, and Salakhutdinov 2016)</ref> also regularizes a neural network by jointly predicting the class label and the context of an instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolutional Networks</head><p>Graph convolutional neural networks (GCNNs) generalize traditional convolutional neural networks to the graph domain. There are mainly two types of GCNNs <ref type="bibr" target="#b0">(Bronstein et al. 2017)</ref>: spatial GCNNs and spectral GCNNs. Spatial GCNNs view the convolution as "patch operator" which constructs a new feature vector for each vertex using its neighborhood information. Spectral GCNNs define the convolution by decomposing a graph signal s ∈ R n (a scalar for each vertex) on the spectral domain and then applying a spectral filter g θ (a function of eigenvalues of L sym ) on the spectral components <ref type="bibr" target="#b1">(Bruna et al. 2014;</ref><ref type="bibr" target="#b6">Sandryhaila and Moura 2013;</ref><ref type="bibr" target="#b7">Shuman et al. 2013</ref>). However this model requires explicitly computing the Laplacian eigenvectors, which is impractical for real large graphs. A way to circumvent this problem is by approximating the spectral filter g θ with Chebyshev polynomials up to K th order <ref type="bibr" target="#b3">(Hammond, Vandergheynst, and Gribonval 2011)</ref>. In (Defferrard, Bresson, and Vandergheynst 2016), Defferrard et al. applied this to build a K-localized ChebNet, where the convolution is defined as:</p><formula xml:id="formula_0">g θ s ≈ K k=0 θ k T k (L sym )s,<label>(1)</label></formula><p>where s ∈ R n is the signal on the graph, g θ is the spectral filter, denotes the convolution operator, T k is the Chebyshev polynomials, and θ ∈ R K is a vector of Chebyshev coefficients. By the approximation, the ChebNet is actually spectrum-free.</p><p>In (Kipf and Welling 2017), Kipf and Welling simplified this model by limiting K = 1 and approximating the largest eigenvalue λ max of L sym by 2. In this way, the convolution becomes</p><formula xml:id="formula_1">g θ s = θ I + D − 1 2 AD − 1 2 s,<label>(2)</label></formula><p>where θ is the only Chebyshev coefficient left. They further applied a normalization trick to the convolution matrix:</p><formula xml:id="formula_2">I + D − 1 2 AD − 1 2 →D − 1 2ÃD − 1 2 ,<label>(3)</label></formula><formula xml:id="formula_3">whereÃ = A + I andD = jÃ ij .</formula><p>Generalizing the above definition of convolution to a graph signal with c input channels, i.e., X ∈ R n×c (each vertex is associated with a c-dimensional feature vector), and using f spectral filters, the propagation rule of this simplified model is:</p><formula xml:id="formula_4">H (l+1) = σ D − 1 2ÃD − 1 2 H (l) Θ (l) ,<label>(4)</label></formula><p>where H (l) is the matrix of activations in the l-th layer, and H (0) = X, Θ (l) ∈ R c×f is the trainable weight matrix in layer l, σ is the activation function, e.g., ReLU (·) = max(0, ·). This simplified model is called graph convolutional networks (GCNs), which is the focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-Supervised Classification with GCNs</head><p>In (Kipf and Welling 2017), the GCN model was applied for semi-supervised classification in a neat way. The model used is a two-layer GCN which applies a softmax classifier on the output features:</p><formula xml:id="formula_5">Z = softmax Â ReLU Â XΘ (0) Θ (1) ,<label>(5)</label></formula><p>whereÂ</p><formula xml:id="formula_6">=D − 1 2ÃD − 1 2 , softmax(x i ) = 1 Z exp(x i ) with Z = i exp(x i ).</formula><p>The loss function is defined as the crossentropy error over all labeled examples: where V l is the set of indices of labeled vertices and F is the dimension of the output features, which is equal to the number of classes. Y ∈ R |V l |×F is a label indicator matrix. The weight parameters Θ (0) and Θ (1) can be trained via gradient descent.</p><formula xml:id="formula_7">L := − i∈V l F f =1 Y if ln Z if ,<label>(6)</label></formula><p>The GCN model naturally combines graph structures and vertex features in the convolution, where the features of unlabeled vertices are mixed with those of nearby labeled vertices, and propagated over the graph through multiple layers. It was reported in (Kipf and Welling 2017) that GCNs outperformed many state-of-the-art methods significantly on some benchmarks such as citation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis</head><p>Despite its promising performance, the mechanisms of the GCN model for semi-supervised learning have not been made clear. In this section, we take a closer look at the GCN model, analyze why it works, and point out its limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why GCNs Work</head><p>To understand the reasons why GCNs work so well, we compare them with the simplest fully-connected networks (FCNs), where the layer-wise propagation rule is</p><formula xml:id="formula_8">H (l+1) = σ H (l) Θ (l) .<label>(7)</label></formula><p>Clearly the only difference between a GCN and a FCN is the graph convolution matrixÂ =D − 1 2ÃD − 1 2 (Eq. (5)) applied on the left of the feature matrix X. To see the impact of the graph convolution, we tested the performances of GCNs and FCNs for semi-supervised classification on the Cora citation network with 20 labels in each class. The results can be seen in <ref type="table" target="#tab_0">Table 1</ref>. Surprisingly, even a one-layer GCN outperformed a one-layer FCN by a very large margin.</p><p>Laplacian Smoothing. Let us first consider a one-layer GCN. It actually contains two steps. 1) Generating a new feature matrix Y from X by applying the graph convolution:</p><formula xml:id="formula_9">Y =D −1/2ÃD−1/2 X.<label>(8)</label></formula><p>2) Feeding the new feature matrix Y to a fully connected layer. Clearly the graph convolution is the key to the huge performance gain. Let us examine the graph convolution carefully. Suppose that we add a self-loop to each vertex in the graph, then the adjacency matrix of the new graph isÃ = A + I. The Laplacian smoothing <ref type="bibr" target="#b8">(Taubin 1995)</ref> on each channel of the input features is defined as: where 0 &lt; γ ≤ 1 is a parameter which controls the weighting between the features of the current vertex and the features of its neighbors. We can write the Laplacian smoothing in matrix form:</p><formula xml:id="formula_10">y i = (1 − γ)x i + γ jã ij d i x j (for 1 ≤ i ≤ n),<label>(9)</label></formula><formula xml:id="formula_11">Y = X − γD −1L X = (I − γD −1L )X,<label>(10)</label></formula><p>whereL =D −Ã. By letting γ = 1, i.e., only using the neighbors' features, we haveŶ =D −1Ã X, which is the standard form of Laplacian smoothing. Now if we replace the normalized LaplacianD −1L with the symmetrically normalized LaplacianD − 1 2LD − 1 2 and let γ = 1, we haveŶ =D −1/2ÃD−1/2 X, which is exactly the graph convolution in Eq. (8). We thus call the graph convolution a special form of Laplacian smoothing -symmetric Laplacian smoothing. Note that here the smoothing still includes the current vertex's features, as each vertex has a self-loop and is its own neighbor.</p><p>The Laplacian smoothing computes the new features of a vertex as the weighted average of itself and its neighbors'. Since vertices in the same cluster tend to be densely connected, the smoothing makes their features similar, which makes the subsequent classification task much easier. As we can see from <ref type="table" target="#tab_0">Table 1</ref>, applying the smoothing only once has already led to a huge performance gain.</p><p>Multi-layer Structure. We can also see from <ref type="table" target="#tab_0">Table 1</ref> that while the 2-layer FCN only slightly improves over the 1layer FCN, the 2-layer GCN significantly improves over the 1-layer GCN by a large margin. This is because applying smoothing again on the activations of the first layer makes the output features of vertices in the same cluster more similar and further eases the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>When GCNs Fail</head><p>We have shown that the graph convolution is essentially a type of Laplacian smoothing. A natural question is how many convolutional layers should be included in a GCN? Certainly not the more the better. On the one hand, a GCN with many layers is difficult to train. On the other hand, repeatedly applying Laplacian smoothing may mix the features of vertices from different clusters and make them indistinguishable. In the following, we illustrate this point with a popular dataset.</p><p>We apply GCNs with different number of layers on the Zachary's karate club dataset <ref type="bibr" target="#b11">(Zachary 1977)</ref>, which has 34 vertices of two classes and 78 edges. The GCNs are untrained with the weight parameters initialized randomly as in <ref type="bibr" target="#b3">(Glorot and Bengio 2010)</ref>. The dimension of the hidden layers is 16, and the dimension of the output layer is 2. The feature vector of each vertex is a one-hot vector. The outputs of each GCN are plotted as two-dimensional points in <ref type="figure" target="#fig_1">Fig. 2</ref>. We can observe the impact of the graph convolution (Laplacian smoothing) on this small dataset. Applying the smoothing once, the points are not well-separated ( <ref type="figure" target="#fig_1">Fig. 2a</ref>). Applying the smoothing twice, the points from the two classes are separated relatively well. Applying the smoothing again and again, the points are mixed <ref type="figure" target="#fig_1">(Fig. 2c, 2d, 2e</ref>). As this is a small dataset and vertices between two classes have quite a number of connections, the mixing happens quickly.</p><p>In the following, we will prove that by repeatedly applying Laplacian smoothing many times, the features of vertices within each connected component of the graph will converge to the same values. For the case of symmetric Laplacian smoothing, they will converge to be proportional to the square root of the vertex degree.</p><p>Suppose that a graph G has k connected components {C i } k i=1 , and the indication vector for the i-th component is denoted by 1 (i) ∈ R n . This vector indicates whether a vertex is in the component C i , i.e.,</p><formula xml:id="formula_12">1 (i) j = 1, v j ∈ C i 0, v j ∈ C i<label>(11)</label></formula><p>Theorem 1. If a graph has no bipartite components, then for any w ∈ R n , and α ∈ (0, 1],</p><formula xml:id="formula_13">lim m→+∞ (I − αL rw ) m w = [1 (1) , 1 (2) , . . . , 1 (k) ]θ 1 , lim m→+∞ (I − αL sym ) m w = D − 1 2 [1 (1) , 1 (2) , . . . , 1 (k) ]θ 2 ,</formula><p>where θ 1 ∈ R k , θ 2 ∈ R k , i.e., they converge to a linear combination of</p><formula xml:id="formula_14">{1 (i) } k i=1 and {D − 1 2 1 (i) } k i=1 respectively.</formula><p>Proof. L rw and L sym have the same n eigenvalues (by multiplicity) with different eigenvectors (Von Luxburg 2007). If a graph has no bipartite components, the eigenvalues all fall in [0,2) <ref type="bibr" target="#b3">(Chung 1997)</ref>. The eigenspaces of L rw and L sym corresponding to eigenvalue 0 are spanned by <ref type="bibr" target="#b9">Von Luxburg 2007)</ref>. For α ∈ (0, 1], the eigenvalues of (I − αL rw ) and (I − αL sym ) all fall into (-1,1], and the eigenspaces of eigenvalue 1 are spanned by {1 (i) } k i=1 and {D − 1 2 1 (i) } k i=1 respectively. Since the absolute value of all eigenvalues of (I − αL rw ) and (I − αL sym ) are less than or equal to 1, after repeatedly multiplying them from the left, the result will converge to the linear combination of eigenvectors of eigenvalue 1, i.e. the linear combination of</p><formula xml:id="formula_15">{1 (i) } k i=1 and {D − 1 2 1 (i) } k i=1 respectively (</formula><formula xml:id="formula_16">{1 (i) } k i=1 and {D − 1 2 1 (i) } k i=1 respec- tively.</formula><p>Note that since an extra self-loop is added to each vertex, there is no bipartite component in the graph. Based on the above theorem, over-smoothing will make the features indistinguishable and hurt the classification accuracy.</p><p>The above analysis raises potential concerns about stacking many convolutional layers in a GCN. Besides, a deep GCN is much more difficult to train. In fact, the GCN used in (Kipf and Welling 2017) is a 2-layer GCN. However, since the graph convolution is a localized filter -a linear combination of the feature vectors of adjacent neighbors, a shallow GCN cannot sufficiently propagate the label information to the entire graph with only a few labels. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the performance of GCNs (with or without validation) drops quickly as the training size shrinks. In fact, the accuracy of GCNs decreases much faster than the accuracy of label propagation. Since label propagation only uses the graph information while GCNs utilize both structural and vertex features, it reflects the inability of the GCN model in exploring the global graph structure.</p><p>Another problem with the GCN model in (Kipf and Welling 2017) is that it requires an additional validation set for early stopping in training, which is essentially using the prediction accuracy on the validation set for model selection.</p><p>If we optimize a GCN on the training data without using the validation set, it will have a significant drop in performance. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the performance of the GCN without validation drops much sharper than the GCN with validation. In (Kipf and Welling 2017), the authors used an additional set of 500 labeled data for validation, which is much more than the total number of training data. This is certainly undesirable as it defeats the purpose of semi-supervised learning. Furthermore, it makes the comparison of GCNs with other methods unfair as other methods such as label propagation may not need the validation data at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Solutions</head><p>We summarize the advantages and disadvantages of the GCN model as follows. The advantages are: 1) the graph convolution -Laplacian smoothing helps making the classification problem much easier; 2) the multi-layer neural network is a powerful feature extractor. The disadvantages are: 1) the graph convolution is a localized filter, which performs unsatisfactorily with few labeled data; 2) the neural network needs considerable amount of labeled data for validation and model selection.</p><p>We want to make best use of the advantages of the GCN model while overcoming its limits. This naturally leads to a co-training (Blum and Mitchell 1998) idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Co-Train a GCN with a Random Walk Model</head><p>We propose to co-train a GCN with a random walk model as the latter can explore the global graph structure, which complements the GCN model. In particular, we first use a random walk model to find the most confident vertices -the nearest neighbors to the labeled vertices of each class, and then add them to the label set to train a GCN. Unlike in (Kipf and Welling 2017), we directly optimize the parameters of a GCN on the training set, without requiring additional labeled data for validation. Add them to the training set with label k 6: end for</p><p>We choose to use the partially absorbing random walks (ParWalks) <ref type="bibr" target="#b9">(Wu et al. 2012)</ref> as our random walk model. A partially absorbing random walk is a second-order Markov chain with partial absorption at each state. It was shown in (Wu, Li, and Chang 2013) that with proper absorption settings, the absorption probabilities can well capture the global graph structure. Importantly, the absorption probabilities can be computed in a closed-form by solving a simple linear system, and can be fast approximated by random walk sampling or scaled up on top of vertex-centric graph engines <ref type="bibr" target="#b3">(Guo et al. 2017)</ref>.</p><p>The algorithm to expand the training set is described in Algorithm 1. First, we calculate the normalized absorption probability matrix P = (L + αΛ) −1 (the choice of Λ may depend on data). P i,j is the probability of a random walk from vertex i being absorbed by vertex j, which represents how likely i and j belong to the same class. Second, we need to measure the confidence of a vertex belonging to class k. We partition the labeled vertices into S 1 , S 2 , ..., where S k denotes the set of labeled data of class k. For each class k, we calculate a confidence vector p = j∈S k P :,j , where p ∈ R n and p i is the confidence of vertex i belonging to class k. Finally, we find the t most confident vertices and add them to the training set with label k to train a GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN Self-Training</head><p>Another way to make a GCN "see" more training examples is to self-train a GCN. Specifically, we first train a GCN with given labels, then select the most confident predictions for each class by comparing the softmax scores, and add them to the label set. We then continue to train the GCN with the expanded label set, using the pre-trained GCN as initialization. This is described in Algorithm 2.</p><p>The most confident instances found by the GCN are supposed to share similar (but not the same) features with the labeled data. Adding them to the labeled set will help training a more robust and accurate classifier. Furthermore, it complements the co-training method in the situation that a graph has many isolated small components and it is not possible to propagate labels with random walks.</p><p>Combine Co-Training and Self-Training. To improve the diversity of labels and train a more robust classifier, we Algorithm 2 Expand the Label Set via Self-Training 1: Z := GCN (X) ∈ R n×F , the output of GCN 2: for each class k do 3:</p><p>Find the top t vertices in Z i,k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Add them to the training set with label k 5: end for propose to combine co-training and self-learning. Specifically, we expand the label set with the most confident predictions found by the random walk and those found by the GCN itself, and then use the expanded label set to continue to train the GCN. We call this method "Union". To find more accurate labels to add to the labeled set, we also propose to add the most confident predictions found by both the random walk and the GCN. We call this method "Intersection".</p><p>Note that we optimize all our methods on the expanded label set, without requiring any additional validation data. As long as the expanded label set contains enough correct labels, our methods are expected to train a good GCN classifier. But how much labeled data does it require to train a GCN? Suppose that the number of layers of the GCN is τ , and the average degree of the underlying graph isd. We propose to estimate the lower bound of the number of labels η = |V l | by solving (d) τ * η ≈ n. The rationale behind this is to estimate how many labels are needed to for a GCN with τ layers to propagate them to cover the entire graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we conduct extensive experiments on real benchmarks to verify our theory and the proposed methods, including Co-Training, Self-Training, Union, and Intersection (see Section 4).</p><p>We compare our methods with several state-of-the-art methods, including GCN with validation (GCN+V); GCN without validation (GCN-V); GCN with Chebyshev filter (Cheby) (Kipf and Welling 2017); label propagation using ParWalks (LP) <ref type="bibr" target="#b9">(Wu et al. 2012)</ref>; Planetoid <ref type="bibr" target="#b10">(Yang, Cohen, and Salakhutdinov 2016)</ref>; DeepWalk <ref type="bibr" target="#b6">(Perozzi, Al-Rfou, and Skiena 2014)</ref>; manifold regularization (ManiReg) <ref type="bibr" target="#b0">(Belkin, Niyogi, and Sindhwani 2006)</ref>; semi-supervised embedding (SemiEmb) <ref type="bibr" target="#b9">(Weston et al. 2008)</ref>; iterative classification algorithm (ICA) <ref type="bibr" target="#b7">(Sen et al. 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>We conduct experiments on three commonly used citation networks: CiteSeer, Cora, and PubMed <ref type="bibr" target="#b7">(Sen et al. 2008)</ref>. The statistics of the datasets are summarized in <ref type="table" target="#tab_1">Table 2</ref>. On each dataset, a document is described by a bag-ofwords feature vector, i.e., a 0/1-valued vector indicating the absence/presence of a certain word. The citation links between documents are described by a 0/1-valued adjacency matrix. The datasets we use for testing are provided by the authors of <ref type="bibr" target="#b10">(Yang, Cohen, and Salakhutdinov 2016)</ref> and <ref type="bibr" target="#b5">(Kipf and Welling 2017)</ref>.</p><p>For ParWalks, we set Λ = I, and α = 10 −6 , following Wu et al.. For GCNs, we use the same hyper-parameters as  <ref type="formula" target="#formula_0">(1)</ref>). We test these methods with 0.5%, 1%, 2%, 3%, 4%, 5% training size on Cora and CiteSeer, and with 0.03%, 0.05%, 0.1%, 0.3% training size on PubMed.</p><p>We choose these labeling rates for easy comparison with (Kipf and Welling 2017), <ref type="bibr" target="#b10">(Yang, Cohen, and Salakhutdinov 2016)</ref>, and other methods. We report the mean accuracy of 50 runs except for the results on PubMed <ref type="bibr" target="#b10">(Yang, Cohen, and Salakhutdinov 2016)</ref>, which are averaged over 10 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Analysis</head><p>The classification results are summarized in <ref type="table" target="#tab_2">Table 3</ref>, 4 and 5, where the highest accuracy in each column is highlighted in bold and the top 3 are underlined. Our methods are displayed at the bottom half of each table.</p><p>We can see that the performance of Co-Training is closely related to the performance of LP. If the data has strong manifold structure, such as PubMed, Co-Training performs the best. In contrast, Self-Training is the worst on PubMed, as it does not utilize the graph structure. But Self-Training does well on CiteSeer where Co-Training is overall the worst. Intersection performs better when the training size is relatively large, because it filters out many labels. Union performs best in many cases since it adds more diverse labels to the training set.</p><p>Comparison with GCNs. At a glance, we can see that on each dataset, our methods outperform others by a large margin in most cases. When the training size is small, all our methods are far better than GCN-V, and much better than GCN+V in most cases. For example, with labeling rate 1% on Cora and CiteSeer, our methods improve over GCN-V by 23% and 28%, and improve over GCN+V by 12% and 7%. With labeling rate 0.05% on PubMed, our methods improve over GCN-V and GCN+V by 37% and 18% respectively. This verifies our analysis that the GCN model cannot effectively propagate labels to the entire graph with small training size. When the training size grows, our methods are still better than GCN+V in most cases, demonstrating the effectiveness of our approaches. When the training size is large enough, our methods and GCNs perform similarly, indicating that the given labels are sufficient for training a good GCN classifier. Cheby does not perform well in most cases, which is probably due to overfitting.  Comparison with other methods. We compare our methods with other state-of-the-art methods in <ref type="table">Table 6</ref>. The experimental setup is the same except that for every dataset, we sample 20 labels for each class, which corresponds to the total labeling rate of 3.6% on CiteSeer, 5.1% on Cora, and 0.3% on PubMed. The results of other baselines are copied from (Kipf and Welling 2017). Our methods perform similarly as GCNs and outperform other baselines significantly. Although we did not directly compare with other baselines, we can see from <ref type="table" target="#tab_2">Table 3</ref>, 4 and 5 that our methods with much fewer labels already outperform many baselines. For example, our method Union on Cora <ref type="table" target="#tab_2">(Table 3)</ref> with 2% labeling rate (54 labels) beats all other baselines with 140 labels ( <ref type="table">Table 6)</ref>.</p><p>Influence of the Parameters. A common parameter of our methods is the number of newly added labels. Adding too many labels will introduce noise, but with too few labels we cannot train a good GCN classifier. As described in the end of Section 4, we can estimate the lower bound of the total number of labels η needed to train a GCN by solving (d) τ * η ≈ n. We use 3η in our experiments. Actually, we found that 2η, 3η and 4η perform similarly in the experiments. We follow Kipf and Welling to set the number of convolutional layers as 2. We also observed in the experiments that 2-layer GCNs performed the best. When the number of convolutional layers grows, the classification accuracy decreases drastically, which is probably due to overfitting.</p><p>Computational Cost. For Co-Training, the overhead is the computational cost of the random walk model, which requires solving a sparse linear system. In our experiments, the time is negligible on Cora and CiteSeer as there are only a few thousand vertices. On PubMed, it takes less than 0.38 seconds in MatLab R2015b. As mentioned in Section 4, the computation can be further speeded up using vertex-centric graph engines <ref type="bibr" target="#b3">(Guo et al. 2017)</ref>, so the scalability of our method is not an issue. For Self-Training, we only need to run a few epochs in addition to training a GCN. It converges fast as it builds on a pre-trained GCN. Hence, the running time of Self-Training is comparable to a GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Understanding deep neural networks is crucial for realizing their full potentials in real applications. This paper contributes to the understanding of the GCN model and its application in semi-supervised classification. Our analysis not only reveals the mechanisms and limitations of the GCN model, but also leads to new solutions overcoming its limits.</p><p>In future work, we plan to develop new convolutional filters which are compatible with deep architectures, and exploit advanced deep learning techniques to improve the performance of GCNs for more graph-based applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Performance comparison of GCNs, label propagation, and our method for semi-supervised classification on the Cora citation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Vertex embeddings of Zachary's karate club network with GCNs with 1,2,3,4,5 layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Expand the Label Set via ParWalks 1: P := (L + αΛ) −1 2: for each class k do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>GCNs vs. Fully-connected networks</figDesc><table><row><cell>One-layer</cell><cell>Two-layer</cell><cell>One-layer</cell><cell>Two-layer</cell></row><row><cell>FCN</cell><cell>FCN</cell><cell>GCN</cell><cell>GCN</cell></row><row><cell cols="4">0.530860 0.559260 0.707940 0.798361</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset statisticsDataset Nodes Edges Classes Features</figDesc><table><row><cell>CiteSeer</cell><cell>3327</cell><cell>4732</cell><cell>6</cell><cell>3703</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>5429</cell><cell>7</cell><cell>1433</cell></row><row><cell cols="3">PubMed 19717 44338</cell><cell>3</cell><cell>500</cell></row><row><cell cols="5">in (Kipf and Welling 2017): a learning rate of 0.01, 200 max-</cell></row><row><cell cols="5">imum epochs, 0.5 dropout rate, 5 × 10 −4 L2 regularization</cell></row><row><cell cols="5">weight, 2 convolutional layers, and 16 hidden units, which</cell></row><row><cell cols="5">are validated on Cora by Kipf and Welling. For each run,</cell></row><row><cell cols="5">we randomly split labels into a small set for training, and a</cell></row><row><cell cols="5">set with 1000 samples for testing. For GCN+V, we follow</cell></row><row><cell cols="5">(Kipf and Welling 2017) to sample additional 500 labels for</cell></row><row><cell cols="5">validation. For GCN-V, we simply optimize the GCN using</cell></row><row><cell cols="5">training accuracy. For Cheby, we set the polynomial degree</cell></row><row><cell>K = 2 (see Eq.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification Accuracy On Cora</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Label Rate</cell><cell>0.5%</cell><cell>1%</cell><cell>2%</cell><cell>3%</cell><cell>4%</cell><cell>5%</cell></row><row><cell>LP</cell><cell cols="6">56.4 62.3 65.4 67.5 69.0 70.2</cell></row><row><cell>Cheby</cell><cell cols="6">38.0 52.0 62.4 70.8 74.1 77.6</cell></row><row><cell>GCN-V</cell><cell cols="6">42.6 56.9 67.8 74.9 77.6 79.3</cell></row><row><cell>GCN+V</cell><cell cols="6">50.9 62.3 72.2 76.5 78.4 79.7</cell></row><row><cell>Co-training</cell><cell cols="6">56.6 66.4 73.5 75.9 78.9 80.8</cell></row><row><cell cols="7">Self-training 53.7 66.1 73.8 77.2 79.4 80.0</cell></row><row><cell>Union</cell><cell cols="6">58.5 69.9 75.9 78.5 80.4 81.7</cell></row><row><cell>Intersection</cell><cell cols="6">49.7 65.0 72.9 77.1 79.4 80.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Classification Accuracy on CiteSeer</figDesc><table><row><cell></cell><cell></cell><cell cols="2">CiteSeer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Label Rate</cell><cell>0.5%</cell><cell>1%</cell><cell>2%</cell><cell>3%</cell><cell>4%</cell><cell>5%</cell></row><row><cell>LP</cell><cell cols="6">34.8 40.2 43.6 45.3 46.4 47.3</cell></row><row><cell>Cheby</cell><cell cols="6">31.7 42.8 59.9 66.2 68.3 69.3</cell></row><row><cell>GCN-V</cell><cell cols="6">33.4 46.5 62.6 66.9 68.4 69.5</cell></row><row><cell>GCN+V</cell><cell cols="6">43.6 55.3 64.9 67.5 68.7 69.6</cell></row><row><cell>Co-training</cell><cell cols="6">47.3 55.7 62.1 62.5 64.5 65.5</cell></row><row><cell cols="7">Self-training 43.3 58.1 68.2 69.8 70.4 71.0</cell></row><row><cell>Union</cell><cell cols="6">46.3 59.1 66.7 66.7 67.6 68.2</cell></row><row><cell>Intersection</cell><cell cols="6">42.9 59.1 68.6 70.1 70.8 71.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Classification Accuracy On PubMed</figDesc><table><row><cell></cell><cell cols="2">PubMed</cell><cell></cell><cell></cell></row><row><cell>Label Rate</cell><cell cols="4">0.03% 0.05% 0.1% 0.3%</cell></row><row><cell>LP</cell><cell>61.4</cell><cell>66.4</cell><cell>65.4</cell><cell>66.8</cell></row><row><cell>Cheby</cell><cell>40.4</cell><cell>47.3</cell><cell>51.2</cell><cell>72.8</cell></row><row><cell>GCN-V</cell><cell>46.4</cell><cell>49.7</cell><cell>56.3</cell><cell>76.6</cell></row><row><cell>GCN+V</cell><cell>60.5</cell><cell>57.5</cell><cell>65.9</cell><cell>77.8</cell></row><row><cell>Co-training</cell><cell>62.2</cell><cell>68.3</cell><cell>72.7</cell><cell>78.2</cell></row><row><cell>Self-training</cell><cell>51.9</cell><cell>58.7</cell><cell>66.8</cell><cell>77.0</cell></row><row><cell>Union</cell><cell>58.4</cell><cell>64.0</cell><cell>70.7</cell><cell>79.2</cell></row><row><cell>Intersection</cell><cell>52.0</cell><cell>59.3</cell><cell>69.4</cell><cell>77.6</cell></row><row><cell cols="5">Table 6: Accuracy under 20 Labels per Class</cell></row><row><cell>Method</cell><cell cols="4">CiteSeer Cora Pubmed</cell></row><row><cell>ManiReg</cell><cell>60.1</cell><cell>59.5</cell><cell></cell><cell>70.7</cell></row><row><cell>SemiEmb</cell><cell>59.6</cell><cell>59.0</cell><cell></cell><cell>71.7</cell></row><row><cell>LP</cell><cell>45.3</cell><cell>68.0</cell><cell></cell><cell>63.0</cell></row><row><cell>DeepWalk</cell><cell>43.2</cell><cell>67.2</cell><cell></cell><cell>65.3</cell></row><row><cell>ICA</cell><cell>69.1</cell><cell>75.1</cell><cell></cell><cell>73.9</cell></row><row><cell>Planetoid</cell><cell>64.7</cell><cell>75.7</cell><cell></cell><cell>77.2</cell></row><row><cell>GCN-V</cell><cell>68.1</cell><cell>80.0</cell><cell></cell><cell>78.2</cell></row><row><cell>GCN+V</cell><cell>68.9</cell><cell>80.3</cell><cell></cell><cell>79.1</cell></row><row><cell>Co-training</cell><cell>64.0</cell><cell>79.6</cell><cell></cell><cell>77.1</cell></row><row><cell>Self-training</cell><cell>67.8</cell><cell>80.2</cell><cell></cell><cell>76.9</cell></row><row><cell>Union</cell><cell>65.7</cell><cell>80.5</cell><cell></cell><cell>78.3</cell></row><row><cell>Intersection</cell><cell>69.9</cell><cell>79.8</cell><cell></cell><cell>77.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research received support from the grant 1-ZVJJ funded by the Hong Kong Polytechnic University. The authors would like to thank the reviewers for their insightful comments and useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyogi</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sindhwani] Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delalleau</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le Roux]</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rwebangira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th annual conference on Computational learning theory</title>
		<meeting>the 11th annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="18" to="42" />
		</imprint>
	</monogr>
	<note>IEEE Signal Processing Magazine</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semisupervised classification by low density separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zien] Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
	<note type="report_type">Max-Planck-Gesellschaft</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bresson</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vandergheynst] Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vandergheynst</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="636" to="648" />
		</imprint>
	</monogr>
	<note>International Conference on Database Systems for Advanced Applications</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Humanlevel concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Welling] Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salakhutdinov</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
		</imprint>
	</monogr>
	<note>Semisupervised classification with graph convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Rfou</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1644" to="1656" />
		</imprint>
	</monogr>
	<note>Discrete signal processing on graphs. IEEE transactions on signal processing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="442" to="457" />
		</imprint>
	</monogr>
	<note>New regularized algorithms for transductive learning</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A signal processing approach to fair surface design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 22nd annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="351" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analyzing the harmonic structure in graph-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Luxburg] Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="3077" to="3085" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cohen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International conference on Machine learning</title>
		<meeting>the 33rd International conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An information flow model for conflict and fission in small groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Zachary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of anthropological research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="452" to="473" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 16</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Goldberg] Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghahramani</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lafferty] Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2003</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
	<note>Introduction to semi-supervised learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
