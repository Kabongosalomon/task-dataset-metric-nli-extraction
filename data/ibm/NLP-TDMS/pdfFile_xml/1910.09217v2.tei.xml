<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2020 DECOUPLING REPRESENTATION AND CLASSIFIER FOR LONG-TAILED RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
							<email>kang@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
							<email>yannisk@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2020 DECOUPLING REPRESENTATION AND CLASSIFIER FOR LONG-TAILED RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The long-tail distribution of the visual world poses great challenges for deep learning based classification models on how to handle the class imbalance problem. Existing solutions usually involve class-balancing strategies, e.g. by loss re-weighting, data re-sampling, or transfer learning from head-to tail-classes, but most of them adhere to the scheme of jointly learning representations and classifiers. In this work, we decouple the learning procedure into representation learning and classification, and systematically explore how different balancing strategies affect them for long-tailed recognition. The findings are surprising: (1) data imbalance might not be an issue in learning high-quality representations; (2) with representations learned with the simplest instance-balanced (natural) sampling, it is also possible to achieve strong long-tailed recognition ability by adjusting only the classifier. We conduct extensive experiments and set new state-of-the-art performance on common long-tailed benchmarks like ImageNet-LT, Places-LT and iNaturalist, showing that it is possible to outperform carefully designed losses, sampling strategies, even complex modules with memory, by using a straightforward approach that decouples representation and classification. Our code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visual recognition research has made rapid advances during the past years, driven primarily by the use of deep convolutional neural networks (CNNs) and large image datasets, most importantly the ImageNet Challenge <ref type="bibr">(Russakovsky et al., 2015)</ref>. Such datasets are usually artificially balanced with respect to the number of instances for each object/class in the training set. Visual phenomena, however, follow a long-tailed distribution that many standard approaches fail to properly model, leading to a significant drop in accuracy. Motivated by this, a number of works have recently emerged that try to study long-tailed recognition, i.e., recognition in a setting where the number of instances in each class highly varies and follows a long-tailed distribution.</p><p>When learning with long-tailed data, a common challenge is that instance-rich (or head) classes dominate the training procedure. The learned classification model tends to perform better on these classes, while performance is significantly worse for instance-scarce (or tail) classes. To address this issue and to improve performance across all classes, one can re-sample the data or design specific loss functions that better facilitate learning with imbalanced data <ref type="bibr" target="#b1">(Chawla et al., 2002;</ref><ref type="bibr" target="#b3">Cui et al., 2019;</ref><ref type="bibr" target="#b0">Cao et al., 2019)</ref>. Another direction is to enhance recognition performance of the tail classes by transferring knowledge from the head classes <ref type="bibr" target="#b16">(Wang et al., 2017;</ref><ref type="bibr" target="#b22">Zhong et al., 2019;</ref><ref type="bibr" target="#b19">Liu et al., 2019)</ref>. Nevertheless, the common belief behind existing approaches is that designing proper sampling strategies, losses, or even more complex models, is useful for learning high-quality representations for long-tailed recognition.</p><p>Most aforementioned approaches thus learn the classifiers used for recognition jointly with the data representations. However, such a joint learning scheme makes it unclear how the long-tailed recognition ability is achieved-is it from learning a better representation or by handling the data imbalance better via shifting classifier decision boundaries? To answer this question, we take one step back and decouple long-tail recognition into representation learning and classification. For learning rep-resentations, the model is exposed to the training instances and trained through different sampling strategies or losses. For classification, upon the learned representations, the model recognizes the long-tailed classes through various classifiers. We evaluate the performance of various sampling and classifier training strategies for long-tailed recognition under both joint and decoupled learning schemes.</p><p>Specifically, we first train models to learn representations with different sampling strategies, including the standard instance-based sampling, class-balanced sampling and a mixture of them. Next, we study three different basic approaches to obtain a classifier with balanced decision boundaries, on top of the learned representations. They are 1) re-training the parametric linear classifier in a class-balancing manner (i.e., re-sampling); 2) non-parametric nearest class mean classifier, which classifies the data based on their closest class-specific mean representations from the training set; and 3) normalizing the classifier weights, which adjusts the weight magnitude directly to be more balanced, adding a temperature to modulate the normalization procedure.</p><p>We conduct extensive experiments to compare the aforementioned instantiations of the decoupled learning scheme with the conventional scheme that jointly trains the classifier and the representations. We also compare to recent, carefully designed and more complex models, including approaches using memory (e.g., <ref type="bibr">OLTR (Liu et al., 2019)</ref>) as well as more sophisticated losses <ref type="bibr" target="#b3">(Cui et al., 2019)</ref>. From our extensive study across three long-tail datasets, ImageNet-LT, Places-LT and iNaturalist, we make the following intriguing observations:</p><p>• We find that decoupling representation learning and classification has surprising results that challenge common beliefs for long-tailed recognition: instance-balanced sampling learns the best and most generalizable representations.</p><p>• It is advantageous in long-tailed recognition to re-adjust the decision boundaries specified by the jointly learned classifier during representation learning: Our experiments show that this can either be achieved by retraining the classifier with class-balanced sampling or by a simple, yet effective, classifier weight normalization which has only a single hyperparameter controlling the "temperature" and which does not require additional training.</p><p>• By applying the decoupled learning scheme to standard networks (e.g., ResNeXt), we achieve significantly higher accuracy than well established state-of-the-art methods (different sampling strategies, new loss designs and other complex modules) on multiple longtailed recognition benchmark datasets, including ImageNet-LT, Places-LT, and iNaturalist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Long-tailed recognition has attracted increasing attention due to the prevalence of imbalanced data in real-world applications <ref type="bibr" target="#b16">(Wang et al., 2017;</ref><ref type="bibr" target="#b23">Zhou et al., 2017;</ref><ref type="bibr">Mahajan et al., 2018;</ref><ref type="bibr" target="#b22">Zhong et al., 2019;</ref><ref type="bibr">Gupta et al., 2019)</ref>. Recent studies have mainly pursued the following three directions:</p><p>Data distribution re-balancing. Along this direction, researchers have proposed to re-sample the dataset to achieve a more balanced data distribution. These methods include over-sampling <ref type="bibr" target="#b1">(Chawla et al., 2002;</ref><ref type="bibr">Han et al., 2005)</ref> for the minority classes (by adding copies of data), undersampling <ref type="bibr" target="#b5">(Drummond et al., 2003)</ref> for the majority classes (by removing data), and class-balanced sampling <ref type="bibr" target="#b13">(Shen et al., 2016;</ref><ref type="bibr">Mahajan et al., 2018)</ref> based on the number of samples for each class.</p><p>Class-balanced Losses. Various methods are proposed to assign different losses to different training samples for each class. The loss can vary at class-level for matching a given data distribution and improving the generalization of tail classes <ref type="bibr" target="#b3">(Cui et al., 2019;</ref><ref type="bibr">Khan et al., 2017;</ref><ref type="bibr" target="#b0">Cao et al., 2019;</ref><ref type="bibr">Khan et al., 2019;</ref><ref type="bibr" target="#b22">Huang et al., 2019)</ref>. A more fine-grained control of the loss can also be achieved at sample level, e.g. with Focal loss <ref type="bibr" target="#b8">(Lin et al., 2017)</ref>, Meta-Weight-Net <ref type="bibr" target="#b14">(Shu et al., 2019)</ref>, re-weighted training <ref type="bibr" target="#b10">(Ren et al., 2018)</ref>, or based on Bayesian uncertainty <ref type="bibr">(Khan et al., 2019)</ref>. <ref type="bibr">Recently, Hayat et al. (2019)</ref> proposed to balance the classification regions of head and tail classes using an affinity measure to enforce cluster centers of classes to be uniformly spaced and equidistant.</p><p>Transfer learning from head-to tail classes. Transfer-learning based methods address the issue of imbalanced training data by transferring features learned from head classes with abundant training instances to under-represented tail classes. Recent work includes transferring the intra-class variance  and transferring semantic deep features <ref type="bibr" target="#b19">(Liu et al., 2019)</ref>. However it is usually a non-trivial task to design specific modules (e.g. external memory) for feature transfer.</p><p>A benchmark for low-shot recognition was proposed by <ref type="bibr">Hariharan &amp; Girshick (2017)</ref> and consists of a representation learning phase without access to the low-shot classes and a subsequent low-shot learning phase. In contrast, the setup for long-tail recognition assumes access to both head and tail classes and a more continuous decrease in in class labels. <ref type="bibr">Recently, Liu et al. (2019)</ref> and <ref type="bibr" target="#b0">Cao et al. (2019)</ref> adopt re-balancing schedules that learn representation and classifier jointly within a two-stage training scheme. OLTR <ref type="bibr" target="#b19">(Liu et al., 2019)</ref> uses instance-balanced sampling to first learn representations that are fine-tuned in a second stage with class-balanced sampling together with a memory module. LDAM <ref type="bibr" target="#b0">(Cao et al., 2019)</ref> introduces a label-distribution-aware margin loss that expands the decision boundaries of few-shot classes. In Section 5 we exhaustively compare to OLTR and LDAM, since they report state-of-the-art results for the ImageNet-LT, Places-LT and iNaturalist datasets. In our work, we argue for decoupling representation and classification. We demonstrate that in a long-tailed scenario, this separation allows straightforward approaches to achieve high recognition performance, without the need for designing sampling strategies, balance-aware losses or adding memory modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING REPRESENTATIONS FOR LONG-TAILED RECOGNITION</head><p>For long-tailed recognition, the training set follows a long-tailed distribution over the classes. As we have less data about infrequent classes during training, the models trained using imbalanced datasets tend to exhibit under-fitting on the few-shot classes. But in practice we are interested in obtaining the model capable of recognizing all classes well. Various re-sampling strategies <ref type="bibr" target="#b1">(Chawla et al., 2002;</ref><ref type="bibr" target="#b13">Shen et al., 2016;</ref><ref type="bibr" target="#b0">Cao et al., 2019)</ref>, loss reweighting and margin regularization over few-shot classes are thus proposed. However, it remains unclear how they achieve performance improvement, if any, for long-tailed recognition. Here we systematically investigate their effectiveness by disentangling representation learning from classifier learning, in order to identify what indeed matters for longtailed recognition.</p><p>Notation. We define the notation used through the paper. Let X = {x i , y i }, i ∈ {1, . . . , n} be a training set, where y i is the label for data point x i . Let n j denote the number of training sample for class j, and let n = C j=1 n j be the total number of training samples. Without loss of generality, we assume that the classes are sorted by cardinality in decreasing order, i.e., if i &lt; j, then n i ≥ n j . Additionally, since we are in a long-tail setting, n 1 n C . Finally, we denote with f (x; θ) = z the representation for x, where f (x; θ) is implemented by a deep CNN model with parameter θ. The final class predictionỹ is given by a classifier function g, such thatỹ = arg max g(z). For the common case, g is a linear classifier, i.e., g(z) = W z + b, where W denotes the classifier weight matrix, and b is the bias. We present other instantiations of g in Section 4.</p><p>Sampling strategies. In this section we present a number of sampling strategies that aim at rebalancing the data distribution for representation and classifier learning. For most sampling strategies presented below, the probability p j of sampling a data point from class j is given by:</p><formula xml:id="formula_0">p j = n q j C i=1 n q i ,<label>(1)</label></formula><p>where q ∈ [0, 1] and C is the number of training classes. Different sampling strategies arise for different values of q and below we present strategies that correspond to q = 1, q = 0, and q = 1/2.</p><p>Instance-balanced sampling. This is the most common way of sampling data, where each training example has equal probability of being selected. For instance-balanced sampling, the probability p IB j is given by Equation 1 with q = 1, i.e., a data point from class j will be sampled proportionally to the cardinality n j of the class in the training set.</p><p>Class-balanced sampling. For imbalanced datasets, instance-balanced sampling has been shown to be sub-optimal <ref type="bibr" target="#b13">(Huang et al., 2016;</ref><ref type="bibr" target="#b16">Wang et al., 2017)</ref> as the model under-fits for few-shot classes leading to lower accuracy, especially for balanced test sets. Class-balanced sampling has been used to alleviate this discrepancy, as, in this case, each class has an equal probability of being selected. The probability p CB j is given by Eq.</p><p>(1) with q = 0, i.e., p CB j = 1/C. One can see this as a two-stage sampling strategy, where first a class is selected uniformly from the set of classes, and then an instance from that class is subsequently uniformly sampled.</p><p>Square-root sampling. A number of variants of the previous sampling strategies have been explored. A commonly used variant is square-root sampling <ref type="bibr">(Mikolov et al., 2013;</ref><ref type="bibr">Mahajan et al., 2018)</ref>, where q is set to 1/2 in Eq. (1) above.</p><p>Progressively-balanced sampling. Recent approaches <ref type="bibr" target="#b2">(Cui et al., 2018;</ref><ref type="bibr" target="#b0">Cao et al., 2019)</ref> utilized mixed ways of sampling, i.e., combinations of the sampling strategies presented above. In practice this involves first using instance-balanced sampling for a number of epochs, and then class-balanced sampling for the last epochs. These mixed sampling approaches require setting the number of epochs before switching the sampling strategy as an explicit hyper-parameter. Here, we experiment with a softer version, progressively-balanced sampling, that progressively "interpolates" between instancebalanced and class-balanced sampling as learning progresses. Its sampling probability/weight p j for class j is now a function of the epoch t,</p><formula xml:id="formula_1">p PB j (t) = (1 − t T )p IB j + t T p CB j ,<label>(2)</label></formula><p>where T is the total number of epochs. <ref type="figure" target="#fig_3">Figure 3</ref> in appendix depicts the sampling probabilities.</p><p>Loss re-weighting strategies. Loss re-weighting functions for imbalanced data have been extensively studied, and it is beyond the scope of this paper to examine all related approaches. What is more, we found that some of the most recent approaches reporting high performance were hard to train and reproduce and in many cases require extensive, dataset-specific hyper-parameter tuning. In Section A of the Appendix we summarize the latest, best performing methods from this area. In Section 5 we show that, without bells and whistles, baseline methods equipped with a properly balanced classifier can perform equally well, if not better, than the latest loss re-weighting approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CLASSIFICATION FOR LONG-TAILED RECOGNITION</head><p>When learning a classification model on balanced datasets, the classifier weights W and b are usually trained jointly with the model parameters θ for extracting the representation f (x i ; θ) by minimizing the cross-entropy loss between the ground truth y i and prediction W f (x i ; θ)+b. This is also a typical baseline for long-tailed recognition. Though various approaches of re-sampling, reweighting and transferring representations from head to tail classes have been proposed, the general scheme remains the same: classifiers are either learned jointly with the representations either endto-end, or via a two-stage approach where the classifier and the representation are jointly fine-tuned with variants of class-balanced sampling as a second stage <ref type="bibr" target="#b2">(Cui et al., 2018;</ref><ref type="bibr" target="#b0">Cao et al., 2019)</ref>.</p><p>In this section, we consider decoupling the representation from the classification in long-tailed recognition. We present ways of learning classifiers aiming at rectifying the decision boundaries on head-and tail-classes via fine-tuning with different sampling strategies or other non-parametric ways such as nearest class mean classifiers. We also consider an approach to rebalance the classifier weights that exhibits a high long-tailed recognition accuracy without any additional retraining.</p><p>Classifier Re-training (cRT). A straightforward approach is to re-train the classifier with classbalanced sampling. That is, keeping the representations fixed, we randomly re-initialize and optimize the classifier weights W and b for a small number of epochs using class-balanced sampling.</p><p>A similar methodology was also recently used in <ref type="bibr" target="#b21">(Zhang et al., 2019)</ref> for action recognition on a long-tail video dataset.</p><p>Nearest Class Mean classifier (NCM). Another commonly used approach is to first compute the mean feature representation for each class on the training set and then perform nearest neighbor search either using cosine similarity or the Euclidean distance computed on L 2 normalized mean features <ref type="bibr" target="#b15">(Snell et al., 2017;</ref><ref type="bibr" target="#b7">Guerriero et al., 2018;</ref><ref type="bibr" target="#b9">Rebuffi et al., 2017)</ref>. Despite its simplicity, this is a strong baseline (cf . the experimental evaluation in Section 5); the cosine similarity alleviates the weight imbalance problem via its inherent normalization (see also <ref type="figure">Figure 4</ref>).</p><p>τ -normalized classifier (τ -normalized). We investigate an efficient approach to re-balance the decision boundaries of classifiers, inspired by an empirical observation: after joint training with instance-balanced sampling, the norms of the weights w j are correlated with the cardinality of the classes n j , while, after fine-tuning the classifiers using class-balanced sampling, the norms of the classifier weights tend to be more similar (cf . <ref type="figure" target="#fig_1">Figure 2</ref>-left).</p><p>Inspired by the above observations, we consider rectifying imbalance of decision boundaries by adjusting the classifier weight norms directly through the following τ -normalization procedure. Formally, let W = {w j } ∈ R d×C , where w j ∈ R d are the classifier weights corresponding to class j.</p><p>We scale the weights of W to get W = { w j } by:</p><formula xml:id="formula_2">w i = w i ||w i || τ ,<label>(3)</label></formula><p>where τ is a hyper-parameter controlling the "temperature" of the normalization, and || · || denotes the L 2 norm. When τ = 1, it reduces to standard L 2 -normalization. When τ = 0, no scaling is imposed. We empirically choose τ ∈ (0, 1) such that the weights can be rectified smoothly. After τ -normalization, the classification logits are given byŷ = W f (x; θ). Note that we discard the bias term b here due to its negligible effect on the logits and final predictions.</p><p>Learnable weight scaling (LWS). Another way of interpreting τ -normalization would be to think of it as a re-scaling of the magnitude for each classifier w i keeping the direction unchanged. This could be written as</p><formula xml:id="formula_3">w i = f i * w i , where f i = 1 ||w i || τ .<label>(4)</label></formula><p>Although for τ -normalized in general τ is chosen through cross-validation, we further investigate learning f i on the training set, using class-balanced sampling (like cRT In the first representation learning stage, the backbone network is usually trained for 90 epochs. In the second stage, i.e., for retraining a classifier (cRT), we restart the learning rate and train it for 10 epochs while keeping the backbone network fixed. Colored markers denote the sampling strategies used to learn the representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SAMPLING STRATEGIES AND DECOUPLED LEARNING</head><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, we compare different sampling strategies for the conventional joint training scheme to a number of variations of the decoupled learning scheme on the ImageNet-LT dataset. For the joint training scheme (Joint), the linear classifier and backbone for representation learning are jointly trained for 90 epochs using a standard cross-entropy loss and different sampling strategies, i.e., Instance-balanced, Class-balanced, Square-root, and Progressively-balanced. For the decoupled learning schemes, we present results when learning the classifier in all the ways presented in Section 4, i.e., re-initialize and re-train (cRT), Nearest Class Mean (NCM) as well as τ -normalized classifier. Below, we discuss a number of key observations.</p><p>Sampling matters when training jointly. From the Joint results in <ref type="figure" target="#fig_0">Figure 1</ref> across sampling methods and splits, we see consistent gains in performance when using better sampling strategies (see also <ref type="table" target="#tab_7">Table 5</ref>). The trends are consistent for the overall performance as well as the medium-and fewshot classes, with progressively-balanced sampling giving the best results. As expected, instancebalanced sampling gives the highest performance for the many-shot classes. This is well expected since the resulted model is highly skewed to the many-shot classes. Our results for different sampling strategies on joint training validate related works that try to design better data sampling methods. Joint or decoupled learning? For most cases presented in <ref type="figure" target="#fig_0">Figure 1</ref>, performance using decoupled methods is significantly better in terms of overall performance, as well as all splits apart from the many-shot case. Even the nonparametric NCM approach is highly competitive in most cases, while cRT and τ -normalized outperform the jointly trained baseline by a large margin (i.e. 5% higher than the jointly learned classifier), and even achieving 2% higher overall accuracy than the best jointly trained setup with progressively-balanced sampling. The gains are even higher for mediumand few-shot classes at 5% and 11%, respectively.</p><p>To further justify our claim that it is beneficial to decouple representation and classifier, we experiment with fine-tuning the backbone network (ResNeXt-50) jointly with the linear classifier. In <ref type="table" target="#tab_1">Table 1</ref>, we present results when fine-tuning the whole network with standard or smaller (0.1×) learning rate, fine-tuning only the last block in the backbone, or only retraining the linear classifier and fixing the representation. Fine-tuning the whole network yields the worst performance (46.3% and 48.8%), while keeping the representation frozen performs best (49.5%). The trend is even more evident for the medium/few-shot classes. This result suggests that decoupling representation and classifier is desirable for long-tailed recognition. Instance-balanced sampling gives the most generalizable representations. Among all decoupled methods, when it comes to overall performance and all splits apart from the many-shot classes, we see that Instance-balanced sampling gives the best results. This is particularly interesting, as it implies that data imbalance might not be an issue learning high-quality representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Index</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Many Medium Few</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight norm visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">HOW TO BALANCE YOUR CLASSIFIER?</head><p>Among the ways of balancing the classifier explored in <ref type="figure" target="#fig_0">Figure 1</ref>, the non-parametric NCM seems to perform slightly worse than cRT and τ -normalization. Those two methods are consistently better in most cases apart from the few-shot case, where NCM performs comparably. The biggest drop for the NCM approach comes from the many-shot case. It is yet still somehow surprising that both the NCM and τ -normalized cases give competitive performance even though they are free of additional training and involve no additional sampling procedure. As discussed in Section 4, their strong performance may stem from their ability to adaptively adjust the decision boundaries for many-, medium-and few-shot classes (see also <ref type="figure">Figure 4</ref>).</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref> (left) we empirically show the L 2 norms of the weight vectors for all classifiers, as well as the training data distribution sorted in a descending manner with respect to the number of instances in the training set. We can observe that the weight norm of the joint classifier (blue line) is positively correlated with the number of training instances of the corresponding class. More-shot classes tend to learn a classifier with larger magnitudes. As illustrated in <ref type="figure">Figure 4</ref>, this yields a wider classification boundary in feature space, allowing the classifier to have much higher accuracy on data-rich classes, but hurting data-scarce classes. τ -normalized classifiers (gold line) alleviate this issue to some extent by providing more balanced classifier weight magnitudes. For retraining (green line), the weights are almost balanced except that few-shot classes have slightly larger classifier weight norms. Note that the NCM approach would give a horizontal line in the figure as the mean vectors are L 2 -normalized before nearest neighbor search.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref> (right), we further investigate how the performance changes as the temperature parameter τ for the τ -normalized classifier varies. The figure shows that as τ increases from 0, many-shot accuracy decays dramatically while few-shot accuracy increases dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">COMPARISON WITH THE STATE-OF-THE-ART ON LONG-TAILED DATASETS</head><p>In this section, we compare the performance of the decoupled schemes to other recent works that report state-of-the-art results on on three common long-tailed benchmarks: ImageNet-LT, iNaturalist and Places-LT. Results are presented in <ref type="table" target="#tab_2">Tables 2, 3</ref> and 4, respectively. ImageNet-LT. <ref type="table" target="#tab_2">Table 2</ref> presents results for ImageNet-LT. Although related works present results with <ref type="bibr">ResNet-10 (Liu et al., 2019)</ref>, we found that using bigger backbone architectures increases performance significantly on this dataset. We therefore present results for three backbones: ResNet-10, ResNeXt-50 and the larger ResNeXt-152. For the state-of-the-art OLTR method of Liu et al. <ref type="formula" target="#formula_0">(2019)</ref> we adopt results reported in the paper, as well as results we reproduced using the authors' opensourced codebase 2 with two training settings: the one suggested in the codebase and the one using our training setting for the representation learning. From the table we see that the non-parametric decoupled NCM method performs on par with the state-of-the-art for most architectures. We also see that when re-balancing the classifier properly, either by re-training or τ -normalizing, we get results that, without bells and whistles outperform the current state-of-the-art for all backbone architectures. We further experimented with adding the memory mechanism of Liu et al. (2019) on top of our decoupled cRT setup, but the memory mechanism didn't seem to further boost performance (see Appendix B.4).</p><p>iNaturalist 2018. We further evaluate our decoupled methods on the iNaturalist 2018 dataset. We present results after 90 and 200 epochs, as we found that 90 epochs were not enough for the representation learning stage to converge; this is different from <ref type="bibr" target="#b0">Cao et al. (2019)</ref> where they train for 90 epochs. From <ref type="table" target="#tab_4">Table 3</ref> we see that results are consistent with the ImageNet-LT case: re-balancing the classifier gives results that outperform CB-Focal <ref type="bibr" target="#b3">(Cui et al., 2019)</ref>. Our performance, when training only for 90 epochs, is slightly lower than the very recently proposed LDAM+DRW <ref type="bibr" target="#b0">(Cao et al., 2019)</ref>. However, with 200 training epochs and classifier normalization, we achieve a new state-of-the-art of 69.3 with ResNet-50 that can be further improved to 72.5 for ResNet-152. It is further worth noting that we cannot reproduce the numbers reported in <ref type="bibr" target="#b0">Cao et al. (2019)</ref>. We find that the τ -normalized classifier performs best and gives a new state-of-the-art for the dataset, while surprisingly achieving similar accuracy (69%/72% for ResNet-50/ResNet-152) across all many-, medium-and few-shot class splits, a highly desired result for long-tailed recognition. Complete results, i.e., for all splits and more backbone architectures can be found in  <ref type="bibr" target="#b20">(Zhang et al., 2017)</ref>, FSLwF <ref type="bibr" target="#b6">(Gidaris &amp; Komodakis, 2018)</ref> and <ref type="bibr">OLTR (Liu et al., 2019)</ref>. Once again, the τ -normalized classifier give the top performance, with impressive gains for the medium-and few-shot classes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this work, we explore a number of learning schemes for long-tailed recognition and compare jointly learning the representation and classifier to a number of straightforward decoupled methods.</p><p>Through an extensive study we find that although sampling strategies matter when jointly learning representation and classifiers, instance-balanced sampling gives more generalizable representations that can achieve state-of-the-art performance after properly re-balancing the classifiers and without need of carefully designed losses or memory units. We set new state-of-the-art performance for three long-tailed benchmarks and believe that our findings not only contribute to a deeper understanding of the long-tailed recognition task, but can offer inspiration for future work. A LOSS RE-WEIGHTING STRATEGIES Here, we summarize some of the best performing loss re-weighting methods that we compare against in Section 5. Introduced in the context of object detection where imbalance exists in most common benchmarks, the Focal loss <ref type="bibr" target="#b8">(Lin et al., 2017)</ref> aims to balance the sample-wise classification loss for model training by down-weighing easy samples. To this end, given a probability prediction h i for the sample x i over its true category y i , it adds a re-weighting factor (1 − h i ) γ with γ &gt; 0 into the standard cross-entropy loss L CE :</p><formula xml:id="formula_4">L focal := (1 − h i ) γ L CE = −(1 − h i ) γ log(h i ).<label>(5)</label></formula><p>For easy samples (which may dominate the training samples) with large predicted probability h i for their true categories, their corresponding cross entropy loss will be down weighted. Recently, <ref type="bibr" target="#b3">Cui et al. (2019)</ref> presented a class balanced variant of the focal loss and applied it to long-tailed recognition. They modulated the Focal loss for a sample from class j with a balance-aware coefficient equal to (1 − β)/(1 − β n j ). Very recently, <ref type="bibr" target="#b0">Cao et al. (2019)</ref> proposed a label-distribution-aware margin (LDAM) loss that encourages few-shot classes to have larger margins, and their final loss is formulated as a cross-entropy loss with enforced margins:</p><formula xml:id="formula_5">L LDAM := − log eŷ j −∆j eŷ j −∆j + c = jeŷ c −∆c ,<label>(6)</label></formula><p>whereŷ are the logits and ∆ j is a class-aware margin, inversely proportional to n 1/4 j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B FURTHER ANALYSIS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 SAMPLING STRATEGIES</head><p>In <ref type="figure" target="#fig_3">Figure 3</ref> we visualize the sampling weights for the four sampling strategies we explore. In <ref type="table" target="#tab_7">Table 5</ref> we present accuracy on ImageNet-LT for "all" classes when training the representation and classifier jointly. It is clear that better sampling strategies help when jointly training the classifier with the representations/backbone architecture.  In <ref type="figure">Figure 4</ref> we illustrate the classifier decision boundaries before/after normalization with Eq.(3), as well as when using cosine distance. Balancing the norms also leads to more balanced decision boundaries, allowing the classifiers for few-shot classes to occupy more space.  <ref type="figure">Figure 4</ref>: Illustrations on different classifiers and their corresponding decision boundaries, where w i and w j denote the classification weight for class i and j respectively, C i is the classification cone belongs to class i in the feature space, m i is the feature mean for class i. From left to right: τ -normalized classifiers with τ → 0: the classifier with larger weights have wider decision boundaries; τ -normalized classifiers with τ → 1: the decision boundaries are more balanced for different classes; NCM with cosine-similarity whose decision boundary is independent of the classifier weights; NCM with Euclidean-similarity whose decision boundaries partition the feature space into Voronoi cells. <ref type="table" target="#tab_9">Table 6</ref> presents some comparative analysis for the four different ways of learning the classifier that are presented in Section 4.</p><formula xml:id="formula_6">w i w j C i C j C i C j ⌧ ! 0 w i w j C i C j ⌧ ! 1 Cosine m i m j m k L 2 m j m i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 CLASSIFIER LEARNING COMPARISON TABLE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 VARYING THE BACKBONE ARCHITECTURE SIZE</head><p>ImageNet-LT. In <ref type="figure" target="#fig_4">Figure 5</ref> we compare the performance of different backbone architecture sizes (model capacity) under different methods, including of different methods 1) OLTR <ref type="bibr" target="#b19">(Liu et al., 2019)</ref> using the authors' codebase settings (OLTR*); 2) OLTR using the representation learning stage detailed in Section 5 (OLTR**); 3) cRT with the memory module from Liu et al. (2019) while training the classifier; 4) cRT; and 5) τ -normalized. we see that a) the authors' implementation of OLTR over-fits for larger models, b) overfitting can be alleviated with our training setup (different training and LR schedules) c) adding the memory unit when re-training the classifier doesn't increase performance. Additional results of <ref type="table" target="#tab_2">Table 2</ref> are given in <ref type="table" target="#tab_10">Table 7</ref>. iNaturalist 2018. In <ref type="table" target="#tab_3">Table 8</ref> we present an extended version of the results of <ref type="table" target="#tab_4">Table 3</ref>. We show results per split as well as results with a ResNet-101 backbone. As we see from the table and mentioned in Section 5, training only for 90 epochs gives sub-optimal representations, while both large models and longer training result in much higher accuracy on this challenging, large-scale task. What is even more interesting, we see performance across the many-, medium-and few-shot splits being approximately equal after re-balancing the classifier, with only a small advantage for the many-shot classes.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 ON THE EXPLORATION OF DETERMINING τ</head><p>The current tau-normalization strategy does require a validation set to choose tau, which could be a disadvantage depending on the practical scenario. Can we do better?</p><p>Finding τ value on training set. We also attempted to select τ directly on the training dataset. Surprisingly, final performance on testing set is very similar, with τ selected using training set only.</p><p>We achieve this goal by simulating a balanced testing distribution from the training set. We first feed the whole training set through the network to get the top-1 accuracy for each of the classes. Then, we average the class-specific accuracies and use the averaged accuracy as the metric to determine the tau value. As shown in <ref type="table" target="#tab_11">Table 9</ref>, we compare the τ found on training set and validation set for all three datasets. We can see that both the vale of τ and the overall performances are very close to each other, which demonstrates the effectiveness of searching for τ on training set. This strategy offers a practical way to find τ even when validation set is not available.</p><p>Learning τ value on training set. We further investigate if we can automatically learn the τ value instead of grid search. To this end, following cRT, we set τ as a learnable parameter and learn it on the training set with balanced sampling, while keeping all the other parameters fixed (including both the backbone network and classifier). Also, we compare the learned τ value and the corresponding results in the Table 9 (denoted by "learn" = ). This further reduces the manual effort of searching best τ values and make the strategy more accessible for practical usage. We experimented with MLPs with different layers (2 or 3) and different number of hidden neurons (2048 or 512). We use ReLU as activation function, set the batch size to be 512, and train the MLP using balanced sampling on fixed representation for 10 epochs with a cosine learning rate schedule, which gradually decrease the learning rate to zero. We conducted experiments on two datasets.</p><p>On ImageNet-LT, we use ResNeXt50 as the backbone network. The results are summarized in <ref type="table" target="#tab_1">Table 10</ref>. We can see that when the MLP going deeper, the performance are getting worse. It probably means the backbone network is enough to learn discriminative representation. For iNaturalist, we use the representation from a ResNet50 model trained for 200 epochs. We only consider a hidden dimension of 2048, as this dataset contains much more classes. The results are shown in <ref type="table" target="#tab_1">Table 11</ref>, and show that performance drop is even more severe when a deeper classifier is used. We tried to replace the linear classifier with a cosine similarity classifier with (denoted by "cos") and without (denoted by "cos(noRelu)") the last ReLU activation function, following <ref type="bibr" target="#b6">Gidaris &amp; Komodakis (2018)</ref>. We summarize the results in <ref type="table" target="#tab_1">Table 12</ref>, which show that they are comparable to each other. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The performance of different classifiers for each split on ImageNet-LT with ResNeXt-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left: Classifier weight norms for ImageNet-LT validation set when classes are sorted by descending values of n j . Blue line: classifier weights learned with instance-balanced sampling. Green line: weights after fine-tuning with class-balanced sampling. Gold line: after τ normalization. Brown line: weights by learnable weight scaling. Right: Accuracy with different values of the normalization parameter τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Sampling weights p j for ImageNet-LT. Classes are ordered with decreasing n j on the x-axis. Left: instance-balanced, class-balanced and square-root sampling. Right: Progressivelybalanced sampling; as epochs progress, sampling goes from instance-balanced to class-balanced sampling.B.2 CLASSIFIER DECISION BOUNDARIES FOR τ -NORMALIZED AND NCM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Accuracy on ImageNet-LT for different backbones</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). In this case, we keep both the representations and classifier weights fixed and only learn the scaling factors f i . We denote this variant as Learnable Weight Scaling (LWS) in our experiments. We perform extensive experiments on three large-scale long-tailed datasets, including Places-LT<ref type="bibr" target="#b19">(Liu et al., 2019)</ref>,ImageNet-LT (Liu et al., 2019), and iNaturalist 2018 (iNatrualist,  2018. Places-LT and ImageNet-LT are artificially truncated from their balanced versions (Places-2<ref type="bibr" target="#b23">(Zhou et al., 2017)</ref> and ImageNet-2012<ref type="bibr" target="#b4">(Deng et al., 2009)</ref>) so that the labels of the training set follow a long-tailed distribution. Places-LT contains images from 365 categories and the number of images per class ranges from 4980 to 5. ImageNet-LT has 1000 classes and the number of images per class ranges from 1280 to 5 images. iNaturalist 2018 is a real-world, naturally long-tailed dataset, consisting of samples from 8,142 species.Evaluation Protocol. After training on the long-tailed datasets, we evaluate the models on the corresponding balanced test/validation datasets and report the commonly used top-1 accuracy over all classes, denoted as All. To better examine performance variations across classes with different number of examples seen during training, we follow<ref type="bibr" target="#b19">Liu et al. (2019)</ref> and further report accuracy on three splits of the set of classes: Many-shot (more than 100 images), Medium-shot (20∼100 images) and Few-shot (less than 20 images). Accuracy is reported as a percentage.Implementation. We use the PyTorch<ref type="bibr" target="#b8">(Paszke et al., 2017)</ref> framework for all experiments 1 . For Places-LT, we choose ResNet-152 as the backbone network and pretrain it on the full ImageNet-2012 dataset, following<ref type="bibr" target="#b19">Liu et al. (2019)</ref>. On ImageNet-LT, we report results withResNet- {10,50,101,152} (He et al., 2016)  and ResNeXt-{50,101,152}(32x4d)<ref type="bibr" target="#b18">(Xie et al., 2017)</ref> but mainly</figDesc><table><row><cell>5 EXPERIMENTS</cell></row><row><cell>5.1 EXPERIMENTAL SETUP</cell></row><row><cell>Datasets.</cell></row></table><note>use ResNeXt-50 for analysis. Similarly, ResNet-{50,101,152} is also used for iNaturalist 2018. For all experiements, if not specified, we use SGD optimizer with momentum 0.9, batch size 512, cosine learning rate schedule (Loshchilov &amp; Hutter, 2016) gradually decaying from 0.2 to 0 and image res- olution 224 × 224.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Re-train</cell><cell cols="3">Many Medium Few</cell><cell>All</cell></row><row><cell>B+C</cell><cell>55.4</cell><cell>45.3</cell><cell cols="2">24.5 46.3</cell></row><row><cell>B+C(0.1×lr)</cell><cell>61.9</cell><cell>45.6</cell><cell cols="2">22.8 48.8</cell></row><row><cell>LB+C</cell><cell>61.4</cell><cell>45.8</cell><cell cols="2">24.5 48.9</cell></row><row><cell>C</cell><cell>61.5</cell><cell>46.2</cell><cell cols="2">27.0 49.5</cell></row></table><note>Retraining/finetuning different parts of a ResNeXt-50 model on ImageNet-LT. B: back- bone; C: classifier; LB: last block.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Long-tail recognition accuracy on ImageNet-LT for different backbone architectures. † denotes results directly copied from<ref type="bibr" target="#b19">Liu et al. (2019)</ref>. * denotes results reproduced with the authors' code. ** denotes OLTR with our representation learning stage.</figDesc><table><row><cell>Method</cell><cell cols="3">ResNet-10 ResNeXt-50 ResNeXt-152</cell></row><row><cell>FSLwF † (Gidaris &amp; Komodakis, 2018)</cell><cell>28.4</cell><cell>-</cell><cell>-</cell></row><row><cell>Focal Loss † (Lin et al., 2017)</cell><cell>30.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Range Loss † (Zhang et al., 2017)</cell><cell>30.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Lifted Loss † (Oh Song et al., 2016)</cell><cell>30.8</cell><cell>-</cell><cell>-</cell></row><row><cell>OLTR † (Liu et al., 2019)</cell><cell>35.6</cell><cell>-</cell><cell>-</cell></row><row><cell>OLTR*</cell><cell>34.1</cell><cell>37.7</cell><cell>24.8</cell></row><row><cell>OLTR**</cell><cell>37.3</cell><cell>46.3</cell><cell>50.3</cell></row><row><cell>Joint</cell><cell>34.8</cell><cell>44.4</cell><cell>47.8</cell></row><row><cell>NCM</cell><cell>35.5</cell><cell>47.3</cell><cell>51.3</cell></row><row><cell>cRT</cell><cell>41.8</cell><cell>49.5</cell><cell>52.4</cell></row><row><cell>τ -normalized</cell><cell>40.6</cell><cell>49.4</cell><cell>52.8</cell></row><row><cell>LWS</cell><cell>41.4</cell><cell>49.9</cell><cell>53.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 8</head><label>8</label><figDesc>of the Appendix. Places-LT. For Places-LT we follow the protocol of Liu et al. (2019) and start from a ResNet-152 backbone pre-trained on the full ImageNet dataset. Similar to Liu et al. (2019), we then fine-tune the backbone with Instance-balanced sampling for representation learning. Classification follows with fixed representations for our decoupled methods. As we see inTable 4, all three decoupled methods outperform the state-of-the-art approaches, including Lifted Loss(Oh Song et al., 2016),</figDesc><table /><note>Focal Loss (Lin et al., 2017), Range Loss</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Overall accuracy on iNaturalist 2018. Rows with † denote results directly copied from<ref type="bibr" target="#b0">Cao et al. (2019)</ref>. We present results when training for 90/200 epochs.</figDesc><table><row><cell>Method</cell><cell cols="2">ResNet-50 ResNet-152</cell></row><row><cell>CB-Focal †</cell><cell>61.1</cell><cell>-</cell></row><row><cell>LDAM †</cell><cell>64.6</cell><cell>-</cell></row><row><cell>LDAM+DRW †</cell><cell>68.0</cell><cell>-</cell></row><row><cell>Joint</cell><cell>61.7/65.8</cell><cell>65.0/69.0</cell></row><row><cell>NCM</cell><cell>58.2/63.1</cell><cell>61.9/67.3</cell></row><row><cell>cRT</cell><cell>65.2/67.6</cell><cell>68.5/71.2</cell></row><row><cell>τ -normalized</cell><cell>65.6/69.3</cell><cell>68.8/72.5</cell></row><row><cell>LWS</cell><cell>65.9/69.5</cell><cell>69.1/72.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results on Places-LT, starting from an ImageNet pre-trained ResNet152. † denotes results directly copied from<ref type="bibr" target="#b19">Liu et al. (2019)</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Many Medium Few</cell><cell>All</cell></row><row><cell>Lifted Loss †</cell><cell>41.1</cell><cell>35.4</cell><cell cols="2">24.0 35.2</cell></row><row><cell>Focal Loss †</cell><cell>41.1</cell><cell>34.8</cell><cell cols="2">22.4 34.6</cell></row><row><cell>Range Loss †</cell><cell>41.1</cell><cell>35.4</cell><cell cols="2">23.2 35.1</cell></row><row><cell>FSLwF †</cell><cell>43.9</cell><cell>29.9</cell><cell cols="2">29.5 34.9</cell></row><row><cell>OLTR †</cell><cell>44.7</cell><cell>37.0</cell><cell cols="2">25.3 35.9</cell></row><row><cell>Joint</cell><cell>45.7</cell><cell>27.3</cell><cell>8.2</cell><cell>30.2</cell></row><row><cell>NCM</cell><cell>40.4</cell><cell>37.1</cell><cell cols="2">27.3 36.4</cell></row><row><cell>cRT</cell><cell>42.0</cell><cell>37.6</cell><cell cols="2">24.9 36.7</cell></row><row><cell>τ -normalized</cell><cell>37.8</cell><cell>40.7</cell><cell cols="2">31.8 37.9</cell></row><row><cell>LWS</cell><cell>40.6</cell><cell>39.1</cell><cell cols="2">28.6 37.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5356-5364, 2019. Hui Han, Wen-Yuan Wang, and Bing-Huan Mao. Borderline-smote: a new over-sampling method in imbalanced data sets learning. In International conference on intelligent computing, pp. 878-887.</figDesc><table><row><cell>Springer, 2005.</cell></row><row><cell>Bharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating</cell></row><row><cell>features. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3018-</cell></row><row><cell>3027, 2017.</cell></row><row><cell>Munawar Hayat, Salman Khan, Waqas Zamir, Jianbing Shen, and Ling Shao. Max-margin class</cell></row><row><cell>imbalanced learning with gaussian affinity. arXiv preprint arXiv:1901.07711, 2019.</cell></row><row><cell>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-</cell></row><row><cell>nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.</cell></row><row><cell>770-778, 2016.</cell></row><row><cell>Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Learning deep representation for</cell></row><row><cell>imbalanced classification. In Proceedings of the IEEE conference on computer vision and pattern</cell></row><row><cell>recognition, pp. 5375-5384, 2016.</cell></row><row><cell>Chen Huang, Yining Li, Change Loy Chen, and Xiaoou Tang. Deep imbalanced learning for face</cell></row><row><cell>recognition and attribute prediction. IEEE transactions on pattern analysis and machine intelli-</cell></row><row><cell>gence, 2019.</cell></row><row><cell>iNatrualist. The inaturalist 2018 competition dataset. https://github.com/visipedia/inat</cell></row><row><cell>comp/tree/master/2018, 2018.</cell></row><row><cell>Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right</cell></row><row><cell>balance with uncertainty. In The IEEE Conference on Computer Vision and Pattern Recognition</cell></row><row><cell>(CVPR), June 2019.</cell></row><row><cell>Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and Roberto Togneri.</cell></row><row><cell>Cost-sensitive learning of deep feature representations from imbalanced data. IEEE transactions</cell></row><row><cell>on neural networks and learning systems, 29(8):3573-3587, 2017.</cell></row><row><cell>Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense</cell></row><row><cell>object detection. In Proceedings of the IEEE international conference on computer vision, pp.</cell></row><row><cell>2980-2988, 2017.</cell></row><row><cell>Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale</cell></row><row><cell>long-tailed recognition in an open world. In Proceedings of the IEEE Conference on Computer</cell></row><row><cell>Vision and Pattern Recognition, pp. 2537-2546, 2019.</cell></row><row><cell>Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv</cell></row><row><cell>preprint arXiv:1608.03983, 2016.</cell></row><row><cell>Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,</cell></row><row><cell>Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised</cell></row><row><cell>pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 181-</cell></row><row><cell>196, 2018.</cell></row><row><cell>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-</cell></row><row><cell>tations of words and phrases and their compositionality. In Advances in neural information pro-</cell></row><row><cell>cessing systems, pp. 3111-3119, 2013.</cell></row></table><note>Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4004-4012, 2016.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Accuracy on ImageNet-LT when jointly learning the representation and classifier using different sampling strategies. Results in this Table are a subset of the results presented inFigure 1.</figDesc><table><row><cell>Sampling</cell><cell cols="3">Many Medium Few</cell><cell>All</cell></row><row><cell>Instance-balanced</cell><cell>65.9</cell><cell>37.5</cell><cell>7.7</cell><cell>44.4</cell></row><row><cell>Class-balanced</cell><cell>61.8</cell><cell>40.1</cell><cell cols="2">15.5 45.1</cell></row><row><cell>Square-root</cell><cell>64.3</cell><cell>41.2</cell><cell cols="2">17.0 46.8</cell></row><row><cell>Progressively-balanced</cell><cell>61.9</cell><cell>43.2</cell><cell cols="2">19.4 47.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparative analysis for different ways of learning the classifier for long-tail recognition.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comprehensive results on ImageNet-LT with different backbone networks {ResNet,</figDesc><table><row><cell cols="2">ResNeXt}-{50, 101,152}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Backbone Method</cell><cell cols="3">ResNet Many Medium Few</cell><cell>All</cell><cell cols="3">ResNeXt Many Medium Few</cell><cell>All</cell></row><row><cell></cell><cell>Joint</cell><cell>64.0</cell><cell>33.8</cell><cell>5.8</cell><cell>41.6</cell><cell>65.9</cell><cell>37.5</cell><cell>7.7</cell><cell>44.4</cell></row><row><cell>*-50</cell><cell>NCM cRT</cell><cell>53.1 58.8</cell><cell>42.3 44.0</cell><cell cols="2">26.5 44.3 26.1 47.3</cell><cell>56.6 61.8</cell><cell>45.3 46.2</cell><cell cols="2">28.1 47.3 27.4 49.6</cell></row><row><cell></cell><cell>τ -normalized</cell><cell>56.6</cell><cell>44.2</cell><cell cols="2">27.4 46.7</cell><cell>59.1</cell><cell>46.9</cell><cell cols="2">30.7 49.4</cell></row><row><cell></cell><cell>LWS</cell><cell>57.1</cell><cell>45.2</cell><cell cols="2">29.3 47.7</cell><cell>60.2</cell><cell>47.2</cell><cell cols="2">30.3 49.9</cell></row><row><cell></cell><cell>Joint</cell><cell>66.6</cell><cell>36.8</cell><cell>7.1</cell><cell>44.2</cell><cell>66.2</cell><cell>37.8</cell><cell>8.6</cell><cell>44.8</cell></row><row><cell>*-101</cell><cell>NCM cRT</cell><cell>56.8 61.6</cell><cell>45.1 46.5</cell><cell cols="2">28.8 47.4 28.0 49.8</cell><cell>57.2 61.7</cell><cell>45.5 46.0</cell><cell cols="2">29.5 47.8 27.0 49.4</cell></row><row><cell></cell><cell>τ -normalized</cell><cell>59.4</cell><cell>47.0</cell><cell cols="2">30.6 49.6</cell><cell>59.1</cell><cell>47.0</cell><cell cols="2">31.7 49.6</cell></row><row><cell></cell><cell>LWS</cell><cell>60.1</cell><cell>47.6</cell><cell cols="2">31.2 50.2</cell><cell>60.5</cell><cell>47.2</cell><cell cols="2">31.2 50.1</cell></row><row><cell></cell><cell>Joint</cell><cell>66.9</cell><cell>27.7</cell><cell>7.7</cell><cell>44.9</cell><cell>69.1</cell><cell>41.4</cell><cell cols="2">10.4 47.8</cell></row><row><cell>*-152</cell><cell>NCM cRT</cell><cell>56.9 61.8</cell><cell>45.6 46.8</cell><cell cols="2">29.9 47.8 28.4 50.1</cell><cell>60.3 64.7</cell><cell>49.0 49.1</cell><cell cols="2">33.6 51.3 29.4 52.4</cell></row><row><cell></cell><cell>τ -normalized</cell><cell>59.6</cell><cell>47.5</cell><cell cols="2">32.2 50.1</cell><cell>62.2</cell><cell>50.1</cell><cell cols="2">35.8 52.8</cell></row><row><cell></cell><cell>LWS</cell><cell>60.6</cell><cell>47.8</cell><cell cols="2">31.4 50.5</cell><cell>63.5</cell><cell>50.4</cell><cell cols="2">34.2 53.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Determining τ on the training set</figDesc><table><row><cell>Dataset</cell><cell cols="2">split learn τ</cell><cell cols="3">Many Medium Few</cell><cell>All</cell></row><row><cell></cell><cell>val</cell><cell>0.7</cell><cell>59.1</cell><cell>46.9</cell><cell cols="2">30.7 49.4</cell></row><row><cell>ImageNet-LT</cell><cell>train</cell><cell>0.7</cell><cell>59.1</cell><cell>46.9</cell><cell cols="2">30.7 49.4</cell></row><row><cell></cell><cell>train</cell><cell>0.6968</cell><cell>59.2</cell><cell>46.9</cell><cell cols="2">30.6 49.4</cell></row><row><cell></cell><cell>val</cell><cell>0.3</cell><cell>65.6</cell><cell>65.3</cell><cell cols="2">65.9 65.6</cell></row><row><cell>iNaturalist</cell><cell>train</cell><cell>0.2</cell><cell>69.0</cell><cell>65.2</cell><cell cols="2">63.6 65.0</cell></row><row><cell></cell><cell>train</cell><cell>0.3146</cell><cell>65.1</cell><cell>65.2</cell><cell cols="2">66.1 65.6</cell></row><row><cell></cell><cell>val</cell><cell>0.8</cell><cell>37.8</cell><cell>40.7</cell><cell cols="2">31.8 37.9</cell></row><row><cell>Places-LT</cell><cell>train</cell><cell>0.6</cell><cell>41.4</cell><cell>39.3</cell><cell cols="2">25.3 37.4</cell></row><row><cell></cell><cell>train</cell><cell>0.5246</cell><cell>42.6</cell><cell>38.3</cell><cell cols="2">22.7 36.8</cell></row><row><cell cols="5">B.6 COMPARING MLP CLASSFIIER WITH LINEAR CLASSIFIER</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>MLP classifier on ImageNet-LT</figDesc><table><row><cell>Layers</cell><cell cols="3">hid-dim : 2048 Many Medium Few</cell><cell>All</cell><cell cols="3">hid-dim : 512 Many Medium Few</cell><cell>All</cell></row><row><cell>1</cell><cell>61.7</cell><cell>45.9</cell><cell cols="2">26.8 49.4</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>60.8</cell><cell>44.4</cell><cell cols="2">24.5 48.0</cell><cell>59.9</cell><cell>44.3</cell><cell>25.1 47.7</cell></row><row><cell>3</cell><cell>60.3</cell><cell>44.3</cell><cell cols="2">23.7 47.7</cell><cell>59.3</cell><cell>43.7</cell><cell>23.9 47.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>MLP classifier on iNaturalst</figDesc><table><row><cell cols="4">Layers Many Medium Few</cell><cell>All</cell></row><row><cell>1</cell><cell>73.2</cell><cell>68.8</cell><cell cols="2">66.1 68.2</cell></row><row><cell>2</cell><cell>60.4</cell><cell>61.8</cell><cell cols="2">60.6 61.2</cell></row><row><cell>3</cell><cell>68.5</cell><cell>63.6</cell><cell cols="2">60.1 62.8</cell></row><row><cell cols="3">B.7 COSINE SIMILARITY FOR CLASSIFICATION</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Cosine similarity Classifier</figDesc><table><row><cell>Classifier</cell><cell cols="3">Many Medium Few</cell><cell>All</cell></row><row><cell>NCM</cell><cell>56.6</cell><cell>45.3</cell><cell cols="2">28.1 47.3</cell></row><row><cell>cRT</cell><cell>61.7</cell><cell>45.9</cell><cell cols="2">26.8 49.4</cell></row><row><cell>τ -normalized</cell><cell>59.1</cell><cell>46.9</cell><cell cols="2">30.7 49.4</cell></row><row><cell>cos</cell><cell>60.4</cell><cell>46.8</cell><cell cols="2">29.3 49.7</cell></row><row><cell>cos(noRelu)</cell><cell>60.7</cell><cell>46.9</cell><cell cols="2">28.0 49.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We will open-source our codebase and models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/zhmiao/OpenLongTailRecognition-OLTR</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4109" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">C4. 5, class imbalance, and cost sensitivity: why undersampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on learning from imbalanced datasets II</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep nearest class mean classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samantha</forename><surname>Guerriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07379</idno>
		<title level="m">Meta-weightnet: Learning an explicit mapping for sample weighting</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature transfer learning for face recognition with under-represented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>eeding of IEEE Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A study on action detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12993</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unequal-training for deep face recognition with long-tailed noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianteng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohai</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">Table 8: Comprehensive results on iNaturalist 2018 with different backbone networks (ResNet-50, ResNet-101 &amp; ResNet-152) and different training epochs</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
