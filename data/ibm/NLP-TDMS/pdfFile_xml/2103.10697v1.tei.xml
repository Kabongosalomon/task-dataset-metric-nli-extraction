<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stéphane D&amp;apos;ascoli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research 2</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research 2</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratoire de Physique de l&apos;École normale supérieure</orgName>
								<orgName type="institution" key="instit1">ENS</orgName>
								<orgName type="institution" key="instit2">Université PSL</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Université de Paris</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Leavitt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research 2</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research 2</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Biroli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research 2</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional architectures have proven extremely successful for vision tasks. Their hard inductive biases enable sample-e cient learning, but come at the cost of a potentially lower performance ceiling. Vision Transformers (ViTs) rely on more exible selfattention layers, and have recently outperformed CNNs for image classi cation. However, they require costly pre-training on large external datasets or distillation from pre-trained convolutional networks. In this paper, we ask the following question: is it possible to combine the strengths of these two architectures while avoiding their respective limitations? To this end, we introduce gated positional self-attention (GPSA), a form of positional self-attention which can be equipped with a "soft" convolutional inductive bias. We initialize the GPSA layers to mimic the locality of convolutional layers, then give each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. The resulting convolutional-like ViT architecture, ConViT, outperforms the DeiT [1] on ImageNet, while o ering a much improved sample e ciency. We further investigate the role of locality in learning by rst quantifying how it is encouraged in vanilla self-attention layers, then analyzing how it is escaped in GPSA layers. We conclude by presenting various ablations to better understand the success of the ConViT. Our code and models are released publicly. * stephane.dascoli@ens.fr † Work performed as part of the Facebook AI Residency</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of deep learning over the last decade has largely been fueled by models with strong inductive biases, allowing e cient training across domains <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. The use of Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, which have become ubiquitous in computer vision since the success of AlexNet in 2012 <ref type="bibr" target="#b5">[6]</ref>, epitomizes this trend. Inductive biases are hard-coded into the architectural structure of CNNs in the form of two strong constraints on the weights: locality and weight sharing. By encouraging translation equivariance (without pooling layers) and translation invariance (with pooling layers) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3]</ref>, the convolutional inductive bias makes models more samplee cient and parameter-e cient <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Similarly, for sequence-based tasks, recurrent networks with hardcoded memory cells have been shown to simplify the learning of long-range dependencies (LSTMs) and outperform vanilla recurrent neural networks in a variety of settings <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>However, the rise of models based purely on attention in recent years calls into question the necessity of hard-coded inductive biases. First introduced as an add-on to recurrent neural networks for Sequence-to-Sequence models <ref type="bibr">[14]</ref>, attention has led to a breakthrough in Natural Language Processing through the emergence of Transformer models, which rely solely on a particular kind of attention: Self-Attention (SA) <ref type="bibr" target="#b14">[15]</ref>. The strong performance of these models when pre-trained on large datasets has quickly led to Transformer-based approaches becoming the default choice over recurrent models like LSTMs <ref type="bibr" target="#b15">[16]</ref>.</p><p>In vision tasks, the locality of CNNs impairs the ability to capture long-range dependencies, whereas attention does not su er from this limitation. Chen et al. <ref type="bibr" target="#b16">[17]</ref> and Bello et al. <ref type="bibr" target="#b17">[18]</ref> leveraged this comple-  <ref type="figure">Figure 1</ref>: Soft inductive biases can help models learn without being restrictive. Hard inductive biases, such as the architectural constraints of CNNs, can greatly improve the sample-e ciency of learning, but can become constraining when the size of the dataset is not an issue. The soft inductive biases introduced by the ConViT avoid this limitation by vanishing away when not required.</p><p>mentarity by augmenting convolutional layers with attention. More recently, Ramachandran et al. <ref type="bibr" target="#b18">[19]</ref> ran a series of experiments replacing some or all convolutional layers in ResNets with attention, and found the best performing models used convolutions in early layers and attention in later layers. The Vision Transformer (ViT), introduced by Dosovitskiy et al. <ref type="bibr" target="#b19">[20]</ref>, entirely dispenses with the convolutional inductive bias by performing SA across embeddings of patches of pixels. The ViT is able to match or exceed the performance of CNNs but requires pre-training on vast amounts of data. More recently, the Data-e cient Vision Transformer (DeiT) <ref type="bibr" target="#b0">[1]</ref> was able to reach similar performances without any pre-training on supplementary data, instead relying on Knowledge Distillation <ref type="bibr" target="#b20">[21]</ref> from a convolutional teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft inductive biases</head><p>The recent success of the ViT demonstrates that while convolutional constraints can enable strongly sample-e cient training in the small-data regime, they can also become limiting as the dataset size is not an issue. In data-plentiful regimes, hard inductive biases can be overly restrictive and learning the most appropriate inductive bias can prove more e ective. The practitioner is therefore confronted with a dilemma between using a convolutional model, which has a high performance oor but a potentially lower performance ceiling due to the hard inductive biases, or a self-attention based model, which has a lower oor but a higher ceiling. This dilemma leads to the following question: can one get the best of both worlds, and obtain the bene ts of the convolutional inductive biases without su ering from its limitations (see <ref type="figure">Fig. 1</ref>)?</p><p>In this direction, one successful approach is the combination of the two architectures in "hybrid" models. These models, which interleave or combine convolutional and self-attention layers, have fueled successful results in a variety of tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Another approach is that of Knowledge Distillation <ref type="bibr" target="#b20">[21]</ref>, which has recently been applied to transfer the inductive bias of a convolutional teacher to a student transformer <ref type="bibr" target="#b0">[1]</ref>. While these two methods o er an interesting compromise, they forcefully induce convolutional inductive biases into the Transformers, potentially a ecting the Transformer with their limitations.</p><p>Contribution In this paper, we take a new step towards bridging the gap between CNNs and Transformers, by presenting a new method to "softly" introduce a convolutional inductive bias into the ViT. The idea is to let each SA layer decide whether to behave as a convolutional layer or not, depending on the context. We make the following contributions:</p><p>1. We present a new form of SA layer, named gated positional self-attention (GPSA), which one can initialize as a convolutional layer. Each attention head then has the freedom to recover expressivity by adjusting a gating parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">We then perform experiments based on the</head><p>DeiT <ref type="bibr" target="#b0">[1]</ref>, with a certain number of SA layers replaced by GPSA layers. The resulting Convolutional Vision Transformer (ConViT) outperforms the DeiT while boasting a much improved sample-e ciency ( <ref type="figure" target="#fig_3">Fig. 2</ref>). 3. We analyze quantitatively how local attention is naturally encouraged in vanilla ViTs, then investigate the inner workings of the ConViT and perform ablations to investigate how it bene ts from the convolution initialization.</p><p>Overall, our work demonstrates the e ectiveness of "soft" inductive biases, especially in the low-data regime where the learning model is highly underspeci ed (see <ref type="figure">Fig. 1</ref>), and motivates the exploration of further methods to induce them.</p><p>Related work Our work is motivated by combining the recent success of pure Transformer models <ref type="bibr" target="#b19">[20]</ref> with the formalized relationship between SA and convolution. Indeed, Cordonnier, Loukas and Jaggi <ref type="bibr" target="#b31">[32]</ref> showed that a SA layer with N h heads can express a convolution of kernel size √ N h , if each head   with that of the DeiT-S by training them on restricted portions of ImageNet-1k, where we only keep a certain fraction of the images of each class. Both models are trained with the hyperparameters reported in <ref type="bibr" target="#b0">[1]</ref>. We display the the relative improvement of the ConViT over the DeiT in green. Right: we compare the top-1 accuracies of our ConViT models with those of other ViTs (diamonds) and CNNs (squares) on ImageNet-1k. The performance of other models on ImageNet are taken from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>focuses on one of the pixels in the kernel patch. By investigating the qualitative aspect of attention maps of models trained on CIFAR-10, it is shown that SA layers with relative positional encodings naturally converge towards convolutional-like con gurations, suggesting that some degree of convolutional inductive bias is desirable.</p><p>Conversely, the restrictiveness of hard locality constraints has been proven by Elsayed et al. <ref type="bibr" target="#b32">[33]</ref>. A breadth of approaches have been taken to imbue CNN architectures with nonlocality <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b27">28</ref>]. Another line of research is to induce a convolutional inductive bias is di erent architectures. For example, Neyshabur <ref type="bibr" target="#b36">[37]</ref> uses a regularization method to encourage fully-connected networks (FCNs) to learn convolutions from scratch throughout training.</p><p>Most related to our approach, d'Ascoli et al. <ref type="bibr" target="#b37">[38]</ref> explored a method to initialize FCNs networks as CNNs. This enables the resulting FCN to reach much higher performance than achievable with standard initialization. Moreover, if the FCN is initialized from a partially trained CNN, the recovered degrees of freedom allow the FCN to outperform the CNN it stems from. This method relates more generally to "warm start" approaches such as those used in spiked tensor models <ref type="bibr" target="#b38">[39]</ref>, where a smart initialization, containing prior information on the problem, is used to ease the learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>We provide an open-source implementation of our method as well as pretrained models at https://github.com/facebookresearch/ convit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We begin by introducing the basics of SA layers, and show how positional attention can allow SA layers to express convolutional layers.</p><p>Multi-head self-attention The attention mechanism is based on a trainable associative memory with (key, query) vector pairs. A sequence of L 1 "query" embeddings Q ∈ R L 1 ×D h is matched against another sequence of L 2 "key" embeddings K ∈ R L 2 ×D h using inner products. The result is an attention matrix whose entry (ij) quanti es how semantically "relevant" Q i is to K j :</p><formula xml:id="formula_0">A = softmax QK √ D h ∈ R L 1 ×L 2 ,<label>(1)</label></formula><p>where (softmax [X]) ij = e X ij / k e X ik . Self-attention is a special case of attention where a sequence is matched to itself, to extract the semantic dependencies between its parts. In the ViT, the queries and keys are linear projections of the embeddings of 16 × 16 pixel patches X ∈ R L×D emb . Hence, we have Q = W qry X and K = W key X, where W key , W qry ∈ R D emb ×D h .</p><p>Multi-head SA layers use several self-attention heads in parallel to allow the learning of di erent kinds of interdependencies. They take as input a sequence of L embeddings of dimension D emb = N h D h , and output a sequence of L embeddings of the same dimension through the following mechanism:</p><formula xml:id="formula_1">MSA(X) := concat h∈[N h ] [SA h (X)] W out + b out , (2) where W out ∈ R D emb ×D emb , b out ∈ R D emb .</formula><p>Each self-attention head h performs the following operation:</p><formula xml:id="formula_2">SA h (X) := A h XW h val ,<label>(3)</label></formula><p>where W h val ∈ R D emb ×D h is the value matrix.  However, in the vanilla form of Eq. 1, SA layers are position-agnostic: they do not know how the patches are located according to each other. To incorporate positional information, there are several options. One is to add some positional information to the input at embedding time, before propagating it through the SA layers: <ref type="bibr" target="#b19">[20]</ref> use this approach in their ViT. Another possibility is to replace the vanilla SA with positional self-attention (PSA), using embeddings r ij of the relative position of patches i and j <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_3">A h ij := softmax Q h i K h j + v h pos r ij<label>(4)</label></formula><p>Each attention head uses a trainable embedding v h pos ∈ R Dpos , and the relative positional embeddings r ij ∈ R Dpos only depend on the distance between pixels i and j, denoted denoted by a two-dimensional vector δ ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-attention as a generalized convolution</head><p>Cordonnier, Loukas and Jaggi <ref type="bibr" target="#b31">[32]</ref> show that a multihead PSA layer with N h heads and learnable relative positional embeddings (Eq. 4) of dimension D pos ≥ 3 can express any convolutional layer of lter size √ N h × √ N h , by setting the following:</p><formula xml:id="formula_4">     v h pos := −α h 1, −2∆ h 1 , −2∆ h 2 , 0, . . . 0 r δ := δ 2 , δ 1 , δ 2 , 0, . . . 0 W qry = W key := 0, W val := I (5)</formula><p>In the above,</p><p>• The center of attention ∆ h ∈ R 2 is the position to which head h pays most attention to, relative to the query patch. For example, in <ref type="figure" target="#fig_5">Fig. 3</ref>(c), the four heads correspond, from left to right, to ∆ 1 = (−1, 1),</p><formula xml:id="formula_5">∆ 2 = (−1, −1), ∆ 3 = (1, 1), ∆ 4 = (1, −1).</formula><p>• The locality strength α h &gt; 0 determines how focused the attention is around its center ∆ h (it can also by understood as the "temperature" of the softmax in Eq. 1). When α h is large, the attention is focused only on the patch(es) located at ∆ h , as in <ref type="figure" target="#fig_5">Fig. 3(d)</ref>; when α h is small, the attention is spread out into a larger area, as in <ref type="figure" target="#fig_5">Fig. 3(c)</ref>.</p><p>Thus, the PSA layer can achieve a strictly convolutional attention map by setting the centers of attention ∆ h to each of the possible positional o sets of a √ N h × √ N h convolutional kernel, and sending the locality strengths α h to some large value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Building on the insight of <ref type="bibr" target="#b31">[32]</ref>, we introduce the Con-Vit, a variant of the ViT <ref type="bibr" target="#b19">[20]</ref> obtained by replacing some of the SA layers by a new type of layer which we call gated positional self-attention (GPSA) layers. The core idea is to enforce the "informed" convolutional con guration of Eqs. 5 in the GPSA layers at initialization, then let them decide whether to stay convolutional or not. However, the standard parameterization of PSA layers (Eq. 4) su ers from two limitations, which lead us two introduce two modi cations.</p><p>Adaptive attention span The rst caveat in PSA is the vast number of trainable parameters involved, since the number of relative positional embeddings r δ is quadratic in the number of patches. This led some authors to restrict the attention to a subset of patches around the query patch <ref type="bibr" target="#b18">[19]</ref>, at the cost of losing long-range information. Because GPSA layers involve positional information, the class token is concatenated with hidden representation after the last GPSA layer. In this paper, we typically take 10 GPSA layers followed by 2 vanilla SA layers. FFN: feedforward network (2 linear layers separated by a GeLU activation); W qry : query weights; W key : key weights; v pos : attention center and span embeddings (learned); r qk : relative position embeddings ( xed); λ: gating parameter (learned); σ: sigmoid function.</p><p>To avoid this, we leave the relative positional embeddings r δ xed, and train only the embeddings v h pos which determine the center and span of the attention heads; this approach relates to the adaptive attention span introduced in Sukhbaatar et al. <ref type="bibr" target="#b39">[40]</ref> for Language Transformers. The initial values of r δ and v h pos are given by Eq. 5, where we take D pos = 3 to get rid of the useless zero components. Thanks to D pos D h , number of parameters involved in the positional attention is negligible compared to the number of parameters involved in the content attention.</p><p>Positional gating The second issue with standard PSA is the fact that the content and positional terms in Eq. 4 are potentially of di erent magnitudes, in which case the softmax will ignore the smallest of the two. In particular, the convolutional initialization scheme discussed above involves highly concentrated attention scores, i.e. high-magnitude values in the softmax. In  practice, we observed that using a convolutional initialization scheme on vanilla PSA layers gives a boost in early epochs, but degrades late-time performance as the attention mechanism lazily ignores the content information (see SM. A).</p><p>To avoid this, GPSA layers sum the content and positional terms after the softmax, with their relative importances governed by a learnable gating parameter λ h (one for each attention head). Finally, we normalize the resulting sum of matrices (whose terms are positive) to ensure that the resulting attention scores de ne a probability distribution. The resulting GPSA layer is therefore parametrized as follows (see also <ref type="figure" target="#fig_6">Fig. 4</ref>):</p><formula xml:id="formula_6">GPSA h (X) := normalize A h XW h val (6) A h ij := (1 − σ(λ h )) softmax Q h i K h j + σ(λ h ) softmax v h pos r ij ,<label>(7)</label></formula><p>where  14×14 pixels, embeds them into vectors of dimension D emb = 64N h using a convolutional stem, then feeds them through 12 blocks which keep the number and dimension of patches constant. Each block consists in a SA layer followed by a 2-layer Feed-Forward Network (FFN) with GeLU activation, both equipped with residual connections. The ConViT is simply a ViT where the rst 10 blocks replace the SA layers by a GPSA layer with a convolutional initialization. Similar to language Transformers like BERT <ref type="bibr" target="#b15">[16]</ref>, the ViT uses an extra "class token", appended to the sequence of patches to predict the class of the input. Since this class token does not carry any positional information, the SA layers of the ViT do not use positional attention: the positional information is injected into the input before being propagated through the layers. However, as GPSA layers involve positional attention, they are not well suited for the class token approach. We solve this problem by appending the class token to the patches after the last GPSA layer (see <ref type="figure" target="#fig_6">Fig. 4</ref></p><formula xml:id="formula_7">(normalize [A]) ij = A ij / k A ik and σ : x → 1 /(1+e −x ) is</formula><formula xml:id="formula_8">) 1 .</formula><p>Training details We based our ConVit on the DeiT <ref type="bibr" target="#b0">[1]</ref>, a hyperparameter-optimized version of the ViT which has been open-sourced 2 . Thanks to its ability to achieve competitive results without using any external data, the DeiT both an excellent baseline and relatively easy to train: the largest model <ref type="figure">(DeiT-B)</ref> only requires a few days of training on 8 GPUs.</p><p>To mimic 2 × 2, 3 × 3 and 4 × 4 convolutional lters, we consider three di erent ConViT models with 4, 9 and 16 attention heads (see Tab. 1). Their number of heads are slightly larger than the DeiT-Ti, ConViT-S and ConViT-B of Touvron et al. <ref type="bibr" target="#b0">[1]</ref>, which respectively use 3, 6 and 12 attention heads. To obtain models of similar sizes, we use two methods of comparison.</p><p>• To establish a direct comparison with Touvron et al. <ref type="bibr" target="#b0">[1]</ref>, we lower the embedding dimension of the ConViTs to D emb /N h = 48 instead of 64 used for the DeiTs. Importantly, we leave all hyperparameters (scheduling, data-augmentation, regularization) presented in <ref type="bibr" target="#b0">[1]</ref> unchanged in order to achieve a fair comparison. The resulting models are named ConViT-Ti, ConViT-S and ConViT-B.</p><p>• We also trained DeiTs and ConViTs using the same number of heads and D emb /N h = 64. This leads to slightly larger models denoted with a "+" in Tab. 1. To maintain stable training while tting these models on 8 GPUs, we lowered the learning rate from 0.0005 to 0.0004 and the batch size from 1024 to 512. These minimal hyperparameter changes lead the DeiT-B+ to perform less well than the DeiT-S+, which is not the case for the ConViT.</p><p>Performance of the ConViT In Tab. 1, we display the top-1 accuracy achieved by these models evaluated on the ImageNet test set after 300 epochs of training, alongside their number of parameters, number of ops and throughput. Each ConViT outperforms its DeiT of same size and same number of ops by a margin. Importantly, although the positional self-attention does slow down the throughput of the ConViTs, they also outperform the DeiTs at equal throughput. For example, The ConViT-S+ reaches a top-1 of 82.2%, outperforming the original DeiT-B with less parameters and higher throughput. Without any tuning, the ConViT also reaches high performance on CIFAR100, see SM. C where we also report learning curves.</p><p>Note that our ConViT is compatible with the distillation methods introduced in Touvron et al. <ref type="bibr" target="#b0">[1]</ref> at no extra cost. As shown in SM. B, hard distillation improves performance, enabling the hard-distilled ConViT-S+ to reach 82.9% top-1 accuracy, on the same footing as the hard-distilled DeiT-B with half the number of parameters. However, while distillation requires an additional forward pass through a pre-trained CNN at each step of training, ConViT has no such requirement, providing similar bene ts to distillation without additonal computational requirements.</p><p>Sample e ciency of the ConViT In Tab. 2, we investigate the sample-e ciency of the ConViT in a systematic way, by subsampling each class of the ImageNet-1k dataset by a fraction f = {0.05, 0.1, 0.3, 0.5, 1} while multiplying the number of epochs by 1/f so that the total number images presented to the model remains constant. As one might expect, the top-1 accuracy of both the DeiT-S and its ConViT-S counterpart drops as f decreases. However, the ConViT su ers much less: while training on only 10% of the data, the ConVit reaches 59.5% top-1 accuracy, compared to 46.5% for its DeiT counterpart.</p><p>This result can be directly compared to <ref type="bibr" target="#b40">[41]</ref>, which after testing several thousand convolutional models reaches a top-1 accuracy of 56.4%; the ConViT is therefore highly competitive in terms of sample e ciency. These ndings con rm our hypothesis that the convolutional inductive bias is most helpful on small datasets, as depicted in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Investigating the role of locality</head><p>In this section, we demonstrate that locality is naturally encouraged in standard SA layers, and examine how the ConViT bene ts from locality being imposed at initialization.</p><p>SA layers are pulled towards locality We begin by investigating whether the hypothesis that PSA layers are naturally encouraged to become "local" over the course of training <ref type="bibr" target="#b31">[32]</ref> holds for the vanilla SA layers used in ViTs, which do not bene t from positional attention. To quantify this, we de ne a measure of "nonlocality" by summing, for each query patch i, the distances δ ij to all the key patches j weighted by their attention score A ij . We average the number obtained over the query patch to obtain the nonlocality metric of head h, which can then be averaged over the attention heads to obtain the nonlocality of the whole layer :  Intuitively, D loc is the number of patches between the center of attention and the query patch: the further the attention heads look from the query patch, the higher the nonlocality. In <ref type="figure" target="#fig_7">Fig. 5 (left panel)</ref>, we show how the nonlocality metric evolves during training across the 12 layers of a DeiT-S trained for 300 epochs on ImageNet. During the rst few epochs, the nonlocality falls from its initial value in all layers, con rming that the DeiT becomes more "convolutional". During the later stages of training, the nonlocality metric stays low for lower layers, and gradually climbs back up for upper layers, revealing that the latter capture long range dependencies, as observed for language Transformers <ref type="bibr" target="#b39">[40]</ref>.</p><formula xml:id="formula_9">D ,h loc := 1 L ij A h, ij δ ij , D loc := 1 N h h D ,h loc<label>(8)</label></formula><p>These observations are particularly clear when examining the attention maps ( <ref type="figure" target="#fig_7">Fig. 15</ref> of the SM), and point to the bene cial e ect of locality in lower layers. In <ref type="figure" target="#fig_13">Fig. 10</ref> of the SM., we also show that the nonlocality metric is lower when training with distillation from a convolutional network as in Touvron et al. <ref type="bibr" target="#b0">[1]</ref>, suggesting that the locality of the teacher is partly transferred to the student <ref type="bibr" target="#b41">[42]</ref>.</p><p>GPSA layers escape locality In the ConViT, strong locality is imposed at the beginning of training in the GPSA layers thanks to the convolutional initialization. In <ref type="figure" target="#fig_7">Fig. 5 (right panel)</ref>, we see that this local con guration is escaped throughout training, as the nonlocality metric grows in all the GPSA layers. However, the nonlocality at the end of training is lower than that reached by the DeiT, showing that some information about the initialization is preserved throughout training. Interestingly, the nal nonlocality does not increase monotonically throughout the To gain more understanding, we examine the dynamics of the gating parameters in <ref type="figure" target="#fig_8">Fig. 6</ref>. We see that in all layers, the average gating parameter E h σ(λ h ) (in black), which re ects the average amount of attention paid to positional information versus content, decreases throughout training. This quantity reaches 0 in layers 6-10, meaning that positional information is practically ignored. However, in layers 1-5, some of the attention heads keep a high value of σ(λ h ), hence take advantage of positional information. Interestingly, the ConViT-Ti only uses positional information up to layers 4, whereas the ConViT-B uses it up to layer 6 (see App. D), suggesting that larger modelswhich are more under-speci ed -bene t more from the convolutional prior. These observations highlight the usefulness of the gating parameter in terms of interpretability.</p><p>The inner workings of the ConViT are further revealed by the attention maps of <ref type="figure" target="#fig_9">Fig. 7</ref>. In layer 10, (bottom row), the attention maps of DeiT and Con-ViT look qualitatively similar: they both perform content-based attention. In layer 2 however (top row), the attention maps of the ConViT are more varied: some heads pay attention to content (heads 1 and 2) whereas other focus mainly on position (heads 3 and 4). Among the heads which focus on position, some stay highly localized (head 4) whereas others broaden their attention span (head 3). In each map, we indicated the value of the gating parameter in a color varying from white (for heads paying attention to content) to red (for heads paying attention to position). Attention maps for more images and more heads are shown in SM. E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strong locality is desirable</head><p>We next investigate how the performance of the ConViT is a ected by two important hyperparameters of the ConViT: the locality strength, α, which determines how focused the heads are around their center of attention, and the number of SA layers replaced by GPSA layers. We examined the e ects of these hyperparameters on ConViT-S, trained on the rst 100 classes of ImageNet. As shown in <ref type="figure">Fig. 8(a)</ref>, nal test accuracy increases both with the locality strength and with the number of GPSA layers; in other words, the more convolutional, the better. In <ref type="figure">Fig. 8(b)</ref>, we show how performance at various stages of training is impacted by the presence of GPSA layers. We see that the boost due to GPSA is particularly strong during the early stages of training: after 20 epochs, using 9 GPSA layers leads the test-accuracy to almost double, suggesting that the convolution ini-  <ref type="figure">Figure 8</ref>: The bene cial e ect of locality. Left: As we increase the locality strength (i.e. how focused each attention head is its associated patch) and the number of GPSA layers of a ConViT-S+, the nal top-1 accuracy increases signi cantly. Right: The bene cial e ect of locality is particularly strong in the early epochs.  <ref type="table">Table 3</ref>: Gating and convolutional initialization play nicely together. We ran an ablation study on the ConViT-S+ trained for 300 epochs on the rst 100 classes of ImageNet-1k. From the left column to right column, we experimented freezing the gating parameters to 0, removing the convolutional initialization, removing the GPSA layers altogether and freezing all attention modules. tialization gives the model a substantial "head start". This speedup is of practical interest in itself, on top of the boost in nal performance.</p><p>Ablation study In Tab. 3, we present an ablation study on the ConViT, denoted as (a). We experiment removing the positional gating (b) 3 , the convolutional initialization (c), both gating and the convolutional initialization (d), and the GPSA altogether ((e), which leaves us with a plain DeiT). Surprisingly, GPSA alone (d) already brings a substantial bene t over the DeiT (+2.5 top-1), demonstrating the importance of positional attention. Adding the convolutional initialization (b) yields further improvement (+2.9), which becomes even stronger when training the gating parameters ((a), +3.1). Interestingly, gating is useful in presence of the convolutional initialization, but lowers performance otherwise, as (c) performs worse than (d).</p><p>We also investigated the performance of the Con-ViT with all attention modules frozen in the GPSA layers, leaving only the patch embeddings and FFNs to be trained. As one could expect, performance is strongly degraded if we initialize the attention modules randomly ((f), -8.5 compared to (a)). But remarkably, the convolutional initialization enables the frozen Con-ViT to reach a very decent performance ((g), -3.6 compared to (a)), almost equalling that of the DeiT (e). In other words, replacing SA layers by random convolutions hardly impacts performance. This naturally begs the question: is attention really key to the success of ViTs?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and perspectives</head><p>The present work investigates the importance of initialization and inductive biases in learning with the ViT. By showing that one can take advantage of convolutional constraints in a soft way, we merge the bene ts of architectural priors and expressive power. The result is a simple recipe that improves trainability and sample e ciency, without increasing model size or requiring any tuning.</p><p>Our approach can be summarized as follows: instead of interleaving convolutional layers with SA layers as in hybrid models, let the layers decide whether to be convolutional or not by adjusting a set of gating parameters. More generally, combining the biases of varied architectures and letting the model choose which ones are best for a given task could become a promising direction, reducing the need for greedy architectural search while o ering higher interpretability. We plan to explore this direction in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The importance of positional gating</head><p>In the main text, we discussed the importance of using GPSA layers instead of the standard PSA layers, where content and positional information are summed before the softmax and lead the attention heads to focus only on the positional information. We give evidence for this claim in <ref type="figure" target="#fig_12">Fig. 9</ref>, where we train a ConViT-B for 300 epochs on ImageNet, but replace the GPSA by standard PSA. The convolutional initialization of the PSA still gives the ConViT a large advantage over the DeiT baseline early in training. However, the ConViT stays in the convolutional con guration and ignores the content information, as can be seen by looking at the attention maps  B The e ect of distillation Nonlocality In <ref type="figure" target="#fig_13">Fig. 10</ref>, we compare the nonlocality curves of <ref type="figure" target="#fig_7">Fig. 5</ref> of the main text with those obtained when the DeiT is trained via hard distillation from a RegNetY-16GF (84M parameters) <ref type="bibr" target="#b42">[43]</ref>, as in Touvron et al. <ref type="bibr" target="#b0">[1]</ref>. In the distillation setup, the nonlocality still drops in the early epochs of training, but increases less at late times compared to without distillation. Hence, the nal internal states of the DeiT are more "local" due to the distillation. This suggests that knowledge distillation transfers the locality of the convolutional teacher to the student, in line with the results of <ref type="bibr" target="#b41">[42]</ref>.</p><p>Performance The hard distillation introduced in Touvron et al. <ref type="bibr" target="#b0">[1]</ref> greatly improves the performance of the DeiT. We have veri ed the complementarity of their distillation methods with our ConViT. In the same way as in the DeiT paper, we used a RegNet-16GF teacher and experimented hard distillation during 300 epochs on ImageNet. The results we obtain are summarized in Tab. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>DeiT  hints to the fact that the convolutional inductive bias transferred from the teacher is redundant with its own convolutional prior. Nevertheless, the performance improvement obtained by the ConViT with hard distillation demonstrates that instantiating soft inductive biases directly in a model can yield bene ts on top of those obtained by instantiating such biases indirectly, in this case via distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Further performance results</head><p>In <ref type="figure">Fig. 11</ref>, we display the time evolution of the top-1 accuracy of our ConViT+ models on CIFAR100, ImageNet and subsampled ImageNet, along with a comparison with the corresponding DeiT+ models.</p><p>For CIFAR100, we kept all hyperparameters unchanged, but rescaled the images to 224 × 224 and increased the number of epochs (adapting the learning rate schedule correspondingly) to mimic the ImageNet scenario. After 1000 epochs, the ConViTs shows clear signs of over tting, but reach impressive performances (82.1% top-1 accuracy with 10M parameters, which is better than the E cientNets reported in <ref type="bibr" target="#b43">[44]</ref>).</p><p>In <ref type="figure" target="#fig_3">Fig. 12</ref>, we study the impact of the various ingredients of the ConViT (presence and number of GPSA layers, gating parameters, convolutional initialization) on the dynamics of learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D E ect of model size</head><p>In <ref type="figure" target="#fig_5">Fig. 13</ref>, we show the analog of <ref type="figure" target="#fig_7">Fig. 5</ref> of the main text for the tiny and base models. Results are qualitatively similar to those observed for the small model. Interestingly, the rst layers of DeiT-B and ConViT-B reach signi cantly higher nonlocality than those of the DeiT-Ti and ConViT-Ti.</p><p>In <ref type="figure" target="#fig_6">Fig. 14, we</ref> show the analog of <ref type="figure" target="#fig_8">Fig. 6</ref> of the main text for the tiny and base models. Again, results are qualitatively similar: the average weight of the positional attention, E h σ(λ h ), decreases over time, so that more attention goes to the content of the image. Note that in the ConViT-Ti, only the rst 4 layers still pay attention to position at the end of training (average gating parameter smaller than one), whereas for ConViT-S, the 5 rst layers still do, and for the ConViT-B, the 6 rst layers still do. This suggests that the larger (i.e. the more underspeci ed) the model is, the more layers make use of the convolutional prior.  <ref type="figure">Figure 11</ref>: The convolutional inductive bias is particularly useful for large models applied to small datasets. Each of the three panels displays the top-  <ref type="figure" target="#fig_5">Figure 13</ref>: The bigger the model, the more non-local the attention. We plotted the nonlocality metric de ned in Eq. 8 of the main text (the higher, the further the attention heads look from the query pixel) throughout 300 epochs of training on ImageNet-1k.  <ref type="figure" target="#fig_6">Figure 14</ref>: The bigger the model, the more layers pay attention to position. We plotted the gating parameters of various heads and various layers, as in <ref type="figure" target="#fig_8">Fig. 6</ref> of the main text (the lower, the less attention is paid to positional information) throughout 300 epochs of training on ImageNet-1k. Note that the ConViT-Ti only has 4 attention heads whereas the ConViT-B has 16, hence the di erent number of curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Attention maps</head><p>Attention maps of the DeiT reveal locality In <ref type="figure" target="#fig_7">Fig. 15</ref>, we give some visual evidence for the fact that vanilla SA layers extract local information by averaging the attention map of the rst and tenth layer of the DeiT over 100 images. Before training, the maps look essentially random. After training, however, most of the attention heads of the rst layer focus on the query pixel and its immediate surroundings, whereas the attention heads of the tenth layer capture long-range dependencies.  <ref type="figure" target="#fig_7">Figure 15</ref>: The averaged attention maps of the DeiT reveal locality at the end of training. To better visualise the center of attention, we averaged the attention maps over 100 images. Top: before training, the attention patterns exhibit a random structure. Bottom: after training, most of the attention is devoted to the query pixel, and the rest is focused on its immediate surroundings.</p><p>Attention maps of the ConViT reveal the diversity of the attention heads In <ref type="figure" target="#fig_8">Fig. 16</ref>, we show a comparison of the attention maps of Deit-Ti and ConViT-Ti for di erent images of the ImageNet validation set. In <ref type="figure" target="#fig_9">Fig. 17</ref>, we compare the attention maps of DeiT-S and ConViT-S. In all cases, results are qualitatively similar: the DeiT attention maps look similar across di erent heads and di erent layers, whereas those of the ConViT perform very di erent operations. Notice that in the second layer, the three rst heads decided to stay local whereas the fourth head focuses on the content. In the last layer, all the heads ignore positional information, focusing only on content. ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 (c) ConViT <ref type="figure" target="#fig_8">Figure 16</ref>: Left: input image which is embedded then fed into the models. The query patch is highlighted by a red box and the colormap is logarithmic to better reveal details. Center: attention maps obtained by a DeiT-Ti after 300 epochs of training on ImageNet. Right: Same for ConViT-Ti. In each map, we indicated the value of the gating parameter in a color varying from white (for heads paying attention to content) to red (for heads paying attention to position). ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 (c) ConViT <ref type="figure" target="#fig_9">Figure 17</ref>: Top: attention maps obtained by a DeiT-S after 300 epochs of training on ImageNet. Bottom: Same for ConViT-S. In each map, we indicated the value of the gating parameter in a color varying from white (for heads paying attention to content) to red (for heads paying attention to position).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The ConViT outperforms the DeiT both in sample and parameter e ciency. Left: we compare the sample e ciency of our ConViT-S (see Tab. 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Convolutional initialization, strength α = 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Positional self-attention layers can be initialized as convolutional layers. (a): Input image from ImageNet, where the query patch is highlighted by a red box. (b),(c),(d): attention maps of an untrained SA layer (b) and those of a PSA layer using the convolutional-like initialization scheme of Eq. 5 with two di erent values of the locality strength parameter, α (c, d). Note that the shapes of the image can easily be distinguished in (b), but not in (c) or (d), when the attention is purely positional.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Architecture of the ConViT. The ConViT (left) is a version of the ViT in which some of the self-attention (SA) layers are replaced with gated positional self-attention layers (GPSA; right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>SA layers try to become local, GPSA layers escape locality. We plot the nonlocality metric de ned in Eq. 8 (the higher, the further the attention heads look from the query pixel) throughout the training of a DeiT-S and a ConViT-S on ImageNet. Similar results for DeiT-Ti/ConViT-Ti and DeiT-B/ConViT-B are shown in SM. D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>The gating parameters reveal the inner workings of the ConViT. For each layer, the colored lines (one for each of the 9 attention heads) quantify how much attention head h pays to positional information versus content, i.e. the value of σ(λ h ), see Eq. 7. The black line represents the value averaged over all heads. We trained the ConViT-S for 300 epochs on Im-ageNet. Similar results for ConViT-Ti and ConViT-B are shown in SM D. layers as for the DeiT. The rst layer and the nal layers strongly escape locality, whereas the intermediate layers (particularly the second layer) stay more local.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>The ConViT learns more diverse attention maps. Left: input image which is embedded then fed into the models. The query patch is highlighted by a red box and the colormap is logarithmic to better reveal details. Center: attention maps obtained by a DeiT-Ti after 300 epochs of training on ImageNet. Right: Same for ConViT-Ti.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(not shown). Later in training, the DeiT catches up and surpasses the performance of the ConViT by utilizing content information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Convolutional initialization without GPSA is helfpul during early training but deteriorates nal performance. We trained the ConViT-B along with its DeiT-B counterpart for 300 epochs on ImageNet, replacing the GPSA layers of the ConViT-B by vanilla PSA layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Distillation pulls the DeiT towards a more local con guration. We plotted the nonlocality metric de ned in Eq. 8 throughout training, for the DeiT-S trained on ImageNet. Left: regular training. Right: training with hard distillation from a RegNet teacher, by means of the distillation introduced in ([1]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>2 ( 10 ( 2 (</head><label>2102</label><figDesc>) = 0.00 ( ) = 0.09 ( ) = 0.51 ( ) = 0.73 Layer ) = 0.00 ( ) = 0.09 ( ) = 0.51 ( ) = 0.73 Layer 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>3 (</head><label>3</label><figDesc>) = 0.67 ( ) = 0.80 ( ) = 0.65 ( ) = 0.88 ( ) = 0.00 ( ) = 0.51 ( ) = 0.00 ( ) = 0.88 ( ) = 0.00 Layer 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance of the models considered, trained from scratch on ImageNet.</figDesc><table><row><cell>Speed is the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>the sigmoid function. By setting the gating parameter λ h to a large positive value at initialization, one has σ(λ h ) 1 : the GPSA bases its attention purely on position, dispensing with the need of setting W qry and W key to zero as in Eq. 5. However, to avoid the ConViT staying stuck at λ h 1, we initialize λ h = 1 for all layers and all heads.Architectural details The ViT slices input images of size 224 into 16 × 16 non-overlapping patches of</figDesc><table><row><cell>Train</cell><cell></cell><cell>Top-1</cell><cell></cell><cell></cell><cell>Top-5</cell><cell></cell></row><row><cell>size</cell><cell cols="6">DeiT ConViT Gap DeiT ConViT Gap</cell></row><row><cell>5%</cell><cell>34.8</cell><cell>47.8</cell><cell>37%</cell><cell>57.8</cell><cell>70.7</cell><cell>22%</cell></row><row><cell>10%</cell><cell>48.0</cell><cell>59.6</cell><cell>24%</cell><cell>71.5</cell><cell>80.3</cell><cell>12%</cell></row><row><cell>30%</cell><cell>66.1</cell><cell>73.7</cell><cell>12%</cell><cell>86.0</cell><cell>90.7</cell><cell>5%</cell></row><row><cell>50%</cell><cell>74.6</cell><cell>78.2</cell><cell>5%</cell><cell>91.8</cell><cell>93.8</cell><cell>2%</cell></row><row><cell>100%</cell><cell>79.9</cell><cell>81.4</cell><cell>2%</cell><cell>95.0</cell><cell>95.8</cell><cell>1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The convolutional inductive bias strongly improves sample e ciency. We compare the top-1 and top-5 accuracy of our ConViT-S with that of the DeiT-S, both trained using the original hyperparameters of the DeiT<ref type="bibr" target="#b0">[1]</ref>, as well as the relative improvement of the ConViT over the DeiT.</figDesc><table><row><cell>Both models are trained on a subsampled version of</cell></row><row><cell>ImageNet-1k, where we only keep a variable frac-</cell></row><row><cell>tion (leftmost column) of the images of each class for</cell></row><row><cell>training.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Top-1 accuracies of the ConViT-S+ compared to the DeiT-S and DeiT-B, both trained for 300 epochs on ImageNet.Just like the DeiT, the ConViT bene ts from distillation, albeit somewhat less than the DeiT, as can be seen from the DeiT-B performing less well than the ConViT-S+ without distillation but better with distillation. This</figDesc><table><row><cell></cell><cell cols="3">-S (22M) DeiT-B (86M) ConViT-S+ (48M)</cell></row><row><cell>No distillation</cell><cell>79.8</cell><cell>81.8</cell><cell>82.2</cell></row><row><cell>Hard distillation</cell><cell>80.9</cell><cell>83.0</cell><cell>82.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>1 accuracy of the ConViT+ model and their corresponding DeiT+ throughout training, as well as the relative improvement between the best top-1 accuracy reached by the DeiT+ and that reached by the ConViT+. Left: tiny, small and base models trained for 3000 epochs on CIFAR100. Middle: tiny, small and base models trained for 300 epochs on ImageNet-1k. The relative improvement of the ConViT over the DeiT increases with model size. Right: small model trained on a subsampled version of ImageNet-1k, where we only keep a fraction f ∈ {0.1, 0.3, 1} of the images of each class. The relative improvement of the ConViT over the DeiT increases with the size of the dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Non-locality</cell><cell>3 4 5 6 7</cell><cell>0</cell><cell>100 200 300 Epochs DeiT</cell><cell>3 4 5 6 7</cell><cell>0</cell><cell>100 200 300 Epochs ConViT</cell><cell>Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) DeiT-Ti and ConViT-Ti</cell></row><row><cell>Top-1 accuracy</cell><cell>40 50 60 70 80</cell><cell>0</cell><cell cols="6">100 Epochs 200 0 GPSA layers 300 0 100 200 300 Epochs 3 4 5 6 7 Non-locality DeiT 3 GPSA layers 9 GPSA layers 6 GPSA layers (b) DeiT-B and ConViT-B 0 100 200 300 Epochs 3 4 5 6 7 ConViT</cell><cell>Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also experimented incorporating the class token as an extra patch of the image to which all heads pay attention to at initialization, but results were worse than concatenating the class token after the GPSA layers (not shown).2 https://github.com/facebookresearch/deit</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">To remove gating, we freeze all gating parameters to λ = 0 so that the same amount of attention is paid to content and position.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Hervé Jégou and Francisco Massa for helpful discussions. We acknowledge funding from the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Training data-e cient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The need for biases in learning generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Laboratory for Computer Science Research . . .</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet classi cation with deep convolutional neural networks&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>Rey E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM 60</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="84" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition&apos;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15825-4_10</idno>
	</analytic>
	<monogr>
		<title level="m">Arti cial Neural Networks -IC-ANN 2010</title>
		<editor>Konstantinos Diamantaras, Wlodek Duch and Lazaros S. Iliadis</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
	<note>Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview&apos;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2014.09.003</idno>
		<idno>10 . 1016 / j . neunet . 2014 . 09 . 003. : http :</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="893" to="6080" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural image statistics and neural representation&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruno A Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1193" to="1216" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistics of natural images: Scaling in the woods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">814</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">LSTM neural networks for language modeling&apos;. In: Thirteenth annual conference of the international speech communication association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LSTM: A Search Space Odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gre</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2016.2582924</idno>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2162" to="2388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A2-Nets: Double Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11579</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>Rey Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Uniter: Universal imagetext representation learning&apos;. In: European Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Object-centric learning with slot attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15055</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bottleneck Transformers for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<idno>arXiv: 2101 . 11605</idno>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Visual Transformers: Tokenbased Image Representation and Processing for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<idno>arXiv: 2006.03677</idno>
		<ptr target="http://arxiv.org/abs/2006.03677" />
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
	<note>cs, eess. visited on 15/10/2020</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR. 2019</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tokens-to-Token ViT: Training Vision Transformers from Scratch on Im-ageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On the relationship between selfattention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03584</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Revisiting spatial invariance with low-rank local connectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamaleldin</forename><surname>Elsayed</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="2868" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31. Ed. by S. Bengio et al. Curran Associates</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9401" to="9411" />
		</imprint>
	</monogr>
	<note>Visited on 19/10/2020</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00745</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local Neural Networks&apos;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00813</idno>
		<idno>10 . 1109 / CVPR . 2018 . 00813</idno>
		<ptr target="https://ieeexplore.ieee.org/document/8578911/(visitedon15/10/2020" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards learning convolutions from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Behnam Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Finding the Needle in the Haystack with Convolutions: on the benets of architectural bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stéphane D&amp;apos;ascoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9334" to="9345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Homotopy analysis for tensor pca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09322</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07799</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Transferring inductive biases through knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00555</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</meeting>
		<imprint>
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">SplitNet: Divide and Cotraining&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14660</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Right: ablation on various ingredients of the ConViT, as in Tab. 3. The baseline is the DeiT-S+ (pink)</title>
	</analytic>
	<monogr>
		<title level="m">Impact of various ingredients of the ConViT on the dynamics of learning. In both cases, we train the ConViT-S+ for 300 epochs on rst 100 classes of ImageNet</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>GPSA&quot;) (ii) freezing the gating parameter of the GPSA layers. frozen gate&quot;); (iii) removing the convolutional initialization (&quot;conv&quot;); (iv) freezing all attention modules in the GPSA layers (&quot;frozen&quot;). The nal top-1 accuracy of the various models trained is reported in the legend</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
