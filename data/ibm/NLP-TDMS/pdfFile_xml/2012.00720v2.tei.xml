<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fully Convolutional Networks for Panoptic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<addrLine>Kong 3 MEGVII Technology 4 SmartMore 5</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fully Convolutional Networks for Panoptic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a conceptually simple, strong, and efficient framework for panoptic segmentation, called Panoptic FCN. Our approach aims to represent and predict foreground things and background stuff in a unified fully convolutional pipeline. In particular, Panoptic FCN encodes each object instance or stuff category into a specific kernel weight with the proposed kernel generator and produces the prediction by convolving the high-resolution feature directly. With this approach, instance-aware and semantically consistent properties for things and stuff can be respectively satisfied in a simple generate-kernel-thensegment workflow. Without extra boxes for localization or instance separation, the proposed approach outperforms previous box-based and -free models with high efficiency on COCO, Cityscapes, and Mapillary Vistas datasets with single scale input. Our code is made publicly available at https://github.com/Jia-Research-Lab/PanopticFCN. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Panoptic segmentation, aiming to assign each pixel with a semantic label and unique identity, is regarded as a challenging task. In panoptic segmentation <ref type="bibr" target="#b18">[19]</ref>, countable and uncountable instances (i.e., things and stuff) are expected to be represented and resolved in a unified workflow. One main difficulty impeding unified representation comes from conflicting properties requested by things and stuff. Specifically, to distinguish among various identities, countable things usually rely on instance-aware features, which vary with objects. In contrast, uncountable stuff would prefer semantically consistent characters, which ensures consistent predictions for pixels with the same semantic meaning. An example is given in <ref type="figure">Fig. 1</ref>, where embedding of individuals should be diverse for inter-class variations, while characters of grass should be similar for intra-class consistency. <ref type="bibr" target="#b0">1</ref> Part of the work was done in MEGVII Research.  <ref type="figure">Figure 1</ref>. Compared with traditional methods, which often utilize separate branches to handle things and stuff in 1(a), the proposed Panoptic FCN 1(b) represents things and stuff uniformly with generated kernels. Here, an example with box-based stream for things is given in 1(a). The shared backbone is omitted for concision.</p><p>For conflict at feature level, specific modules are usually tailored for things and stuff separately, as presented in <ref type="figure">Fig. 1(a)</ref>. In particular, instance-aware demand of things is satisfied mainly from two streams, namely box-based <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b24">25]</ref> and box-free <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6]</ref> methods. Meanwhile, the semantic-consistency of stuff is met in a pixel-by-pixel manner <ref type="bibr" target="#b32">[33]</ref>, where similar semantic features would bring identical predictions. A classic case is Panoptic FPN <ref type="bibr" target="#b17">[18]</ref>, which utilizes Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> and FCN <ref type="bibr" target="#b32">[33]</ref> in separated branches to respectively classify things and stuff, similar to that of <ref type="figure">Fig. 1(a)</ref>. Although attempt <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6]</ref> has been made to predict things without boxes, extra predictions (e.g., affinities <ref type="bibr" target="#b9">[10]</ref>, and offsets <ref type="bibr" target="#b50">[51]</ref>) together with postprocess procedures are still needed to distinguish among instances, which slow down the whole system and hinder it from being fully convolutional. Consequently, a unified representation is required to bridge this gap.</p><p>In this paper, we propose a fully convolutional framework for unified representation, called Panoptic FCN. In particular, Panoptic FCN encodes each instance into a specific kernel and generates the prediction by convolutions directly. Thus, both things and stuff can be predicted together with a same resolution. In this way, instance-aware and semantically consistent properties for things and stuff can be respectively satisfied in a unified workflow, which is briefly illustrated in <ref type="figure">Fig. 1(b)</ref>. To sum up, the key idea of Panoptic FCN is to represent and predict things and stuff uniformly with generated kernels in a fully convolutional pipeline.</p><p>To this end, kernel generator and feature encoder are respectively designed for kernel weights generation and shared feature encoding. Specifically, in kernel generator, we draw inspirations from point-based object detectors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b54">55]</ref> and utilize the position head to locate as well as classify foreground objects and background stuff by object centers and stuff regions, respectively. Then, we select kernel weights <ref type="bibr" target="#b16">[17]</ref> with the same positions from the kernel head to represent corresponding instances. For the instance-awareness and semantic-consistency described above, a kernel-level operation, called kernel fusion, is further proposed, which merges kernel weights that are predicted to have the same identity or semantic category. With a naive feature encoder, which preserves the high-resolution feature with details, each prediction of things and stuff can be produced by convolving with generated kernels directly.</p><p>In general, the proposed method can be distinguished from two aspects. Firstly, different from previous work for things generation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45]</ref>, which outputs dense predictions and then utilizes NMS for overlaps removal, the deigned framework generates instance-aware kernels and produces each specific instance directly. Moreover, compared with traditional FCN-based methods for stuff prediction <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>, which select the most likely category in a pixel-by-pixel manner, our approach aggregates global context into semantically consistent kernels and presents results of existing semantic classes in a whole-instance manner.</p><p>The overall approach, named Panoptic FCN, can be easily instantiated for panoptic segmentation, which will be fully elaborated in Sec. 3. To demonstrate its superiority, we give extensive ablation studies in Sec. 4.2. Furthermore, experimental results are reported on COCO <ref type="bibr" target="#b28">[29]</ref>, Cityscapes <ref type="bibr" target="#b7">[8]</ref>, and Mapillary Vistas <ref type="bibr" target="#b34">[35]</ref> datasets. Without bells-and-whistles, Panoptic FCN outperforms previous methods with efficiency, and respectively attains 44.3% PQ and 47.5% PQ on COCO val and test-dev set. Meanwhile, it surpasses all similar box-free methods by large margins and achieves leading performance on Cityscapes and Mapillary Vistas val set with 61.4% PQ and 36.9% PQ, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Panoptic segmentation. Traditional approaches mainly conduct segmentation for things and stuff separately. The benchmark for panoptic segmentation <ref type="bibr" target="#b18">[19]</ref> directly combines predictions of things and stuff from different models, causing heavy computational overhead. To solve this problem, methods have been proposed by dealing with things and stuff in one model but in separate branches, including Panoptic FPN <ref type="bibr" target="#b17">[18]</ref>, AUNet <ref type="bibr" target="#b24">[25]</ref>, and UPSNet <ref type="bibr" target="#b49">[50]</ref>. From the view of instance representation, previous work mainly formats things and stuff from different perspectives. Foreground things are usually separated and represented with boxes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24]</ref> or aggregated according to center offsets <ref type="bibr" target="#b50">[51]</ref>, while background stuff is often predicted with a parallel FCN <ref type="bibr" target="#b32">[33]</ref> branch. Although methods of <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10]</ref> represent things and stuff uniformly, the inherent ambiguity cannot be resolved well merely with the pixel-level affinity, which yields the performance drop in complex scenarios. In contrast, the proposed Panoptic FCN represents things and stuff in a uniform and fully convolutional framework with decent performance and efficiency.</p><p>Instance segmentation. Instance segmentation aims to discriminate objects in the pixel level, which is a finer representation compared with detected boxes. For instanceawareness, previous works can be roughly divided into two streams, i.e., box-based methods and box-free approaches. Box-based methods usually utilize detected boxes to locate or separate objects <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38]</ref>. Meanwhile, box-free approaches are designed to generate instances without assistance of object boxes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. Recently, AdaptIS <ref type="bibr" target="#b39">[40]</ref> and CondInst <ref type="bibr" target="#b41">[42]</ref> are proposed to utilize point-proposal for instance segmentation. However, the instance aggregation or object-level removal is still needed for results. In this paper, we represent objects in a box-free pipeline, which generates the kernel for each object and produces results by convolving the detail-rich feature directly, with no need for object-level duplicates removal <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37]</ref>. Semantic segmentation. Semantic segmentation assigns each pixel with a semantic category, without considering diverse object identities. In recent years, rapid progress has been made on top of FCN <ref type="bibr" target="#b32">[33]</ref>. Due to the semantically consistent property, several attempts have been made to capture contextual cues from wider perception fields <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> or establish pixel-wise relationship for long-range dependencies <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref>. There is also work to design network architectures for semantic segmentation automatically <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26]</ref>, which is beyond the scope of this paper. Our proposed Panoptic FCN adopts a similar method to represent things and stuff, which aggregates global context into a specific kernel to predict corresponding semantic category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Panoptic FCN</head><p>Panoptic FCN is conceptually simple: kernel generator is introduced to generate kernel weights for things and stuff with different categories; kernel fusion is designed to merge kernel weights with the same identity from multiple stages; and feature encoder is utilized to encode the high-resolution feature. In this section, we elaborate on the above components as well as the training and inference scheme. <ref type="figure">Figure 2</ref>. The framework of Panoptic FCN. The proposed framework mainly contains three components, namely kernel generator, kernel fusion, and feature encoder. In kernel generator, position head is designed to locate and classify object centers along with stuff regions; kernel head in each stage is used to generate kernel weights for both things and stuff. Then, kernel fusion is utilized to merge kernel weights with the same identity from different stages. And feature encoder is adopted to encode the high-resolution feature with details. With the generated kernel weight for each instance, both things and stuff can be predicted with a simple convolution directly. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Kernel Generator</head><p>Given a single stage feature X i from the i-th stage in FPN <ref type="bibr" target="#b26">[27]</ref>, the proposed kernel generator aims at generating the kernel weight map G i with positions for things L th i and stuff L st i , as depicted in <ref type="figure">Fig. 2</ref>. To this end, position head is utilized for instance localization and classification, while kernel head is designed for kernel weight generation.</p><p>Position head. With the input X i ∈ R Ci×Wi×Hi , we simply adopt stacks of convolutions to encode the feature map and generate X i , as presented in <ref type="figure">Fig. 2</ref>. Then we need to locate and classify each instance from the shared feature map X i . However, according to the definition <ref type="bibr" target="#b18">[19]</ref>, things can be distinguished by object centers, while stuff is uncountable. Thus, we adopt object centers and stuff regions to respectively represent position of each individual and stuff category. It means background regions with the same semantic meaning are viewed as one instance. In particular, object map L th i ∈ R N th ×Wi×Hi and stuff map L st i ∈ R Nst×Wi×Hi can be generated by convolutions directly with the shared feature map X i , where N th and N st denote the number of semantic category for things and stuff, respectively.</p><p>To better optimize L th i and L st i , different strategies are adopted to generate the ground truth. For the k-th object in class c, we split positive key-points onto the c-th channel of the heatmap Y th i ∈ [0, 1] N th ×Wi×Hi with Gaussian kernel, similar to that in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b54">55]</ref>. With respect to stuff, we produce the ground truth Y st i ∈ [0, 1] Nst×Wi×Hi by bilinear interpolating the one-hot semantic label to corresponding sizes. Hence, the position head can be optimized with L th pos and L st pos for object centers and stuff regions, respectively.</p><formula xml:id="formula_0">L th pos = i FL(L th i , Y th i )/N th , L st pos = i FL(L st i , Y st i )/W i H i , L pos =L th pos + L st pos ,<label>(1)</label></formula><p>where FL(·, ·) represents the Focal Loss <ref type="bibr" target="#b27">[28]</ref> for optimization. For inference, D th i = (x, y) : 1(L th i,c,x,y ) = 1 and D st i = (x, y) : 1(L st i,c,x,y ) = 1 are selected to respectively represent the existence of object centers and stuff regions in corresponding positions with predicted categories O i . This process will be further explained in Sec. 3.4.</p><p>Kernel head. In kernel head, we first capture spatial cues by directly concatenating relative coordinates to the feature X i , which is similar with that in CoordConv <ref type="bibr" target="#b30">[31]</ref>. With the concatenated feature map X i ∈ R (Ci+2)×Wi×Hi , stacks of convolutions are adopted to generate the kernel weight map G i ∈ R Ce×Wi×Hi , as presented in <ref type="figure">Fig. 2</ref>. Given predictions D th i and D st i from the position head, kernel weights with the same coordinates in G i are chosen to represent corresponding instances. For example, assuming candidate (x c , y c ) ∈ D th i , kernel weight G i,:,xc,yc ∈ R Ce×1×1 is selected to generate the result with predicted category c. The same is true for D st i . We represent the selected kernel weights in i-th stage for things and stuff as G th i and G st i , respectively. Thus, the kernel weight G th i and G st i together with predicted categories O i in the i-th stage can be produced with the proposed kernel generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Kernel Fusion</head><p>Previous work <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b44">45]</ref> utilized NMS to remove duplicate boxes or instances in the post-processing stage. Different from them, the designed kernel fusion operation merges repetitive kernel weights from multiple FPN stages before final instance generation, which guarantees instanceawareness and semantic-consistency for things and stuff, respectively. In particular, given aggregated kernel weights G th and G st from all the stages, the j-th kernel weight K j ∈ R Ce×1×1 is achieved by</p><formula xml:id="formula_1">K j = AvgCluster(G j ),<label>(2)</label></formula><p>where AvgCluster denotes average-clustering operation, and the candidate set G j = {G m : ID(G m ) = ID(G j )} includes all the kernel weights, which are predicted to have the same identity ID with G j . For object centers, kernel weight G th m is viewed as identical with G th j if the cosine similarity between them surpasses a given threshold thres, which will be further investigated in <ref type="table" target="#tab_1">Table 3</ref>. For stuff regions, all kernel weights in G st , which share a same category with G st j , are marked as one identity ID. With the proposed approach, each kernel weight K th</p><formula xml:id="formula_2">j in K th = {K th 1 , ..., K th m } ∈ R M ×Ce×1×1</formula><p>can be viewed as an embedding for single object, where the total number of objects is M . Therefore, kernels with the same identity are merged as a single embedding for things, and each kernel in K th represents an individual object, which satisfies the instance-awareness for things. Meanwhile, kernel weight K st j in K st = {K st 1 , ..., K st n } ∈ R N ×Ce×1×1 represents the embedding for all j-th class pixels, where the existing number of stuff is N . With this method, kernels with the same semantic category are fused into a single embedding, which guarantees the semantic-consistency for stuff. Thus, both properties requested by things and stuff can be fulfilled with the proposed kernel fusion operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Encoder</head><p>To preserve details for instance representation, highresolution feature F h ∈ R Ce×W/4×H/4 is utilized for feature encoding. Feature F h can be generated from FPN in several ways, e.g., P2 stage feature, summed features from all stages, and features from semantic FPN <ref type="bibr" target="#b17">[18]</ref>. These methods are compared in <ref type="table">Table 6</ref>. Given the feature F h , a similar strategy with that in kernel head is applied to encode positional cues and generate the encoded feature F e ∈ R Ce×W/4×H/4 , as depicted in <ref type="figure">Fig. 2</ref>. Thus, given M and N kernel weights for things K th and stuff K st from the kernel fusion, each instance is produced by</p><formula xml:id="formula_3">P j = K j ⊗ F e .</formula><p>Here, P j denotes the j-th prediction, and ⊗ indicates the convolutional operation. That means M +N kernel weights generate M +N instance predictions with resolution W/4× H/4 for the whole image. Consequently, the panoptic result can be produced with a simple process <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and Inference</head><p>Training scheme. In the training stage, the central point in each object and all the points in stuff regions are utilized to generate kernel weights for things and stuff, respectively. Here, Dice Loss <ref type="bibr" target="#b33">[34]</ref> is adopted to optimize the predicted segmentation, which is formulated as</p><formula xml:id="formula_4">L seg = j Dice(P j , Y seg j )/(M + N ),<label>(3)</label></formula><p>where Y seg j denotes ground truth for the j-th prediction P j . To further release the potential of kernel generator, multiple positives inside each object are sampled to represent the instance. In particular, we select k positions with top predicted scores s inside each object in L th i , resulting in k × M kernels as well as instances in total. This will be explored in <ref type="table">Table 7</ref>. As for stuff regions, the factor k is set to 1, which means all the points in same category are equally treated. Then, we replace the original loss with a weighted version</p><formula xml:id="formula_5">WDice(P j , Y seg j ) = k w k Dice(P j,k , Y seg j ),<label>(4)</label></formula><p>where w k denotes the k-th weighted score with w k = s k / i s i . According to Eqs. <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_4">(3)</ref>, optimized target L is defined with the weighted Dice Loss L seg as</p><formula xml:id="formula_6">L seg = j WDice(P j , Y seg j )/(M + N ),<label>(5)</label></formula><formula xml:id="formula_7">L = λ pos L pos + λ seg L seg .<label>(6)</label></formula><p>Inference scheme. In the inference stage, Panoptic FCN follows a simple generate-kernel-then-segment pipeline. Specifically, we first aggregate positions D th i , D st i and corresponding categories O i from the i-th position head, as illustrated in the Sec. 3.1. For object centers, we preserve the peak points in MaxPool(L th i ) utilizing a similar method with that in <ref type="bibr" target="#b54">[55]</ref>. Thus, the indicator for things 1(L th i,c,x,y ) is marked as positive if point (x, y) in the c-th channel is preserved as the peak point. Similarly, the indicator for stuff regions 1(L st i,c,x,y ) is viewed as positive if point (x, y) with category c is kept. With the designed kernel fusion and the feature encoder, the prediction P can be easily produced. Specifically, we keep the top 100 scoring kernels of objects and all the kernels of stuff after kernel fusion for instance generation. The threshold 0.4 is utilized to convert predicted soft masks to binary results. It should be noted that both the heuristic process or direct argmax could be used to generate non-overlap panoptic results. The argmax could accelerate the inference but bring performance drop (1.4% PQ). For fair comparison both from speed and accuracy, the heuristic procedure <ref type="bibr" target="#b17">[18]</ref> is adopted in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce experimental settings for Panoptic FCN. Then we conduct abundant studies on the COCO <ref type="bibr" target="#b28">[29]</ref> val set to reveal the effect of each component. Finally, comparison with previous methods on COCO <ref type="bibr" target="#b28">[29]</ref>, Cityscapes <ref type="bibr" target="#b7">[8]</ref>, and Mapillary Vistas <ref type="bibr" target="#b7">[8]</ref> dataset is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>Architecture. From the perspective of network architecture, ResNet <ref type="bibr" target="#b12">[13]</ref> with FPN <ref type="bibr" target="#b26">[27]</ref> are utilized for backbone instantiation. P3 to P7 stages in FPN are used to provide single stage feature X i for the kernel generator that is shared across all stages. Meanwhile, P2 to P5 stages are adopted to generate the high-resolution feature F h , which will be further investigated in <ref type="table">Table 6</ref>. All convolutions in kernel generator are equipped with GroupNorm <ref type="bibr" target="#b46">[47]</ref> and ReLU activation. Moreover, a naive convolution is adopted at the end of each head in kernel generator for feature projection.</p><p>Datasets. COCO dataset <ref type="bibr" target="#b28">[29]</ref> is a widely used benchmark, which contains 80 thing classes and 53 stuff classes. It involves 118K, 5K, and 20K images for training, validation, and testing, respectively. Cityscapes dataset <ref type="bibr" target="#b7">[8]</ref> consists of 5,000 street-view fine annotations with size 1024 × 2048, which are divided into 2,975, 500, and 1,525 images for training, validation, and testing, respectively. Mapillary Vistas <ref type="bibr" target="#b7">[8]</ref> is a traffic-related dataset with resolutions ranging from 1024 × 768 to more than 4000 × 6000. It includes 37 thing classes and 28 stuff classes with 18K, 2K, and 5K images for training, validation, and testing, respectively.</p><p>Optimization. Network optimization is conducted using SGD with weight decay 1e −4 and momentum 0.9. And poly schedule with power 0.9 is adopted. Experimentally, λ pos is set to a constant 1, and λ seg are respectively set to 3, 4, and 3 for COCO, Cityscapes, and Mapillary Vistas datasets. For COCO, we set initial rate to 0.01 and follow the 1× strategy in Detectron2 <ref type="bibr" target="#b47">[48]</ref> by default. We randomly flip and rescale the shorter edge from 640 to 800 pixels with 90K iterations. Herein, annotated object centers with instance scale range {(1,64), (32,128), (64,256), (128,512), (256,2048)} are assigned to P3-P7 stages, respectively. For Cityscapes, we optimize the network for 65K iterations with an initial rate 0.02 and construct each mini-batch with 32 random 512 × 1024 crops from images that are randomly rescaled from 0.5 to 2.0×. For Mapillary Vistas, the network is optimized for 150K iterations with an initial rate 0.02. In each iteration, we randomly resize images from 1024 to 2048 pixels at the shorted side and build 32 crops with the size 1024 × 1024. Due to the variation in scale distribution, we modify the assigning strategy to {(1,128), (64,256), (128,512), (256,1024), (512,2048)} for Cityscapes and Mapillary Vistas datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Component-wise Analysis</head><p>Kernel generator. Kernel generator plays a vital role in Panoptic FCN. Here, we compare several settings inside kernel generator to improve the kernel expressiveness in each stage. As presented in <ref type="table">Table 1</ref>, with the number of convolutions in each head increasing, the network performance improves steadily and achieves the peak PQ with 3 stacked Conv3 × 3 whose channel number is 256. Simi-  <ref type="bibr" target="#b55">[56]</ref> are adopted in position head to extend the receptive field, which brings further improvement, especially in stuff regions (1.4% PQ).</p><p>Position embedding. Due to the instance-aware property of objects, position embedding is introduced to provide essential cues. In <ref type="table">Table 2</ref>, we compare among several positional settings by attaching relative coordinates <ref type="bibr" target="#b30">[31]</ref> to different heads. An interesting finding is that the improvement is minor (up to 0.3% PQ) if coordinates are attached to the kernel head or feature encoder only, but it boosts to 1.4% PQ when given the positional cues to both heads. It can be attributed to the constructed correspondence in the position between kernel weights and the encoded feature.   <ref type="bibr" target="#b6">[7]</ref>. The latency is measured end-to-end from single input to panoptic result. Details are given in <ref type="table">Table 11</ref>.</p><p>Kernel fusion. Kernel fusion is a core operation in the proposed method, which guarantees the required properties for things and stuff, as elaborated in Sec. 3.2. We investigate the fusion type class-aware and similarity thresholds thres in <ref type="table" target="#tab_1">Table 3</ref>. As shown in the table, the network attains the best performance with thres 0.90. And the class-agnostic manner could dismiss some similar instances with different categories, which yields drop in AP. Furthermore, we compare kernel fusion with Matrix NMS <ref type="bibr" target="#b45">[46]</ref> which is utilized for pixel-level removal. As presented in <ref type="table">Table 4</ref>, the performance saturates with the simple kernel-level fusion method, and extra NMS brings no more gain.</p><p>Feature encoder. To enhance expressiveness of the encoded feature F e , we further explore the channel number and feature type used in feature encoder. As illustrated in <ref type="table">Table 5</ref>, the network achieves 41.3% PQ with 64 channels, and extra channels contribute little improvement. For effi- <ref type="table">Table 11</ref>. Comparisons with previous methods on the COCO val set. Panoptic FCN-400, 512, and 600 denotes utilizing smaller input instead of the default setting. All our results are achieved on the same device with single input and no flipping. FPS is measured end-to-end from single input to panoptic result with an average speed over 1,000 images, which could be further improved with more optimizations. The simple enhanced version is marked with *. The model testing by ourselves according to released codes is denoted as †. ciency, we set the channel number of feature encoder to 64 by default. As for high-resolution feature generation, three types of methods are further discussed in <ref type="table">Table 6</ref>. It is clear that Semantic FPN <ref type="bibr" target="#b17">[18]</ref>, which combines features from four stages in FPN, achieves the top performance 41.3% PQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Weighted dice loss. The designed weighted dice loss aims to release the potential of kernel generator by sampling k positive kernels inside each object. Compared with the original dice loss, which selects a single central point in each object, improvement brought by the weighted dice loss reaches 1.1% PQ, as presented in <ref type="table">Table 7</ref>. This is achieved by sampling 7 top-scoring kernels to generate results of each instance, which are optimized together in each step.</p><p>Training schedule. To fully optimize the network, we prolong the training iteration to the 3× training schedule, which is widely adopted in recent one-stage instance-level approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. As shown in <ref type="table">Table 8</ref>, 2× training schedule brings 1.9% PQ improvements and increasing iterations to 3× schedule contributes extra 0.4% PQ.</p><p>Enhanced version. We further explore model capacity by combining existing simple enhancements, e.g., deformable convolutions and extra channels. As illustrated in <ref type="table" target="#tab_3">Table 9</ref>, the simple reinforcement contributes 0.7% improvement over the default setting, marked as Panoptic FCN*.</p><p>Upper-bound analysis. In <ref type="table">Table 10</ref>, we give analysis to the upper-bound of generate-kernel-then-segment fashion with Res50-FPN backbone on the COCO val set. As illustrated in the table, given ground truth positions of object centers L th i and stuff regions L st i , the network yields 6.2% PQ from more precise locations. And it will bring extra boost (16.1% PQ) to the network if we assign ground truth categories to the position head. Compared with the baseline method, there still remains huge potential to be explored (22.3% PQ in total), especially for stuff regions which could have even up to 33.7% PQ and 42.8% mIoU gains.</p><p>Speed-accuracy. To verify the network efficiency, we plot the end-to-end speed-accuracy trade-off curve on the COCO val set. As presented in <ref type="figure" target="#fig_1">Fig. 3</ref>, the proposed Panoptic FCN surpasses all previous box-free models by large margins on both performance and efficiency. Even compared with the well-optimized Panoptic FPN <ref type="bibr" target="#b17">[18]</ref> from Detectron2 <ref type="bibr" target="#b47">[48]</ref>, our approach still attains a better speed-accuracy balance with different image scales. Details about these data points are included in <ref type="table">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main Results</head><p>We further conduct experiments on different scenarios, namely COCO dataset for common context, Cityscapes and Mapillary Vistas datasets for traffic-related environments.  Cityscapes. Furthermore, we carry out experiments on Cityscapes val set in <ref type="table" target="#tab_1">Table 13</ref>. Panoptic FCN exceeds the top box-free model <ref type="bibr" target="#b5">[6]</ref> with 1.7% PQ and attains 61.4% PQ. Even compared with the leading box-based model <ref type="bibr" target="#b23">[24]</ref>, which utilizes Lovasz loss for further optimization, the proposed method still achieves comparable performance.</p><p>Mapillary Vistas. In <ref type="table" target="#tab_6">Table 14</ref>, we compare with other state-of-the-art models on the large-scale Mapillary Vistas val set with Res50-FPN backbone. As presented in the table, the proposed Panoptic FCN exceeds previous box-free methods by a large margin in both things and stuff. Specifically, Panoptic FCN surpasses the leading box-based <ref type="bibr" target="#b35">[36]</ref> and box-free [6] models with 0.7% and 3.6% PQ, and attains 36.9% PQ with simple enhancement in the feature encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented the Panoptic FCN, a conceptually simple yet effective framework for panoptic segmentation. The key difference from prior works lies on that we represent and predict things and stuff in a fully convolutional manner. To this end, kernel generator and kernel fusion are proposed to generate the unique kernel weight for each object instance or semantic category. With the high-resolution feature produced by feature encoder, prediction is achieved by convolutions directly. Meanwhile, instance-awareness and semantic-consistency for things and stuff are respectively satisfied with the designed workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Details</head><p>Herein, we provide more technical details of the training and inference process in the proposed Panoptic FCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Training Details</head><p>Position head. To better optimize the position head, we have explored different types of center assigning strategy. For object center generation, mass center is utilized to provide the k-th ground-truth coordinate x k and y k of the main paper. As presented in <ref type="table" target="#tab_7">Table 15</ref>, compared with the box center for ground-truth generation, we find that mass center brings superior performance and higher robustness, especially in the designed weighted dice loss, which samples 7 top-scoring points inside each object. It could be attributed to that most of mass centers are located within the object area, while it is not the case for box centers. Moreover, the object size-adaptive deviation σ k of the main paper is set to (2r + 1)/3, where r denotes the gaussian radius of the k-th object similar to that in CornerNet <ref type="bibr" target="#b19">[20]</ref>.</p><p>Weighted dice loss. For objects, we select k positions whose scores are predicted to be top-k within the object region of L th i,c , where c is the annotated class. This procedure utilizes k top-scoring kernels to represent the same object and generates k results of prediction P j . Thus, we generate the same ground-truth Y seg j for k results in P j , which can be optimized with Eq. <ref type="bibr" target="#b4">(5)</ref>. For stuff regions, we merge all positions with the same category in each stage using AvgCluster, which brings a specific kernel. Hence, the factor k can be viewed as 1 for each stuff region. Due to the variation of scale distribution among datasets, we simply modify some parameters in the inference stage. In particular, for COCO dataset, we preserve 100 topscoring kernels for object prediction and utilize threshold 0.4 to convert soft masks to binary results, as illustrated in Sec. 3.4 of the main paper. For Cityscapes and Mapillary Vistas datasets, 200 kernels with top predicted scores are kept for object prediction, and cosine threshold 0.95 and mask threshold 0.5 are utilized to fuse repetitive kernels and convert binary masks, respectively. Meanwhile, a similar strategy with that in SOLO <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> is adopted to adjust predicted object scores. Furthermore, our codes for training and inference will be released to provide more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Results</head><p>We further visualize qualitative results of Panoptic FCN on several datasets with common context and traffic-related scenarios, i.e., COCO, Cityscapes, and Mapillary Vistas.</p><p>COCO. As presented in <ref type="figure" target="#fig_2">Fig. 4</ref>, Panoptic FCN gives detailed characterization to the daily environment. Thanks to the pixel-by-pixel handling manner and unified representation, details in foreground things and background stuff can be preserved. Moreover, the proposed approach also validates its effectiveness on objects with various scales in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><p>Cityscapes. As for the street view, we visualize panoptic results on the Cityscapes val set, as illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>. In addition to the well-depicted cars and pedestrians, the proposed approach achieves satisfactory performance on slender objects, like street lamps and traffic lights.</p><p>Mapillary Vistas. In <ref type="figure" target="#fig_4">Fig. 6</ref>, we further present panoptic results on the Mapillary Vistas val set, which contains larger scales traffic-related scenes. It is clear in the figure that the proposed Panoptic FCN achieves surprising results, especially on vehicles and traffic signs. The coherence of qualitative results also reflects the fulfillment of instanceawareness and semantic-consistency in Panoptic FCN.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Speed-Accuracy trade-off curve on the COCO val set. All results are compared with Res50 except DeeperLab [51] based on Xception-71</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of panoptic results on the COCO val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of panoptic results on the Cityscapes val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of panoptic results on the Mapillary Vistas val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison with different settings of the kernel generator on the COCO val set. deform and conv num respectively denote deformable convolutions for position head and number of convolutions in both heads of the kernel generator. deform conv num PQ PQ th PQ st AP mIoU Comparison with different positional settings on the COCO val set. coordw and coord f denote combining coordinates for the kernel head, and feature encoder, respectively.</figDesc><table><row><cell>1</cell><cell>38.4 43.4 31.0 28.3 39.9</cell></row><row><cell>2</cell><cell>38.9 44.1 31.1 28.9 40.1</cell></row><row><cell>3</cell><cell>39.2 44.7 31.0 29.6 40.2</cell></row><row><cell>4</cell><cell>39.2 44.9 30.8 29.4 39.9</cell></row><row><cell>3</cell><cell>39.9 45.0 32.4 29.9 41.2</cell></row><row><cell>coord w coord f</cell><cell>PQ PQ th PQ st AP mIoU</cell></row><row><cell></cell><cell>39.9 45.0 32.4 29.9 41.2</cell></row><row><cell></cell><cell>39.9 45.0 32.2 30.0 41.1</cell></row><row><cell></cell><cell>40.2 45.3 32.5 30.4 41.6</cell></row><row><cell></cell><cell>41.3 46.9 32.9 32.1 41.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>AP mIoU</cell></row></table><note>Comparison with different similarity thresholds of kernel fusion on the COCO val set. class-aware denotes only merging kernel weights with the same predicted class. And thres indicates the cosine similarity threshold thres for kernel fusion in Sec. 3.2.class-aware thres PQ PQ th PQ stTable 4. Comparison with different methods of removing repetitive predictions. kernel-fusion and nms indicates the proposed kernel- level fusion method and Matrix NMS [46], respectively.kernel-fusion nms PQ PQ th PQ st AP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .Table 6 .Table 7 .Table 8 .</head><label>5678</label><figDesc>Comparison with different channel numbers of the feature encoder on the COCO val set. channel num represents the channel number Ce of the feature encoder. Comparison with different feature types for the feature encoder on the COCO val set. feature type denotes the method to generate high-resolution feature F h in Sec. 3.3. Comparison with different settings of weighted dice loss on the COCO val set. weighted and k denote weighted dice loss and the number of sampled points in Sec. 3.4, respectively. weighted k PQ PQ th PQ st AP mIoU -40.2 45.5 32.4 31.0 41.3 1 40.0 45.1 32.4 30.9 41.4 3 41.0 46.4 32.7 31.6 41.4 5 41.0 46.5 32.9 32.1 41.7 7 41.3 46.9 32.9 32.1 41.7 9 41.3 46.8 32.9 32.1 41.8 Comparison with different training schedules on the COCO val set. 1×, 2×, and 3× schedule denote the 90K, 180K, and 270K training iterations in Detectron2 [48], respectively. schedule PQ PQ th PQ st AP mIoU</figDesc><table><row><cell>channel num</cell><cell>PQ PQ th PQ st AP mIoU</cell></row><row><cell>16</cell><cell>39.9 45.0 32.1 30.8 41.3</cell></row><row><cell>32</cell><cell>40.8 46.3 32.5 31.7 41.6</cell></row><row><cell>64</cell><cell>41.3 46.9 32.9 32.1 41.7</cell></row><row><cell>128</cell><cell>41.3 47.0 32.6 32.6 41.7</cell></row><row><cell>feature type</cell><cell>PQ PQ th PQ st AP mIoU</cell></row><row><cell>FPN-P2</cell><cell>40.6 46.0 32.4 31.6 41.3</cell></row><row><cell>FPN-Summed</cell><cell>40.5 46.0 32.1 31.7 41.1</cell></row><row><cell cols="2">Semantic FPN [18] 41.3 46.9 32.9 32.1 41.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 9 .</head><label>9</label><figDesc>Comparison with different settings of the feature encoder on the COCO val set. deform and channel num represent deformable convolutions and the channel number Ce, respectively. deform channel num PQ PQ th PQ st AP mIoU 64 43.6 49.3 35.0 34.5 43.8 256 44.3 50.0 35.6 35.5 44.0Table 10. Upper-bound analysis on the COCO val set. gt position and gt class denote utilizing the ground-truth position Gi and class Oi in each position head for kernel generation, respectively.</figDesc><table><row><cell cols="5">gt position gt class</cell><cell>PQ</cell><cell>PQ th</cell><cell>PQ st</cell><cell>AP</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>43.6</cell><cell>49.3</cell><cell>35.0</cell><cell>34.5</cell><cell>43.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>49.8</cell><cell>52.2</cell><cell>46.1</cell><cell>38.2</cell><cell>54.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>65.9</cell><cell>64.1</cell><cell>68.7</cell><cell>45.5</cell><cell>86.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">+22.3 +14.8 +33.7 +11.0 +42.8</cell></row><row><cell>PQ (%)</cell><cell>38 40 42 44</cell><cell cols="4">RealTimePan Panoptic FPN-1x CIAE Ours-400 Panoptic FPN-3x Ours-512 Ours Ours-600</cell><cell>Ours* UPSNet</cell><cell>PCV Panoptic FCN Box-based Box-free</cell></row><row><cell></cell><cell>36</cell><cell></cell><cell cols="2">Panoptic-DeepLab</cell><cell></cell><cell></cell></row><row><cell></cell><cell>34</cell><cell></cell><cell></cell><cell></cell><cell cols="2">DeeperLab</cell></row><row><cell></cell><cell></cell><cell>40</cell><cell>60</cell><cell>80</cell><cell cols="2">100 Latency (ms) 120</cell><cell>140</cell><cell>160</cell><cell>180</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Backbone PQ SQ RQ PQ th SQ th RQ th PQ st SQ st RQ st Device FPS</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>box-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Panoptic FPN [18]</cell><cell cols="2">Res50-FPN 39.0</cell><cell>-</cell><cell>-</cell><cell>45.9</cell><cell>-</cell><cell>-</cell><cell>28.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Panoptic FPN  † -1×</cell><cell cols="5">Res50-FPN 39.4 77.8 48.3 45.9</cell><cell>80.9</cell><cell>55.3</cell><cell cols="3">29.6 73.3 37.7</cell><cell cols="2">V100 17.5</cell></row><row><cell>Panoptic FPN  † -3×</cell><cell cols="5">Res50-FPN 41.5 79.1 50.5 48.3</cell><cell>82.2</cell><cell>57.9</cell><cell cols="3">31.2 74.4 39.5</cell><cell cols="2">V100 17.5</cell></row><row><cell>AUNet [25]</cell><cell cols="2">Res50-FPN 39.6</cell><cell>-</cell><cell>-</cell><cell>49.1</cell><cell>-</cell><cell>-</cell><cell>25.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CIAE [11]</cell><cell cols="2">Res50-FPN 40.2</cell><cell>-</cell><cell>-</cell><cell>45.3</cell><cell>-</cell><cell>-</cell><cell>32.3</cell><cell>-</cell><cell>-</cell><cell cols="2">2080Ti 12.5</cell></row><row><cell>UPSNet  † [50]</cell><cell cols="5">Res50-FPN 42.5 78.0 52.5 48.6</cell><cell>79.4</cell><cell>59.6</cell><cell cols="3">33.4 75.9 41.7</cell><cell>V100</cell><cell>9.1</cell></row><row><cell>Unifying [24]</cell><cell cols="5">Res50-FPN 43.4 79.6 53.0 48.6</cell><cell>-</cell><cell>-</cell><cell>35.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>box-free</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeeperLab [51]</cell><cell cols="2">Xception-71 33.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">V100 10.6</cell></row><row><cell>Panoptic-DeepLab [6]</cell><cell>Res50</cell><cell>35.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">V100 20.0</cell></row><row><cell>AdaptIS [40]</cell><cell>Res50</cell><cell>35.9</cell><cell>-</cell><cell>-</cell><cell>40.3</cell><cell>-</cell><cell>-</cell><cell>29.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RealTimePan [14]</cell><cell cols="2">Res50-FPN 37.1</cell><cell>-</cell><cell>-</cell><cell>41.0</cell><cell>-</cell><cell>-</cell><cell>31.3</cell><cell>-</cell><cell>-</cell><cell cols="2">V100 15.9</cell></row><row><cell>PCV [43]</cell><cell cols="5">Res50-FPN 37.5 77.7 47.2 40.0</cell><cell>78.4</cell><cell>50.0</cell><cell cols="5">33.7 76.5 42.9 1080Ti 5.7</cell></row><row><cell>SOLO V2 [46]</cell><cell cols="2">Res50-FPN 42.1</cell><cell>-</cell><cell>-</cell><cell>49.6</cell><cell>-</cell><cell>-</cell><cell>30.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Panoptic FCN-400</cell><cell cols="5">Res50-FPN 40.7 80.5 49.3 44.9</cell><cell>82.0</cell><cell>54.0</cell><cell cols="3">34.3 78.1 42.1</cell><cell cols="2">V100 20.9</cell></row><row><cell>Panoptic FCN-512</cell><cell cols="5">Res50-FPN 42.3 80.9 51.2 47.4</cell><cell>82.1</cell><cell>56.9</cell><cell cols="3">34.7 79.1 42.7</cell><cell cols="2">V100 18.9</cell></row><row><cell>Panoptic FCN-600</cell><cell cols="5">Res50-FPN 42.8 80.6 51.6 47.9</cell><cell>82.6</cell><cell>57.2</cell><cell cols="3">35.1 77.4 43.1</cell><cell cols="2">V100 16.8</cell></row><row><cell>Panoptic FCN</cell><cell cols="5">Res50-FPN 43.6 80.6 52.6 49.3</cell><cell>82.6</cell><cell>58.9</cell><cell cols="3">35.0 77.6 42.9</cell><cell cols="2">V100 12.5</cell></row><row><cell>Panoptic FCN  *</cell><cell cols="5">Res50-FPN 44.3 80.7 53.0 50.0</cell><cell>83.4</cell><cell>59.3</cell><cell cols="3">35.6 76.7 43.5</cell><cell>V100</cell><cell>9.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 12 .Table 13 .</head><label>1213</label><figDesc>Experiments on the COCO test-dev set. All our results are achieved with single scale input and no flipping. The simple enhanced version and val set for training are marked with * and ‡. Experiments on the Cityscape val set. All our results are achieved with single scale input and no flipping. The simple enhanced version is marked with *. InTable 11, we conduct experiments on COCO val set. Compared with recent approaches, Panoptic FCN achieves superior performance with efficiency, which surpasses leading box-based<ref type="bibr" target="#b23">[24]</ref> and box-free<ref type="bibr" target="#b42">[43]</ref> methods over 0.2% and 1.5% PQ, respectively. With simple enhancement, the gap enlarges to 0.9% and 2.2% PQ. Mean-</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">PQ PQ th PQ st</cell></row><row><cell></cell><cell>box-based</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Panoptic FPN [18]</cell><cell>Res101-FPN</cell><cell cols="3">40.9 48.3 29.7</cell></row><row><cell>CIAE [11]</cell><cell>DCN101-FPN</cell><cell cols="3">44.5 49.7 36.8</cell></row><row><cell>AUNet [25]</cell><cell cols="4">ResNeXt152-FPN 46.5 55.8 32.5</cell></row><row><cell>UPSNet [50]</cell><cell>DCN101-FPN</cell><cell cols="3">46.6 53.2 36.7</cell></row><row><cell>Unifying  ‡ [24]</cell><cell>DCN101-FPN</cell><cell cols="3">47.2 53.5 37.7</cell></row><row><cell>BANet [5]</cell><cell>DCN101-FPN</cell><cell cols="3">47.3 54.9 35.9</cell></row><row><cell></cell><cell>box-free</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeeperLab [51]</cell><cell>Xception-71</cell><cell cols="3">34.3 37.5 29.6</cell></row><row><cell>SSAP [10]</cell><cell>Res101-FPN</cell><cell cols="3">36.9 40.1 32.0</cell></row><row><cell>PCV [43]</cell><cell>Res50-FPN</cell><cell cols="3">37.7 40.7 33.1</cell></row><row><cell>Panoptic-DeepLab [6]</cell><cell>Xception-71</cell><cell cols="3">39.7 43.9 33.2</cell></row><row><cell>AdaptIS [40]</cell><cell>ResNeXt-101</cell><cell cols="3">42.8 53.2 36.7</cell></row><row><cell>Axial-DeepLab [44]</cell><cell>Axial-ResNet-L</cell><cell cols="3">43.6 48.9 35.6</cell></row><row><cell>Panoptic FCN</cell><cell>Res101-FPN</cell><cell cols="3">45.5 51.4 36.4</cell></row><row><cell>Panoptic FCN</cell><cell>DCN101-FPN</cell><cell cols="3">47.0 53.0 37.8</cell></row><row><cell>Panoptic FCN  *</cell><cell>DCN101-FPN</cell><cell cols="3">47.1 53.2 37.8</cell></row><row><cell>Panoptic FCN  *  ‡</cell><cell>DCN101-FPN</cell><cell cols="3">47.5 53.7 38.2</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">PQ PQ th PQ st</cell></row><row><cell></cell><cell>box-based</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Panoptic FPN [18]</cell><cell cols="4">Res101-FPN 58.1 52.0 62.5</cell></row><row><cell>AUNet [25]</cell><cell cols="4">Res101-FPN 59.0 54.8 62.1</cell></row><row><cell>UPSNet [50]</cell><cell cols="4">Res50-FPN 59.3 54.6 62.7</cell></row><row><cell>SOGNet [52]</cell><cell cols="4">Res50-FPN 60.0 56.7 62.5</cell></row><row><cell>Seamless [36]</cell><cell cols="4">Res50-FPN 60.2 55.6 63.6</cell></row><row><cell>Unifying [24]</cell><cell cols="4">Res50-FPN 61.4 54.7 66.3</cell></row><row><cell></cell><cell>box-free</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PCV [43]</cell><cell cols="4">Res50-FPN 54.2 47.8 58.9</cell></row><row><cell>DeeperLab [51]</cell><cell cols="2">Xception-71 56.5</cell><cell>-</cell><cell>-</cell></row><row><cell>SSAP [10]</cell><cell cols="3">Res50-FPN 58.4 50.6</cell><cell>-</cell></row><row><cell>AdaptIS [40]</cell><cell>Res50</cell><cell cols="3">59.0 55.8 61.3</cell></row><row><cell>Panoptic-DeepLab [6]</cell><cell>Res50</cell><cell>59.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Panoptic FCN</cell><cell cols="4">Res50-FPN 59.6 52.1 65.1</cell></row><row><cell>Panoptic FCN  *</cell><cell cols="4">Res50-FPN 61.4 54.8 66.6</cell></row><row><cell>COCO.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>while, Panoptic FCN outperforms all top-ranking models on COCO test-dev set, as illustrated in Table 12. In par- ticular, the proposed method surpasses the state-of-the-art approach in box-based stream with 0.2% PQ and attains</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 14 .</head><label>14</label><figDesc>Experiments on the Mapillary Vistas val set. All our results are achieved with single scale input and no flipping. The simple enhanced version is marked with *.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">PQ PQ th PQ st</cell></row><row><cell></cell><cell>box-based</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BGRNet [49]</cell><cell cols="4">Res50-FPN 31.8 34.1 27.3</cell></row><row><cell>TASCNet [22]</cell><cell cols="4">Res50-FPN 32.6 31.1 34.4</cell></row><row><cell>Seamless [36]</cell><cell cols="4">Res50-FPN 36.2 33.6 40.0</cell></row><row><cell></cell><cell>box-free</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeeperLab [51]</cell><cell cols="2">Xception-71 32.0</cell><cell>-</cell><cell>-</cell></row><row><cell>AdaptIS [40]</cell><cell>Res50</cell><cell cols="3">32.0 26.6 39.1</cell></row><row><cell>Panoptic-DeepLab [6]</cell><cell>Res50</cell><cell>33.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Panoptic FCN</cell><cell cols="4">Res50-FPN 34.8 30.6 40.5</cell></row><row><cell>Panoptic FCN  *</cell><cell cols="4">Res50-FPN 36.9 32.9 42.3</cell></row><row><cell cols="5">47.5% PQ with single scale inputs. Compared with the sim-</cell></row><row><cell cols="5">ilar box-free fashion, our method improves 1.9% PQ over</cell></row><row><cell cols="4">Axial-DeepLab [44] which adopts stronger backbone.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 15 .</head><label>15</label><figDesc>Comparisons among different settings of center type on the COCO val set. weighted and center type denote weighted dice loss and center type for ground-truth generation, respectively. weighted center type PQ PQ th PQ st AP mIoU box 39.7 44.7 32.3 30.3 41.2 mass 40.2 45.5 32.4 31.0 41.3 box 40.6 45.7 32.8 31.7 41.5 mass 41.3 46.9 32.9 32.1 41.7 A.2. Inference Details.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was partially supported by National Key R&amp;D Program of China (No. 2017YFA0700800), and Beijing Academy of Artificial Intelligence (BAAI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Banet: Bidirectional aggregation network with occlusion handling for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ssap: Single-shot instance segmentation with affinity pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning category-and instance-aware pixel embedding for fast panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13342</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time panoptic segmentation from dense detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Centermask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Tagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01192</idno>
		<title level="m">Learning to fuse things and stuff</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unifying training and inference for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention-guided unified network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning dynamic routing for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Seamless scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Colovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequential context encoding for duplicate removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pointins: Point-based instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06148</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adaptis: Adaptive instance selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learnable tree filter for structure-preserving feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05664</idno>
		<title level="m">Conditional convolutions for instance segmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pixel consensus voting for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Solo: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Solov2: Dynamic, faster and stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bidirectional graph reasoning network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05093</idno>
		<title level="m">Deeperlab: Single-shot image parser</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sognet: Scene overlap graph network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
