<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Content Enhanced BERT-based Text-to-SQL Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Never Stop Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huilin</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">China Electronic Technology Group Corporation Information Science Academy</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Content Enhanced BERT-based Text-to-SQL Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep Learning · Semantic Parsing · Database</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple methods to leverage the table content for the BERT-based model to solve the text-to-SQL problem. Based on the observation that some of the table content match some words in question string and some of the table header also match some words in question string, we encode two addition feature vector for the deep model. Our methods also benefit the model inference in testing time as the tables are almost the same in training and testing time. We test our model on the WikiSQL dataset and outperform the BERT-based baseline by 3.7% in logic form and 3.7% in execution accuracy and achieve stateof-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing is the tasks of translating natural language to logic form. Mapping from natural language to SQL (NL2SQL) is an important semantic parsing task for question answering system. In recent years, deep learning and BERTbased model have shown significant improvement on this task. However, past methods did not encode the table content for the input of deep model. For industry application, the table of training time and the table of testing time are the same. So the table content can be encoded as external knowledge for the deep model.</p><p>In order to solve the problem that the table content is not used for model, we propose our effective encoding methods, which could incorporate database designing information into the model. Our key contribution are three folds:</p><p>1. We use the match info of all the table cells and question string to mark the question and produce a feature vector which is the same length to the question.</p><p>2. We use the match inf of all the table column name and question string to mark the column and produce a feature vector which is the same length to the table header.</p><p>3. We design the whole BERT-based model and take the two feature vector above as external inputs. The experiment results outperform the baseline <ref type="bibr" target="#b2">[3]</ref>. The code is available. 1 BERT <ref type="bibr" target="#b3">[4]</ref> is a very deep transformer-based <ref type="bibr" target="#b4">[5]</ref> model. It first pre-train on very large corpus using the mask language model loss and the next-sentence loss. And then we could fine-tune BERT on a variety of specific tasks like text classification, text matching and natural language inference and set new state-of-the-art performance on them. In this section we describe our encoding methods based on the word matching of table content and question string and the word matching of table header and question string. The full algorithms are shown in Algorithm 1 and Algorithm 2. In the Algorithm 1, the value 1 stand for 'START' tag, value 2 stand for 'MIDDLE' tag, value 3 stand for 'END' tag. In the Algorithm 2, we think that the column,  We use BERT as the representation layer. The question and table header are concat and then input to BERT, so that the question and table header have the attention interaction information of each other. We denote the BERT output of question and table header as Q and H</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">External Feature Vector Encoding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BERT embedding layer</head><p>Given the question tokens w 1 , w 2 , ..., w n and the table header h 1 , h 2 , ..., h n , we follow the BERT convention and concat the question tokens and table header for BERT input. The detail encoding is below:</p><formula xml:id="formula_0">[CLS], w 1 , w 2 , ..., w n , [SEP ], h 1 , [SEP ], h 2 , [SEP ], ..., h n , [SEP ]</formula><p>The output embeddings of BERT are shared in all the downstream tasks. We think the concatenation input for BERT can produce some kind of 'global' attention for the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SELECT column</head><p>Our goal is to predict the column name in the </p><p>where QV and HV is the external feature vectors that are described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SELECT agg</head><p>Our goal is to predict the agg slot. The inputs are Q with QV and the output are the probability of SELECT agg:</p><formula xml:id="formula_2">P (sa|Q, QV )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">WHERE number</head><p>Our goal is to predict the where number slot. The inputs are Q and H with QV and HV . The output are the probability of WHERE number:</p><formula xml:id="formula_3">P (wn|Q, H, QV, HV )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">WHERE column</head><p>Our goal is to predict the where column slot for each condition of WHERE clause. The inputs are Q, H and P wn with QV and HV . The output are the top wherenumber probability of WHERE column: P (wc|Q, H, P wn , QV, HV ) (4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">WHERE op</head><p>Our goal is to predict the where column slot for each condition of WHERE clause. The inputs are Q, H, P wc and P wn . The output are the probability of WHERE op slot: P (wo|Q, H, P wn , P wc ) (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">WHERE value</head><p>Our goal is to predict the where column slot for each condition of WHERE clause. The inputs are Q, H, P wn , P wc and P wo with QV and HV . The output are the probability of WHERE value slot: P (wv|Q, H, P wn , P wc , P wo , QV, HV )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section we describe detail of experiment parameters and show the experiment result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment result</head><p>In this section, we evaluate our methods versus other approachs on the WikiSQL dataset. See <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref> for detail. The SQLova <ref type="bibr" target="#b2">[3]</ref> result use the BERT-Base-Uncased pretrained model and run on our machine without executionguided decoding(EG) <ref type="bibr" target="#b5">[6]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Based on the observation that the table data is almost the same in training time and testing time and to solve the problem that the table content is lack for deep model. We propose a simple encoding methods that can leverage the table content as external feature for the BERT-based deep model, demonstrate its good performance on the WikiSQL task, and achieve state-of-the-art on the datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An example of WikiSQL dataset 2 Related Work WikiSQL [1] is a large semantic parsing dataset. It has 80654 natural language and corresponding SQL pairs. The examples of WikiSQL are shown in fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>The construction for question mark vector vector = [0]*question length for cell in table do if contains(question,cell) then start index = get index(question, cell) vector[start index:start index+len(cell)] = 2 vector[start index] = 1 vector[start index+len(cell)] = 3 break end if end for for one in header do if contains(question,one) then index = get index(question, one) vector[index] = 4 end if end for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The deep model 4 The Deep Neural Model Based on the Wikisql dataset, we also use three sub-model to predict the SE-LECT part, AGG part and WHERE part. The whole model is shown in fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 2 The construction for table header mark vector which contains the matched cell, should be marked. The final question mark vector is named QV and the final table header mark vector is named HV . For industry application, we could refer to Algorithm 1 and Algorithm 2 to encode external knowledge flexibly.</figDesc><table><row><cell>vector = [0]*header length</cell></row><row><cell>index = 0</cell></row><row><cell>for one in header do</cell></row><row><cell>if contains(question, one) then</cell></row><row><cell>vector[index] = 1</cell></row><row><cell>end if</cell></row><row><cell>index = index + 1</cell></row><row><cell>end for</cell></row><row><cell>for cell in table do</cell></row><row><cell>if contains(question,cell) then</cell></row><row><cell>vector[get column index(table, cell)] = 2</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>table header .</head><label>header</label><figDesc>The inputs are the question Q and table header H. The output are the probability of SELECT column:</figDesc><table /><note>P (sc|Q, H, QV, HV )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Overall result on the WikiSQL task Break down result on the WikiSQL dataset.The detail results are shown atTable 3. The header vector mainly improve the result of WHERE OP and the question vector mainly improve the result of WHERE VALUE.</figDesc><table><row><cell>Model</cell><cell cols="6">Logic Form Dev Acc Execution Dev Acc Logic Form Test Acc Execution Test Acc</cell></row><row><cell>SQLova[3]</cell><cell>80.6%</cell><cell>86.5%</cell><cell>80.0%</cell><cell></cell><cell>85.5%</cell><cell></cell></row><row><cell>Our methods</cell><cell>84.3%</cell><cell>90.3%</cell><cell>83.7%</cell><cell></cell><cell>89.2%</cell><cell></cell></row><row><cell cols="2">Our methods + EG 85.4%</cell><cell>91.1%</cell><cell>84.5%</cell><cell></cell><cell>90.1%</cell><cell></cell></row><row><cell>Model</cell><cell cols="6">SELECT column SELECT agg WHERE number WHERE column WHERE op WHERE value</cell></row><row><cell>SQLova[3]</cell><cell>97.0%</cell><cell>90.1%</cell><cell>98.5%</cell><cell>94.4%</cell><cell>97.3%</cell><cell>95.5%</cell></row><row><cell>Our methods</cell><cell>97.4%</cell><cell>90.0%</cell><cell>99.1%</cell><cell>97.9%</cell><cell>98.1%</cell><cell>97.6%</cell></row><row><cell cols="2">Our methods + EG 97.4%</cell><cell>90.4%</cell><cell>98.9%</cell><cell>97.9%</cell><cell>97.7%</cell><cell>97.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The results of ablation study</figDesc><table><row><cell>Model</cell><cell cols="6">SELECT col SELECT agg WHERE num WHERE col WHERE op WHERE value</cell></row><row><cell>without all vector</cell><cell>97.0%</cell><cell>90.1%</cell><cell>98.5%</cell><cell>94.4%</cell><cell>97.3%</cell><cell>95.5%</cell></row><row><cell cols="2">without header vector 97.0%</cell><cell>90.1%</cell><cell>98.5%</cell><cell>94.4%</cell><cell>97.3%</cell><cell>97.3%</cell></row><row><cell cols="2">without question vector 97.0%</cell><cell>90.1%</cell><cell>98.5%</cell><cell>97.8%</cell><cell>97.3%</cell><cell>95.5%</cell></row><row><cell>all</cell><cell>97.4%</cell><cell>90.0%</cell><cell>99.1%</cell><cell>97.9%</cell><cell>98.1%</cell><cell>97.6%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/guotong1988/NL2SQL-RULE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00103</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Typesql: Knowledge-based type-aware neural text-to-sql generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09769</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01069</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention is all you need[C]//Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Robust text-to-sql generation with execution-guided decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tatwawadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03100</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
